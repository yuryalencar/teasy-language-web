% Encoding: UTF-8
@inproceedings{Dukhan:2015:PMO:2835857.2835860,
 author = {Dukhan, Marat},
 title = {PeachPy Meets Opcodes: Direct Machine Code Generation from Python},
 booktitle = {Proceedings of the 5th Workshop on Python for High-Performance and Scientific Computing},
 series = {PyHPC '15},
 year = {2015},
 isbn = {978-1-4503-4010-6},
 location = {Austin, Texas},
 pages = {3:1--3:6},
 articleno = {3},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2835857.2835860},
 doi = {10.1145/2835857.2835860},
 acmid = {2835860},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Opcode, assembly, domain-specific language},
}

@inproceedings{Hallenberg:2012:DAT:2663608.2663628,
 author = {Hallenberg, Niels and Carlsen, Philip Lykke},
 title = {Declarative Automated Test},
 booktitle = {Proceedings of the 7th International Workshop on Automation of Software Test},
 series = {AST '12},
 year = {2012},
 isbn = {978-1-4673-1822-8},
 location = {Zurich, Switzerland},
 pages = {96--102},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=2663608.2663628},
 acmid = {2663628},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {F\#, automated testing, domain specific language, functional testing},
}

@inproceedings{King:2014:LAD:2610384.2628048,
 author = {King, Tariq M. and Nunez, Gabriel and Santiago, Dionny and Cando, Adam and Mack, Cody},
 title = {Legend: An Agile DSL Toolset for Web Acceptance Testing},
 booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
 series = {ISSTA 2014},
 year = {2014},
 isbn = {978-1-4503-2645-2},
 location = {San Jose, CA, USA},
 pages = {409--412},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2610384.2628048},
 doi = {10.1145/2610384.2628048},
 acmid = {2628048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Agile Development, Behavior-Driven Development, Domain-Specific Languages, Software Testing, Test Automation},
}

@inproceedings{Felderer:2018:PEE:3210459.3210479,
 author = {Felderer, Michael and Jeschko, Fabian},
 title = {A Process for Evidence-Based Engineering of Domain-Specific Languages},
 booktitle = {Proceedings of the 22Nd International Conference on Evaluation and Assessment in Software Engineering 2018},
 series = {EASE'18},
 year = {2018},
 isbn = {978-1-4503-6403-4},
 location = {Christchurch, New Zealand},
 pages = {169--174},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3210459.3210479},
 doi = {10.1145/3210459.3210479},
 acmid = {3210479},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DSL engineering, domain-specific languages, empirical research, evidence-based software engineering, repository mining},
}

@inproceedings{Haser:2016:ITE:2915970.2916010,
 author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
 title = {An Integrated Tool Environment for Experimentation in Domain Specific Language Engineering},
 booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
 series = {EASE '16},
 year = {2016},
 isbn = {978-1-4503-3691-8},
 location = {Limerick, Ireland},
 pages = {20:1--20:5},
 articleno = {20},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/2915970.2916010},
 doi = {10.1145/2915970.2916010},
 acmid = {2916010},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {controlled experiment, domain specific languages (DSLs), empirical evaluation, experimentation, language engineering, meta programming system (MPS), tool support},
}

@article{Zhou:2014:AWT:2659118.2659136,
 author = {Zhou, Jingang and Yin, Kun},
 title = {Automated Web Testing Based on Textual-visual UI Patterns: The UTF Approach},
 journal = {SIGSOFT Softw. Eng. Notes},
 issue_date = {September 2014},
 volume = {39},
 number = {5},
 month = sep,
 year = {2014},
 issn = {0163-5948},
 pages = {1--6},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2659118.2659136},
 doi = {10.1145/2659118.2659136},
 acmid = {2659136},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, domain-specific language, user-interface pattern, web application},
}

@inproceedings{Martinez:2014:EMA:2631890.2631893,
 author = {Martinez, Jorge and Thomas, Troy and King, Tariq M.},
 title = {Echo: A Middleware Architecture for Domain-specific UI Test Automation},
 booktitle = {Proceedings of the 2014 Workshop on Joining AcadeMiA and Industry Contributions to Test Automation and Model-Based Testing},
 series = {JAMAICA 2014},
 year = {2014},
 isbn = {978-1-4503-2933-0},
 location = {San Jose, CA, USA},
 pages = {13--15},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/2631890.2631893},
 doi = {10.1145/2631890.2631893},
 acmid = {2631893},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Domain-Specific Languages, Middleware, Software Architecture, Software Testing, Test Automation, User Interfaces},
}

@article{Sinha:2006:HMT:1151695.1151697,
 author = {Sinha, Avik and Smidts, Carol},
 title = {HOTTest: A Model-based Test Design Technique for Enhanced Testing of Domain-specific Applications},
 journal = {ACM Trans. Softw. Eng. Methodol.},
 issue_date = {July 2006},
 volume = {15},
 number = {3},
 month = jul,
 year = {2006},
 issn = {1049-331X},
 pages = {242--278},
 numpages = {37},
 url = {http://doi.acm.org/10.1145/1151695.1151697},
 doi = {10.1145/1151695.1151697},
 acmid = {1151697},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Haskell, HaskellDB, Test case generation, database-specific test case generation, domain-specific languages, domain-specific testing, model-based testing, test generation tools},
}

@inproceedings{Al-Sibahi:2016:SEH:2997364.2997382,
 author = {Al-Sibahi, Ahmad Salim and Dimovski, Aleksandar S. and W\k{a}sowski, Andrzej},
 title = {Symbolic Execution of High-level Transformations},
 booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
 series = {SLE 2016},
 year = {2016},
 isbn = {978-1-4503-4447-0},
 location = {Amsterdam, Netherlands},
 pages = {207--220},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2997364.2997382},
 doi = {10.1145/2997364.2997382},
 acmid = {2997382},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated white-box test generation, model transformation, program transformation, symbolic execution},
}

@inproceedings{Heider:2012:URT:2362536.2362563,
 author = {Heider, Wolfgang and Rabiser, Rick and Gr\"{u}nbacher, Paul and Lettner, Daniela},
 title = {Using Regression Testing to Analyze the Impact of Changes to Variability Models on Products},
 booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
 series = {SPLC '12},
 year = {2012},
 isbn = {978-1-4503-1094-9},
 location = {Salvador, Brazil},
 pages = {196--205},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2362536.2362563},
 doi = {10.1145/2362536.2362563},
 acmid = {2362563},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {product line evolution, regression testing, variability models},
}

@inproceedings{Blichmann:2015:CWA:2837185.2837219,
 author = {Blichmann, Gregor and Radeck, Carsten and Hahn, Sergej and Mei\ssner, Klaus},
 title = {Component-based Workspace Awareness Support for Composite Web Applications},
 booktitle = {Proceedings of the 17th International Conference on Information Integration and Web-based Applications \& Services},
 series = {iiWAS '15},
 year = {2015},
 isbn = {978-1-4503-3491-4},
 location = {Brussels, Belgium},
 pages = {61:1--61:10},
 articleno = {61},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2837185.2837219},
 doi = {10.1145/2837185.2837219},
 acmid = {2837219},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaboration, mashups, widgets, workspace awareness},
}

@article{Makki:2016:ART:3093335.2993257,
 author = {Makki, Majid and Van Landuyt, Dimitri and Joosen, Wouter},
 title = {Automated Regression Testing of BPMN 2.0 Processes: A Capture and Replay Framework for Continuous Delivery},
 journal = {SIGPLAN Not.},
 issue_date = {March 2017},
 volume = {52},
 number = {3},
 month = oct,
 year = {2016},
 issn = {0362-1340},
 pages = {178--189},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3093335.2993257},
 doi = {10.1145/3093335.2993257},
 acmid = {2993257},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BPMN 2.0, Business Process Execution, Node Mocking, Performance Overhead, Regression Testing, Test Automation, jBPM},
}

@article{Harrold:1996:SCA:226295.309037,
 author = {Harrold, Mary Jean and Rothermel, Gregg},
 title = {Separate Computation of Alias Information for Reuse},
 journal = {SIGSOFT Softw. Eng. Notes},
 issue_date = {May 1996},
 volume = {21},
 number = {3},
 month = may,
 year = {1996},
 issn = {0163-5948},
 pages = {107--120},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/226295.309037},
 doi = {10.1145/226295.309037},
 acmid = {309037},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Boussaa:2016:ANT:3093335.2993256,
 author = {Boussaa, Mohamed and Barais, Olivier and Baudry, Benoit and Suny{\'e}, Gerson},
 title = {Automatic Non-functional Testing of Code Generators Families},
 journal = {SIGPLAN Not.},
 issue_date = {March 2017},
 volume = {52},
 number = {3},
 month = oct,
 year = {2016},
 issn = {0362-1340},
 pages = {202--212},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3093335.2993256},
 doi = {10.1145/3093335.2993256},
 acmid = {2993256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generator, code quality, non-functional properties, testing},
}

@inproceedings{Mustyatsa:2015:BER:2855667.2855678,
 author = {Mustyatsa, Vadim},
 title = {BDD by Example: Russian Bylina Written in Gherkin Language},
 booktitle = {Proceedings of the 11th Central \&\#38; Eastern European Software Engineering Conference in Russia},
 series = {CEE-SECR '15},
 year = {2015},
 isbn = {978-1-4503-4130-1},
 location = {Moscow, Russia},
 pages = {10:1--10:15},
 articleno = {10},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/2855667.2855678},
 doi = {10.1145/2855667.2855678},
 acmid = {2855678},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Agile, Gherkin, GitHub, JBehave, Scrum, behaviour-driven development, byliny, cucumber, epic poetry, examples, extreme programming, user stories},
}

@inproceedings{Gafurov:2018:ATA:3238147.3240463,
 author = {Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin},
 title = {Achieving Test Automation with Testers Without Coding Skills: An Industrial Report},
 booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
 series = {ASE 2018},
 year = {2018},
 isbn = {978-1-4503-5937-5},
 location = {Montpellier, France},
 pages = {749--756},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/3238147.3240463},
 doi = {10.1145/3238147.3240463},
 acmid = {3240463},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DSL for test automation, Helsenorge, Test automation, keyword-driven test automation, process-driven test automation},
}

@Comment{jabref-meta: databaseType:bibtex;}
@inproceedings{2002477229400 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Plastic ball grid arrays, a qualified packaging technology for high reliability space applications},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
author = {Massey, Mary C. and Parrish, Brian E. and McMullen, William E. and Estes, Thomas J.},
volume = {4828},
year = {2002},
pages = {175 - 180},
issn = {0277786X},
address = {Reno, NV, United states},
abstract = {TRW has qualified a new advanced digital packaging technology for critical space applications; Plastic Ball Grid Array (PBGA) assemblies. This achievement enables use of high I/O, state-of-the-art ASIC designs and promises improved digital subsystem performance at significantly reduced weight and cost. Plastic ball grid array packages accommodating die as large as 17 mm sq. with over 800 I/O are planned on future satellite programs surviving extensive component-level and product-level qualification testing. TRW's space-qualification of laminate-based (organic) area array packaging technology responds to the ever increasing demand for reliable, cost effective, high density interconnect (HDI) solutions while leveraging the commercial standard plastic encapsulated microcircuit (PEM). This paper describes the product line approach used to produce qualification test vehicles representative of proposed flight configurations. TRW's radiation hardened 32-bit processor ASIC was used as an electrically functional test vehicle. The initial research focused on package reliability without hermeticity (RWOH) and solder joint reliability. Test results from a controlled insertion of plastic ball grid array packages on flight-like dual sequential laminated (DSL) boards are presented, where thousands of solder joints are continuously monitored using an inexpensive and highly reliable on-board fault detection circuit. The data demonstrates how this technology can provide a superior integrated circuit (IC) packaging solution over traditional ceramic-based assemblies. Space programs will benefit from aggressive insertion of JEDEC standard PBGA packages to achieve higher performance designs with increased circuit densities, while reducing electronic product's size, weight, power, and cost.},
key = {Electronics packaging},
keywords = {Electric discharges;Integrated circuits;Interconnection networks;Laminates;Printed circuit boards;Reliability;Satellites;Space applications;},
note = {Plastic ball grid arrays (PBGA);},
} 


@inproceedings{20103513188582 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {A C++-embedded domain-specific language for programming the MORA soft processor array},
journal = {Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors},
author = {Vanderbauwhede, W. and Margala, M. and Chalamalasetti, S.R. and Purohit, S.},
year = {2010},
pages = {141 - 148},
issn = {10636862},
address = {Rennes, France},
abstract = {MORA is a novel platform for high-level FPGA programming of streaming vector and matrix operations, aimed at multimedia applications. It consists of soft array of pipelined low-complexity SIMD processors-in-memory (PIM). We present a Domain-Specific Language (DSL) for high-level programming of the MORA soft processor array. The DSL is embedded in C++, providing designers with a familiar language framework and the ability to compile designs using a standard compiler for functional testing before generating the FPGA bitstream using the MORA toolchain. The paper discusses the MORA-C++ DSL and the compilation route into the assembly for the MORA machine and provides examples to illustrate the programming model and performance. &copy; 2010 IEEE.<br/>},
key = {Pipeline processing systems},
keywords = {Ability testing;C++ (programming language);Computer architecture;Digital subscriber lines;Field programmable gate arrays (FPGA);Multimedia systems;Problem oriented languages;},
note = {Domain specific language (DSL);Domain specific languages;Embedded domain specific languages;High-level programming;Multimedia applications;Multimedia processing;Reconfigurable processors;Soft processors;},
URL = {http://dx.doi.org/10.1109/ASAP.2010.5540750},
} 


@inproceedings{20110113552572 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {An integrated domain specific language for post-processing and visualizing electrophysiological signals in Java},
journal = {2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC'10},
author = {Strasser, T. and Peters, T. and Jagle, H. and Zrenner, E. and Wilke, R.},
year = {2010},
pages = {4687 - 4690},
abstract = {Electrophysiology of vision - especially the electroretinogram (ERG) - is used as a non-invasive way for functional testing of the visual system. The ERG is a combined electrical response generated by neural and non-neuronal cells in the retina in response to light stimulation. This response can be recorded and used for diagnosis of numerous disorders. For both clinical practice and clinical trials it is important to process those signals in an accurate and fast way and to provide the results as structured, consistent reports. Therefore, we developed a freely available and open-source framework in Java (http://www.eye.uni-tuebingen.de/projectlidsI4sigproc). The framework is focused on an easy integration with existing applications. By leveraging well-established software patterns like pipes-and-filters and fluent interfaces as well as by designing the application programming interfaces (API) as an integrated domain specific language (DSL) the overall framework provides a smooth learning curve. Additionally, it already contains several processing methods and visualization features and can be extended easily by implementing the provided interfaces. In this way, not only can new processing methods be added but the framework can also be adopted for other areas of signal processing. This article describes in detail the structure and implementation of the framework and demonstrate its application through the software package used in clinical practice and clinical trials at the University Eye Hospital Tuebingen one of the largest departments in the field of visual electrophysiology in Europe. &copy; 2010 IEEE.<br/>},
key = {Java programming language},
keywords = {Application programming interfaces (API);Electrophysiology;Experiments;Neurology;Processing;Signal processing;Visualization;},
note = {Clinical practices;Clinical trial;Domain specific languages;Electrical response;Electroretinograms;Functional testing;Learning curves;Light stimulation;Neuronal cell;Non-invasive way;Open source frameworks;Post processing;Processing method;Software patterns;Visual systems;},
URL = {http://dx.doi.org/10.1109/IEMBS.2010.5626417},
} 


@inproceedings{20154001337207 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Legend: An agile DSL toolset for web acceptance testing},
journal = {2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
author = {King, Tariq M. and Nunez, Gabriel and Santiago, Dionny and Cando, Adam and Mack, Cody},
year = {2014},
pages = {409 - 412},
address = {San Jose, CA, United states},
abstract = {Agile development emphasizes collaborations among customers, business analysts, domain experts, developers, and testers. However, the large scale and rapid pace of many agile projects presents challenges during testing activities. Large sets of test artifacts must be comprehensible and available to various stakeholders, traceable to requirements, and easily maintainable as the software evolves. In this paper we describe Legend, a toolset that leverages domain-specific language to streamline functional testing in agile projects. Some key features of the toolset include test template generation from user stories, model-based automation, test inventory synchronization, and centralized test tagging.<br/> Copyright 2014 ACM.},
key = {Acceptance tests},
keywords = {Digital subscriber lines;Graphical user interfaces;Problem oriented languages;Software testing;},
note = {Acceptance testing;Agile development;Behavior-driven development;Business analysts;Domain specific languages;Functional testing;Model-based OPC;Test Automation;},
} 


@inproceedings{20113714315180 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Using Haskell to script combinatoric testing of web services},
journal = {Proceedings of the 6th Iberian Conference on Information Systems and Technologies, CISTI 2011},
author = {Prasetya, I.S.W.B. and Amorim, J. and Vos, T.E.J. and Baars, A.},
year = {2011},
abstract = {The Classification Tree Method (CTM) is a popular approach in functional testing as it allows the testers to systematically partition the input domain of an SUT, and specifies the combinations they want. We have implemented the approach as a small domain specific language (DSL) embedded in the functional language Haskell. Such an embedding leads to clean syntax and moreover we can natively access Haskell's full features. This paper will explain the approach, and how it is applied for testing Web Services. &copy; 2011 AISTI.<br/>},
key = {Web services},
keywords = {Combinatorial mathematics;Problem oriented languages;Websites;},
note = {Automated testing;Classification tree method;Domain specific language (DSL);Functional languages;Functional testing;Haskell;},
} 


@article{20164102895331 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Is business domain language support beneficial for creating test case specifications: A controlled experiment},
journal = {Information and Software Technology},
author = {Haser, Florian and Felderer, Michael and Breu, Ruth},
volume = {79},
year = {2016},
pages = {52 - 62},
issn = {09505849},
abstract = {Context: Behavior Driven Development (BDD), widely used in modern software development, enables easy creation of acceptance test case specifications and serves as a communication basis between business- and technical-oriented stakeholders. BDD is largely facilitated through simple domain specific languages (DSL) and usually restricted to technical test domain concepts. Integrating business domain concepts to implement a ubiquitous language for all members of the development team is an appealing test language improvement issue. But the integration of business domain concepts into BDD toolkits has so far not been investigated. Objective: The objective of the study presented in this paper is to examine whether supporting the ubiquitous language features inside a DSL, by extending a DSL with business domain concepts, is beneficial over using a DSL without those concepts. In the context of the study, benefit is measured in terms of perceived quality, creation time and length of the created test case specifications. In addition, we analyze if participants feel supported when using predefined business domain concepts. Method: We investigate the creation of test case specifications, similar to BDD, in a controlled student experiment performed with graduate students based on a novel platform for DSL experimentation. The experiment was carried out by two groups, each solving a similar comparable test case, one with the simple DSL, the other one with the DSL that includes business domain concepts. A crossover design was chosen for evaluating the perceived quality of the resulting specifications. Results: Our experiment indicates that a business domain aware language allows significant faster creation of documents without lowering the perceived quality. Subjects felt better supported by the DSL with business concepts. Conclusion: Based on our findings we propose that existing BDD toolkits could be further improved by integrating business domain concepts.<br/> &copy; 2016 Elsevier B.V.},
key = {Acceptance tests},
keywords = {Boolean functions;Digital subscriber lines;Problem oriented languages;Software design;Software testing;Specifications;Students;},
note = {Behavior driven development;Controlled experiment;Development teams;Domain specific language (DSL);Graduate students;Language features;Student experiments;Test case specifications;},
URL = {http://dx.doi.org/10.1016/j.infsof.2016.07.001},
} 


@article{20143600043933 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Model transformations for migrating legacy deployment models in the automotive industry},
journal = {Software and Systems Modeling},
author = {Selim, Gehan M. K. and Wang, Shige and Cordy, James R. and Dingel, Juergen},
volume = {14},
number = {1},
year = {2013},
pages = {365 - 381},
issn = {16191366},
abstract = {Many companies in the automotive industry have adopted model-driven development in their vehicle software development. As a major automotive company, General Motors (GM) has been using a custom-built, domain-specific modeling language, implemented as an internal proprietary metamodel, to meet the modeling needs in its control software development. Since AUTomotive Open System ARchitecture (AUTOSAR) has been developed as a standard to ease the process of integrating components provided by different suppliers and manufacturers, there has been a growing demand to migrate these GM-specific, legacy models to AUTOSAR models. Given that AUTOSAR defines its own metamodel for various system artifacts in automotive software development, we explore applying model transformations to address the challenges in migrating GM-specific, legacy models to their AUTOSAR equivalents. As a case study, we have built and validated a model transformation using the MDWorkbench tool, the Atlas Transformation Language, and the Metamodel Coverage Checker tool. This paper reports on the case study, makes observations based on our experience to assist in the development of similar types of transformations, and provides recommendations for further research.<br/> &copy; 2013, Springer-Verlag Berlin Heidelberg.},
key = {Software design},
keywords = {Automobile manufacture;Automotive industry;Black-box testing;Embedded systems;Modeling languages;Open systems;Specification languages;},
note = {Automotive control softwares;AutoSAR;Model transformation;Model-driven development;Transformation languages;},
URL = {http://dx.doi.org/10.1007/s10270-013-0365-1},
} 


@inproceedings{20124415610620 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Application of build-in self test in functional test of DSL},
journal = {IPC APEX EXPO 2012},
author = {Gu, YaJun and Qin, Ye and Wang, ZhiJun and Wei, David and Ho, Andrew and Chen, Stephen and Feng, Zhen and Kurwa, Murad},
volume = {2},
year = {2012},
pages = {945 - 959},
address = {San Diego, CA, United states},
} 


@inproceedings{20124415610585 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Application of build-in self test in functional test of DSL},
journal = {IPC APEX EXPO 2012},
author = {Gu, YaJun and Qin, Ye and Wang, ZhiJun and Wei, David and Ho, Andrew and Chen, Stephen and Feng, Zhen and Kurwa, Murad},
volume = {1},
year = {2012},
pages = {233 - 234},
address = {San Diego, CA, United states},
} 


@inproceedings{20180504691895 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Evaluation of an integrated tool environment for experimentation in DSL engineering},
journal = {Lecture Notes in Business Information Processing},
author = {Haser, Florian and Felderer, Michael and Breu, Ruth},
volume = {302},
year = {2018},
pages = {147 - 168},
issn = {18651348},
address = {Vienna, Austria},
abstract = {Domain specific languages (DSL) are a popular means for providing customized solutions to a certain problem domain. So far, however, language workbenches lack sufficient built-in features in providing decision support when it comes to language design and improvement. Controlled experiments can provide data-driven decision support for both, researchers and language engineers, for comparing different languages or language features. This paper provides an evaluation of an integrated end-to-end tool environment for performing controlled experiments in DSL engineering. The experimentation environment is presented by a running example from engineering domain specific languages for acceptance testing. The tool is built on and integrated into the Meta Programming System (MPS) language workbench. For each step of an experiment the language engineer is supported by suitable DSLs and tools all within the MPS platform. The evaluation, from the viewpoint of the experiments subject, is based on the technology acceptance model (TAM). Results reveal that the subjects found the DSL experimentation environment intuitive and easy to use.<br/> &copy; Springer International Publishing AG 2018.},
key = {Digital subscriber lines},
keywords = {Acceptance tests;Computer software selection and evaluation;Decision support systems;DSL;Problem oriented languages;},
note = {Domain specific languages;Empirical Software Engineering;Experimentation;Language engineering;Meta Programming;Model based software engineering;},
URL = {http://dx.doi.org/10.1007/978-3-319-71440-0_9},
} 


@inproceedings{20165203173126 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Automated regression testing of BPMN 2.0 processes a capture and replay framework for continuous delivery},
journal = {GPCE 2016 - Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, co-located with SPLASH 2016},
author = {Makki, Majid and Van Landuyt, Dimitri and Joosen, Wouter},
year = {2016},
pages = {178 - 189},
address = {Amsterdam, Netherlands},
abstract = {Regression testing is a form of software quality assurance (QA) that involves comparing the behavior of a newer version of a software artifact to its earlier correct behavior, and signaling the QA engineer when deviations are detected. Given the large potential in automated generation and execution of regression test cases for business process models in the context of running systems, powerful tools are required to make this practically feasible, more specifically to limit the potential impact on production systems, and to reduce the manual effort required from QA engineers. In this paper, we present a regression testing automation framework that implements the capture &amp; replay paradigm in the context of BPMN 2.0, a domain-specific language for modeling and executing business processes. The framework employs parallelization techniques and efficient communication patterns to reduce the performance overhead of capturing. Based on inputs from the QA engineer, it manipulates the BPMN2 model before executing tests for isolating the latter from external dependencies (e.g. human actors or expensive web services) and for avoiding undesired sideeffects. Finally, it performs a regression detection algorithm and reports the results to the QA engineer. We have implemented our framework on top of a BPMN2-compliant execution engine, namely jBPM, and performed functional validations and evaluations of its performance and fault-Tolerance. The results, indicating 3:9% average capturing performance overhead, demonstrate that the implemented framework can be the foundation of a practical regression testing tool for BPMN 2.0, and a key enabler for continuous delivery of business process-driven applications and services.<br/> &copy;2016 ACM.},
key = {Software testing},
keywords = {Automatic programming;Automation;Computer software selection and evaluation;Engineers;Fault tolerance;Modeling languages;Problem oriented languages;Regression analysis;Web services;},
note = {BPMN 2.0;Business process execution;JBPM;Node mocking;Performance overhead;Regression testing;Test Automation;},
URL = {http://dx.doi.org/10.1145/2993236.2993257},
} 


@article{2000505390853 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Programmable DSP-based DSL chipsets streamline system testing},
journal = {EE: Evaluation Engineering},
author = {Vogel, Erich and Halbach, Robert and Sheppard, Ben},
volume = {39},
number = {10},
year = {2000},
pages = {4 pp - 4 pp},
issn = {01490370},
abstract = {The streamline testing of programmable digital signal processing (DSP)-based digital subscriber line (DSL) was discussed. The programmable DSP-based solutions were tested by performing signal analysis and generation functions in software. The programmable solution allowed manufacturing tests to be customized to individual system requirements. The manufacturing verification process for DSL modems was completed in three stages of in-circuit, parametric and functional testing.},
key = {Microprocessor chips},
keywords = {Costs;Digital signal processing;Integrated circuit testing;Microprogramming;Modems;},
note = {Digital subscriber lines (DSL);In-circuit testing (ICT);},
} 


@inproceedings{20160501861464 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Model based testing of an interactive music system},
journal = {Proceedings of the ACM Symposium on Applied Computing},
author = {Poncelet, Clement and Jacquemard, Florent},
volume = {13-17-April-2015},
year = {2015},
pages = {1759 - 1764},
address = {Salamanca, Spain},
abstract = {The role of an interactive music system (IMS) is to accompany musicians during live performances, like a real musician. It reacts in realtime to audio signals from musicians, according to a timed specification called mixed score, written in a domain specific language. Such goals imply strong requirements of temporal reliability and robustness to unforeseen errors in input, yet not so much studied in the computer music community. We present the application of model-based testing techniques and tools to a state-of-the-art IMS, including the following tasks: generation of relevant input data for testing (including timing values) following coverage criteria, computation of the corresponding expected output, according to the semantics of a given mixed score, black-box execution of the test data and verdict. Our method is based on formal models compiled directly from mixed scores, and passed, after conversion to timed automata, to the model-checker Uppaal. This fully automatic approach has been applied to real mixed scores used in concerts and the results obtained have permitted to identify bugs in the target IMS.<br/> Copyright 2015 ACM.},
key = {Model checking},
keywords = {Automata theory;Black-box testing;Computer music;Graphical user interfaces;Problem oriented languages;Real time systems;Semantics;},
note = {Automatic approaches;Coverage criteria;Domain specific languages;Interactive music systems;Model based testing;State of the art;Temporal reliability;Timed Automata;},
URL = {http://dx.doi.org/10.1145/2695664.2695804},
} 


@article{20164502986281 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Model-based testing for building reliable realtime interactive music systems},
journal = {Science of Computer Programming},
author = {Poncelet, Clement and Jacquemard, Florent},
volume = {132},
year = {2016},
pages = {143 - 172},
issn = {01676423},
abstract = {The role of an Interactive Music System (IMS) is to accompany musicians during live performances, acting like a real musician. It must react in realtime to audio signals from musicians, according to a timed high-level requirement called mixed score, written in a domain specific language. Such goals imply strong requirements of temporal reliability and robustness to unforeseen errors in input, yet not much addressed by the computer music community. We present the application of Model-Based Testing techniques and tools to a state-of-the-art IMS, including in particular: offline and on-the-fly approaches for the generation of relevant input data for testing (including timing values), with coverage criteria, the computation of the corresponding expected output, according to the semantics of a given mixed score, the black-box execution of the test data on the System Under Test and the production of a verdict. Our method is based on formal models in a dedicated intermediate representation, compiled directly from mixed scores (high-level requirements), and either passed, to the model-checker Uppaal (after conversion to Timed Automata) in the offline approach, or executed by a virtual machine in the online approach. Our fully automatic framework has been applied to real mixed scores used in concerts and the results obtained have permitted to identify bugs in the target IMS.<br/> &copy; 2016 Elsevier B.V.},
key = {Model checking},
keywords = {Automata theory;Black-box testing;Computer music;Graphical user interfaces;Problem oriented languages;Semantics;Testing;},
note = {Coverage criteria;Domain specific languages;Interactive music systems;Intermediate representations;Model based testing;Off-line approaches;Temporal reliability;Timed Automata;},
URL = {http://dx.doi.org/10.1016/j.scico.2016.08.002},
} 


@article{20063810118906 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {HOTTest: A model-based test design technique for enhanced testing of domain-specific applications},
journal = {ACM Transactions on Software Engineering and Methodology},
author = {Sinha, Avik and Smidts, Carol},
volume = {15},
number = {3},
year = {2006},
pages = {242 - 278},
issn = {1049331X},
abstract = {Model-based testing is an effective black-box test generation technique for applications. Existing model-based testing techniques, however, fail to capture implicit domain-specific properties, as they overtly rely on software artifacts such as design documents, requirement specifications, etc., for completeness of the test model. This article presents a technique, HOTTest, which uses a strongly typed domain-specific language to model the system under test. This allows extraction of type-related system invariants, which can be related to various domain-specific properties of the application. Thus, using HOTTest, it is possible to automatically extract and embed domain-specific requirements into the test models. In this article we describe HOTTest, its principles and methodology, and how it is possible to relate domain-specific properties to specific type constraints. HOTTest is described using the example of HaskellDB, which is a Haskell-based embedded domain-specific language for relational databases. We present an example application of the technique and compare the results to some other commonly used Model-based test automation techniques like ASML-based testing, UML-based testing, and EFSM-based testing. &copy; 2006 ACM.},
key = {Software engineering},
keywords = {Automation;Computer programming languages;Database systems;Embedded systems;Mathematical models;},
note = {Database-specific test case generation;Domain-specific languages;Domain-specific testing;HaskellDB;Haskells;Model-based testing;Test case generation;Test generation tools;},
URL = {http://dx.doi.org/10.1145/1151695.1151697},
} 


@inproceedings{20123115287320 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Declarative automated test},
journal = {2012 7th International Workshop on Automation of Software Test, AST 2012 - Proceedings},
author = {Hallenberg, Niels and Carlsen, Philip Lykke},
year = {2012},
pages = {96 - 102},
address = {Zurich, Switzerland},
abstract = {Automated tests at the business level can be expensive to develop and maintain. One common approach is to have a domain expert instruct a QA developer to implement what she would do manually in the application. Though there exist record-replay tools specifically developed for this, these tend to scale poorly for more complicated test scenarios. We present a different solution: An Embedded Domain Specific Language (EDSL) in F#, containing the means to model the user interface, and the various manipulations of it. We hope that this DSL will bridge the gap between the business domain and technical domain of applications to such a degree that domain experts may be able to construct automatic tests without depending on QA developers, and that these tests will prove more maintainable. &copy; 2012 IEEE.<br/>},
key = {Software testing},
keywords = {Automation;Fluorine;Problem oriented languages;User interfaces;},
note = {Automated test;Automated testing;Business domain;Domain experts;Domain specific languages;Embedded domain specific languages;Functional testing;Record-replay;},
URL = {http://dx.doi.org/10.1109/IWAST.2012.6228998},
} 


@inproceedings{20150400440526 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Test process improvement with documentation driven integration testing},
journal = {Proceedings - 2014 9th International Conference on the Quality of Information and Communications Technology, QUATIC 2014},
author = {Haser, Florian and Felderer, Michael and Breu, Ruth},
year = {2014},
pages = {156 - 161},
address = {Guimaraes, Portugal},
abstract = {Improving the maturity of the test process in an organization, especially but not limited to integration testing, involves obstacles and risks, such as the additional work overhead of the new process. In addition, integration testing descriptions are often too technical not addressing the language needs of the domain. In research cooperations with companies from the insurance and banking domain it turned out that test descriptions and reports are one of the most useful testing artifacts, while doing adhoc testing. This paper presents a bottom up testing approach, which first helps the integration tester in producing a semi-formal test description and report, up to be an enabler for automatic model-based testing in the very end. The presented approach is based on a textual domain specific language that is able to evolve over time. This is done by analyzing the test descriptions and reports automatically with machine learning techniques as well as manually by integration testers. Often recurring test steps or used components are integrated into the test language, making it specially tailored for a specific organization. For each test step implementations can be attached, preparing it for the next iteration. In this paper the methodology and architecture of our integration testing approach are presented together with the underlying language concepts.<br/> &copy; 2014 IEEE.},
key = {Integration testing},
keywords = {Integration;Iterative methods;Learning systems;Model checking;Online systems;Problem oriented languages;Process engineering;},
note = {Automatic modeling;Domain specific languages;Machine learning techniques;Model-based integrations;Regression testing;Research cooperation;Test process;Underlying language;},
URL = {http://dx.doi.org/10.1109/QUATIC.2014.29},
} 


@inproceedings{20172703897368 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
volume = {Part F128404},
year = {2016},
pages = {City of Lugano; Hasler Foundation; Oracle Labs; Universita della Svizzera Italiana, Faculty of Informatics - },
address = {Lugano, Switzerland},
abstract = {The proceedings contain 18 papers. The topics discussed include: deeply reifying running code for constructing a domain-specific language; a distributed selectors runtime system for Java applications; efficient memory traces with full pointer information; extraction-based regression test selection; inference and checking of object immutability; integrating asynchronous task parallelism and data-centric atomicity; JCrypt: towards computation over encrypted data; multi-tier data synchronization based on an optimized concurrent linked-list; preexistence and concrete type analysis in the context of multiple inheritance; and prioritizing regression tests for desktop and web-applications based on the execution frequency of modified code.<br/>},
} 


@inproceedings{20124815732189 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Computer Applications for Software Engineering, Disaster Recovery, and Business Continuity - International Conferences, ASEA and DRBC 2012, Held in Conjunction with GST 2012, Proceedings},
journal = {Communications in Computer and Information Science},
volume = {340 CCIS},
year = {2012},
issn = {18650929},
address = {Jeju Island, Korea, Republic of},
abstract = {The proceedings contain 62 papers. The topics discussed include: impact on realistic mobility model for aircraft ad hoc networks; technology network model using bipartite social network analysis; mobile application development using component features and inheritance; view, level and fragment: commonalities in 'Architecture 101' and software modelling; highly analysable, reusable, and realisable architectural designs with XCD; ARSL: a domain specific language for aircraft separation minima determination; regression testing of object-oriented software: a technique based on use cases and associated tool; development of an instant meeting Android application using Wi-Fi direct APIs; developer support for understanding preprocessor macro expansions; towards building method level maintainability models based on expert evaluations; and a study on the improved stability of inverter through history management of semiconductor elements for power supply.},
} 


@inproceedings{20181805112947 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
volume = {2017-December},
year = {2018},
issn = {15301362},
address = {Nanjing, Jiangsu, China},
abstract = {The proceedings contain 93 papers. The topics discussed include: extracting traceability between predicates in event-B refinement; an improved approach to traceability recovery based on word embeddings; application of LSSVM and SMOTE on seven open source projects for predicting refactoring at class level; detecting full initialization points of objects to support code refactorings; a cloud-based trust evaluation scheme using a vehicular social network environment; Noff: a novel extendible parallel library for high-performance network traffic monitoring; a reusable framework for modeling and verifying in-vehicle networking systems in the presence of CAN and FlexRay; cost-effective regression testing using bloom filters in continuous integration development environments; correlation between the frequent use of gang-of-four design patterns and structural complexity; method level text summarization for java code using nano-patterns; flexible components for development of embedded systems with GPUs; Exniffer: learning to prioritize crashes by assessing the exploitability from memory dump; modeling and verifying identity authentication security of HDFS using CSP; a goal-driven framework in support of knowledge management; extracting insights from the topology of the JavaScript package ecosystem; improving bug localization with an enhanced convolutional neural network; mining handover process in open source development: an exploratory study; an analysis method of safety requirements for automotive software systems; and domain-specific language facilitates scheduling in model checking.<br/>},
} 


@inproceedings{20123015276547 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Modelling Foundations and Applications - 8th European Conference, ECMFA 2012, Proceedings},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {7349 LNCS},
year = {2012},
issn = {03029743},
address = {Kgs. Lyngby, Denmark},
abstract = {The proceedings contain 30 papers. The topics discussed include: executable UML: from multi-domain to multi-core; models meeting automotive design challenges; a commutative model composition operator to support software adaptation; comparative study of model-based and multi-domain system engineering approaches for industrial settings; strengthening SAT-based validation of UML/OCL models by representing collections as relations; model interchange testing: a process and a case study; an internal domain-specific language for constructing OPC UA queries and event filters; combining UML sequence and state machine diagrams for data-flow based integration testing; model transformations for migrating legacy models: an industrial case study; derived features for EMF by integrating advanced model queries; a lightweight approach for managing XML documents with MDE languages; and bridging the gap between requirements and aspect state machines to support non-functional testing: industrial case studies.},
} 


@inproceedings{20123515370190 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of EuroPLoP 2009 - 14th Annual European Conference on Pattern Languages of Programming},
journal = {Proceedings of EuroPLoP 2009 - 14th Annual European Conference on Pattern Languages of Programming},
year = {2009},
address = {Irsee, Germany},
abstract = {The proceedings contain 32 papers. The topics discussed include: enterprise architecture management patterns for enterprise architecture visioning; roles in a software project; applied pattern for strategy management for technology entrepreneurship and innovation MSc program; performance of open source projects; the role of analysis patterns in systems analysis; applying architectural patterns for parallel programming: solving the one-dimensional heat equation; towards formalized adaptation patterns for adaptive interactive systems; a pattern driven approach against architectural knowledge vaporization; reusable architectural decisions for DSL design: foundational decisions in DSL projects; a pattern vocabulary for project distribution; business patterns for knowledge audit implementation; applying distributed development patterns; and a pattern language of black-box test design for reactive software systems.},
} 


@inproceedings{20124315599205 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Applying industrial-strength testing techniques to critical care medical equipment},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Woskowski, Christoph},
volume = {7612 LNCS},
year = {2012},
pages = {62 - 73},
issn = {03029743},
address = {Magdeburg, Germany},
abstract = {Hardware and software development of embedded systems interdependently gear into each other. Even more so if the device under development is intended for use in critical care facilities such as intensive care units. Especially in this case, safety measures and risk mitigation techniques are implemented using both hardware and software components. Thus applying hardware and software testing approaches in combination is inevitable as well. The increasing utilization of test domain-specific languages (Test DSLs), code generators and keyword-driven interpreters tends to raise the level of abstraction in test development. This approach aims to enhance productivity by generating executable tests from a non-programming language created for describing test cases. A second goal is to increase coverage by generating tests for as many as possible combinations of input values (black box test) or for all reasonable paths of a program flow (white box test). In combination with hardware-supported signal generation and fault injection this can be a very powerful strategy for testing safety-critical embedded devices. This article introduces an example of this strategy - the usage of a keyword-driven testing technique in cooperation with additional test hardware - in the context of an embedded medical device development, all the while emphasizing the benefit of combining different approaches. It discusses the utilization of commercial off-the-shelf (COTS) testing hardware as well as the application of an in-house developed test box. It also highlights the integration of commercial software - for requirements engineering, test management and continuous integration - with a self-developed testing framework powered by its own keyword-based test DSL. &copy; 2012 Springer-Verlag.<br/>},
key = {Integration testing},
keywords = {Biomedical equipment;Computer hardware;Digital subscriber lines;Embedded systems;Hardware;Intensive care units;Problem oriented languages;Risk assessment;Safety engineering;Safety testing;Software design;},
note = {Commercial off-the shelves;Continuous integrations;Domain specific languages;Hardware and software components;Keyword driven;Medical device development;Medical Devices;Testing hardwares;},
URL = {http://dx.doi.org/10.1007/978-3-642-33678-2_6},
} 


@inproceedings{20083811553195 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {Developing automated test system for ADSL equipment},
journal = {Conference Record - IEEE Instrumentation and Measurement Technology Conference},
author = {Syed, Tariq and Das, Sunil R. and Biswas, Satyendra N. and Petriu, Emil M.},
year = {2008},
pages = {1833 - 1838},
issn = {10915281},
address = {Victoria, BC, Canada},
abstract = {The requirement for an automated test system has immensely increased due to the realization that manual testing is associated with additional resources and staffing constraints. In order to achieve a competitive edge, reduced development cost, timely product delivery, and product quality are mandatory in today's organization. Manual testing requires skilled operators that increase cost, time, and product delivery. The low cost computer based automated system helps to get an edge by fulfilling these organizational demands. In this paper, an automated testing system has been developed to support functional testing of Nortel Network's modem system (1-Meg SUT). The modem is an inherently complex asymmetric digital subscriber line (ADSL) product and its testing is far more complex than just verification ofprocess faults. The complexity ofADSL system renders automated test system an important and imperative part ofADSL testing. The subject paper demonstrates the indispensable need of automated test system for ADSL testing and its advantages in providing a competitive edge for the organization. &copy;2008 IEEE.<br/>},
key = {Asymmetric digital subscriber lines (ADSL)},
keywords = {Automation;Complex networks;Costs;DSL;Modems;Software testing;Telephone lines;},
note = {Automated systems;Automated test systems;Automated testing;Competitive edges;Development costs;Functional testing;Hardware platform;Software platforms;},
URL = {http://dx.doi.org/10.1109/IMTC.2008.4547343},
} 


@inproceedings{20183905860278 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {A process for evidence-based engineering of domain-specific languages},
journal = {ACM International Conference Proceeding Series},
author = {Felderer, Michael and Jeschko, Fabian},
volume = {Part F137700},
year = {2018},
pages = {SIGNAL; Software Innovation - },
address = {Christchurch, New zealand},
abstract = {Domain-specific languages (DSLs) are mainly designed ad-hoc and gut feeling resulting in languages that are often not well suited for their users and engineers. In this paper we develop a process for evidence-based language engineering to design domain-specific languages based on empirical evidence to support decision in language engineering. The developed process comprises an iterative execution of the phases DSL engineering, issue identification, data collection and evidence appraisal. We exemplify the concept by designing a DSL for Gherkin, a language test-driven acceptance testing in Xtext. The required evidence is derived by mining and analyzing all GitHub projects until July 1, 2017 that apply Gherkin.<br/> &copy; 2018 Association for Computing Machinery.},
key = {Software engineering},
keywords = {Acceptance tests;Digital subscriber lines;Problem oriented languages;},
note = {Acceptance testing;Domain specific languages;Empirical research;Evidence Based Software Engineering;Issue identifications;Iterative executions;Language engineering;Repository mining;},
URL = {http://dx.doi.org/10.1145/3210459.3210479},
} 


@inproceedings{20163002635787 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
copyright = {Compendex},
title = {16th International Symposium on Trends in Functional Programming, TFP 2015},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {9547},
year = {2016},
pages = {1 - 156},
issn = {03029743},
address = {Sophia Antipolis, France},
abstract = {The proceedings contain 8 papers. The special focus in this conference is on Trends in Functional Programming. The topics include: lightweight higher-order rewriting in haskell; towards a theory of reach; functional testing of java programs; type class instances for type-level lambdas in haskell; on the role of slicing in functional data-flow programming; a shallow embedded type safe extendable DSL for the arduino and programmable signatures and termination proofs for recursive functions in FoCaLiZe.},
} 



@INPROCEEDINGS{7133548, 
author={M. Rahman and J. Gao}, 
booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
title={A Reusable Automated Acceptance Testing Architecture for Microservices in Behavior-Driven Development}, 
year={2015}, 
volume={}, 
number={}, 
pages={321-325}, 
abstract={Cloud Computing and Mobile Cloud Computing are reshaping the way applications are being developed and deployed due to their unique needs such as massive scalability, guaranteed fault tolerance, near zero downtime, etc. and also daunting challenges such as security, reliability, continuous deployment and update capability. Microservices architecture, where application is composed of a set of independently deployable services, is increasingly becoming popular due to its capability to address most of these needs and challenges. In recent years, the Behavior-Driven Development (BDD) has become one of the most popular agile software development processes, and frequently used in microservices development. The key to success of BDD is the executable acceptance tests that describe the expected behavior of a feature and its acceptance criteria in the form of scenarios using simple and business people readable syntax. The reusability, auditability, and maintainability become some of the major concerns when BDD test framework is applied for each microservice repository and no previous research addresses these concerns. In this paper, we present a reusable automated acceptance testing architecture to address all these concerns.}, 
keywords={cloud computing;mobile computing;program testing;software prototyping;reusable automated acceptance testing architecture;cloud computing;mobile cloud computing;behavior-driven development;agile software development process;BDD test framework;Data structures;Boolean functions;Business;Testing;Software;Maintenance engineering;executable automated acceptance testing; Gherkin; functional testing; behavior-driven development; microservice}, 
doi={10.1109/SOSE.2015.55}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5069055, 
author={D. Talby}, 
booktitle={2009 ICSE Workshop on Automation of Software Test}, 
title={The perceived value of authoring and automating acceptance tests using a model driven development toolset}, 
year={2009}, 
volume={}, 
number={}, 
pages={154-157}, 
abstract={One approach to applying keyword driven testing in a model-driven development environment is by defining a domain specific language for test cases. The toolset then provides test editors, versioning, validation, reporting and hyperlinks across models - in addition to enabling automated test execution. This case study evaluates the effectiveness of such a solution as perceived by two teams of professional testers, who used it to test several products over a two year period. The results suggest that in addition to the expected benefits of automation, the solution reduces the time and effort required to write tests, maintain tests and plan the test authoring and execution efforts - at the expense of requiring longer training and a higher bar for recruiting testers.}, 
keywords={authoring systems;program testing;specification languages;perceived value;acceptance tests;model driven development toolset;keyword driven testing;domain specific language;automated test execution;test authoring;Automatic testing;DSL;Logic testing;Domain specific languages;Application software;Automation;Recruitment;Programming;Metamodeling;Context modeling}, 
doi={10.1109/IWAST.2009.5069055}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8387650, 
author={V. Pinkevich and A. Platunov}, 
booktitle={2018 IEEE Industrial Cyber-Physical Systems (ICPS)}, 
title={Model-driven functional testing of cyber-physical systems using deterministic replay techniques}, 
year={2018}, 
volume={}, 
number={}, 
pages={141-146}, 
abstract={Specialized embedded computer systems are one of the core technologies of modern industrial cyber-physical systems. They implement application algorithms and perform data acquisition, processing and transfer. The article presents the original approach to functional testing of embedded computer systems to overcome a number of restrictions imposed by specifics of the process of their design and development.}, 
keywords={cyber-physical systems;embedded systems;production engineering computing;program testing;application algorithms;embedded computer systems;deterministic replay techniques;model-driven functional testing;industrial cyber-physical systems;cyber-physical systems;embedded systems;record and deterministic replay;computer architecture;high-level modeling;functional testing;verification;debug}, 
doi={10.1109/ICPHYS.2018.8387650}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4464015, 
author={J. Diaz and A. Yague and P. P. Alarcon and J. Garbajosa}, 
booktitle={Seventh International Conference on Composition-Based Software Systems (ICCBSS 2008)}, 
title={A Generic Gateway for Testing Heterogeneous Components in Acceptance Testing Tools}, 
year={2008}, 
volume={}, 
number={}, 
pages={110-119}, 
abstract={Acceptance testing tools and Systems Under Test (SUT) require a gateway that will set up the communication link between them. Nevertheless, SUTs are often large systems composed of heterogeneous components that are executed in heterogeneous networks and platforms. Therefore, a non trivial communication problem between testing tools and these SUT heterogeneous components arises. A significant effort is invested in designing and implementing gateways for each specific component interface to cope with heterogeneity. This problem may be addressed through the use of middleware technologies that hide heterogeneity. However, this solution is too specific for each SUT domain. It may require a noteworthy effort to support the wide range of currently available interface standards that are provided by the different platforms and networks. An approach for testing heterogeneous components based on a generic gateway is presented in this paper. The generic gateway implements a service-oriented middleware named OSGi (Open Service Gateway initiative). OSGi helps to solve the heterogeneity problem and reduces the impact of designing a gateway for each specific SUT domain. The solution has been validated using the acceptance testing tool TOPEN (Test and Operation ENvironment) in a home automation scenario.}, 
keywords={distributed processing;middleware;object-oriented methods;program testing;software tools;generic gateway;heterogeneous component testing;acceptance testing tools;systems under test;communication link;heterogeneous networks;SUT heterogeneous components;interface standards;service-oriented middleware;OSGi;Open Service Gateway initiative;heterogeneity problem;SUT domain;TOPEN;Test and Operation ENvironment;home automation scenario;System testing;Automatic testing;Middleware;Software systems;Software testing;Home automation;Embedded system;Intelligent sensors;Computer architecture;Software tools;test automation;middleware;complex systems testing;acceptance testing tools;OSGI;TOPEN;gateway;validation}, 
doi={10.1109/ICCBSS.2008.31}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8449605, 
author={E. C. Dos Santos and P. Vilain and D. Hiura Longo}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={Poster: A Systematic Literature Review to Support the Selection of User Acceptance Testing Techniques}, 
year={2018}, 
volume={}, 
number={}, 
pages={418-419}, 
abstract={User Acceptance Testing (UAT) aims to determine whether or not a software satisfies users acceptance criteria. Although some studies have used acceptance tests as software requirements, no previous study has collected information about available UAT techniques and established a comparison of them, to support an organization in the selection of one over another. This work presents a Systematic Literature Review on UAT to find out available techniques and compare their main features. We selected 80 studies and found out 21 UAT techniques. As result, we created a comparative table summarizing these techniques and their features.}, 
keywords={Testing;Software;Natural languages;Tools;Bibliographies;Software engineering;Systematics;User acceptance testing;techniques;classification;features}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@INPROCEEDINGS{6569751, 
author={A. Trsel}, 
booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
title={A Testing Tool for Web Applications Using a Domain-Specific Modelling Language and the NuSMV Model Checker}, 
year={2013}, 
volume={}, 
number={}, 
pages={383-390}, 
abstract={Test case generation from formal models using model checking software is an established method. This paper presents a model-based testing approach for web applications based on a domain-specific language model. It is shown how the domain-specific language is transformed into the input language of the NuSMV model checker and how the resulting traces are converted into executable test scripts for various test automation tools. The presented approach has been implemented with comprehensive automation in a research tool which architecture is outlined.}, 
keywords={automatic programming;formal verification;Internet;program testing;specification languages;Web applications;domain-specific modelling language;NuSMV model checker software;test case generation;model-based testing approach;test automation tools;executable test scripts;DSL;Automation;Adaptation models;Software;Web pages;Model checking;web applications;model-based testing;model checking;test automation}, 
doi={10.1109/ICST.2013.54}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{7367051, 
author={R. Kumar and V. Kumar}, 
booktitle={2015 World Congress on Information Technology and Computer Applications (WCITCA)}, 
title={Process optimization for testing of domain specific languages in industrial automation}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Software testing is essential part of software development. The goal of the testing process is not only to enhance the quality and robustness of the software but also verify the correctness and non functional requirements of the software under all working conditions. Large software has their large testing suites to verify the stability of legacy features. Testing processes have huge challenges to maintain effectiveness and efficiency of the legacy test cases. There are many different processes and techniques available all technique or processes have their advantages and limitations. A tailored testing process has been tried to utilize all technique together to improvise the benefits and efficiency of testing in the industrial automation domain. This paper tries to explain a customized approach of utilizing the available testing techniques in such a way that it enhances the effectiveness and efficiency of regression testing, thus improving the time to market of large product-line Industrial automation software.}, 
keywords={factory automation;production engineering computing;program testing;program verification;software maintenance;software quality;process optimization;domain specific language testing;software testing;software development;software quality;software verification;legacy test cases;regression testing;product-line industrial automation software;Automation;Software testing;Fault detection;Software engineering;Software maintenance;Regression Testing;Test Effectiveness;Test suite optimization;Software Testing process;Test Automation;Industial automation}, 
doi={10.1109/WCITCA.2015.7367051}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{5540750, 
author={W. Vanderbauwhede and M. Margala and S. R. Chalamalasetti and S. Purohit}, 
booktitle={ASAP 2010 - 21st IEEE International Conference on Application-specific Systems, Architectures and Processors}, 
title={A C++-embedded Domain-Specific Language for programming the MORA soft processor array}, 
year={2010}, 
volume={}, 
number={}, 
pages={141-148}, 
abstract={MORA is a novel platform for high-level FPGA programming of streaming vector and matrix operations, aimed at multimedia applications. It consists of soft array of pipelined low-complexity SIMD processors-in-memory (PIM). We present a Domain-Specific Language (DSL) for high-level programming of the MORA soft processor array. The DSL is embedded in C++, providing designers with a familiar language framework and the ability to compile designs using a standard compiler for functional testing before generating the FPGA bitstream using the MORA toolchain. The paper discusses the MORA-C++ DSL and the compilation route into the assembly for the MORA machine and provides examples to illustrate the programming model and performance.}, 
keywords={C++ language;field programmable gate arrays;multimedia computing;parallel processing;pipeline processing;program compilers;specification languages;C++ embedded domain specific language;MORA soft processor array programming;high level FPGA programming;vector streaming;matrix operations;multimedia;pipelined low complexity SIMD processors-in-memory;DSL;MORA machine;compiler;Domain specific languages;Field programmable gate arrays;Parallel processing;DSL;Streaming media;Application specific integrated circuits;Parallel programming;Concurrent computing;Algorithm design and analysis;Programming profession;Reconfigurable Processor;Soft Processor Array;Multimedia Processing;Domain-Specific Language}, 
doi={10.1109/ASAP.2010.5540750}, 
ISSN={1063-6862}, 
month={July},}
@INPROCEEDINGS{7570913, 
author={P. Pandit and S. Tahiliani and M. Sharma}, 
booktitle={2016 Symposium on Colossal Data Analysis and Networking (CDAN)}, 
title={Distributed agile: Component-based user acceptance testing}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Testing is conducted at multiple levels during the development of software. User Acceptance Testing conforms that the software meets user's criteria. In this paper, User Acceptance Testing is automatically conducted based on acceptance criteria. The acceptance criteria are written in the form of Given-When-Then Template. These acceptance criteria are broken down into steps and numbered. The dependencies among the steps are determined as Given-&gt;When-&gt;Then. Henceforth, the steps are arranged in a dependency graph. This graph further leads to the creation of a decision table in which the outcome of one step leads to the outcomes of its dependent steps. The decision table forms the basis of generation of a binary weighted dependency tree. This tree becomes the means to form test coverage (number of combinations to test) which forms the basis of generation of acceptance test cases.}, 
keywords={decision tables;human factors;object-oriented programming;program testing;software prototyping;trees (mathematics);distributed agile;component-based user acceptance testing;software development;acceptance criteria;Given-When-Then template;dependency graph;decision table;binary weighted dependency tree;acceptance test case generation;Testing;Online banking;Data analysis;Credit cards;Software;Algorithm design and analysis}, 
doi={10.1109/CDAN.2016.7570913}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5463692, 
author={R. Chatley and J. Ayres and T. White}, 
booktitle={2010 Third International Conference on Software Testing, Verification, and Validation Workshops}, 
title={LiFT: Driving Development Using a Business-Readable DSL for Web Testing}, 
year={2010}, 
volume={}, 
number={}, 
pages={460-468}, 
abstract={This paper describes the development and evolution of LiFT, a framework for writing automated tests in a style that makes them very readable, even for non-programmers. We call this style 'literate testing'. By creating a domain-specific language embedded within Java, we were able to write automated tests that read almost like natural language, allowing business requirements to be expressed very clearly. This allows development to be driven from tests that are created by developers and customers together, helping give all stakeholders confidence that the right things are being tested and hence a correct system being built. We discuss the experiences of a team using these tools and techniques in a large commercial project, and the lessons learned from the experience.}, 
keywords={business data processing;Internet;Java;LiFT;driving development;business readable DSL;Web testing;literate testing;domain specific language;Java;natural language;DSL;Automatic testing;System testing;Writing;Software testing;Domain specific languages;Java;Natural languages;Business communication;Formal specifications;TDD;acceptance testing;DSL}, 
doi={10.1109/ICSTW.2010.12}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6200141, 
author={E. Borjesson}, 
booktitle={2012 IEEE Fifth International Conference on Software Testing, Verification and Validation}, 
title={Industrial Applicability of Visual GUI Testing for System and Acceptance Test Automation}, 
year={2012}, 
volume={}, 
number={}, 
pages={475-478}, 
abstract={The software market is becoming more challenging as demands for faster time-to-market and higher software quality continue to grow. These challenges are embedded in all areas of Software Engineering, including Verification and Validation where they are proposed as solvable with automated testing. However, most automated testing techniques focus on low system level testing and are not suitable for high level tests, i.e. System and Acceptance tests, leaving industrial needs for test automation unfulfilled. In this paper we present a research plan to evaluate a novel automated testing technique, called visual GUI testing, based on image recognition algorithms and scripts that interact through the system GUI to automate complex scenario based tests. The technique has been evaluated at the company Saab AB where industrial, safety critical, scenario based, test cases were automated showing the industrial applicability of the technique. However, many factors are still unknown regarding the techniques industrial applicability, i.e. script maintenance costs, usability and learn ability, etc. Our research aims to uncover these unknown factors with the final research goal to show that visual GUI testing is a viable and cost-effective technique that will fill the gap in industry for a cost-effective, simple, robust, high-level test automation technique.}, 
keywords={graphical user interfaces;program testing;program verification;software houses;software quality;time to market;industrial applicability;visual GUI testing;system test automation;acceptance test automation;software market;time-to-market;software quality;software engineering;verification;validation;automated testing;Testing;Graphical user interfaces;Visualization;Industries;Automation;Robustness;Maintenance engineering;V&V;Automated testing;Visual GUI testing;Image recognition;Scripted testing}, 
doi={10.1109/ICST.2012.129}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{8411756, 
author={B. Elodie and A. Fabrice and L. Bruno and B. Arnaud}, 
booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={Lightweight Model-Based Testing for Enterprise IT}, 
year={2018}, 
volume={}, 
number={}, 
pages={224-230}, 
abstract={Model-Based Testing (MBT) popularity in IT is growing at a very slow pace. A recent survey stated that no more than 14% of respondents use MBT in their projects. Our experience, presented in this paper, demonstrates that the complexity in use of the current MBT approaches for the average tester is the main reason for this low dissemination. Then we introduce a lightweight MBT approach and a tool, called Yest, dedicated to business process-based testing of enterprise information systems. This tool uses a workflow-based graphical representation linked with decision tables to be used by functional testers without requiring any kind of modeling skill (such as UML for example). These approach and tool are dedicated to a particular class of applications (i.e. enterprise IT applications such as ERP and bespoke business applications). This focus strongly helps to simplify the approach and to adapt the tooling to the targeted users (namely IT functional testers). Finally, we discuss the way MBT may support emerging Acceptance Test Driven Development practices in agile.}, 
keywords={business data processing;enterprise resource planning;information systems;information technology;program testing;Unified Modeling Language;business process-based testing;MBT popularity;Yest;IT functional testers;model-based testing;enterprise IT;average tester;Acceptance Test Driven Development practices;business applications;modeling skill;functional testers;decision tables;workflow-based graphical representation;enterprise information systems;Tools;Testing;Unified modeling language;Task analysis;Password;Electronic mail;Model-based-testing;Lightweight MBT;MBT tool;Test cases generation;Business process-based testing}, 
doi={10.1109/ICSTW.2018.00053}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7128889, 
author={N. Visic and H. Fill and R. A. Buchmann and D. Karagiannis}, 
booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
title={A domain-specific language for modeling method definition: From requirements to grammar}, 
year={2015}, 
volume={}, 
number={}, 
pages={286-297}, 
abstract={The core process a modeling method engineer needs to accomplish starts with the acquisition of domain knowledge and requirements, and ends with the deployment of a usable modeling tool. In between, a key intermediate deliverable of this process is the modeling method specification which, ideally, should be platform independent. On one hand, it takes input from a structured understanding of the application domain and scenarios; on the other hand, it provides sufficiently structured input to support the implementation of tool support for modeling activities. It is quite common that such modeling methods are domain-specific, in the sense that they provide concepts from the domain as first-class modeling citizens. However, for the purposes of this paper, we raise the level of abstraction for domain specificity and consider modeling method engineering as the application domain. Consequently, we raise several research questions - whether a domain-specific language can support this domain, and what would be its requirements, properties, constructs and grammar. We propose an initial draft of such a language - one that abstracts away from meta-modeling platforms by establishing a meta2layer of abstraction where a modeling method can be defined in a declarative manner, then the final modeling tool is generated by automated compilation of the method definition for the meta-modeling environment of choice.}, 
keywords={formal specification;grammars;knowledge based systems;domain-specific language;grammar;domain knowledge;modeling method specification;domain specificity;modeling method engineering;metamodeling platform;Unified modeling language;Analytical models;DSL;Metamodeling;Semantics;Computational modeling;Domain specific languages;domain-specific language;modeling method;meta-modeling;modeling tool}, 
doi={10.1109/RCIS.2015.7128889}, 
ISSN={2151-1349}, 
month={May},}
@INPROCEEDINGS{5254116, 
author={A. Miller and B. Kumar and A. Singhal}, 
booktitle={2009 33rd Annual IEEE International Computer Software and Applications Conference}, 
title={Photon: A Domain-Specific Language for Testing Converged Applications}, 
year={2009}, 
volume={2}, 
number={}, 
pages={269-274}, 
abstract={Automated testing of converged applications can be complex, as it is rare for a single testing tool to provide a single solution for all access points which a given application supports. As such, testing teams often create customized testing frameworks, which integrate several different testing tools, and a myriad of programming languages and scripting tools. When an applicationpsilas unique set of access points changes, or a new testing tool comes to market which offers a competitive advantage over existing test tools, the cost of updating these customized frameworks can be difficult to justify. This paper provides a solution to this problem by introducing ldquoPhotonese,rdquo a domain-specific language which testers can use to compose automation scripts which are independent of the test tool used for automation. In this way, the tester creates reusable testing assets in a framework which is reusable across multiple projects.}, 
keywords={authoring languages;program testing;domain-specific language;testing converged application;automated testing;customized testing framework;programming language;scripting tool;Photonese;automation script;reusable testing asset;Domain specific languages;Automatic testing;Automation;Software testing;Life testing;Telephony;Books;Application software;Displays;Computer applications;Software quality;Software reusability;Software testing}, 
doi={10.1109/COMPSAC.2009.143}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{6228998, 
author={N. Hallenberg and P. L. Carlsen}, 
booktitle={2012 7th International Workshop on Automation of Software Test (AST)}, 
title={Declarative automated test}, 
year={2012}, 
volume={}, 
number={}, 
pages={96-102}, 
abstract={Automated tests at the business level can be expensive to develop and maintain. One common approach is to have a domain expert instruct a QA developer to implement what she would do manually in the application. Though there exist record-replay tools specifically developed for this, these tend to scale poorly for more complicated test scenarios. We present a different solution: An Embedded Domain Specific Language (EDSL) in F#, containing the means to model the user interface, and the various manipulations of it. We hope that this DSL will bridge the gap between the business domain and technical domain of applications to such a degree that domain experts may be able to construct automatic tests without depending on QA developers, and that these tests will prove more maintainable.}, 
keywords={program testing;software quality;user interfaces;declarative automated testing;business level;domain expert;QA developer;record-replay tool;embedded domain specific language;F#;user interface;technical domain;Testing;DSL;Documentation;Phantoms;Engines;Business;User interfaces;Functional Testing;Automated Testing;Domain Specific Language;F#}, 
doi={10.1109/IWAST.2012.6228998}, 
ISSN={}, 
month={June},}
@ARTICLE{8440671, 
author={A. M. Mirza and M. N. A. Khan}, 
journal={IEEE Access}, 
title={An Automated Functional Testing Framework for Context-Aware Applications}, 
year={2018}, 
volume={6}, 
number={}, 
pages={46568-46583}, 
abstract={In the modern era of mobile computing, context-aware computing is an emerging paradigm due to its widespread applications. Context-aware applications are gaining increasing popularity in our daily lives since these applications can determine and react according to the situational context and help users to enhance usability experience. However, testing these applications is not straightforward since it poses several challenges, such as generating test data, designing context-coupled test cases, and so on. However, the testing process can be automated to a greater extent by employing model-based testing technique for context-aware applications. To achieve this goal, it is necessary to automate model transformation, test data generation, and test case execution processes. In this paper, we propose an approach for behavior modeling of context-aware application by extending the UML activity diagram. We also propose an automated model transformation approach to transform the development model, i.e., extended UML activity diagram into the testing model in the form of function nets. The objective of this paper is to automate the context-coupled test case generation and execution. We propose a functional testing framework for automated execution of keyword-based test cases. Our functional testing framework can reduce the testing time and cost, thus enabling the test engineers to execute more testing cycles to attain a higher degree of test coverage.}, 
keywords={mobile computing;program testing;Unified Modeling Language;keyword-based test cases;functional testing framework;test engineers;test coverage;context-aware computing;test data generation;test case execution processes;automated model transformation approach;context-coupled test case generation;mobile computing;UML activity diagram;Unified modeling language;Testing;Petri nets;Context-aware services;Sensors;Context modeling;Context-aware applications;model based testing;function net;petri net;model transformation}, 
doi={10.1109/ACCESS.2018.2865213}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{6470771, 
author={D. Mayrhofer and C. Huemer}, 
booktitle={2012 IEEE 14th International Conference on Commerce and Enterprise Computing}, 
title={REA-DSL: Business Model Driven Data-Engineering}, 
year={2012}, 
volume={}, 
number={}, 
pages={9-16}, 
abstract={An accounting information system (AIS) manages data about a company's financial and economic status. The contribution of this paper is closing the gap between the languages used by business domain experts and IT-experts in analyzing the relevant data. A well accepted approach for an accountability infrastructure is the Resource-Event-Agent (REA) ontology. Although REA has been based on well-established concepts of the accounting theory, its representation has not been intuitive to domain experts. In previous work, we developed the REA-DSL, a dedicated and easy-to-understand graphical domain specific modeling language for the REA ontology. Evidently, a model-driven approach requires to transform the REA-DSL artifacts to code. In this paper we present the transformation of the REA-DSL to a relational database for AIS. This approach offers the advantage that a domain expert verifies the relevant data in an "accounting language", whereas the IT expert is able to work with traditional data base structures.}, 
keywords={accounts data processing;ontologies (artificial intelligence);relational databases;simulation languages;software engineering;REA-DSL language;business model driven data-engineering;accounting information system;AIS;resource-event-agent ontology;REA ontology;accounting theory;graphical domain specific modeling language;relational database;domain expert;accounting language;Economics;Marine animals;Business;Ontologies;Unified modeling language;Marketing and sales;Analytical models;REA;domain-specific language;business models;relational schema}, 
doi={10.1109/CEC.2012.12}, 
ISSN={2378-1963}, 
month={Sept},}
@INPROCEEDINGS{7005185, 
author={J. Peltola and S. Sierla and V. Vyatkin}, 
booktitle={Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)}, 
title={Adapting Keyword driven test automation framework to IEC 61131-3 industrial control applications using PLCopen XML}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Factory Acceptance Testing should involve customer's experts and knowledge in defining, reading and validating tests, while keeping labor costs at moderate level. This involvement requires a testing approach, which hides implementation details and emphasizes domain terminology. Keyword driven testing is seen a viable test automation solution to reduce cost and enable customer involvement in acceptance testing. We propose an approach for adaptation of Keyword driven testing framework to IEC 61131-3 industrial process control applications. It utilizes importing of application elements, presented with PLCopen XML, and transforming them to proxy objects to be used as variables in test code, with domain specific names. Benefits include simplification of test and keyword specifications and hiding of implementation details from testers.}, 
keywords={automatic testing;control engineering computing;IEC standards;process control;programmable controllers;quality assurance;XML;factory acceptance testing;customer experts;keyword driven testing;test automation solution;cost reduction;IEC 61131-3 industrial process control applications;PLCopen XML;domain specific names;keyword specifications;Testing;Libraries;XML;Automation;Process control;IEC standards;Radio frequency;Industrial Process Control System;IEC 61131-3;PLCopen XML;Factory Acceptance Testing;Test Automation;Keyword Driven Testing;Test Framework}, 
doi={10.1109/ETFA.2014.7005185}, 
ISSN={1946-0740}, 
month={Sept},}
@INPROCEEDINGS{7928002, 
author={A. Dwarakanath and D. Era and A. Priyadarshi and N. Dubash and S. Podder}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Accelerating Test Automation through a Domain Specific Language}, 
year={2017}, 
volume={}, 
number={}, 
pages={460-467}, 
abstract={Test automation involves the automatic execution of test scripts instead of being manually run. This significantly reduces the amount of manual effort needed and thus is of great interest to the software testing industry. There are two key problems in the existing tools &amp; methods for test automation - a) Creating an automation test script is essentially a code development task, which most testers are not trained on, and b) the automation test script is seldom readable, making the task of maintenance an effort intensive process. We present the Accelerating Test Automation Platform (ATAP) which is aimed at making test automation accessible to non-programmers. ATAP allows the creation of an automation test script through a domain specific language based on English. The English-like test scripts are automatically converted to machine executable code using Selenium WebDriver. ATAP's English-like test script makes it easy for non-programmers to author. The functional flow of an ATAP script is easy to understand as well thus making maintenance simpler (you can understand the flow of the test script when you revisit it many months later). ATAP has been built around the Eclipse ecosystem and has been used in a real-life testing project. We present the details of the implementation of ATAP and the results from its usage in practice.}, 
keywords={program testing;specification languages;Selenium WebDriver;Eclipse ecosystem;ATAP;accelerating test automation platform;code development task;software testing industry;test script execution;domain specific language;test automation;Automation;DSL;Tools;Selenium;Natural languages;Java;Programming;Test automation;Selenium;Xtext;DSL}, 
doi={10.1109/ICST.2017.52}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8241120, 
author={A. Jumagaliyev and J. Whittle and Y. Elkhatib}, 
booktitle={2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)}, 
title={Using DSML for Handling Multi-tenant Evolution in Cloud Applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={272-279}, 
abstract={Multi-tenancy is sharing a single application's resources to serve more than a single group of users (i.e. tenant). Cloud application providers are encouraged to adopt multi-tenancy as it facilitates increased resource utilization and ease of maintenance, translating into lower operational and energy costs. However, introducing multi-tenancy to a single-tenant application requires significant changes in its structure to ensure tenant isolation, configurability and extensibility. In this paper, we analyse and address the different challenges associated with evolving an application's architecture to a multi-tenant cloud deployment. We focus specifically on multi-tenant data architectures, commonly the prime candidate for consolidation and multi-tenancy. We present a Domain-Specific Modeling language (DSML) to model a multi-tenant data architecture, and automatically generate source code that handles the evolution of the application's data layer. We apply the DSML on a representative case study of a single-tenant application evolving to become a multi-tenant cloud application under two resource sharing scenarios. We evaluate the costs associated with using this DSML against the state of the art and against manual evolution, reporting specifically on the gained benefits in terms of development effort and reliability.}, 
keywords={cloud computing;resource allocation;specification languages;DSML;resource utilization;multitenant cloud deployment;multitenant data architecture;multitenant cloud application;multitenancy;resource sharing;multitenant evolution handling;maintenance ease;Domain-Specific Modeling language;Databases;Data models;Load modeling;Computer architecture;Cloud computing;Software as a service;Business}, 
doi={10.1109/CloudCom.2017.31}, 
ISSN={2330-2186}, 
month={Dec},}
@INPROCEEDINGS{8425200, 
author={S. Petruzza and S. Treichler and V. Pascucci and P. Bremer}, 
booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
title={BabelFlow: An Embedded Domain Specific Language for Parallel Analysis and Visualization}, 
year={2018}, 
volume={}, 
number={}, 
pages={463-473}, 
abstract={The rapid growth in simulation data requires large-scale parallel implementations of scientific analysis and visualization algorithms, both to produce results within an acceptable timeframe and to enable in situ deployment. However, efficient and scalable implementations, especially of more complex analysis approaches, require not only advanced algorithms, but also an in-depth knowledge of the underlying runtime. Furthermore, different machine configurations and different applications may favor different runtimes, i.e., MPI vs Charm++ vs Legion, etc., and different hardware architectures. This diversity makes developing and maintaining a broadly applicable analysis software infrastructure challenging. We address some of these problems by explicitly separating the implementation of individual tasks of an algorithm from the dataflow connecting these tasks. In particular, we present an embedded domain specific language (EDSL) to describe algorithms using a new task graph abstraction. This task graph is then executed on top of one of several available runtimes (MPI, Charm++, Legion) using a thin layer of library calls. We demonstrate the flexibility and performance of this approach using three different large scale analysis and visualization use cases, i.e., topological analysis, rendering and compositing dataflow, and image registration of large microscopy scans. Despite the unavoidable overheads of a generic solution, our approach demonstrates performance portability at scale, and, in some cases, outperforms hand-optimized implementations.}, 
keywords={application program interfaces;data visualisation;graph theory;message passing;parallel processing;software libraries;specification languages;MPI;dataflow;embedded domain specific language;task graph abstraction;parallel analysis;large-scale parallel implementations;scientific analysis;machine configurations;hardware architectures;software infrastructure;BabelFlow;visualization algorithms;library calls;visualization use cases;Task analysis;Runtime;Software algorithms;Payloads;Software;Libraries;Rendering (computer graphics);Embedded DSL;user productivity;in situ analytics;Simulation runtime systems;programming models}, 
doi={10.1109/IPDPS.2018.00056}, 
ISSN={1530-2075}, 
month={May},}
@BOOK{7899157, 
author={Marco Brambilla and Jordi Cabot and Manuel Wimmer and Luciano Baresi}, 
booktitle={Model-Driven Software Engineering in Practice: Second Edition}, 
title={Model-Driven Software Engineering in Practice: Second Edition}, 
year={2017}, 
volume={}, 
number={}, 
pages={}, 
abstract={<p>This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE).</p> <p>MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis.</p> <p>The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away.</p> <p>The book is organized into two main parts.</p> <ul> <li>The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios, and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes.</li> <li>The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects.</li> </ul> <p>The second edition of the book features:</p> <ul> <li>a set of completely new topics, including: full example of the creation of a new modeling language (IFML), discussion of modeling issues and approaches in specific domains, like business process modeling, user interaction modeling, and enterprise architecture</li> <li>complete revision of examples, figures, and text, for improving readability, understandability, and coherence</li> <li>better formulation of definitions, dependencies between concepts and ideas</li> <li>addition of a complete index of book content</li> </ul> <p>In addition to the contents of the book, more resources are provided on the book's website http://www.mdse-book.com, including the examples presented in the book.</p>}, 
keywords={modeling;software engineering;UML;domain-specific language;model-driven engineering;code generation;reverse engineering;model transformation;MDD;MDA;MDE;MDSE;OMG;DSL;EMF;Eclipse}, 
doi={}, 
ISSN={}, 
publisher={Morgan & Claypool}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7899157},}
@INPROCEEDINGS{7862407, 
author={J. Wienke and S. Wrede}, 
booktitle={2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)}, 
title={Continuous regression testing for component resource utilization}, 
year={2016}, 
volume={}, 
number={}, 
pages={273-280}, 
abstract={Unintended changes in the utilization of resources like CPU and memory can lead to severe problems for the operation of robotics and intelligent systems. Still, systematic testing for such performance regressions has largely been ignored in this domain. We present a method to specify and execute performance tests for individual components of component-based robotics systems based on their component interfaces. The method includes an automatic analysis of each component revision against previous ones that reports potential changes to the resource usage characteristics. This informs developers about the impact of their changes. We describe the design of the framework and present evaluation results for the automatic detection of performance changes based on tests for a variety of robotics components.}, 
keywords={program testing;robots;continuous regression testing;component resource utilization;performance tests;component-based robotic system;component interfaces;automatic component revision analysis;resource usage characteristics;automatic performance change detection;robotic component;Robots;Testing;Middleware;Intelligent systems;Radiation detectors;Systematics}, 
doi={10.1109/SIMPAR.2016.7862407}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{5626417, 
author={T. Strasser and T. Peters and H. Jgle and E. Zrenner and R. Wilke}, 
booktitle={2010 Annual International Conference of the IEEE Engineering in Medicine and Biology}, 
title={An integrated domain specific language for post-processing and visualizing electrophysiological signals in Java}, 
year={2010}, 
volume={}, 
number={}, 
pages={4687-4690}, 
abstract={Electrophysiology of vision - especially the electroretinogram (ERG) - is used as a non-invasive way for functional testing of the visual system. The ERG is a combined electrical response generated by neural and non-neuronal cells in the retina in response to light stimulation. This response can be recorded and used for diagnosis of numerous disorders. For both clinical practice and clinical trials it is important to process those signals in an accurate and fast way and to provide the results as structured, consistent reports. Therefore, we developed a freely available and open-source framework in Java (http://www.eye.uni-tuebingen.de/project/idsI4sigproc). The framework is focused on an easy integration with existing applications. By leveraging well-established software patterns like pipes-and-filters and fluent interfaces as well as by designing the application programming interfaces (API) as an integrated domain specific language (DSL) the overall framework provides a smooth learning curve. Additionally, it already contains several processing methods and visualization features and can be extended easily by implementing the provided interfaces. In this way, not only can new processing methods be added but the framework can also be adopted for other areas of signal processing. This article describes in detail the structure and implementation of the framework and demonstrate its application through the software package used in clinical practice and clinical trials at the University Eye Hospital Tuebingen one of the largest departments in the field of visual electrophysiology in Europe.}, 
keywords={application program interfaces;data visualisation;electroretinography;Java;medical signal processing;neurophysiology;vision;integrated domain specific language;post-processing;electrophysiological signals;Java;vision;electroretinogram;ERG;neural cells;light stimulation;pipes-and-filters;fluent interfaces;application programming interfaces;smooth learning curve;visualization features;University Eye Hospital Tuebingen;visual electrophysiology;Lead;Java;Artificial neural networks;Hospitals;HTML;Visualization;Algorithms;Computer Graphics;Diagnosis, Computer-Assisted;Electroretinography;Humans;Programming Languages;Retinal Diseases;Software;User-Computer Interface}, 
doi={10.1109/IEMBS.2010.5626417}, 
ISSN={1094-687X}, 
month={Aug},}
@INPROCEEDINGS{6984109, 
author={F. Hser and M. Felderer and R. Breu}, 
booktitle={2014 9th International Conference on the Quality of Information and Communications Technology}, 
title={Test Process Improvement with Documentation Driven Integration Testing}, 
year={2014}, 
volume={}, 
number={}, 
pages={156-161}, 
abstract={Improving the maturity of the test process in an organization, especially but not limited to integration testing, involves obstacles and risks, such as the additional work overhead of the new process. In addition, integration testing descriptions are often too technical not addressing the language needs of the domain. In research cooperations with companies from the insurance and banking domain it turned out that test descriptions and reports are one of the most useful testing artifacts, while doing adhoc testing. This paper presents a bottom up testing approach, which first helps the integration tester in producing a semi-formal test description and report, up to be an enabler for automatic model-based testing in the very end. The presented approach is based on a textual domain specific language that is able to evolve over time. This is done by analyzing the test descriptions and reports automatically with machine learning techniques as well as manually by integration testers. Often recurring test steps or used components are integrated into the test language, making it specially tailored for a specific organization. For each test step implementations can be attached, preparing it for the next iteration. In this paper the methodology and architecture of our integration testing approach are presented together with the underlying language concepts.}, 
keywords={learning (artificial intelligence);program testing;machine learning;textual domain specific language;automatic model-based testing;documentation driven integration testing;test process improvement;Testing;Documentation;Unified modeling language;DSL;Insurance;Companies;Model-Based Integration Testing;Test Process Improvement;Regression Testing}, 
doi={10.1109/QUATIC.2014.29}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{5974321, 
author={I. S. W. B. Prasetya and J. Amorim and T. E. J. Vos and A. Baars}, 
booktitle={6th Iberian Conference on Information Systems and Technologies (CISTI 2011)}, 
title={Using Haskell to script combinatoric testing of Web Services}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The Classification Tree Method (CTM) is a popular approach in functional testing as it allows the testers to systematically partition the input domain of an SUT, and specifies the combinations they want. We have implemented the approach as a small domain specific language (DSL) embedded in the functional language Haskell. Such an embedding leads to clean syntax and moreover we can natively access Haskell's full features. This paper will explain the approach, and how it is applied for testing Web Services.}, 
keywords={pattern classification;program testing;tree data structures;Web services;Haskell;script combinatoric testing;web services;classification tree method;CTM;functional testing;SUT;domain specific language;DSL;functional language Haskell;Cities and towns;Strips;automated testing;combinatoric testing}, 
doi={}, 
ISSN={2166-0727}, 
month={June},}
@INPROCEEDINGS{7323087, 
author={J. Iber and N. Kajtazovic and A. Hller and T. Rauter and C. Kreiner}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Ubtl UML testing profile based testing language}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={The continuous increase of software complexity is one of the major problems associated with the development of today's complex technical systems. In particular, for safety-critical systems, which usually require to be thoroughly verified and validated, managing such a complexity is of high importance. To this end, industry is utilizing Model-Driven Development (MDD) in many aspects of systems engineering, including verification and validation activities. Until now many specifications and standards have been released by the MDD community to support those activities by putting models in focus. The general problem is, however, that applying those specifications is often difficult, since they comprise a broader scope than usually required to solve specific problems. In this paper we propose a domain-specific language (DSL) that allows to specify tests from the UML Testing Profile (UTP). The main contribution is that only particular aspects of UTP are captured, thereby allowing the MDD process to be narrowed to specific needs, such as supporting code generation facilities for certain types of tests or even specific statements in tests. In the end we show the application of the DSL using a simple example within a MDD process, and we report on performance of that process.}, 
keywords={program compilers;program testing;program verification;safety-critical software;software metrics;Unified Modeling Language;code generation facilities;UTP;test specification;DSL;domain-specific language;validation activity;verification activity;systems engineering;MDD;model-driven development;safety-critical systems;complex technical systems;software complexity;testing language;UML testing profile;Ubtl;Unified modeling language;Testing;Generators;Biological system modeling;DSL;Software;Concrete;UML Testing Profile;UML;Textual Domain-Specific Language;Test Specification Language;Software Testing;Model-Driven Development}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{6058739, 
author={W. J. Headrick and M. A. Bodkin and R. R. Fox and T. W. Davis and K. Dusch and D. Wolfe}, 
booktitle={2011 IEEE AUTOTESTCON}, 
title={Signal Based Domain Specific Language (SBDSL) a proposal for a next generation test}, 
year={2011}, 
volume={}, 
number={}, 
pages={240-244}, 
abstract={Signal Based Domain Specific Language (SBDSL) is a domain specific language which combines the use of ATLAS Signal statements with high-level programming language constructs. The goals of this new language are: facilitate the writing of concurrent test programs, provide a language that is easy to extend with new constructs, maintain backwards compatibility with ATLAS Family of languages, enable interoperability between test stations, and enable engineers' fresh out of college to quickly become productive with a test programming language. This paper will cover how the design of the SBDSL language, SBDSL Integrated Development Environment (IDE) and runtime executable will accomplish these goals and present results from the technology demonstration developed.}, 
keywords={open systems;program testing;specification languages;signal based domain specific language;next generation test;ATLAS signal statements;high level programming language constructs;concurrent test programs;backwards compatibility;ATLAS language family;interoperability;test programming language;SBDSL integrated development environment;Hardware;Instruments;Visualization;Syntactics;Debugging;Programming;Libraries}, 
doi={10.1109/AUTEST.2011.6058739}, 
ISSN={1558-4550}, 
month={Sept},}
@INPROCEEDINGS{802127, 
author={A. A. Reyes and D. Richardson}, 
booktitle={14th IEEE International Conference on Automated Software Engineering}, 
title={Siddhartha: a method for developing domain-specific test driver generators}, 
year={1999}, 
volume={}, 
number={}, 
pages={81-90}, 
abstract={Siddhartha applies the domain-specific language (DSL) paradigm to solve difficult problems in specification-based testing (SBT). Domain-specific test case data specifications (TestSpecs) and difficult-to-test program design styles engender difficult SBT problems, which are the essential phenomena of interest to Siddhartha. Difficult-to-test program design styles are explicitly represented by domain-specific, unit test driver reference designs that accommodate the problematic program design styles. DSLs are developed to represent both TestSpecs and Driver reference designs. A DSL language processing tool (a translator) is developed that maps TestSpecs into Drivers. We developed a prototype implementation of Siddhartha via Reasoning SDK (formerly known as Software Refinery) and developed two domain-specific TestSpec/spl rarr/Driver translators. Each translator generated Drivers that revealed new failures in a real-world digital flight control application program.}, 
keywords={formal specification;program testing;automatic testing;program interpreters;aerospace computing;Siddhartha;domain-specific language paradigm;domain-specific test driver generator development;specification-based testing;domain-specific test case data specifications;difficult-to-test program design styles;domain-specific unit test driver reference designs;TestSpecs;Driver reference designs;DSL language processing tool;translator;Reasoning SDK;real-world digital flight control application program;Automatic testing;Application software;DSL;Aerospace control;Computer science;Aerospace electronics;Domain specific languages;Prototypes;Formal specifications;Automatic control}, 
doi={10.1109/ASE.1999.802127}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4052587, 
author={C. Yilmaz and A. Porter and A. S. Krishna and A. M. Memon and D. C. Schmidt and A. S. Gokhale and B. Natarajan}, 
journal={IEEE Transactions on Software Engineering}, 
title={Reliable Effects Screening: A Distributed Continuous Quality Assurance Process for Monitoring Performance Degradation in Evolving Software Systems}, 
year={2007}, 
volume={33}, 
number={2}, 
pages={124-141}, 
abstract={Developers of highly configurable performance-intensive software systems often use in-house performance-oriented "regression testing" to ensure that their modifications do not adversely affect their software's performance across its large configuration space. Unfortunately, time and resource constraints can limit in-house testing to a relatively small number of possible configurations, followed by unreliable extrapolation from these results to the entire configuration space. As a result, many performance bottlenecks escape detection until systems are fielded. In our earlier work, we improved the situation outlined above by developing an initial quality assurance process called "main effects screening". This process 1) executes formally designed experiments to identify an appropriate subset of configurations on which to base the performance-oriented regression testing, 2) executes benchmarks on this subset whenever the software changes, and 3) provides tool support for executing these actions on in-the-field and in-house computing resources. Our initial process had several limitations, however, since it was manually configured (which was tedious and error-prone) and relied on strong and untested assumptions for its accuracy (which made its use unacceptably risky in practice). This paper presents a new quality assurance process called "reliable effects screening" that provides three significant improvements to our earlier work. First, it allows developers to economically verify key assumptions during process execution. Second, it integrates several model-driven engineering tools to make process configuration and execution much easier and less error prone. Third, we evaluate this process via several feasibility studies of three large, widely used performance-intensive software frameworks. Our results indicate that reliable effects screening can detect performance degradation in large-scale systems more reliably and with significantly less resources than conventional techniques}, 
keywords={program testing;software performance evaluation;software quality;software reliability;reliable effects screening;distributed continuous quality assurance process;performance degradation monitoring;evolving software systems;performance intensive software systems;regression testing;software performance;in house testing;performance bottlenecks;main effects screening;configuration subset;software benchmarks;tool support;process configuration;process execution;Quality assurance;Monitoring;Degradation;Software systems;Performance evaluation;Software performance;Software testing;System testing;Time factors;Extrapolation;Distributed continuous quality assurance;performance-o-ri-ented regression testing;design-of-experiments theory.}, 
doi={10.1109/TSE.2007.20}, 
ISSN={0098-5589}, 
month={Feb},}
@INPROCEEDINGS{6233406, 
author={1. Landhuer and A. Genaid}, 
booktitle={2012 Third International Workshop on Recommendation Systems for Software Engineering (RSSE)}, 
title={Connecting User Stories and code for test development}, 
year={2012}, 
volume={}, 
number={}, 
pages={33-37}, 
abstract={User Stories are short feature descriptions from the user's point of view. Functional tests ensure that the feature described by a User Story is fully implemented. We present a tool that builds an ontology for code and links completed User Stories in natural language with the related code artifacts. The ontology also contains links to API components that were used to implement the functional tests. Preliminary results show that these links can be used to recommend reusable test steps for new User Stories.}, 
keywords={application program interfaces;functional programming;natural languages;ontologies (artificial intelligence);program testing;software prototyping;software reusability;functional testing;feature description;user story;ontology;natural language;code artifact;API component;reusable testing;Ontologies;Natural languages;Software;Data structures;Boolean functions;Compounds;Testing;code mining;functional testing;reasoning;traceability;ontology}, 
doi={10.1109/RSSE.2012.6233406}, 
ISSN={2327-0934}, 
month={June},}
@INPROCEEDINGS{8115712, 
author={L. Meftah and M. Gomez and R. Rouvoy and I. Chrisment}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={ANDROFLEET: Testing WiFi peer-to-peer mobile apps in the large}, 
year={2017}, 
volume={}, 
number={}, 
pages={961-966}, 
abstract={WiFi P2P allows mobile apps to connect to each other via WiFi without an intermediate access point. This communication mode is widely used by mobile apps to support interactions with one or more devices simultaneously. However, testing such P2P apps remains a challenge for app developers as i) existing testing frameworks lack support for WiFi P2P, and ii) WiFi P2P testing fails to scale when considering a deployment on more than two devices. In this paper, we therefore propose an acceptance testing framework, named Androfleet, to automate testing of WiFi P2P mobile apps at scale. Beyond the capability of testing point-to-point interactions under various conditions, An-drofleet supports the deployment and the emulation of a fleet of mobile devices as part of an alpha testing phase in order to assess the robustness of a WiFi P2P app once deployed in the field. To validate Androfleet, we demonstrate the detection of failing black-box acceptance tests for WiFi P2P apps and we capture the conditions under which such a mobile app can correctly work in the field. The demo video of Androfleet is made available from https://youtu.be/gJ5_Ed7XL04.}, 
keywords={mobile computing;peer-to-peer computing;program testing;wireless LAN;P2P testing;acceptance testing framework;An-drofleet;point-to-point interaction testing;WiFi peer-to-peer mobile application testing;black-box acceptance tests;Androfleet;alpha testing phase;mobile devices;Wireless fidelity;Peer-to-peer computing;Testing;Mobile communication;Androids;Humanoid robots;Mobile handsets}, 
doi={10.1109/ASE.2017.8115712}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{713476, 
author={F. H. Daou}, 
booktitle={1998 IEEE AUTOTESTCON Proceedings. IEEE Systems Readiness Technology Conference. Test Technology for the 21st Century (Cat. No.98CH36179)}, 
title={Overview of ADSL test requirement towards conformance, performance and interoperability}, 
year={1998}, 
volume={}, 
number={}, 
pages={413-420}, 
abstract={The enormous installed base of copper in the access network with the right transmission techniques present a huge potential for delivering broadband services to bandwidth hungry customers. Various Digital Subscriber line technologies (xDSL) employ various transmission methods and efficiently utilize the last available bandwidth on existing copper wires. Asymmetric Digital Subscriber Line (ADSL) delivers up to 6 Mbps to the user. This transmission of 6 Mbps is achieved using sophisticated modulation and compression techniques in a spectrum up to 1.1 MHz, pushing the physical limit on the usable bandwidth in the copper. This technology co-exists with impairments, noise intrusions, bridge taps, and other non-spectrally compatible transmissions. This paper presents an overview of xDSL Technology (which includes ADSL), the test challenges facing ADSL technology, and outlines the three areas of ADSL tests needed to ensure product conformance and cross vendor interoperability.}, 
keywords={digital subscriber lines;telecommunication equipment testing;conformance testing;automatic test equipment;automatic testing;open systems;ADSL test requirement;conformance;performance;interoperability;broadband services;available bandwidth;copper wires;asymmetric DSL;usable bandwidth;noise intrusions;impairments;bridge taps;nonspectrally compatible transmissions;test challenges;CAP/QAM spectral separation;compliance;functional tester;ATE;6 Mbit/s;Testing;Copper;Bandwidth;Bit rate;OFDM modulation;Quadrature amplitude modulation;DSL;Modulation coding;Working environment noise;Phase modulation}, 
doi={10.1109/AUTEST.1998.713476}, 
ISSN={1088-7725}, 
month={Aug},}
@INPROCEEDINGS{7437001, 
author={N. Hafidhoh and I. Liem and F. N. Azizah}, 
booktitle={2015 International Conference on Data and Software Engineering (ICoDSE)}, 
title={Source code generator for automating business rule implementation}, 
year={2015}, 
volume={}, 
number={}, 
pages={219-224}, 
abstract={Business rules can be implemented on business processes, business behavior, people, or software in an organization. Aligned with software development, business rules are captured from requirement elicitation and analysis, then designed and implemented in the software. The changes of business environment may affect business rules. The changes of the business rules may bring impact in the software, so that the software needs to be redeveloped. In this paper, we present a source code generator to automate business rule implementation using business rule approach. We propose a Domain Specific Language (DSL) of business rule to help expressing business rules in a business-friendly language. We also develop a business rule generator to generate source codes based on a DSL script for expressing the business rules. The proposed solution has been tested in two case studies. It is shown that the generator can help the implementation of the business rules in source codes and that the generated code can be used in business applications.}, 
keywords={business data processing;formal specification;source code (software);source code generator;business rule implementation automation;software development;requirement elicitation;requirement analysis;business environment;domain-specific language;business-friendly language;business rule generator;DSL script;DSL;Natural languages;Software;Generators;Organizations;Grammar;business rule;implementation;source code generator;DSL}, 
doi={10.1109/ICODSE.2015.7437001}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4600272, 
author={S. M. R. Sadri and Y. N. Harandi and M. Pirhadi and M. Y. Waskasi and A. I. Tabrizipoor and M. Mirzabaghi}, 
booktitle={2007 International Symposium on High Capacity Optical Networks and Enabling Technologies}, 
title={Test strategy for DSL broadband IP access services}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={In this paper the test methodology has been expanded for evaluation of DSL broadband services. This is a new strategy for testing and considering different aspects of test. This strategy can precisely test various features of DSL broadband equipment and services which can be delivered by them in terms of different aspects of testing such as functionality, performance, conformance, etc. It was practically executed over a designed testbed in Iran Telecommunications Research Center (ITRC) where a lab named NGN Pilot exists. The introduced strategy has a hierarchical structure from the topmost level of testing including different categories to the bottommost level including detailed test topics and test cases. This paper elaborates how the strategy was designed and how it can be applied to a desired environment to compare different DSL scenarios with each other. The strategy can then be used by a service provider to help deciding which solutions can achieve the best outcome.}, 
keywords={broadband networks;digital subscriber lines;IP networks;DSL broadband service;IP access services;DSL broadband equipment;Iran Telecommunications Research Center;NGN Pilot;hierarchical structure;service provider;Modems;Broadband communication;IP networks;Multiprotocol label switching;Throughput;Quality of service;Next generation networking;DSL;Broadband Access;Test Strategy;Testbed;Pilot}, 
doi={10.1109/HONET.2007.4600272}, 
ISSN={1949-4092}, 
month={Nov},}
@INPROCEEDINGS{7985680, 
author={R. Rolim and G. Soares and L. D'Antoni and O. Polozov and S. Gulwani and R. Gheyi and R. Suzuki and B. Hartmann}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)}, 
title={Learning Syntactic Program Transformations from Examples}, 
year={2017}, 
volume={}, 
number={}, 
pages={404-415}, 
abstract={Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.}, 
keywords={automatic programming;program processors;syntactic program transformations learning;REFAZER;programming-by-example methodology;domain-specific language;domain-specific deductive algorithms;DSL;code edits;C# open-source projects;DSL;Programming profession;Tools;C# languages;Pattern matching;Open source software;Program transformation;program synthesis;tutoring systems;refactoring}, 
doi={10.1109/ICSE.2017.44}, 
ISSN={1558-1225}, 
month={May},}
@BOOK{6813569, 
author={Marco Brambilla and Jordi Cabot and Manuel Wimmer}, 
booktitle={Model-Driven Software Engineering in Practice}, 
title={Model-Driven Software Engineering in Practice}, 
year={2012}, 
volume={}, 
number={}, 
pages={}, 
abstract={This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The book is targeted to a diverse set of readers, spanning: professionals, CTOs, CIOs, and team managers that need to have a bird's eye vision on the matter, so as to take the appropriate decisions when it comes to choosing the best development techniques for their company or team; software analysts, developers, or designers that expect to use MDSE for improving everyday work productivity, either by applying the basic modeling techniques and notations or by defining new domain-specific modeling languages and applying end-to-end MDSE practices in the software factory; and academic teachers and students to address undergrad and postgrad courses on MDSE. In addition to the contents of the book, more resources are provided on the book's website, including the examples presented in the book. Table of Contents: Introduction / MDSE Principles / MDSE Use Cases / Model-Driven Architecture (MDA) / Integration of MDSE in your Development Process / Modeling Languages at a Glance / Developing your Own Modeling Language / Model-to-Model Transformations / Model-to-Text Transformations / Managing Models / Summary}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Morgan & Claypool}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6813569},}
@INPROCEEDINGS{5261058, 
author={C. McMahon}, 
booktitle={2009 Agile Conference}, 
title={History of a Large Test Automation Project Using Selenium}, 
year={2009}, 
volume={}, 
number={}, 
pages={363-368}, 
abstract={In 2007 I started work as a tester for a company called Socialtext. When I joined the company there was already a Selenium-based test framework in place, but there were only a couple of automated test cases created; we had about 400 test steps, or individual assertions about the behavior of the application. When I left Socialtext two years later, we had just surpassed 10,000 test steps in the main set of regression tests. We also had browser-specific test sets in place, an automated test case for visually checking the application, and a Continuous-Integration-like script that ran all day and all night against the latest version of the code. At about 4000 test steps, regression bugs released to production dropped essentially to zero. The other 6000 test steps covered ongoing new features in the project, and more robust testing of the older application functions. This report discusses how I helped grow this system, and the things we learned along the way that helped it be such a successful ongoing project. The report covers initial conditions and test design; discusses issues in application feature coverage; how and when to grow the system quickly; a couple of test design smells that caused us problems along the way; how we treat Continuous Integration in a system like this; and how we coped when significant parts of the User Interface were completely re-engineered.}, 
keywords={program testing;regression analysis;large test automation project;selenium;Socialtext;regression tests;browser specific test sets;continuous-integration-like script;robust testing;application function;application feature coverage;test design;continuous integration;user interface;History;Automatic testing;System testing;Quality management;Failure analysis;User interfaces;Engineering profession;Home automation;Radio access networks;Computer bugs;UI automation;Selenium;test design;experience}, 
doi={10.1109/AGILE.2009.9}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8101621, 
author={O. Olajubu and S. Ajit and M. Johnson and S. Thomson and M. Edwards and S. Turner}, 
booktitle={2017 9th Computer Science and Electronic Engineering (CEEC)}, 
title={Automated test case generation from high-level logic requirements using model transformation techniques}, 
year={2017}, 
volume={}, 
number={}, 
pages={178-182}, 
abstract={It is not uncommon for industries to use natural language to represent high-level software requirement specifications. It is also not uncommon for these requirement specifications to be translated into design and used further for implementation and generation of test cases in the software engineering life-cycle. These requirements are often ambiguous, incorrect, and incomplete. Finding them late in the development lifecycle proves very expensive and lowers the productivity. This paper reports on the experience of applying model-based technologies from academia to a real-world problem domain in the aviation industry to improve the productivity. The paper focuses on the application of a model-based technique to automatically generate test cases to satisfy Modified Condition/Decision Coverage (MC/DC) from high-level logic requirements expressed in a Domain Specific Language (DSL).}, 
keywords={automatic testing;formal specification;program testing;specification languages;high-level software requirement specifications;software engineering life-cycle;high-level logic requirements;Domain Specific Language;automated test case generation;model transformation techniques;natural language;productivity;model-based technique;decision coverage;modified condition;Legged locomotion;DSL;Software;Testing;Industries;Productivity;Natural languages}, 
doi={10.1109/CEEC.2017.8101621}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7102636, 
author={S. Herbold and A. De Francesco and J. Grabowski and P. Harms and L. M. Hillah and F. Kordon and A. Maesano and L. Maesano and C. Di Napoli and F. De Rosa and M. A. Schneider and N. Tonellotto and M. Wendland and P. Wuillemin}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={The MIDAS Cloud Platform for Testing SOA Applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={While Service Oriented Architectures (SOAs) are for many parts deployed online, and today often in a cloud, the testing of the systems still happens mostly locally. In this paper, we want to present the MIDAS Testing as a Service (TaaS), a cloud platform for the testing of SOAs. We focus on the testing of whole SOA orchestrations, a complex task due to the number of potential service interactions and the increasing complexity with each service that joins an orchestration. Since traditional testing does not scale well with such a complex setup, we employ a Model-based Testing (MBT) approach based on the Unified Modeling Language (UML) and the UML Testing Profile (UTP) within MIDAS. Through this, we provide methods for functional testing, security testing, and usage-based testing of service orchestrations. Through harnessing the computational power of the cloud, MIDAS is able to generate and execute complex test scenarios which would be infeasible to run in a local environment.}, 
keywords={cloud computing;program testing;service-oriented architecture;Unified Modeling Language;MIDAS cloud platform;SOA application testing;service oriented architecture;MIDAS testing-as-a-service;TaaS platform;SOA orchestration;model-based testing approach;MBT approach;unified modeling language;UML testing profile;functional testing;security testing;usage-based testing;Testing;Unified modeling language;DSL;Cloud computing;Service-oriented architecture;Monitoring}, 
doi={10.1109/ICST.2015.7102636}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{5069041, 
author={T. Clark}, 
booktitle={2009 ICSE Workshop on Automation of Software Test}, 
title={Model based functional testing using pattern directed filmstrips}, 
year={2009}, 
volume={}, 
number={}, 
pages={53-61}, 
abstract={Model driven functional system testing generates test scenarios from behavioural and structural models. In order to autmatically generate tests, conditions such as invariants and pre-/post-conditions must be precisely defined. UML provides the Object Constraint Language (OCL) for this purpose; however OCL expressions can become very complex. This paper describes an approach that allows many commonly found OCL patterns to be expressed as snapshot patterns that correspond directly to the information model diagrams. Behaviour is constructed as chains of snapshots, or filmstrips. Snapshots and filmstrips are as expressive as UML behaviour models and OCL but it is argued that they are more accessible and more modular.}, 
keywords={program testing;Unified Modeling Language;pattern directed filmstrips;model driven functional system testing;Object Constraint Language;UML;Snapshots;System testing;Unified modeling language;Logic testing;Software testing;Context modeling;Software engineering;Test pattern generators;Process design;Concrete;Graphical user interfaces}, 
doi={10.1109/IWAST.2009.5069041}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7095983, 
author={M. Kayama and S. Ogata and T. Nagai and H. Yokoka and K. Masumoto and M. Hashimoto}, 
booktitle={2015 IEEE Global Engineering Education Conference (EDUCON)}, 
title={Effectiveness of Model-Driven Development in conceptual modeling education for university freshmen}, 
year={2015}, 
volume={}, 
number={}, 
pages={274-282}, 
abstract={The purpose of this study is to explore educational methods for conceptual modeling for novices. In this research, the subjects are mainly freshmen in university. Model driven development (MDD) and a domain specific language (DSL) are key factors in this study. By using MDD, learners are expected to be able to evaluate their own model by observing the target device's behavior. By using a DSL, teachers can control the difficulty of the problems given to their learners. In this paper, we describe our research approach using MDD and a DSL, then, show our experiment design and results. We also discuss the effectiveness of MDD in university freshmen courses with proposed educational methodology.}, 
keywords={computer science education;educational courses;educational institutions;model-driven development;conceptual modeling education;domain specific language;DSL;MDD;university freshmen courses;educational methodology;Unified modeling language;Object oriented modeling;DSL;Computational modeling;Robots;Conferences;Analytical models;model driven development;university freshmen;state machine diagram;model quality;achievment level}, 
doi={10.1109/EDUCON.2015.7095983}, 
ISSN={2165-9567}, 
month={March},}
@INPROCEEDINGS{6498461, 
author={E. Duclos and S. Le Digabel and Y. Guhneuc and B. Adams}, 
booktitle={2013 17th European Conference on Software Maintenance and Reengineering}, 
title={ACRE: An Automated Aspect Creator for Testing C++ Applications}, 
year={2013}, 
volume={}, 
number={}, 
pages={121-130}, 
abstract={We present ACRE, an Automated aspect creator, to use aspect-oriented programming (AOP) to perform memory, invariant and interferences testing for software programs written in C++. ACRE allows developers without knowledge in AOP to use aspects to test their programs without modifying the behavior of their source code. ACRE uses a domain-specific language (DSL), which statements testers insert into the source code like comments to describe the aspects to be used. The presence of DSL statements in the code does not modify the program's compilation and behavior. ACRE parses the DSL statements and automatically generates appropriate aspects that are then weaved into the source code to identify bugs due to memory leaks, incorrect algorithm implementation, or interference among threads. Thanks to the use of aspects and ACRE, testers can add or remove tests easily. Using an aspect generated by ACRE, we find a memory leak in a complex C++ software program, NOMAD, used in both industry and research. We also verify a crucial mathematical point of the algorithm behind NOMAD and collect data to find possible interference bugs, in NOMAD.}, 
keywords={aspect-oriented programming;C++ language;program compilers;program debugging;program testing;ACRE;automated aspect creator;C++ application testing;aspect-oriented programming;AOP;memory testing;invariant testing;interferences testing;software program;domain-specific language;statements tester;source code;DSL statement;program compilation;bug identification;memory leak;NOMAD;Testing;DSL;Interference;Computer bugs;Radiation detectors;Java;Weaving;AOP;C++;NOMAD;interference bug pattern;memory testing;invariant testing}, 
doi={10.1109/CSMR.2013.22}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7323106, 
author={K. Beckmann}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Integrating existing proprietary system models into a model-driven test process for an industrial automation scenario}, 
year={2015}, 
volume={}, 
number={}, 
pages={255-262}, 
abstract={The introduction of modern model-driven software development methodologies into the industrial practise still proves to be a challenge. Especially small or medium-sized enterprises (SMEs) need an incremental and continuous modernisation process, which incorporates existing projects, is customised and cost-effective. Particularly, suitable solutions for model-based or -driven testing with test automation to increase the efficiency are in demand. This paper presents an approach for integrating existing proprietary system models of an SME partner for describing industrial automation processes into a model-driven test process, utilising a domain-specific language for the test specification. The test objectives focuses on the correct implementation of the communication and synchronisation of distributed state machines. The presented approach is integrated into a test framework, which is based on the Eclipse Modelling Framework (EMF) and the Eclipse Test and Performance Tools Platform Project (TPTP) framework. To separate the possibly changeable system and DSL-specific models from the implementation of the test framework, a stable and more generic test meta model was defined.}, 
keywords={factory automation;program testing;small-to-medium enterprises;software engineering;proprietary system model;model-driven test process;industrial automation;modern model-driven software development;small or medium-sized enterprises;SME;domain-specific language;test specification;eclipse modelling framework;EMF;eclipse test and performance tools platform project;generic test meta model;Unified modeling language;Adaptation models;DSL;Object oriented modeling;Biological system modeling;Software;Automation;MDSD;DSL;Metamodelling;Testing;MDT;Model-driven Testing}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7102598, 
author={E. Alegroth and G. Bache and E. Bache}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={On the Industrial Applicability of TextTest: An Empirical Case Study}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Software systems are becoming more complex, not least in their Graphical User Interfaces (GUIs), which presents challenges for existing testing practices. Pressure to reduce time to market leaves less time for manual testing and increases the importance of test automation. Previous research has identified several generations of automated GUI-based test approaches with different cost-benefit tradeoffs. Whilst test automation provides fast quality feedback it can be associated with high costs and inability to identify defects not explicitly anticipated by the test designer. TextTest is a capture-replay tool for GUI-based testing with a novel approach that overcomes several of the challenges experienced with previous approaches. Firstly the tool supports Approval Testing, an approach where ASCII-art representations of the GUI's visual state are used to verify correct application behavior at the system level. Secondly it records and replays test scripts in a user defined domain specific language (DSL) that is readable by all stakeholders. In this paper we present a three phase industrial case study that aims to identify TextTest's applicability in industrial practice. The paper reports that the tool is associated with (1) low script development costs due to recording functionality, (2) low maintenance costs, on average 7 minutes per test case, (3) better defect finding ability than manual system testing, (4) high test case execution performance (In this case 500 test cases in 20 minutes), (5) high script readability due to DSL defined scripts, and (6) test suites that are robust to change (In this case 93 percent per iteration). However, the tool requires a higher degree of technical skill for customization work, test maintainers need skills in designing regular expressions and the tool's applicability is currently restricted to Java and Python based applications.}, 
keywords={graphical user interfaces;Java;program testing;software tools;TextTest;software systems;graphical user interfaces;test automation;automated GUI-based testing;capture-replay tool;approval testing;ASCII-art representations;user defined domain specific language;test case execution performance;DSL defined scripts;Java based applications;Python based applications;Graphical user interfaces;Testing;Maintenance engineering;DSL;Companies;Manuals;Data collection}, 
doi={10.1109/ICST.2015.7102598}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{4547343, 
author={T. Syed and S. R. Das and S. N. Biswas and E. M. Petriu}, 
booktitle={2008 IEEE Instrumentation and Measurement Technology Conference}, 
title={Developing Automated Test System for ADSL Equipment}, 
year={2008}, 
volume={}, 
number={}, 
pages={1833-1838}, 
abstract={The requirement for an automated test system has immensely increased due to the realization that manual testing is associated with additional resources and staffing constraints. In order to achieve a competitive edge, reduced development cost, timely product delivery, and product quality are mandatory in today's organization. Manual testing requires skilled operators that increase cost, time, and product delivery. The low cost computer based automated system helps to get an edge by fulfilling these organizational demands. In this paper, an automated testing system has been developed to support functional testing of Nortel Network's modem system (1-Meg SUT). The modem is an inherently complex asymmetric digital subscriber line (ADSL) product and its testing is far more complex than just verification of process faults. The complexity of ADSL system renders automated test system an important and imperative part of ADSL testing. The subject paper demonstrates the indispensable need of automated test system for ADSL testing and its advantages in providing a competitive edge for the organization.}, 
keywords={automatic testing;digital subscriber lines;modems;automated test system;ADSL equipment;product quality;timely product delivery;manual testing;functional testing;Nortel Network modem system;asymmetric digital subscriber line product;process faults;Automatic testing;System testing;Automation;Modems;Costs;DSL;Information technology;Internet;Graphical user interfaces;Software testing;Asymmetric digital subscriber line (ADSL);automated testing;hardware platform;software platform}, 
doi={10.1109/IMTC.2008.4547343}, 
ISSN={1091-5281}, 
month={May},}
@ARTICLE{8306153, 
author={M. Bures and K. Frajtak and B. S. Ahmed}, 
journal={IEEE Transactions on Reliability}, 
title={Tapir: Automation Support of Exploratory Testing Using Model Reconstruction of the System Under Test}, 
year={2018}, 
volume={67}, 
number={2}, 
pages={557-580}, 
abstract={For a considerable number of software projects, the creation of effective test cases is hindered by design documentation that is either lacking, incomplete, or obsolete. The exploratory testing approach can serve as a sound method in such situations. However, the efficiency of this testing approach strongly depends on the method, the documentation of explored parts of a system, the organization and distribution of work among individual testers on a team, and the minimization of potential (very probable) duplicities in performed tests. In this paper, we present a framework for replacing and automating a portion of these tasks. A screen-flow-based model of the tested system is incrementally reconstructed during the exploratory testing process by tracking testers' activities. With additional metadata, the model serves for an automated navigation process for a tester. Compared with the exploratory testing approach, which is manually performed in two case studies, the proposed framework allows the testers to explore a greater extent of the tested system and enables greater detection of the defects present in the system. The results show that the time efficiency of the testing process improved with the framework support. This efficiency can be increased by team-based navigational strategies that are implemented within the proposed framework, which is documented by another case study presented in this paper.}, 
keywords={program testing;automated navigation process;exploratory testing approach;model reconstruction;sound method;software projects;Testing;Navigation;Lead;Documentation;Browsers;Web pages;Automation;Functional testing;generation of test cases from model;model reengineering;model-based testing (MBT);system under test (SUT) model;web applications testing}, 
doi={10.1109/TR.2018.2799957}, 
ISSN={0018-9529}, 
month={June},}
@INPROCEEDINGS{8369563, 
author={T. Glock and B. Sillman and M. Kobold and S. Rebmann and E. Sax}, 
booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
title={Model-based validation and testing of industry 4.0 plants}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={One of the major technical aspects of Industry 4.0 (I4.0) is the decentralized task and function distribution based on a service-oriented approach. This leads to new challenges by using the existing technologies and planning methods of I4.0 industrial plants. The validation of such dynamic service-oriented architectures requires new validation methods to ensure the correct functionality of the I4.0 plant. The interactions of distributed functions and the integration of functions on several devices play an important role in planning, configuration, and validation of such plants. This work presents a new approach of a continuous model-based validation and testing method of I4.0 plant service-oriented architectures. The method allows to reuse existing functional tests of services and to adjust them with the help of the model-based information of a plant architecture model.}, 
keywords={industrial plants;production engineering computing;production planning;service-oriented architecture;distributed functions;testing method;plant architecture model;planning methods;dynamic service-oriented architectures;continuous model-based validation;industry 4.0 plants;Tools;Unified modeling language;Testing;Computer architecture;Sensors;Industries;Planning;Functional Testing of plants;industry 4.0;service-oriented;distributed systems;reuse of tests;validation of I4.0 plant systems}, 
doi={10.1109/SYSCON.2018.8369563}, 
ISSN={2472-9647}, 
month={April},}
@INPROCEEDINGS{4839232, 
author={J. Daz and A. Yage and J. Garbajosa}, 
booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems}, 
title={A Systematic Process for Implementing Gateways for Test Tools}, 
year={2009}, 
volume={}, 
number={}, 
pages={58-66}, 
abstract={Test automation is facing a new challenge because tools, as well as having to provide conventional test functionalities, must be capable to interact with ever more heterogeneous complex systems under test (SUT). The number of existing software interfaces to access these systems is also a growing number. The problem cannot be analyzed only from a technical or engineering perspective; the economic perspective is as important. This paper presents a process to systematically implement gateways which support the communication between test tools and SUTs with a reduced cost. The proposed solution does not preclude any interface protocol at the SUT side. This process is supported using a generic architecture of a gateway defined on top of OSGi. Any test tool can communicate with the gateway through a unique defined interface. To communicate the gateway and the SUT, basically, the driver corresponding to the SUT software interface has to be loaded.}, 
keywords={computer testing;internetworking;network servers;software architecture;software tools;systematic process;gateways;systems under test;cost reduction;software interface protocol;OSGi;System testing;Automatic testing;Costs;Service oriented architecture;Home automation;Internet;Radiofrequency identification;Intelligent sensors;Software testing;Web services;test automation;gateway;complex systems testing;acceptance testing tools;OSGI;TOPEN}, 
doi={10.1109/ECBS.2009.40}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7980397, 
author={A. Contan and C. Dehelean and L. Miclea}, 
booktitle={2017 14th International Conference on Engineering of Modern Electric Systems (EMES)}, 
title={Applying coding systems in the process of testing software applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={127-131}, 
abstract={The challenges met during the software projects fall into any number of categories. The development and the technical solutions bring about technical challenges, but the situations one is confronted with, may also be sociological, psychological or managerial in nature. Without any knowledge in the field of social sciences, the programmers, testers and managers might interpret the social aspects of the project improperly, and such interpretations lead to the inability to fully understand the problem and, ultimately, to inefficiency in the decision-making process. Furthermore, solid knowledge of theories in the area of the social sciences is required for a better understanding of both the context in which the application runs and of the final users who will use the developed project. The understanding of and the involvement in a software acceptance testing (SAT) project, requires the combination of multiple theories and principles from different disciplines.}, 
keywords={program testing;software management;source code (software);coding systems;software application testing;software project social aspects;decision-making process;software acceptance testing project;SAT project;Encoding;Testing;Software quality;Software engineering;Computational modeling;Psychology;coding models;software testing;software testing;social science}, 
doi={10.1109/EMES.2017.7980397}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7321521, 
author={M. Schuts and J. Hooman}, 
booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
title={Using Domain Specific Languages to improve the development of a power control unit}, 
year={2015}, 
volume={}, 
number={}, 
pages={781-788}, 
abstract={To improve the design of a power control unit at Philips, two Domain Specific Languages (DSLs) have been used. The first DSL provides a concise and readable notation for the essential state transitions. It is used to generate both configuration files and analysis models. In addition, we also generate instances of a second DSL which represents test traces. This second DSL is used to generate test cases for the power control unit. The use of DSLs not only improved productivity, but also the quality of the configuration files and the test set.}, 
keywords={medical computing;domain specific language;power control unit;Philips;DSL;DSL;Power control;Hardware;Generators;X-ray imaging;Software;Voltage control}, 
doi={10.15439/2015F46}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7809817, 
author={D. Ratiu and M. Voelter}, 
booktitle={2016 IEEE/ACM 11th International Workshop in Automation of Software Test (AST)}, 
title={Automated Testing of DSL Implementations - Experiences from Building mbeddr}, 
year={2016}, 
volume={}, 
number={}, 
pages={15-21}, 
abstract={Domain specific languages promise to improve productivity and quality of software by providing problem-adequate abstractions to developers. Projectional language workbenches like JetBrains MPS allow the definition of modular and extensible domain specific languages, generators and development environments. While recent advances in language engineering have enabled the definition of DSLs and tooling in a modular and cost-effective manner, the quality assurance of their implementation is still challenging. In this paper we present our work on testing the implementation of domain specific languages and associated tools, and discuss different approaches to increase the automation of language testing. We illustrate this based on MPS and our experience with testing mbeddr, a set of domain specific languages and tools on top of C tailored to embedded software development.}, 
keywords={embedded systems;program testing;software quality;specification languages;automated testing;DSL;building mbeddr;domain specific languages;productivity;software quality;problem-adequate abstractions;projectional language workbench;JetBrains MPS;language testing;embedded software development;Testing;DSL;Generators;C languages;Software;Semantics;Automation;domain specific languages;quality assurance;automated testing}, 
doi={10.1109/AST.2016.011}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1609829, 
author={Prashant Gandhi and N. C. Haugen and M. Hill and R. Watt}, 
booktitle={Agile Development Conference (ADC'05)}, 
title={Creating a living specification using FIT documents}, 
year={2005}, 
volume={}, 
number={}, 
pages={253-258}, 
abstract={Using FIT for automated acceptance testing supports a process in which developers and customers collaborate on a single executable specification for each story, i.e. the FIT documents. By collaborating closely on the FIT documents, the developers and customers reach a shared understanding of the domain and develop the ubiquitous language of the application. Our experience with this process was ultimately successful but not completely pain free. In this experience report we highlight the benefits and pitfalls and share techniques for achieving successful developer and customer collaboration in specifying executable FIT documents.}, 
keywords={software development management;formal specification;program testing;formal specification;FIT documents;automated acceptance testing;executable specification;successful developer-customer collaboration;Collaboration;Automatic testing;Writing;Pain;Fixtures;Collaborative tools;Encoding;System testing;Automation}, 
doi={10.1109/ADC.2005.19}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{1226344, 
author={N. Caouras and M. Freda and F. Monfet and V. S. Aldea and O. Naeem and Tho Le-Ngoc and B. Champagne}, 
booktitle={CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436)}, 
title={Performance evaluation platform for xDSL deployment in a complex multi-segment environment}, 
year={2003}, 
volume={1}, 
number={}, 
pages={61-64 vol.1}, 
abstract={This paper presents a highly flexible simulation platform catering to the easy and rapid evaluation of existing and future digital subscriber line (DSL) deployments as well as DSL modem performance prediction using practical modem implementations in a complex multi-segment environment. The paper outlines the methodology employed to architect and develop the core software, followed by a description of the performance prediction hooks for a variety of current and future DSL modem technologies. The graphical user interface (GUI) abstracting the core software for the user is described in terms of the various configuration options and the quick and easy graphical design of typical and complex deployment scenarios. The proposed simulator's calculations, notably theoretical SNR margin, maximum theoretical capacity and reach, plus performance evaluation using user-designed modem models, are also outlined. To support the accuracy of the new simulator, results for some example scenarios are presented and compared against other available simulators.}, 
keywords={performance evaluation;digital subscriber lines;modems;graphical user interfaces;telecommunication computing;digital simulation;performance evaluation platform;xDSL deployment;complex multisegment environment;digital subscriber line deployments;DSL modem performance prediction;core software;performance prediction hooks;DSL modem technologies;graphical user interface;configuration options;SNR margin;maximum theoretical capacity;user-designed modem models;twisted-pair simulator;DSL;Crosstalk;Modems;Communication cables;Background noise;Electromagnetic coupling;Copper;Telephony;Frequency;Colored noise}, 
doi={10.1109/CCECE.2003.1226344}, 
ISSN={0840-7789}, 
month={May},}
@INPROCEEDINGS{4626839, 
author={B. Magro and J. Garbajosa and J. Prez}, 
booktitle={2008 12th International Software Product Line Conference}, 
title={A Software Product Line Definition for Validation Environments}, 
year={2008}, 
volume={}, 
number={}, 
pages={45-54}, 
abstract={Functional requirements must be tested to check if the system executes as the end user expects. Validation environments must be able to test multiple kinds of applications that belong to different domains and technologies. Since this wide validation spectrum is very difficult to cope with, validation environments are usually specialized in domains, programming languages, technologies, etc. However, it is possible to identify that the validation processes for different systems share a set of commonalities and variability points. This is a perfect framework to apply the software product line approach to develop domain specific validation environments for testing specific products. In this paper we present our experience of applying software product lines to support the variability of validation environments. We illustrate our product-line experience of developing two domain-specific validation environments for two different case studies: digital TV and slots machines.}, 
keywords={product development;program testing;program verification;software reusability;software product line definition;functional requirements;validation environments;specific products testing;Graphical user interfaces;Software;Engines;Testing;Computer architecture;Computer languages;Databases;tools;CASE;acceptance testing;software product lines;validation environments;validation tools;testing tools;software engineering environments;software development environments}, 
doi={10.1109/SPLC.2008.35}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{786779, 
author={G. Nelson}, 
booktitle={IEEE ATM Workshop '99 Proceedings (Cat. No. 99TH8462)}, 
title={Testing techniques for next-generation IP networks}, 
year={1999}, 
volume={}, 
number={}, 
pages={63-68}, 
abstract={In this paper, we look at some examples of how "IP meets ATM" and then discuss some of the recent advances in IP standards. For the remainder of the paper, we examine testing techniques used in three different scenarios: functional testing of a layer 2/layer 3 switching device; class of service (CoS) contract verification in an IP network; interworking testing of an IP/ATM access device.}, 
keywords={Internet;asynchronous transfer mode;telecommunication equipment testing;quality of service;telecommunication standards;protocols;formal verification;local area networks;next-generation IP networks;ATM;IP standards;functional testing;layer 2/layer 3 switching device;class of service;CoS;contract verification;interworking;access device;LAN;Testing;Next generation networking;IP networks;Asynchronous transfer mode;Multiprotocol label switching;Routing;Telecommunication traffic;Protocols;Packet switching;Proposals}, 
doi={10.1109/ATM.1999.786779}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7999652, 
author={J. L. de Moura and A. S. Charo and J. C. D. Lima and B. de Oliveira Stein}, 
booktitle={2017 17th International Conference on Computational Science and Its Applications (ICCSA)}, 
title={Test case generation from BPMN models for automated testing of Web-based BPM applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={This article proposes an approach to generate test cases from BPMN models, for automated testing of Web applications implemented with the support of BPM suites. The work is primarily focused on functional testing and has the following objectives: (i) identify execution paths from the flow analysis in the BPMN model and (ii) generate the initial code of test scripts to be run on a given Web application testing tool. Throughout the article, we describe the design and implementation of a solution to achieve these goals, targeting automated tests using Selenium and Cucumber as tools. The approach was applied to processes from a public repository and was able to generate test scenarios from different BPMN models.}, 
keywords={Internet;program testing;test case generation;BPMN models;Web-based BPM applications;automated testing;functional testing;Web application testing tool;Selenium;Cucumber;Testing;Tools;Logic gates;Selenium;Process control;XML;Monitoring;Business Process Management;BPMN;automatic software testing;test case generation}, 
doi={10.1109/ICCSA.2017.7999652}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6569742, 
author={D. Di Nardo and N. Alshahwan and L. Briand and Y. Labiche}, 
booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
title={Coverage-Based Test Case Prioritisation: An Industrial Case Study}, 
year={2013}, 
volume={}, 
number={}, 
pages={302-311}, 
abstract={This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.}, 
keywords={program testing;regression analysis;software fault tolerance;coverage-based test case prioritisation;industrial case study;real world system;regression fault;coverage criteria;fault detection rate;Testing;Fault detection;Measurement;Software;Computer aided software engineering;Minimization;Data collection;regression testing;industrial case study;test case prioritisation}, 
doi={10.1109/ICST.2013.27}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{5954390, 
author={C. Efkemann and J. Peleska}, 
booktitle={2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops}, 
title={Model-Based Testing for the Second Generation of Integrated Modular Avionics}, 
year={2011}, 
volume={}, 
number={}, 
pages={55-62}, 
abstract={In this paper the authors present the current research and development activities regarding automated testing of Integrated Modular Avionics controllers in the European research project SCARLETT. The authors describe the goals of the SCARLETT project and explain its background of Integrated Modular Avionics. Furthermore, they explain different levels of testing of components required for certification. A domain-specific modelling language designed for the IMA platform is presented. This language is used to create models from which tests of different levels can be generated automatically. The authors expect significant improvements in terms of effort to create and maintain test procedures compared to conventional test creation.}, 
keywords={aerospace computing;avionics;program testing;simulation languages;model-based testing;integrated modular avionics;SCARLETT research project;domain-specific modelling language;Aerospace electronics;Testing;Generators;Aircraft;Europe;Random access memory;Concrete;IMA;SCARLETT;TTCN-3;avionics;domain-specific modelling;model-based testing}, 
doi={10.1109/ICSTW.2011.72}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7962331, 
author={C. Chiw and G. Kindlmann and J. Reppy}, 
booktitle={2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST)}, 
title={DATm: Diderot's Automated Testing Model}, 
year={2017}, 
volume={}, 
number={}, 
pages={45-51}, 
abstract={Diderot is a parallel domain-specific language forthe analysis and visualization of multidimensional scientific images, such as those produced by CT and MRI scanners. Diderot is designed to support algorithms that are based on differential tensor calculus and produces a higher-order mathematical model which allows direct manipulation of tensor fields. One of the main challenges of the Diderot implementation is bridging this semantic gap by effectively translating high-level mathematical notation of tensor calculus into efficient low-level code in the target language. A key question for a high-level language, such as Diderot, is how do we know that the implementation is correct. We have previously presented and defended a core set of rewriting rules, but the full translation from source to executable requires much more work. In this paper, we present DATm, Diderot's automated testing model to check the correctness of the core operations in the programming language. DATm can automatically create test programs, and predict what the outcome should be. We measure the accuracy of the computations written in the Diderot language, based on how accurately the output of the program represents the mathematical equivalent of the computations. This paper describes a model for testing a high-level language based on correctness. It introduces the pipeline for DATm, a tool that can automatically create and test tens of thousands of Diderot test programs and that has found numerous bugs. We make a case for the necessity of extensive testing by describing bugs that are deep in the compiler, and only could be found with a unique application of operations. Lastly, we demonstrate that the model can be used to create other types of tests by visual verification.}, 
keywords={parallel languages;program compilers;program debugging;program testing;program verification;rewriting systems;specification languages;tensors;DATm;Diderot automated testing model;parallel domain-specific language;multidimensional scientific image analysis;multidimensional scientific image visualization;differential tensor calculus;higher-order mathematical model;high-level language;rewriting rules;programming language;Diderot test programs;bugs;compiler;visual verification;Testing;Tensile stress;Kernel;Shape;Computational modeling;Generators;Calculus;domain specific testing;Diderot;DSL;tensor calc;visual verificaiton}, 
doi={10.1109/AST.2017.5}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7237259, 
author={E. Lavrishcheva}, 
booktitle={2015 Science and Information Conference (SAI)}, 
title={Ontological approach to the formal specification of the standard life cycle}, 
year={2015}, 
volume={}, 
number={}, 
pages={965-972}, 
abstract={Approach is offered to the formal specification of Standard Life Cycle (LC) of the program systems (PS) by the ontology facilities with purpose automation and generation of the variants LC for making the appropriate kinds process for development different PS. Ontological approach to presentation LC model of the standard ISO/IEC 12207-2007 is included the specification of general, organizational and support processes. These processes are presented in the subject-oriented DSL, which than transformed to XML for realization. One of the processes, the testing process is given in terms of Protg systems. An eventual result of this system Protg got generally at accepted to the XML, suitable for implementation tasks testing PS on computer.}, 
keywords={formal specification;ontologies (artificial intelligence);XML;ontological approach;standard life cycle formal specification;program systems;ISO/IEC 12207-2007;subject-oriented DSL;XML;Protg systems;DSL;Ontologies;XML;ISO Standards;IEC Standards;Testing;ontology;the life cycle standard;model of life cycle;processes;actions;task;testing;DSL;description;Protg;XML}, 
doi={10.1109/SAI.2015.7237259}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8456399, 
author={J. G. Quenum and S. Aknine}, 
booktitle={2018 IEEE International Conference on Services Computing (SCC)}, 
title={Towards Executable Specifications for Microservices}, 
year={2018}, 
volume={}, 
number={}, 
pages={41-48}, 
abstract={This paper presents an empirical approach for microservice automated testing. With the rise of the agile methodology, automated testing has gained momentum in software development, including using microservices as an architectural style. However, the tests are not always related to the core specifications of the system being developed. In this paper, we discuss an approach to derive the tests, especially the acceptance tests, from the specifications of the systems. To avoid any ambiguity in the specifications, we focus on the formal specifications of the system. To this end, we introduce intelligent agents as a conceptual unit to encapsulate the formal specifications of services. Indeed, a comparison of microservice tenets and the general characterization of agents reveals that both can be thought of as autonomous software entities, driven by goals and evolving within a distributed environment and communicating with one another. Using a real-world application we show how agent formal specifications can be linked to microservice automated testing.}, 
keywords={formal specification;program testing;software architecture;software prototyping;empirical approach;microservice automated testing;agile methodology;software development;core specifications;acceptance tests;microservice tenets;agent formal specifications;executable specifications;autonomous software entities;Testing;Service-oriented architecture;Intelligent agents;Business;Computer architecture;Curriculum development;Services;Testing;Intelligent Agents;Formal Specifications}, 
doi={10.1109/SCC.2018.00013}, 
ISSN={2474-2473}, 
month={July},}
@INPROCEEDINGS{7302512, 
author={S. Vinogradov and A. Ozhigin and D. Ratiu}, 
booktitle={2015 IEEE International Symposium on Systems Engineering (ISSE)}, 
title={Modern model-based development approach for embedded systems practical experience}, 
year={2015}, 
volume={}, 
number={}, 
pages={56-59}, 
abstract={Control functionality of modern rail vehicles is getting more and more complex. It contains several modules such as the traction control unit or the central control unit, as well as input and output stations, such as driver's cab terminals and process I/Os. A plethora of devices are connected to the vehicle and train bus and are able to communicate. The functions of the vehicle control and traction systems are configured by using function blocks from which loadable programs are generated. The languages used to program the control units are well established in the field. However, one-size-fits-all approach cannot adequately address the increased complexity of the software in modern trains. In this paper we describe our preliminary experience with using the multi-paradigm modeling tool mbeddr in the railway domain. The following aspects have been in focus during the work: (a) matching the application requirements and domain specific language used for implementation; (b) integration of model-based approach into traditional product lifecycle; (c) reengineering existing functionality using modeling and code generation capabilities of mbeddr. The system example we chose was the application logic of automated train driving system implemented in development environment of Siemens process automation framework.}, 
keywords={embedded systems;rail traffic control;traction;model-based development approach;embedded systems;rail vehicle control functionality;traction control unit;central control unit;input stations;output stations;driver cab terminals;I/O process;vehicle bus;train bus;traction systems;function blocks;loadable programs;multiparadigm modeling tool;mbeddr tool;railway domain;application requirements;domain specific language;model-based approach;product lifecycle;reengineering;modeling capabilities;code generation capabilities;automated train driving system;application logic;Siemens process automation framework;Software;Mathematical model;Complexity theory;Control systems;Domain specific languages;Formal verification;Rail transportation;model based development;language engineering}, 
doi={10.1109/SysEng.2015.7302512}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7000030, 
author={M. T. C. F. Albuquerque and G. L. Ramalho and V. Corruble and A. L. M. Santos and F. Freitas}, 
booktitle={2014 Brazilian Symposium on Computer Games and Digital Entertainment}, 
title={Helping Developers to Look Deeper inside Game Sessions}, 
year={2014}, 
volume={}, 
number={}, 
pages={31-40}, 
abstract={Game design and development activities are increasingly relying on the analysis of gamer's behavior and preferences data. Various tools are available to the developers to track and analyze general data concerning acquisition, retention and monetization aspects of game commercialization. This is good enough to give hints on where problems are, but not to enable a precise diagnosis, which demands fine-grained data. For this kind of data, there is not enough support or guidance to decide which data to capture, to write the code to capture it, to choose the best representation of it and to allow an adequate retrieval and presentation of it. This paper introduces GameGuts (GG), a framework devoted to give further assistance to developers in choosing, representing, accessing and presenting game sessions fine-grained data. As a case study, GG recorded sessions of a game platform with over a hundred thousand users. The logs were analyzed using a Visual Domain Specific Language (as a query language) and an ensemble of rules (as a compliance test). The results are encouraging, since we could - among other results - find bugs and catch cheaters, as well as spot design flaws.}, 
keywords={computer games;data analysis;query languages;visual languages;game sessions;game design;game development activities;gamer behavior analysis;preferences data;general data tracking;general data analysis;game commercialization;GameGuts;game platform;visual domain specific language;query language;rule ensemble;bugs;catch cheaters;design flaws;Games;Ontologies;Visualization;Servers;DSL;Database languages;Measurement;game analytics;knowledge representation;game data mining}, 
doi={10.1109/SBGAMES.2014.28}, 
ISSN={2159-6662}, 
month={Nov},}
@INPROCEEDINGS{5967120, 
author={G. Piho and J. Tepandi and M. Roost and M. Parman and V. Puusep}, 
booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
title={From archetypes based domain model via requirements to software: Exemplified by LIMS Software Factory}, 
year={2011}, 
volume={}, 
number={}, 
pages={570-575}, 
abstract={The Archetypes Based Development (ABD) proceeds from archetypes based domain model via requirements to software. We give an overview of ABD and exemplify its application on Laboratory Information Management Systems (LIMS) Software Factory development. ABD is guided by Zachman Framework and utilizes software engineering triptych together with archetypes and archetype patterns. For modelling of domains the Test Driven Modelling (TDM) techniques are used. TDM utilizes test driven development techniques in domain engineering. The resultant domain models serve as the Domain Specific Language for prescribing requirements. Implementation and testing of the LIMS Software Factory proves feasibility of archetypes based techniques in real life systems. ABD helps developers to better understand business requirements, to design cost effective enterprise applications through systematic reuse of archetypal components, as well as to validate and verify requirements resulting in higher quality software.}, 
keywords={business data processing;information management;laboratories;software quality;specification languages;archetypes based domain model development;LIMS software factory;software requirements;laboratory information management systems software factory development;Zachman Framework;software engineering triptych;test driven modelling techniques;domain specific language;business requirements;Software;Production facilities;Laboratories;Capability maturity model;DSL;Organizations;Archetypes;archetype patterns;domain analysis;domain model;domain modelling;software engineering;software factory;laboratory information management system (LIMS);laboratory domain model}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5630332, 
author={M. K. Agaram and B. Laird}, 
booktitle={2010 14th IEEE International Enterprise Distributed Object Computing Conference}, 
title={A Componentized Architecture for Externalized Business Rules}, 
year={2010}, 
volume={}, 
number={}, 
pages={175-183}, 
abstract={Delta Dental is the leading provider of Dental benefits in the Midwest, serving nearly 5.1 million members. Delta Dental of Michigan uses Business Rules to articulate complex claims adjudication rules, from legacy COBOL programs. These rules are entirely authored, managed, tested and governed by Subject Matter Experts with no programming background. The Business Rules Architecture incorporates a componentized approach to authoring and organizing Business Rules which promotes extensive rule reuse at several levels through layered components. To mitigate complexity, the framework provides features for sandbox testing and regression testing of the rules by the Business Users. It also provides a rich reporting and trace ability back to the Policy charter. As a result of this framework a legacy adjudication COBOL program of 50,000+ lines were reduced to 30+ reusable rules. Nearly 94 % of the claims processed are automatically adjudicated by the rules engine. The purpose of this paper is to describe in detail the insights gained from the architecture and the measurable productivity gains accomplished.}, 
keywords={COBOL;dentistry;medical administrative data processing;componentized architecture;externalized business rules;Delta Dental;complex claims adjudication rules;legacy COBOL programs;business rules architecture;sandbox testing;regression testing;Business;Dentistry;Vocabulary;Testing;Production;Runtime;Humans;Architecture;Business Rules;BRIDE;Component}, 
doi={10.1109/EDOC.2010.26}, 
ISSN={1541-7719}, 
month={Oct},}
@INPROCEEDINGS{7102627, 
author={W. Krenn and R. Schlick and S. Tiran and B. Aichernig and E. Jobstl and H. Brandl}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={MoMut::UML Model-Based Mutation Testing for UML}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Model-based mutation testing (MBMT) is a promising testing methodology that relies on a model of the system under test (SUT) to create test cases. Hence, MBMT is a so-called black-box testing approach. It also is fault based, as it creates test cases that are guaranteed to reveal certain faults: after inserting a fault into the model of the SUT, it looks for a test case revealing this fault. This turns MBMT into one of the most powerful and versatile test case generation approaches available as its tests are able to demonstrate the absence of certain faults, can achieve both, control-flow and data-flow coverage of model elements, and also may include information about the behaviour in the failure case. The latter becomes handy whenever the test execution framework is bound in the number of observations it can make and - as a consequence - has to restrict them. However, this versatility comes at a price: MBMT is computationally expensive. The tool MoMuT::UML (https://www.momut.org) is the result of a multi-year research effort to bring MBMT from the academic drawing board to industrial use. In this paper we present the current stable version, share the lessons learnt when applying two generations of MoMuT::UML in an industrial setting, and give an outlook on the upcoming, third,generation.}, 
keywords={data flow computing;program testing;Unified Modeling Language;MoMut::UML model;model-based mutation testing;MBMT;system under test;SUT;test cases;black-box testing approach;test case generation;control-flow coverage;data-flow coverage;test execution framework;Unified modeling language;Testing;Computational modeling;Integrated circuit modeling;Circuit faults;Object oriented modeling;Semantics}, 
doi={10.1109/ICST.2015.7102627}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{7880820, 
author={X. Guo and R. G. Dutta and P. Mishra and Y. Jin}, 
booktitle={2016 17th International Workshop on Microprocessor and SOC Test and Verification (MTV)}, 
title={Automatic RTL-to-Formal Code Converter for IP Security Formal Verification}, 
year={2016}, 
volume={}, 
number={}, 
pages={35-38}, 
abstract={The wide usage of hardware intellectual property (IP) cores from untrusted vendors has raised security concerns in the integrated circuit (IC) industry. Existing testing methods are designed to validate the functionality of the hardware IP cores. These methods often fall short in detecting unspecified (often malicious) logic. Formal methods like Proof-Carrying Hardware (PCH), on the other hand, can help eliminate hardware Trojans and/or design backdoors by formally proving security properties on soft IP cores despite the high proof development cost. One of the causes to the high cost is the manual conversion of the hardware design from RTL code to a domain-specific language prior to verification. To mitigate this issue and to lower the overall cost of PCH framework, we propose an automatic code converter for translating VHDL to Formal-HDL, a domain specific language for representing hardware designs in Coq language. Our code converter provides support to wide variety of hardware designs. Towards the goal of speeding up the verification procedure in our PCH framework, the code converter is the important first step. The applicability of the tool is demonstrated by converting soft IP cores of AES to its Coq equivalent code.}, 
keywords={codes;convertors;formal verification;hardware description languages;logic circuits;microprocessor chips;security;Coq equivalent code;AES;Coq language;formal-HDL;VHDL;domain-specific language;soft IP cores;security properties;hardware Trojans;PCH;proof-carrying hardware;hardware IP cores;integrated circuit industry;intellectual property;IP security formal verification;automatic RTL-to-formal code converter;Security;Hardware;IP networks;Hardware design languages;Trojan horses;Syntactics;Hardware Security;Hardware IP Protection;Formal Verification}, 
doi={10.1109/MTV.2016.23}, 
ISSN={2332-5674}, 
month={Dec},}
@INPROCEEDINGS{7958601, 
author={T. Petsios and A. Tang and S. Stolfo and A. D. Keromytis and S. Jana}, 
booktitle={2017 IEEE Symposium on Security and Privacy (SP)}, 
title={NEZHA: Efficient Domain-Independent Differential Testing}, 
year={2017}, 
volume={}, 
number={}, 
pages={615-632}, 
abstract={Differential testing uses similar programs as cross-referencing oracles to find semantic bugs that do not exhibit explicit erroneous behaviors like crashes or assertion failures. Unfortunately, existing differential testing tools are domain-specific and inefficient, requiring large numbers of test inputs to find a single bug. In this paper, we address these issues by designing and implementing NEZHA, an efficient input-format-agnostic differential testing framework. The key insight behind NEZHA's design is that current tools generate inputs by simply borrowing techniques designed for finding crash or memory corruption bugs in individual programs (e.g., maximizing code coverage). By contrast, NEZHA exploits the behavioral asymmetries between multiple test programs to focus on inputs that are more likely to trigger semantic bugs. We introduce the notion of d-diversity, which summarizes the observed asymmetries between the behaviors of multiple test applications. Based on d-diversity, we design two efficient domain-independent input generation mechanisms for differential testing, one gray-box and one black-box. We demonstrate that both of these input generation schemes are significantly more efficient than existing tools at finding semantic bugs in real-world, complex software. NEZHA's average rate of finding differences is 52 times and 27 times higher than that of Frankencerts and Mucerts, two popular domain-specific differential testing tools that check SSL/TLS certificate validation implementations, respectively. Moreover, performing differential testing with NEZHA results in 6 times more semantic bugs per tested input, compared to adapting state-of-the-art general-purpose fuzzers like American Fuzzy Lop (AFL) to differential testing by running them on individual test programs for input generation. NEZHA discovered 778 unique, previously unknown discrepancies across a wide variety of applications (ELF and XZ parsers, PDF viewers and SSL/TLS libraries), many of which constitute previously unknown critical security vulnerabilities. In particular, we found two critical evasion attacks against ClamAV, allowing arbitrary malicious ELF/XZ files to evade detection. The discrepancies NEZHA found in the X.509 certificate validation implementations of the tested SSL/TLS libraries range from mishandling certain types of KeyUsage extensions, to incorrect acceptance of specially crafted expired certificates, enabling man-in-the-middle attacks. All of our reported vulnerabilities have been confirmed and fixed within a week from the date of reporting.}, 
keywords={program debugging;program testing;programming language semantics;security of data;NEZHA;domain-independent differential testing;program testing;cross-referencing oracles;semantic bug finding;efficient input-format-agnostic differential testing framework;crash finding;memory corruption bug;code coverage maximization;program behavioral asymmetry;d-diversity;domain-independent input generation mechanism;gray-box testing;black-box testing;SSL-TLS certificate validation implementation;general-purpose fuzzers;American Fuzzy Lop;ELF testing;XZ parser;PDF viewer;SSL-TLS libraries;critical security vulnerabilities;critical evasion attacks;ClamAV;arbitrary malicious ELF/XZ files;X.509 certificate validation implementation;KeyUsage extension;expired certificates;man-in-the-middle attack;Testing;Computer bugs;Semantics;Tools;Software;Libraries;Security;nezha;differential testing;fuzzing}, 
doi={10.1109/SP.2017.27}, 
ISSN={2375-1207}, 
month={May},}
@INPROCEEDINGS{7101658, 
author={P. McCormick and C. Sweeney and N. Moss and D. Prichard and S. K. Gutierrez and K. Davis and J. Mohd-Yusof}, 
booktitle={2014 Fourth International Workshop on Domain-Specific Languages and High-Level Frameworks for High Performance Computing}, 
title={Exploring the Construction of a Domain-Aware Toolchain for High-Performance Computing}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={The push towards exascale computing has sparked a new set of explorations for providing new productive programming environments. While many efforts are focusing on the design and development of domain-specific languages (DSLs), few have addressed the need for providing a fully domain-aware toolchain. Without such domain awareness critical features for achieving acceptance and adoption, such as debugger support, pose a long-term risk to the overall success of the DSL approach. In this paper we explore the use of language extensions to design and implement the Scout DSL and a supporting toolchain infrastructure. We highlight how language features and the software design methodologies used within the toolchain play a significant role in providing a suitable environment for DSL development.}, 
keywords={high level languages;parallel processing;software engineering;domain-aware toolchain;high-performance computing;language extensions;Scout DSL;domain-specific languages;software design methodologies;DSL;Runtime;Computer architecture;Syntactics;Graphics processing units;Image color analysis}, 
doi={10.1109/WOLFHPC.2014.9}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7552103, 
author={C. K. Poon and T. Wong and Y. T. Yu and V. C. S. Lee and C. M. Tang}, 
booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)}, 
title={Toward More Robust Automatic Analysis of Student Program Outputs for Assessment and Learning}, 
year={2016}, 
volume={1}, 
number={}, 
pages={780-785}, 
abstract={Automated analysis and assessment of students' programs, typically implemented in automated program assessment systems (APASs), are very helpful to both students and instructors in modern day computer programming classes. The mainstream of APASs employs a black-box testing approach which compares students' program outputs with instructor-prepared outputs. A common weakness of existing APASs is their inflexibility and limited capability to deal with admissible output variants, that is, outputs produced by acceptable correct programs that differ from the instructor's. This paper proposes a more robust framework for automatically modelling and analysing student program output variations based on a novel hierarchical program output structure called HiPOS. Our framework assesses student programs by means of a set of matching rules tagged to the HiPOS, which produces a better verdict of correctness. We also demonstrate the capability of our framework by means of a pilot case study using real student programs.}, 
keywords={computer science education;educational administrative data processing;program testing;automatic analysis;learning;students program assessment;automated program assessment systems;APAS;computer programming classes;black-box testing;instructor-prepared outputs;admissible output variants;student program output variations;hierarchical program output structure;HiPOS;Robustness;Programming profession;Education;Computers;Testing;Natural language processing;automated assessment technology;computer science education;learning computer programming;program output variant;student program analysis}, 
doi={10.1109/COMPSAC.2016.208}, 
ISSN={0730-3157}, 
month={June},}
@ARTICLE{5441292, 
author={X. Amatriain and P. Arumi}, 
journal={IEEE Transactions on Software Engineering}, 
title={Frameworks Generate Domain-Specific Languages: A Case Study in the Multimedia Domain}, 
year={2011}, 
volume={37}, 
number={4}, 
pages={544-558}, 
abstract={We present an approach to software framework development that includes the generation of domain-specific languages (DSLs) and pattern languages as goals for the process. Our model is made of three workflows-framework, metamodel, and patterns-and three phases-inception, construction, and formalization. The main conclusion is that when developing a framework, we can produce with minimal overhead-almost as a side effect-a metamodel with an associated DSL and a pattern language. Both outputs will not only help the framework evolve in the right direction, but will also be valuable in themselves. In order to illustrate these ideas, we present a case study in the multimedia domain. For several years, we have been developing a multimedia framework. The process has produced a full-fledged domain-specific metamodel for the multimedia domain, with an associated DSL and a pattern language.}, 
keywords={multimedia computing;software engineering;specification languages;visual programming;domain-specific languages;multimedia domain;pattern languages;domain-specific metamodel;associated DSL;visual programming;software framework development;Domain specific languages;DSL;Unified modeling language;Vocabulary;Concrete;Software engineering;Computer aided software engineering;Natural languages;Metamodeling;Best practices;Domain-specific architectures;visual programming;life cycle;CASE.}, 
doi={10.1109/TSE.2010.48}, 
ISSN={0098-5589}, 
month={July},}
@INPROCEEDINGS{5464138, 
author={A. C. Noubissi and J. Iguchi-Cartigny and J. Lanet}, 
booktitle={2010 Fifth International Conference on Systems}, 
title={Incremental Dynamic Update for Java-Based Smart Cards}, 
year={2010}, 
volume={}, 
number={}, 
pages={110-113}, 
abstract={One of the most appealing feature for multi-application smart cards is their ability to dynamically download or delete applications once the card has been issued. Applications can be updated by deleting old versions and loading the new ones. Nevertheless, for system components, the update is sligthly more complex because the systems never stop. Indeed, for smart cards based on Java called JavaCard, the virtual machine has a life cycle similar to the card because persistent objects are preserved after the communication sessions with the reader have expired. We present in this paper, our research in dynamic system components updating of JavaCard. Our technique requires a lot of off-card and on-card mechanisms. Our approach uses control flow graph to determine change between versions, a domain specific language to represent the change for minimization of the download overhead throughout the communication link with the card.}, 
keywords={Java;smart cards;software engineering;virtual machines;incremental dynamic update;Java-based smart cards;multiapplication smart cards;JavaCard;virtual machine;off-card mechanism;on-card mechanism;Java;Smart cards;Virtual machining;Runtime;Application software;Domain specific languages;Aerodynamics;Embedded software;DSL;Operating systems;Smart Card;HotSwUp;Java Card;Dynamic update;e-passport}, 
doi={10.1109/ICONS.2010.27}, 
ISSN={}, 
month={April},}
@ARTICLE{6312844, 
author={F. T. Baker}, 
journal={IEEE Transactions on Software Engineering}, 
title={Structured programming in a production programming environment}, 
year={1975}, 
volume={SE-1}, 
number={2}, 
pages={241-252}, 
abstract={Discusses how structured programming methodology has been introduced into a large production programming organization using an integrated but flexible approach. It next analyzes the advantages and disadvantages of each component of the methodology and presents some quantitative results on its use. It concludes with recommendations based on this generally successful experience, which could be useful to other organizations interested in improving reliability and productivity.}, 
keywords={computer facilities;programming;top down programming;production programming environment;structured programming;Programming;DSL;Libraries;Organizations;Encoding;Standards;Guidelines;Chief programmer teams (CPT's);development support libraries (DSL's);structured coding;structured programming;top-down development;top-down programming}, 
doi={10.1109/TSE.1975.6312844}, 
ISSN={0098-5589}, 
month={June},}
@INPROCEEDINGS{5967121, 
author={G. Piho and J. Tepandi and M. Parman and V. Puusep and M. Roost}, 
booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
title={Test Driven domain modelling}, 
year={2011}, 
volume={}, 
number={}, 
pages={576-581}, 
abstract={To write software we have to know requirements; to know requirements we have to know domain; to know the domain we have to analyze and model one. We propose a methodology for applying Test Driven Modelling in engineering of domains, requirements and software. We will restrict ourselves here to enterprise information systems and therefore to business domains. As common for Software Factories, domain models (as well as all other models) are software artefacts, not only documentation artefacts. In our approach Test Driven Modelling utilizes Test Driven Development for domain modelling. Domain models engineered in this way are used as Domain Specific Language for specifying software requirements. The hypothesis is that such domain models can be used for validation of requirements and verification of software, lead developments towards Software Factories, and increase dependability of software.}, 
keywords={business data processing;DP industry;formal specification;information systems;program testing;program verification;software reliability;specification languages;system documentation;test driven domain modelling;domains engineering;requirements engineering;software engineering;enterprise information systems;business domains;software factory;domain models;software artefacts;documentation artefacts;test driven development;domain specific language;software requirements;software verification;software dependability;Software;Measurement units;Time division multiplexing;Analytical models;Business;Silicon;Production facilities;domain analysis and engineering;domain model and domain modelling;software engineering;software factory;software testing;test driven development;test driven modelling;verification and validation}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6890825, 
author={F. Wanderley and A. Silva and J. Araujo and D. S. Silveira}, 
booktitle={2014 IEEE 4th International Model-Driven Requirements Engineering Workshop (MoDRE)}, 
title={SnapMind: A framework to support consistency and validation of model-based requirements in agile development}, 
year={2014}, 
volume={}, 
number={}, 
pages={47-56}, 
abstract={Two fundamental principles and values of agile methods are customer satisfaction by rapid delivery of useful software and the improvement of the communication process by continuous stakeholders' involvement. But, how to deal with customers' satisfaction and find a better visualization model at the requirements level (which stakeholders can understand and be involved) in an agile development context? Also, how this visualization model enhancement can guarantee consistency between agile requirements artefacts (e.g., user stories and domain models)? Thus, to answer these questions, this paper presents the SnapMind framework. This framework aims to make the requirements modelling process more user-centered, through the definition of a visual requirements language, based on mind maps, model-driven and domain specific language techniques. Moreover, through these techniques, the SnapMind framework focuses on support for consistency between user stories and the domain models using a model animation technique called snapshots. The framework was applied to an industrial case study to investigate its feasibility.}, 
keywords={customer satisfaction;software prototyping;model-based requirements;agile methods;customer satisfaction;rapid delivery;useful software;communication process;agile development context;visualization model enhancement;SnapMind framework;domain specific language techniques;user stories;domain models;model animation technique;snapshots;Visualization;Software;Syntactics;Unified modeling language;Adaptation models;Business;Semantics;Agile Software Requirements;Model-Driven Engineering;Domain-Specific Languages;Mind Map;Snapshots}, 
doi={10.1109/MoDRE.2014.6890825}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6062146, 
author={A. Groce and K. Havelund and M. Smith}, 
booktitle={2010 ACM/IEEE 32nd International Conference on Software Engineering}, 
title={From scripts to specifications: the evolution of a flight software testing effort}, 
year={2010}, 
volume={2}, 
number={}, 
pages={129-138}, 
abstract={This paper describes the evolution of a software testing effort during a critical period for the flagship Mars Science Laboratory rover project at the Jet Propulsion Laboratory. Formal specification for post-run analysis of log files, using a domain-specific language, LogScope, replaced scripted real-time analysis. Log analysis addresses the key problems of on-the-fly approaches and cleanly separates specification and execution. Mining the test repository suggested the inadequacy of the scripted approach, and encouraged a partly engineer-driven development. LogScope development should hold insights for others facing the tight deadlines and reactionary nature of testing for critical projects. LogScope received a JPL Mariner Award for "improving productivity and quality of the MSL Flight Software" and has been discussed as an approach for other flight missions. We note LogScope features that most contributed to ease of adoption and effectiveness. LogScope is general and can be applied to any software producing logs.}, 
keywords={aerospace computing;data mining;formal specification;program diagnostics;program testing;software testing effort;flagship Mars Science Laboratory rover project;Jet Propulsion Laboratory;post-run analysis;log files;domain specific language;LogScope;scripted real-time analysis;log analysis;on-the-fly approach;test repository;engineer driven development;LogScope development;JPL Mariner Award;MSL flight software;flight missions;formal specification;Software;Telemetry;Space vehicles;Laboratories;Libraries;Semantics;Python;development practices;logs;runtime verification;space flight software;temporal logic;test infrastructure;testing}, 
doi={10.1145/1810295.1810314}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{5381641, 
author={T. Hartmann}, 
booktitle={2009 Testing: Academic and Industrial Conference - Practice and Research Techniques}, 
title={Model Based Testing of End-to-End Chains Using Domain Specific Languages}, 
year={2009}, 
volume={}, 
number={}, 
pages={82-91}, 
abstract={In this paper, the author explains a new approach of model based end-to-end chain testing using scenarios with original and simulated equipment. The first goal is to automatically derive test data and test cases from the model, which is defined by a domain specific language. Several solvers can be attached to the conversion to quickly create a wide variety of stimuli for the system(s) under test. Furthermore, the system under test can be stimulated by either original equipment - which is connected to the test bench - or the test bench can simulate equipment and create inputs for the tested systems. Any mixture of simulated and original equipment is possible and can be changed on the fly. In the end, the results from the system under test are collected. These results can then be displayed back in the model. This method is currently used and improved in the project "E-Cab" in which the author is involved. Passengers travelling by plane are in the focus of this project. Complete services and service chains - from the booking at home up to leaving the destination airport - are created and used by many systems communicating with each other. The author expects advantages from testing these end-to-end chains with this approach.}, 
keywords={automatic test pattern generation;model based testing;end-to-end chain;domain specific language;automatic test case generation;system under test;E-Cab;Domain specific languages;System testing;Automatic testing;Logic testing;Constraint theory;Logistics;Operating systems;Airports;Complex networks;Robustness;model based;end-to-end chain;testing;automatic test case generation;automatic test data generation}, 
doi={10.1109/TAICPART.2009.25}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{1158072, 
author={A. Saha and V. Mishra and P. Saxena and R. Mundhada and K. Katiyar and S. Kumar}, 
booktitle={15th Annual IEEE International ASIC/SOC Conference}, 
title={Design of broadband controller for residential gateway applications}, 
year={2002}, 
volume={}, 
number={}, 
pages={283-287}, 
abstract={As Internet technology becomes more pervasive, homes are getting connected to cable or DSL. The increasing user demands for "always on" service along with multiple connectivity for voice and data is gradually making the presence of a residential gateway in every household a reality. A residential gateway should have routing and bridging capabilities along with seamless connectivity to contemporary premise networking technologies. Integration of all these features on a single device essentially requires a rich architecture, smart design techniques and thorough verification. This paper describes the architecture and design of a broadband network controller which is a critical component of TI's voice and data centric residential gateway solution.}, 
keywords={home computing;internetworking;network servers;broadband networks;wide area networks;telecommunication network routing;data communication equipment;integrated voice/data communication;Internet;digital subscriber lines;integrated circuit design;integrated circuit testing;home networks;broadband networks;wide area networks;broadband controllers;household/domestic residential gateway applications;residential gateway routing/bridging capabilities;Internet technology;DSL;cable connection;always on Internet services;voice/data multiple connectivity;premise networking technologies seamless connectivity;smart design techniques;device architecture;verification;VoIP;VoATM;VoDSL;Internet;DSL;Broadband communication;Ethernet networks;Universal Serial Bus;Control systems;Instruments;Routing;Home automation;Transceivers}, 
doi={10.1109/ASIC.2002.1158072}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6462664, 
author={B. Combemale and X. Crgut and M. Pantel}, 
booktitle={2012 19th Asia-Pacific Software Engineering Conference}, 
title={A Design Pattern to Build Executable DSMLs and Associated V amp;V Tools}, 
year={2012}, 
volume={1}, 
number={}, 
pages={282-287}, 
abstract={Model executability is now a key concern in model-driven engineering, mainly to support early validation and verification (V&V). Some approaches allow to weave executability into metamodels, defining executable domain-specific modeling languages (DSMLs). Model validation can then be achieved by simulation and graphical animation through direct interpretation of the conforming models. Other approaches address model executability by model compilation, allowing to reuse the virtual machines or V&V tools existing in the target domain. Nevertheless, systematic methods are currently not available to help the language designer in the definition of such an execution semantics and related tools. For instance, simulators are mostly hand-crafted in a tool specific manner for each DSML. In this paper, we propose to reify the elements commonly used to support state-based execution in a DSML. We infer a design pattern (called Executable DSML pattern) providing a general reusable solution for the expression of the executability concerns in DSMLs. It favors flexibility and improves reusability in the definition of semantics-based tools for DSMLs. We illustrate how this pattern can be applied to ease the development of V&V tools.}, 
keywords={simulation languages;software reusability;virtual machines;executable DSML;V&V tools;model executability;model-driven engineering;early validation and verification;metamodels;domain-specific modeling languages;model compilation;virtual machine reuse;state-based execution;design pattern;Semantics;Unified modeling language;Runtime;Computational modeling;Abstracts;Animation;Concrete;Model Driven Engineering;Software Language Engineering;Validation & Verification}, 
doi={10.1109/APSEC.2012.79}, 
ISSN={1530-1362}, 
month={Dec},}
@ARTICLE{6784505, 
author={J. M. Vara and V. A. Bollati and . Jimnez and E. Marcos}, 
journal={IEEE Transactions on Software Engineering}, 
title={Dealing with Traceability in the MDDof Model Transformations}, 
year={2014}, 
volume={40}, 
number={6}, 
pages={555-583}, 
abstract={Traceability has always been acknowledged as a relevant topic in Software Engineering. However, keeping track of the relationships between the different assets involved in a development process is a complex and tedious task. The fact that the main assets handled in any model-driven engineering project are models and model transformations eases the task. In order to take advantage of this scenario, which has not been appropriately capitalized on by the most widely adopted model transformation languages before, this work presents MeTAGeM-Trace, a methodological and technical proposal with which to support the model-driven development of model transformations that include trace generation. The underlying idea is to start from a high-level specification of the transformation which is subsequently refined into lower-level transformation models in terms of a set of DSLs until the source code that implements the transformation can be generated. Running this transformation produces not only the corresponding target models, but also a trace model between the elements of the source and target models. As part of the proposal, an EMF-based toolkit has been developed to support the development of ATL and ETL model transformations. This toolkit has been empirically validated by conducting a set of case studies following a systematic research methodology.}, 
keywords={research and development;software engineering;source code (software);MDD;traceability;software engineering;model-driven engineering project;model transformation languages;MeTAGeM-Trace;trace generation;lower-level transformation models;DSL;source code;EMF-based toolkit;ATL model transformations;ETL model transformations;systematic research methodology;Proposals;Object oriented modeling;Software;DSL;Complexity theory;Data models;Software engineering;Model-driven engineering;model transformations;traceability}, 
doi={10.1109/TSE.2014.2316132}, 
ISSN={0098-5589}, 
month={June},}
@INPROCEEDINGS{7577380, 
author={N. Kapre and S. Bayliss}, 
booktitle={2016 26th International Conference on Field Programmable Logic and Applications (FPL)}, 
title={Survey of domain-specific languages for FPGA computing}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={High-performance FPGA programming has typically been the exclusive domain of a small band of specialized hardware developers. They are capable of reasoning about implementation concerns at the register-transfer level (RTL) which is analogous to assembly-level programming in software. Sometimes these developers are required to push further down to manage even lower levels of abstraction closer to physical aspects of the design such as detailed layout to meet critical design constraints. In contrast, software programmers have long since moved away from textual assembly-level programming towards relying on graphical integrated development environments (IDEs), high-level compilers, smart static analysis tools and runtime systems that optimize, manage and assist the program development tasks. Domain-specific languages (DSLs) can bridge this productivity gap by providing higher levels of abstraction in environments close to the domain of application expert. DSLs carefully limit the set of programming constructs to minimize programmer mistakes while also enabling a rich set of domain-specific optimizations and program transformations. With a large number of DSLs to choose from, an inexperienced FPGA user may be confused about how to select an appropriate one for the intended domain. In this paper, we review a combination of legacy and state-of-the-art DSLs available for FPGA development and provide a taxonomy and classification to guide selection and correct use of the framework.}, 
keywords={electronic engineering computing;field programmable gate arrays;parallel programming;program compilers;program diagnostics;programming environments;software engineering;domain-specific languages;FPGA computing;high-performance FPGA programming;register-transfer level;RTL;textual assembly-level programming;graphical integrated development environments;graphical IDE;high-level compilers;smart static analysis tools;runtime systems;DSL;productivity gap;domain-specific optimizations;program transformations;Field programmable gate arrays;Hardware;DSL;Hardware design languages;Software;Programming;Productivity}, 
doi={10.1109/FPL.2016.7577380}, 
ISSN={1946-1488}, 
month={Aug},}
@INPROCEEDINGS{6605922, 
author={S. Sobernig and B. Hoisl and M. Strembeck}, 
booktitle={2013 13th International Conference on Quality Software}, 
title={Requirements-Driven Testing of Domain-Specific Core Language Models Using Scenarios}, 
year={2013}, 
volume={}, 
number={}, 
pages={163-172}, 
abstract={In this paper, we present an approach for the scenario-based testing of the core language models of domain-specific modeling languages (DSML). The core language model is a crucial artifact in DSML development, because it captures all relevant domain abstractions and specifies the relations between these abstractions. In software engineering, scenarios are used to explore and to define (actual or intended) system behavior as well as to specify user requirements. The different steps in a requirements-level scenario can then be refined through detailed scenarios. In our approach, we use scenarios as a primary design artifact. Non-executable, human-understandable scenario descriptions can be refined into executable test scenarios. To demonstrate the applicability of our approach, we implemented a scenario-based testing framework based on the Eclipse Modeling Framework (EMF) and the Epsilon model-management toolkit.}, 
keywords={formal specification;program testing;specification languages;requirement-driven testing;domain-specific core language models;scenario-based testing;domain-specific modeling languages;DSML development;software engineering;user requirements;Eclipse modeling framework;Epsilon model-management toolkit;EMF;system behavior;Biological system modeling;Testing;Unified modeling language;Software;Prototypes;Metamodeling;domain-specific modeling;scenario-based testing;language engineering;metamodel testing}, 
doi={10.1109/QSIC.2013.56}, 
ISSN={1550-6002}, 
month={July},}
@INPROCEEDINGS{766632, 
author={F. Blome and A. Hamich}, 
booktitle={International Symposium on Switching}, 
title={24 months of commercial isdn experience}, 
year={1990}, 
volume={2}, 
number={}, 
pages={159-164}, 
abstract={}, 
keywords={ISDN;Switches;Telephony;Packet switching;DSL;Telecommunications;Circuits;Roads;Testing;Cities and towns}, 
doi={10.1109/ISS.1990.766632}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{916745, 
author={R. S. Lightfoot}, 
booktitle={Proceedings of the 2000 IEEE International Conference on Management of Innovation and Technology. ICMIT 2000. 'Management in the 21st Century' (Cat. No.00EX457)}, 
title={Establishing long-term viability in the information economy: the strength of the integrated systems engineering process}, 
year={2000}, 
volume={2}, 
number={}, 
pages={526-529 vol.2}, 
abstract={Information technology companies are under increasing pressure to develop high technology products and services at an accelerated pace. They are often rewarded for such efforts through increases in their stock prices only to see these gains drop due to poor quality, failed technology, and poor customer support. In considering these events the question that comes to mind is, "What is the formula for providing long term viability in the high-tech information economy?" This paper presents recommendations that can be applied to the integrated systems/software engineering process which, when implemented, will provide companies with the formula for long-term viability in this rapidly changing fast-paced environment.}, 
keywords={information technology;systems engineering;technical support services;information economy;integrated systems engineering process;long-term viability;stock prices;poor quality;failed technology;poor customer support;high-tech information economy;integrated systems/software engineering process;Information technology;Companies;Systems engineering and theory;Stock markets;IEEE news;Educational institutions;Acceleration;Software engineering;DSL;Modems}, 
doi={10.1109/ICMIT.2000.916745}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6104710, 
author={J. Liu and G. Liu}, 
booktitle={2011 4th International Conference on Intelligent Networks and Intelligent Systems}, 
title={Research and Implementation of SNMP-Based Network Management System}, 
year={2011}, 
volume={}, 
number={}, 
pages={129-132}, 
abstract={After the brief introduction of the current situation of matured system framework based on SNMP, the basic problems of network management framework design are presented in detail. This paper focuses on the construction of SNMP-based platform in terms of software framework. Finally, by the use of the medium-sized data exchange network, it achieves the function of discovering the topology.}, 
keywords={computer network management;protocols;telecommunication network topology;SNMP based network management system;network management framework design;software framework;medium sized data exchange network;topology discovery;simple network management protocol;Topology;Software;Programming;Testing;Synchronization;DSL;Scalability;Network management framework;Auto-Topology control;Scalability;Generality}, 
doi={10.1109/ICINIS.2011.39}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4299957, 
author={D. Kim and H. Lim and S. Lee}, 
booktitle={International Conference on Software Engineering Advances (ICSEA 2007)}, 
title={A Case Study on Testing Activites for KT-OSS Maintenance}, 
year={2007}, 
volume={}, 
number={}, 
pages={77-77}, 
abstract={This paper describes the testing activities for the maintenance of the KT-OSS (Korea Telecom Operations Support System). Since the KT-OSS is a large software, it is essential to continuously perform maintenance activities such as the addition of new services from business departments and new functions requested by users and operators, performance improvement of existing functions, correction of the errors found during operation of the system, and so on. To ensure the successful maintenance of the KT-OSS without any effect on the existing functions and performance, we performed various tests related to functionality, efficiency and others before the added and modified parts were applied to the KT-OSS. In this paper, we show the maintenance process, the various tests related to it, the test organization, and the test environment for controlling the quality of the KT-OSS maintenance. Through these testing activities, we were able to successfully maintain the KT-OSS.}, 
keywords={program testing;software maintenance;telecommunication computing;Korea telecom operation support system;software maintenance;software testing activity;Software maintenance;Error correction;Telecommunications;Preventive maintenance;System testing;Information management;Quality management;DSL;Paper technology;Research and development}, 
doi={10.1109/ICSEA.2007.1}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{410732, 
author={D. Kelly and Q. Hartmann and W. Gude}, 
booktitle={Sixth Annual IEEE International ASIC Conference and Exhibit}, 
title={A multi-FPGA prototype of a DS1/HDSL synchronizer and desynchronizer prior to ASIC fabrication}, 
year={1993}, 
volume={}, 
number={}, 
pages={332-335}, 
abstract={As the speed and complexity of today's ASICs continues to grow, conventional prototyping techniques for algorithm verification begin to break down. A novel implementation of DS1/HDSL synchronizer and desynchronizer utilizing an array of four FPGA devices to verify algorithm performances prior to ASIC fabrication is described. The utilization of FPGA devices for ASIC prototyping can significantly reduce the risk, cost, and time-to-market involved with complex ASIC devices.<<ETX>>}, 
keywords={high level synthesis;hardware description languages;field programmable gate arrays;logic CAD;application specific integrated circuits;subscriber loops;synchronisation;flowcharting;VLSI;programmable logic arrays;high-bit-rate digital subscriber line;VHDL;compiled datapath circuits;VLSI;fuse generation;timing analysis;multi-FPGA prototype;DS1/HDSL synchronizer;desynchronizer;algorithm performances;ASIC prototyping;risk;cost;time-to-market;Prototypes;Application specific integrated circuits;Fabrication;Timing;Field programmable gate arrays;Jitter;Frequency synchronization;Costs;DSL;Testing}, 
doi={10.1109/ASIC.1993.410732}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{739853, 
author={P. J. Pingree}, 
booktitle={17th DASC. AIAA/IEEE/SAE. Digital Avionics Systems Conference. Proceedings (Cat. No.98CH36267)}, 
title={Deep space one integration and test challenges: getting to the launch pad in the faster, better, cheaper world}, 
year={1998}, 
volume={2}, 
number={}, 
pages={H21/1-H21/8 vol.2}, 
abstract={This paper describes the integration and test challenges of verifying and validating the avionics hardware and flight software which have been experienced in meeting the New Millennium Program Deep Space One (DS1) project's faster, better, cheaper requirements. This paper gives a high level overview of the development and application of the two flight system testbeds (DSI Hotbench), the testbed activities supported the DSI Spacecraft Assembly, Test and Launch Operations (ATLO), and the testing performed to prepare for and support post-launch mission operations. In the "Faster, Better, Cheaper" environment of the New Millennium Program, DSI Integration and Test (I&T) has defined new methods and decision criteria to meet our requirements and goals while assessing and minimizing the risk in the paths we have taken. Our successes and failures are largely yet to be seen as we approach our July 1 launch date. This paper describes the challenges that have been faced and some that have been overcome during the DSI I&T phase. It presents the risks that have been accepted in our attempts to test completely the DSI Avionics system in preparation for launch and mission operations.}, 
keywords={avionics;aerospace computing;electronic equipment testing;automatic testing;integration;test;avionics hardware;flight software;Millennium Program Deep Space One project;DSI Hotbench;testbed;post-launch mission;DSI Integration and Test;failures;Atherosclerosis;Aerospace electronics;Space technology;Electronic equipment testing;Software testing;System testing;Hardware;Space vehicles;DSL;Space missions}, 
doi={10.1109/DASC.1998.739853}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{4055046, 
author={D. Kim and H. Lim and S. Lee}, 
booktitle={2006 Canadian Conference on Electrical and Computer Engineering}, 
title={Testing Activities for KT-OSS Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={2397-2400}, 
abstract={This paper describes the testing activities for the development of the KTOSS (Korea Telecom Operations Support System). In this paper, we show the test phases for performing the verification and validation activities for the development and maintenance of KT-OSS. They are based on the general software development lifecycle, with an operational test added to it as an additional phase. To ensure the successful development of the KT-OSS, we performed various tests related to functionality, efficiency and others. Also, the tests were performed for maintenance after the field release. We also show the criteria for them and deal with the test organizations and the test-bed for managing and controlling the quality of the KT-OSS in this paper. Through these testing activities, we were able to successfully develop and release the KT-OSS}, 
keywords={program testing;program verification;software maintenance;software quality;software reliability;software reusability;testing activities;KT-OSS development;Korea Telecom Operations Support System;verification activities;validation activities;software maintenance;software development lifecycle;test-bed;software quality;Life testing;Software testing;Programming;System testing;Laboratories;Performance evaluation;Quality management;DSL;Telecommunications;ISO standards;Test;Operations Support System;Verification;Validation;Software Development Lifecycle}, 
doi={10.1109/CCECE.2006.277822}, 
ISSN={0840-7789}, 
month={May},}
@INPROCEEDINGS{5479285, 
author={Guo-Ming Sung and Yen-Tang Chang and Wen-Huei Chen and Hsiang-Yuan Hsieh}, 
booktitle={2010 International Conference on Networking and Digital Society}, 
title={A new architecture of broadband network system suitable for asymmetric digital subscriber line application}, 
year={2010}, 
volume={1}, 
number={}, 
pages={624-628}, 
abstract={This paper presents the design and implementation on a new architecture of broadband network system which suitable for asymmetric digital subscriber line (ADSL) application. The main design skill is based on the cell-based digital IC design process, and is implemented in 0.18 m 1P6M CMOS process. The main function of this chip is to build a bridge between Ethernet and ATM which is used to substitute for RISC processor, leading to enhance the broadband network switching ability and stability. Furthermore, the clock management system is adopted to manage the packages. By this technique, a small size and low cost chip will be obtained.}, 
keywords={asynchronous transfer mode;broadband networks;CMOS integrated circuits;digital subscriber lines;integrated circuit design;broadband network system;asymmetric digital subscriber line application;ADSL;cell-based digital IC design;CMOS process;Ethernet;ATM;RISC processor;clock management system;size 0.18 micron;Broadband communication;DSL;CMOS integrated circuits;CMOS digital integrated circuits;Process design;CMOS process;Bridges;Ethernet networks;Asynchronous transfer mode;Reduced instruction set computing;component;Asynchronous transfer mode (ATM);asymmetric digital subscriber line (ADSL);Broadband Network}, 
doi={10.1109/ICNDS.2010.5479285}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4696109, 
author={M. Khoshbakhtian and N. HezareMoghadam and M. Mazoochi}, 
booktitle={2008 Third International Conference on Broadband Communications, Information Technology Biomedical Applications}, 
title={Identification of Various Problems in an Environment of Multi Vendor Equipments for PSTN Services in NGN}, 
year={2008}, 
volume={}, 
number={}, 
pages={194-201}, 
abstract={This paper focuses on Iran next generation network (NGN) Pilot, the purpose of which is to achieve appropriate knowledge for a desired move through current telecommunication network of Iran to NGN. On the base of this purpose, obtaining a proper knowledge about capabilities and weaknesses of vendors' NGN equipments is necessary as well as verifying the functionalities of these equipments in a multi-vendor environment. Accordingly, a test bed has been designed with the capability of executing various tests scenarios related to NGN and processing the results. At these stage two types of tests, i.e. Functional tests and Interoperability tests have been done. Since Functional tests are performed in a single-vendor environment; the proper interoperation among multi-vendor equipments can not be guaranteed. Consequently, other types of tests called Interoperability tests have been designed. These tests are performed in a multi-vendor environment and classified into two groups which process interoperation of two Call Servers or a Call Server and Gateway from different vendors, respectively. The following paper represents the results obtained from the mentioned tests as well as the precise assessment of them.}, 
keywords={circuit switching;internetworking;open systems;packet switching;multi vendor equipments;PSTN services;NGN;Iran next generation network pilot;interoperability tests;Call Server;Gateway;Next generation networking;Testing;Cities and towns;Network servers;File servers;Databases;Performance evaluation;Modems;Access protocols;Switching circuits;interoperability;Megaco;NGN;SIP}, 
doi={10.1109/BROADCOM.2008.14}, 
ISSN={}, 
month={Nov},}
@ARTICLE{1094910, 
author={S. Ahamed and P. Bohn and N. Gottfried}, 
journal={IEEE Transactions on Communications}, 
title={A Tutorial on Two-Wire Digital Transmission in the Loop Plant}, 
year={1981}, 
volume={29}, 
number={11}, 
pages={1554-1564}, 
abstract={This paper explores the constraints on the design of twowire repeaterless digital subscriber loop (DSL) systems. Broadly categorized, the design depends on the technical feasibility of the approach used to achieve two-wire transmission, constraints related to compatibility with other systems sharing the same cable, and immunity to central office noise. Each of these varies With the choice of system parameters including the transmission rate, transmit power, choice of line codes, etc. Technical feasibility is evaluated by computer simulation studies. Compatibility with other systems is explored by crosstalk calculations. Noise immunity considerations, as they translate into digital line power levels, are also explored.}, 
keywords={Digital communications;Wire communication subscriber networks;Tutorial;Crosstalk;DSL;Echo cancellers;Wire;Repeaters;Central office;Noise cancellation;Computer simulation;Noise level}, 
doi={10.1109/TCOM.1981.1094910}, 
ISSN={0090-6778}, 
month={November},}
@INPROCEEDINGS{7102616, 
author={F. Haser and R. Breu}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Non-Intrusive Documentation-Driven Integration Testing}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-2}, 
abstract={Powerful development frameworks and adoption of agile development methods are continuously increasing release frequency, thus compress test cycles. Test automation, often relying on model based approaches, helps to reduce test time, however the introduction of related heavy weight processes is often quite challenging. In order to tackle this problem, we propose a bottom up testing approach, which in a nutshell, in the initial phase supports the integration tester in creating a semi-formal test case description and report. The approach, a textual domain specific framework, will guide the test expert in evolving the base language, in order to be tailored for a domain language of an organization. The evolved language can be linked to executable code, which enables in the long run (semi-)automated model based regression testing.}, 
keywords={program testing;software prototyping;specification languages;system documentation;documentation-driven integration testing;agile development method;test automation;bottom up testing approach;textual domain specific framework;domain language;Testing;Automation;Context;Unified modeling language;Business;Writing;Measurement}, 
doi={10.1109/ICST.2015.7102616}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{5959781, 
author={C. Miksovic and O. Zimmermann}, 
booktitle={2011 Ninth Working IEEE/IFIP Conference on Software Architecture}, 
title={Architecturally Significant Requirements, Reference Architecture, and Metamodel for Knowledge Management in Information Technology Services}, 
year={2011}, 
volume={}, 
number={}, 
pages={270-279}, 
abstract={Capturing and sharing design knowledge such as architectural decisions is becoming increasingly important in professional Information Technology (IT) services firms. Methods, models, and tools supporting explicit knowledge management strategies have been proposed in recent years. In this paper, we extend previous work in the architectural knowledge management community to satisfy the requirements of an additional user group: the designers of IT infrastructure solutions that are outsourced from one company to another. Such strategic outsourcing solutions require complex, contractually relevant design decisions concerning many different resources such as IT infrastructures, people, and real estate. In this paper, we present a reference architecture and a decision process-oriented knowledge metamodel that we synthesized from the domain-specific functional requirements and quality attributes. We also present a tool implementation of these decision modeling concepts and discuss user feedback.}, 
keywords={information services;information technology;knowledge management;outsourcing;design knowledge;architectural decision;professional information technology service firm;knowledge management strategy;architectural knowledge management community;IT infrastructure solution;strategic outsourcing solution;decision process oriented knowledge metamodel;domain-specific functional requirement;quality attribute;decision modeling concept;user feedback;Proposals;Knowledge engineering;Knowledge based systems;Computer architecture;Communities;Engines;DSL;knowledge management;outsourcing;workflow}, 
doi={10.1109/WICSA.2011.43}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7323104, 
author={C. Sanz and A. Salas and M. de Miguel and A. Alonso and J. A. de la Puente and C. Benac}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Automated model-based testing based on an agnostic-platform modeling language}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Currently multiple Domain Specific Languages (DSLs) are used for model-driven software development, in some specific domains. Software development methods, such as agile development, are test-centered, and their application in model-based frameworks requires model support for test development. We introduce a specific language to define generic test models, which can be automatically transformed into executable tests for particular testing platforms. The resulting test models represent the test plan for applications also built according to a model-based approach. The approach presented here includes some customisations for the application of the developed languages and transformation tools for some specific testing platforms. These languages and tools have been integrated with some specific DSL designed for software development.}, 
keywords={program testing;software development management;automated model;agnostic platform modeling language;multiple domain specific languages;DSL;model driven software development;software development methods;generic test models;Testing;Unified modeling language;Software;Architecture;Computer architecture;Complexity theory;Engines;Model-based Testing;Automated Testing;Agile Development}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{6737898, 
author={A. Parizad}, 
booktitle={2013 13th International Conference on Environment and Electrical Engineering (EEEIC)}, 
title={Dynamic stability Analysis for damavand power plant considering PMS functions by DIGSILENT software}, 
year={2013}, 
volume={}, 
number={}, 
pages={145-155}, 
abstract={For a number of years Power Management System (PMS) have been used to control islanded power systems. It can control system frequency and voltage as well as generated real and reactive power. The PMS would often have a load shedding facility to prevent the cascade failure of the generation system. The investigation is based on dynamics time domain simulations by DigSilent software and aims to define the main functions for the power management system (PMS) that will govern the operation of the power plant. In this paper, DigSilent Simulation Language (DSL) is used for simulation of Turbine-Governor and excitation system. Also DigSilent Program Language (DPL) is used to simulate action of PMS when it is required. Also, different dynamic contingencies related to Damavand power plant such as Trip of grid connection, Trip of One GT, Trip of load, Trip of Two GTs, Trip of Grid connection (According to governor logic, Automatic change mode acts, Solving frequency deviation by PMS) are considered. In each scenario, power plant stability is investigated and where required, PMS functions (such as load shedding and sending appropriate set points) are applied.}, 
keywords={control engineering computing;frequency control;load management;load shedding;power engineering computing;power system stability;reactive power control;voltage control;dynamic stability analysis;PMS function;islanded power system control;frequency control;reactive power;load shedding;DigSilent software;power management system;DigSilent Simulation Language;DSL;Turbine-Governor;DigSilent Program Language;DPL;Damavand power plant;Trip of One GT;Trip of load;Trip of Two GT;Trip of Grid connection;power plant stability;Power system stability;Turbines;Generators;Load modeling;Velocity control;Power system dynamics;power system stability;power management system (PMS);Digsilent Software;Turbine-Governor model}, 
doi={10.1109/EEEIC-2.2013.6737898}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8004372, 
author={A. A. Ahmad and P. Brereton and P. Andras}, 
booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={A Systematic Mapping Study of Empirical Studies on Software Cloud Testing Methods}, 
year={2017}, 
volume={}, 
number={}, 
pages={555-562}, 
abstract={Context: Software has become more complicated, dynamic, and asynchronous than ever, making testing more challenging. With the increasing interest in the development of cloud computing, and increasing demand for cloud-based services, it has become essential to systematically review the research in the area of software testing in the context of cloud environments. Objective: The purpose of this systematic mapping study is to provide an overview of the empirical research in the area of software cloud-based testing, in order to build a classification scheme. We investigate functional and non-functional testing methods, the application of these methods, and the purpose of testing using these methods. Method: We searched for electronically available papers in order to find relevant literature and to extract and analyze data about the methods used. Result: We identified 69 primary studies reported in 75 research papers published in academic journals, conferences, and edited books. Conclusion: We found that only a minority of the studies combine rigorous statistical analysis with quantitative results. The majority of the considered studies present early results, using a single experiment to evaluate their proposed solution.}, 
keywords={cloud computing;program testing;statistical analysis;software cloud testing methods;classification scheme;nonfunctional testing methods;statistical analysis;Cloud computing;Security;Software testing;Reliability;Systematics;systematic mapping study;cloud software testing methods;software testing;empirical studies}, 
doi={10.1109/QRS-C.2017.94}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{1186689, 
author={S. Brini and D. Benjelloun and F. Castanier}, 
booktitle={2003 Design, Automation and Test in Europe Conference and Exhibition}, 
title={A flexible virtual platform for computational and communication architecture exploration of DMT VDSL modems}, 
year={2003}, 
volume={}, 
number={}, 
pages={164-169 suppl.}, 
abstract={In this paper a high-level SoC architecture exploration of DMT (Discrete Multitone) VDSL transceivers (Very high speed Digital Subscriber Line) is presented. A flexible and complete virtual platform was developed for the purpose, exploiting the paradigm of "orthogonalization of concerns" (functionality independent from architecture) in the framework of Cadence VCC system level design tool. An accurate processor model, obtained through the back-annotation of profiling results on a target DSP core, allowed the exploration of different HW/SW partitioning and the study of the computational units required. A transaction-accurate VCC bus model was developed for the investigation of the on-chip bus architecture and its relevant parameters dimensioning.}, 
keywords={digital subscriber lines;modems;system-on-chip;hardware-software codesign;flexible virtual platform;DMT VDSL modems;communication architecture;high-level SoC architecture;DMT;Discrete Multitone;orthogonalization of concerns;Cadence VCC system level design tool;processor model;back-annotation;HW/SW partitioning;computational units;transaction-accurate VCC bus model;Computer architecture;OFDM modulation;Modems;Transceivers;DSL;System-level design;Digital signal processing;Quality of service;Testing;Libraries}, 
doi={10.1109/DATE.2003.1186689}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{6976605, 
author={M. A. Jimnez and . V. Gmez and N. M. Villegas and G. Tamura and L. Duchien}, 
booktitle={2014 IEEE 8th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems}, 
title={A Framework for Automated and Composable Testing of Component-Based Services}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={The vision of service-oriented computing has been largely developed on the fundamental principle of building systems by composing and orchestrating services in their control flow. Nowadays, software development is notably influenced by service-oriented architectures (SOAs), in which the quality of software systems is determined by the quality of the involved services and their actual composition. Despite the efforts on improving their individual quality, adding or replacing services in an evolving system can introduce failures, thus compromising the satisfaction of the system's functional and extra-functional requirements. These failures erode the trust in the SOA vision. Thus, a key issue for the industrial adoption of SOA is providing service providers, integrators, and consumers the means to build confidence that services behave according to the contracted quality conditions. In this paper we present a first version of PA SCA NI, a framework for specifying and executing test specifications for service-oriented systems. From a test specification, PA SCA NI generates a configuration of testing services compliant with the Service Component Architecture (SCA) specification, which can be composed to integrate different testing strategies, being these tests traceable in an automated way. Our evaluation results show the applicability of the framework and a substantial gain in the tester's effort for developing tests.}, 
keywords={formal specification;object-oriented programming;program testing;service-oriented architecture;software quality;service-oriented computing;control flow;software development;service-oriented architectures;software systems;SOA vision;industrial adoption;service providers;service integrators;service consumers;PASCANI;service-oriented systems;test specification;testing services;service component architecture specification;component-based service testing;system functional requirements;system extra-functional requirements;Testing;Service-oriented architecture;Computer architecture;DSL;Software systems;Software service testing;SOA testing;composable tests;SCA testing}, 
doi={10.1109/MESOCA.2014.9}, 
ISSN={2326-6937}, 
month={Sept},}
@ARTICLE{7270333, 
author={N. Mellegrd and A. Ferwerda and K. Lind and R. Heldal and M. R. V. Chaudron}, 
journal={IEEE Transactions on Software Engineering}, 
title={Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study}, 
year={2016}, 
volume={42}, 
number={3}, 
pages={245-260}, 
abstract={Domain-specific modelling (DSM) is a modern software development technology that aims at enhancing productivity. One of the claimed advantages of DSM is increased maintainability of software. However, current empirical evidence supporting this claim is lacking. In this paper, we contribute evidence from a case study conducted at a software development company. We study how the introduction of DSM affected the maintenance of a legacy system. We collected data about the maintenance phase of a system that was initially developed using manual programming, but which was gradually replaced by DSM development. We performed statistical analyses of the relation between the use of DSM and the time needed to resolve defects, the defect density, and the phase in which defects were detected. The results show that after introducing DSM the defect density is lower, that defects are found earlier, but resolving defects takes longer. Other observed benefits are that the number of developers and the number of person-hours needed for maintaining the system decreased, and the portability to new platforms increased. Our findings are useful for organizations that consider introducing DSM and would like to know which benefits can be realized in software maintenance.}, 
keywords={software maintenance;statistical analysis;domain-specific modelling;software maintenance;DSM;software development technology;software maintainability;software development company;legacy system;manual programming;statistical analysis;DSL;Maintenance engineering;Unified modeling language;Business;Software maintenance;Productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity}, 
doi={10.1109/TSE.2015.2479221}, 
ISSN={0098-5589}, 
month={March},}
@INPROCEEDINGS{1253823, 
author={S. Brini and D. Benjelloun and F. Castanier}, 
booktitle={2003 Design, Automation and Test in Europe Conference and Exhibition}, 
title={A flexible virtual platform for computational and communication architecture exploration of DMT VDSL modems}, 
year={2003}, 
volume={}, 
number={}, 
pages={164-169}, 
abstract={}, 
keywords={Computer architecture;OFDM modulation;Modems;Transceivers;DSL;System-level design;Digital signal processing;Quality of service;Testing;Libraries}, 
doi={10.1109/DATE.2003.1253823}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{8428788, 
author={R. Bussenot and H. Leblanc and C. Percebois}, 
booktitle={2018 13th Annual Conference on System of Systems Engineering (SoSE)}, 
title={Orchestration of Domain Specific Test Languages with a Behavior Driven Development approach}, 
year={2018}, 
volume={}, 
number={}, 
pages={431-437}, 
abstract={An airplane is composed by many complexes and embedded systems. During the integration testing phase, the design office produces requirements of the targeted system, and the test center produces concrete test procedures to be executed on a test bench. In this context, integration tests are mostly written in natural language and manually executed step by step by a tester. In order to formalize integration tests procedures dedicated to each system with domain specific languages approved by testers, and in order to automatize integration tests, we have introduced agile practices in the integration testing phase. We have chosen a Behavior Driven Development (BDD) approach to orchestrate Domain Specific Test Languages produced for the ACOVAS FUI project.}, 
keywords={aircraft;embedded systems;formal specification;program testing;specification languages;domain specific test languages;behavior driven development approach;embedded systems;integration testing phase;targeted system;test center;test bench;domain specific languages;integration test procedures;BDD approach;ACOVAS FUI project;Testing;Software;DSL;Natural languages;Hardware;Aerospace electronics;Aircraft}, 
doi={10.1109/SYSOSE.2018.8428788}, 
ISSN={}, 
month={June},}
@ARTICLE{1519613, 
author={Young-Jae Cho and Seung-Hoon Lee}, 
journal={IEEE Transactions on Circuits and Systems I: Regular Papers}, 
title={An 11b 70-MHz 1.2-mm/sup 2/ 49-mW 0.18-/spl mu/m CMOS ADC with on-chip current/voltage references}, 
year={2005}, 
volume={52}, 
number={10}, 
pages={1989-1995}, 
abstract={This work proposes an 11b 70-MHz CMOS pipelined analog-digital converter (ADC) as one of core circuit blocks for very high speed digital subscriber line system applications. The proposed ADC for the internal use has the strictly limited number of externally connected I/O pins while the ADC employs on-chip CMOS current/voltage references and a merged-capacitor switching technique to improve ADC performances. The ADC implemented in a 0.18-/spl mu/m 1P4M CMOS technology shows the maximum signal-to-noise distortion ratio (SNDR) of 60 dB at 70 MSample/s. The ADC maintains the SNDR of 58 dB and the spurious-free dynamic resistance of 68 dB for input frequencies up to the Nyquist rate at 60 MSample/s. The measured differential and integral nonlinearities of the ADC are within /spl plusmn/0.63 and /spl plusmn/1.21 LSB, respectively. The active chip area is 1.2 mm/sup 2/ and the ADC consumes 49 mW at 70 MSample/s at 1.8 V.}, 
keywords={analogue-digital conversion;CMOS digital integrated circuits;system-on-chip;capacitor switching;digital subscriber lines;VHF circuits;radiofrequency integrated circuits;reference circuits;network topology;integrated circuit design;CMOS ADC;on-chip current reference;on-chip voltage reference;CMOS pipelined analog-digital converters;core circuit blocks;very high speed digital subscriber lines;merged-capacitor switching technique;analogue-digital conversion;CMOS digital integrated circuits;system-on-chip;switched capacitor networks;VHF circuits;radiofrequency integrated circuits;reference circuits;network topology;integrated circuit design;70 MHz;49 mW;0.18 micron;1.8 V;Analog-digital conversion;CMOS technology;CMOS digital integrated circuits;CMOS analog integrated circuits;DSL;Pins;Voltage;Frequency;Distortion measurement;Electrical resistance measurement;Analogdigital converter (ADC);CMOS;low power;on-chip references}, 
doi={10.1109/TCSI.2005.853251}, 
ISSN={1549-8328}, 
month={Oct},}
@INPROCEEDINGS{8227296, 
author={K. S. Tse and P. C. Johnson}, 
booktitle={2017 IEEE Security and Privacy Workshops (SPW)}, 
title={A Framework for Validating Session Protocols}, 
year={2017}, 
volume={}, 
number={}, 
pages={110-119}, 
abstract={Communication protocols are complex, their implementations are difficult, causing many unintended (and severe) vulnerabilities in protocol parsing. While the problem of packet parsing is solved, session parsing remains challenging. Building on existing systems that reliably parse individual messages, we present our four-component framework for implementing protocol session parsers with the goal to improve security of protocol parsing: specification of a protocol message, description of a protocol state machine, testing routines to validate implementations against fake and real data, and graph generation to visualize implementations. This framework enables the creation of a session parser, which validates individual protocol messages in the context of other messages in the same conversation. This is helpful because more secure parsers lead to more secure communication.}, 
keywords={protocols;telecommunication security;protocol parsing;protocol message;protocol state machine;session protocols;communication protocols;packet parsing;session parsing;four-component framework;protocol session parsers;Protocols;Data structures;Security;Testing;DSL;Semantics;Language-theoretic security;protocol state machine;protocol parsing;session parsing}, 
doi={10.1109/SPW.2017.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8080500, 
author={P. Mueller and T. Belschner and R. Reichel}, 
booktitle={2017 IEEE AUTOTESTCON}, 
title={Automated test artifact generation for a distributed avionics platform utilizing abstract state machines}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The development of complex and highly safety-critical avionics systems, such as fly-by-wire, is typically linked with high efforts, risks and thus costs. Especially with regard to certification the testing activities during verification are playing a major role. This paper introduces the automatization complex of the testing artifact generation by use of Abstract State Machines (ASM), which allows a unified approach for system and software testing. The baseline is the Flexible Platform technology (a platform based development approach) currently under development by the Institute of Aircraft Systems (ILS) of the University of Stuttgart. The remaining automatization complex is the automated generation of certification relevant documentation, i.e. the requirements. These three complexes establish the AAA-Process which lays the foundation for an effective total system capability for complex avionics systems while simultaneously mitigating risks and costs. The actual test artifact generation is strictly aligned to development standards used in the aviation industry. Requirements exist as classes in a textual representation as well as in a specification model, represented by ASMs. The functional behavior, as described by the models, serves as a test oracle for test case generation. For this the model is translated into a graph system, instrumented by selectable testing methods and executed. The resulting trace data is used to automatically derive test procedures under consideration of the corresponding test environment as scripts, which are directly executable within our testing infrastructure consisting of a HiL simulation. Furthermore this includes the automatic generation of the associated traceability data and test specification documentation. An initial framework has been defined to support exchangeability of individual tasks in the generation tool-chain. The feasibility of the approach has been demonstrated by testing the complete heterogeneous signal communication of an exemplary avionics system, resp. platform instance, at system level as well as at software high-level.}, 
keywords={aerospace computing;avionics;finite state machines;program testing;safety-critical software;abstract state machines;ASM;software testing;Flexible Platform technology;platform based development approach;certification relevant documentation;complex avionics systems;test oracle;test case generation;graph system;test specification documentation;HiL simulation;University of Stuttgart;safety-critical avionics systems;Institute of Aircraft Systems;distributed avionics platform;automated test artifact generation;Aerospace electronics;Software;Hardware;Tools;Aircraft;DSL;Testing}, 
doi={10.1109/AUTEST.2017.8080500}, 
ISSN={1558-4550}, 
month={Sept},}
@INPROCEEDINGS{1689533, 
author={Dae-Woo Kim and Hyun-Min Lim and Sang-Kon Lee}, 
booktitle={2006 IEEE International Symposium on Consumer Electronics}, 
title={A Case Study on Testing and Evaluation in the KT-OSS Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper describes the test and evaluation activities for the development of the KT-OSS (Korea Telecom Operations Support System). In this paper, we show the test and evaluation phases for the development and maintenance of the KT-OSS. To ensure the successful development of the KT-OSS, we performed various tests related to functionality, efficiency and others. We also show the criteria for them and deal with the test organizations and the test-bed for managing and controlling the quality of the KT-OSS. And we describe our experiences in performing these tests. Through these test and evaluation activities, we were able to successfully develop and release the KT-OSS}, 
keywords={program testing;software performance evaluation;software quality;telecommunication computing;KT-OSS development;Korea Telecom Operations Support System;quality management;test activity;evaluation activity;Computer aided software engineering;Life testing;Software testing;Programming;System testing;Performance evaluation;Quality management;Telecommunications;Information management;DSL;Test;Evaluation;Operations Support System;Software Development Lifecycle}, 
doi={10.1109/ISCE.2006.1689533}, 
ISSN={0747-668X}, 
month={June},}
@INPROCEEDINGS{8327149, 
author={L. P. Binamungu and S. M. Embury and N. Konstantinou}, 
booktitle={2018 IEEE Workshop on Validation, Analysis and Evolution of Software Tests (VST)}, 
title={Detecting duplicate examples in behaviour driven development specifications}, 
year={2018}, 
volume={}, 
number={}, 
pages={6-10}, 
abstract={In Behaviour-Driven Development (BDD), the behaviour of the software to be built is specified as a set of example interactions with the system, expressed using a Given-When-Then structure. The examples are written using customer language, and are readable by end-users. They are also executable, and act as tests that determine whether the implementation matches the desired behaviour or not. This approach can be effective in building a common understanding of the requirements, but it can also face problems. When the suites of examples grow large, they can be difficult and expensive to change. Duplication can creep in, and can be challenging to detect manually. Current tools for detecting duplication in code are also not effective for BDD examples. Moreover, human concerns of readability and clarity can rise. We present an approach for detecting duplication in BDD suites that is based around dynamic tracing, and describe an evaluation based on three open source systems.}, 
keywords={formal specification;program diagnostics;public domain software;duplication;BDD suites;open source systems;behaviour driven development specifications;customer language;duplicate examples detection;software behaviour;Given-When-Then structure;dynamic tracing;Tools;Production;Software;Syntactics;Semantics;Cloning;DSL;behaviour-driven development;duplication detection;dynamic tracing}, 
doi={10.1109/VST.2018.8327149}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6210038, 
author={C. G. Bianchin and R. Demonti and A. R. de Almeida and M. T. da Silva Filho and C. L. da Silva Pinto}, 
booktitle={2012 IEEE International Conference on Industrial Technology}, 
title={Development of static switch with high speed algorithm to fault detection}, 
year={2012}, 
volume={}, 
number={}, 
pages={808-814}, 
abstract={This paper presents the updated results on research for development of a static switch for operation in medium voltage. It shows the results of the algorithm that detects an outage and enables the digital processing to control the static switch. This algorithm was based on the Clarke and Park transforms. The operation of the switch consists of transferring the power supply from the main source, where an outage occurred, to an alternative power source (backup). The load stays off for a time of milliseconds. A low voltage prototype was built, to allow the evaluation of the algorithm. Then a prototype of the static switch was built in voltage of 13,8kV. The first tests - feeding resistive load - are presented in voltage of 13.8 kV.}, 
keywords={power semiconductor switches;thyristors;static switch;high speed algorithm;fault detection;medium voltage;digital processing;power supply;alternative power source;low voltage prototype;resistive load;voltage 13.8 kV;DSL;Switches;algorithm;digital processing;series-connected thyristors;medium voltage}, 
doi={10.1109/ICIT.2012.6210038}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7073229, 
author={C. Mahmoudi and F. Mourlin}, 
booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
title={Business Process Management with mobile routes}, 
year={2014}, 
volume={}, 
number={}, 
pages={420-427}, 
abstract={Business processes are milestone of the information system of any companies. Their availability is a crucial aspect. We provide a solution for the high level of availability of business processes by the use of cluster of enterprise service buses (ESB). Our approach is based on the dynamic creation of the route between the business services and the migration of a runtime context from one ESB to another one. So, we insure the management of business processes over a cluster and measure the impact of such incident. Through the use of log, we also report these events which allow the administrator for preparing updates of the information system. With the use of open source software, we guarantee the reuse of our case study with other kinds of enterprise service bus, which respect open standard exchanges like XML language and REST API.}, 
keywords={application program interfaces;business data processing;mobile computing;public domain software;service-oriented architecture;Web services;XML;business process management;mobile route;information system;enterprise service bus;ESB;business services;open source software;XML language;REST API;Business;Containers;Context;DSL;XML;Servers;Routing;SOA architecture;orchestration;cluster of bus;message routing;web service}, 
doi={10.1109/AICCSA.2014.7073229}, 
ISSN={2161-5330}, 
month={Nov},}
@INPROCEEDINGS{7203010, 
author={R. Abreu and H. Erdogmus and A. Perez}, 
booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
title={CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts}, 
year={2015}, 
volume={2}, 
number={}, 
pages={551-554}, 
abstract={Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plug in mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, Code Aware, for distributed and fine-grained artifact analysis. Code Aware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. Code ware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems, (b) the ability to perform both static and dynamic analyses on these artifacts, and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of Code Aware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges.}, 
keywords={productivity;program diagnostics;software quality;CodeAware;sensor-based fine-grained monitoring;software artifact management;continuous integration tools;CI tools;artifact analysis capabilities;plug in mechanisms;distributed artifact analysis;fine-grained artifact analysis;ecosystem;monitors;code quality improvement;team productivity improvement;static analysis;dynamic analysis;automated actuators;Probes;Software;Ecosystems;Software engineering;Monitoring;Electronic mail;DSL}, 
doi={10.1109/ICSE.2015.192}, 
ISSN={0270-5257}, 
month={May},}
@ARTICLE{1341380, 
author={M. Varsamou and T. Antonakopoulos and N. Papandreou}, 
journal={IEEE Design Test of Computers}, 
title={From protocol models to their implementation: a versatile testing methodology}, 
year={2004}, 
volume={21}, 
number={5}, 
pages={416-428}, 
abstract={The design and test of communication protocols relies extensively on formal description languages. In this protocol design and verification scheme, high-level models serve in generating simulation sequences for low-level models, and all simulation is based on directed testing. The methodology is versatile and flexible, and difficult to set up the first time.}, 
keywords={protocols;formal verification;formal specification;conformance testing;communication protocol testing;formal description languages;protocol design;formal verification;Protocols;Object oriented modeling;System testing;Mathematical model;Design methodology;Computational modeling;Appropriate technology;Process design;DSL;Automata}, 
doi={10.1109/MDT.2004.61}, 
ISSN={0740-7475}, 
month={Sept},}
@ARTICLE{4749742, 
author={P. Scully and R. Skehill and S. McGrath}, 
journal={IEEE Wireless Communications}, 
title={Mobility in an RF-isolated test platform}, 
year={2008}, 
volume={15}, 
number={6}, 
pages={8-15}, 
abstract={Understanding the practical impact of mobility in a wireless network is essential for the wireless networks of tomorrow. Mobility influences the network performance, behavior, and ability to provide seamless service across a wide area. Testing and evaluating the real impact of mobility is difficult in the field with so many variables such as interference, fading, and so on. Experimental wireless evaluation in a test environment must correspond to an actual deployment. Furthermore, it is important to achieve repeatability without sacrificing realism. This study presents a functional test platform that uses real IEEE 802.11 equipment, providing repeatability and reliability. The platform is used to test the practical impact of device movement in a WLAN cell while voice and data applications are running. The mobility characteristics of wireless devices are based on individual models used by researchers with the addition of real aspects of mobility from empirical studies to improve realism.}, 
keywords={Testing;Fading;Wireless networks;Wireless LAN;Radio transmitters;Interference;GSM;3G mobile communication;Receivers;Wireless communication}, 
doi={10.1109/MWC.2008.4749742}, 
ISSN={1536-1284}, 
month={December},}
@INPROCEEDINGS{7785748, 
author={J. Carter and W. B. Gardner}, 
booktitle={2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)}, 
title={BHive: Towards Behaviour-Driven Development Supported by B-Method}, 
year={2016}, 
volume={}, 
number={}, 
pages={249-256}, 
abstract={Behaviour-Driven Development (BDD) is an "outside-in" approach to software development built upon semi-formal mediums for specifying the behaviour of a system as it would be observed externally. Through the representation of a system as a collection of user stories and scenarios using BDD's notation, practitioners automate acceptance tests using examples of desired behaviour for the envisioned system. A formal model created in concert with BDD tests would provide valuable insight into test validity and enhance the visibility of the problem domain. This work called BHive builds upon the formal underpinnings of BDD scenarios by mapping their "Given," "When," and "Then" statements to "Precondition," "Command," and "Postcondition" constructs as introduced by Floyd-Hoare logic. We posit that this mapping allows for a B-Method representation to be created and that such a model is useful for exploring system behaviour and exposing gaps in requirements. We also outline extensions to BDD tooling required for the described integration and present benefits of the BHive approach to integrating formalism within a BDD project.}, 
keywords={formal logic;software engineering;behaviour-driven development;BHive approach;BDD;software development;B-method representation;Floyd-Hoare logic;Software;Testing;Stakeholders;Shape;Documentation;Conferences;BDD;Behaviour-Driven Development;B-Method;Agile}, 
doi={10.1109/IRI.2016.39}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7018484, 
author={B. Hois and S. Sobernig and M. Strembeck}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Natural-language scenario descriptions for testing core language models of domain-specific languages}, 
year={2014}, 
volume={}, 
number={}, 
pages={356-367}, 
abstract={The core language model is a central artifact of domain-specific modeling languages (DSMLs) as it captures all relevant domain abstractions and their relations. Natural-language scenarios are a means to capture requirements in a way that can be understood by technical as well as non-technical stakeholders. In this paper, we use scenarios for the testing of structural properties of DSML core language models. In our approach, domain experts and DSML engineers specify requirements via structured natural-language scenarios. These scenario descriptions are then automatically transformed into executable test scenarios providing forward and backward traceability of domain requirements. To demonstrate the feasibility of our approach, we used Eclipse Xtext to implement a requirements language for the definition of semi-structured scenarios. Transformation specifications generate executable test scenarios that run in our test platform which is built on the Eclipse Modeling Framework and the Epsilon language family.}, 
keywords={Testing;Unified modeling language;Vocabulary;Natural languages;Collaboration;Abstracts;Syntactics;Domain-Specific Modeling;Natural-Language Requirement;Scenario-based Testing;Metamodel Testing;Eclipse Modeling Framework;Xtext;Epsilon;EUnit}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7140371, 
author={S. Gebert and C. Schwartz and T. Zinner and P. Tran-Gia}, 
booktitle={2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
title={Continuously delivering your network}, 
year={2015}, 
volume={}, 
number={}, 
pages={766-769}, 
abstract={Softwarization and cloudification of networks through software defined networking and network functions virtualisation promise a new degree of flexibility and agility. By moving logic from device firmware into software applications and applying software development mechanisms, innovation can be introduced with less effort. Concrete ways how to operate and orchestrate such systems are not yet defined. The process of making changes to a controller software or a virtualized network function in a production network without the risk of network disruption is not covered by literature. Complexity of systems brings the risk of unexpected side-effects and has so long been a show-stopper for administrators applying changes to networking devices. This paper suggests the adaption of the successful concept of continuous delivery into the software defined networking world. Test-driven development and automatic acceptance tests demonstrate that the software engineering community already found ways to ensure that changes do not break. Applied to network engineering, the adaption of continuous delivery can be seen as an enabler for risk-free and frequent changes in production infrastructure through push button deployments.}, 
keywords={cloud computing;firmware;software defined networking;software engineering;virtualisation;network softwarization;network cloudification;software defined networking;network function virtualisation;firmware;software development mechanisms;network engineering;push button deployments;Software;Pipelines;Production;Servers;Measurement;Technological innovation}, 
doi={10.1109/INM.2015.7140371}, 
ISSN={1573-0077}, 
month={May},}
@INPROCEEDINGS{7928008, 
author={H. Ukai and X. Qu}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Test Design as Code: JCUnit}, 
year={2017}, 
volume={}, 
number={}, 
pages={508-515}, 
abstract={In a development process where testing is highly automated, there is a major challenge to cope with issues such as huge test size and test stability. In this paper, we propose a model-based testing (MBT) tool called JCUnit, which generates a test suite from a model given as a Java class. Unlike other tools, it is designed to generate small and stable test suites and supports various popular models. With this tool, developers can apply MBT approach to their products without learning domain-specific language of proprietary MBT tools. Moreover, features such as portability and pluggability make it useful in a wide range of phases from unit testing to system testing. As a result, the efforts required in practical software testing will be reduced.}, 
keywords={Java;program testing;software portability;software tools;software testing;system testing;unit testing;software pluggability;software portability;MBT tool;Java class;test suite;model-based testing;test stability;test size;test design;JCUnit;Tools;Java;Software;Graphical user interfaces;Pipelines;Software testing;automated testing;FSM spec;model-based testing;combinatorial interaction testing}, 
doi={10.1109/ICST.2017.58}, 
ISSN={}, 
month={March},}
@ARTICLE{6200023, 
author={V. Garcia-Diaz and B. C. P. G-Bustelo and O. Sanjuan-Martinez and E. R. Nunez Valdez and J. M. C. Lovelle}, 
journal={IET Software}, 
title={MCTest: towards an improvement of match algorithms for models}, 
year={2012}, 
volume={6}, 
number={2}, 
pages={127-139}, 
abstract={Owing to the increasing importance of model-driven engineering (MDE) and the changes experienced by software systems over their life cycle, the calculation, representation and visualisation of matches and differences between two different versions of the same model are becoming more necessary and useful. This study shows the need for improvement in the algorithms for calculating the relationships between models and presents a tool to test different implementations, thus reducing the effort required to measure, compare or create new algorithms. To demonstrate the need for improvement and the framework developed, the authors have created different models that conform to the metamodel of a domain-specific language. Subsequently, the authors compared these models using the algorithms of the eclipse modelling framework (EMF) Compare tool, part of the eclipse modeling project, which is the framework of reference for MDE. Thus, in the case study, the authors tool is used to measure the quality of the comparisons performed by EMF Compare.}, 
keywords={data structures;data visualisation;software engineering;specification languages;MCTest;match algorithm;model-driven engineering;software system;match calculation;match representation;match visualisation;eclipse modelling framework;EMF Compare tool}, 
doi={10.1049/iet-sen.2011.0040}, 
ISSN={1751-8806}, 
month={April},}
@INPROCEEDINGS{7886903, 
author={D. Menendez and S. Nagarakatte}, 
booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)}, 
title={Termination-Checking for LLVM Peephole Optimizations}, 
year={2016}, 
volume={}, 
number={}, 
pages={191-202}, 
abstract={Mainstream compilers contain a large number of peephole optimizations, which perform algebraic simplification of the input program with local rewriting of the code. These optimizations are a persistent source of bugs. Our recent research on Alive, a domain-specific language for expressing peephole optimizations in LLVM, addresses a part of the problem by automatically verifying the correctness of these optimizations and generating C++ code for use with LLVM. This paper identifies a class of non-termination bugs that arise when a suite of peephole optimizations is executed until a fixed point. An optimization can undo the effect of another optimization in the suite, which results in non-terminating compilation. This paper (1) proposes a methodology to detect non-termination bugs with a suite of peephole optimizations, (2) identifies the necessary condition to ensure termination while composing peephole optimizations, and (3) provides debugging support by generating concrete input programs that cause non-terminating compilation. We have discovered 184 optimization sequences, involving 38 optimizations, that cause non-terminating compilation in LLVM with Alive-generated C++ code.}, 
keywords={C++ language;program compilers;program debugging;termination-checking;LLVM peephole optimizations;nontermination bugs;nonterminating compilation;debugging;input programs;Alive-generated C++ code;Optimization;Computer bugs;C++ languages;Semantics;Concrete;Toxicology;Pattern matching;Compiler Verification;Peephole Optimization;Alive;Termination}, 
doi={10.1145/2884781.2884809}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{5190273, 
author={X. Wang and P. Xu}, 
booktitle={2009 International Conference on Information Technology and Computer Science}, 
title={Build an Auto Testing Framework Based on Selenium and FitNesse}, 
year={2009}, 
volume={2}, 
number={}, 
pages={436-439}, 
abstract={Writing auto testing is a required engineering technique that can save time and money, and help businesses better respond to changes. But if we use testing framework improperly, more problems would possibly be caused. An auto testing framework based on Selenium and FitNesse is discussed in this article which can help with those problems. The framework use Selenium APIs to get page value, DbFit to init database, FitNesse to manage the test fixture, and a special DSL to write test fixture. It could greatly reduce the line numbers of testing code and the project developing period, lower the random error rate, facilitate writing fixture table, improve the coding productivity, and the quality of final product.}, 
keywords={application program interfaces;groupware;Internet;object-oriented programming;program testing;public domain software;software engineering;FitNesse;auto testing framework;Selenium API;page value;DbFit;fixture table writing;coding productivity;Software testing;Fixtures;Java;Automatic testing;Information security;Writing;Open source software;Programming;Information technology;Computer science;Auto Testing Framework;Selenium;FitNesse}, 
doi={10.1109/ITCS.2009.228}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{5525697, 
author={E. Blondel and C. Monney}, 
booktitle={Intelec 2010}, 
title={Efficient powering of communication and IT equipments using rotating UPS}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Today, the demand for uninterrupted power supply for Telecommunication and Internet services increases drastically. Following the same trend, cooling demand explodes. Supplying sufficient power with extremely high reliability becomes even more challenging. The era of telephone exchanges using 48V power supply and 8 hours of battery backup is past. Telecom Operators are currently migrating from POTS to all IP. In addition, broadband Internet access for everyone using DSL or fibre is already reality. More and more applications like TV on demand, streaming services, online gaming or entertainment are very power hungry. For such large systems, static UPS systems are no more efficient. A better alternative is offered by rotating UPS, also known as "No-break". In addition to requiring less space and no batteries, these systems have a better power efficiency too.}, 
keywords={computer centres;computer power supplies;cooling;telecommunication power supplies;uninterruptible power supplies;communication equipment power;IT equipment power;rotating UPS;uninterrupted power supply;telecommunication power supply;Internet service power supply;cooling demand;no-break power;Uninterruptible power systems;Batteries;Alternators;Energy storage;Power quality;Power supplies;Cooling;Ventilation;Inductors;Acoustic testing}, 
doi={10.1109/INTLEC.2010.5525697}, 
ISSN={0275-0473}, 
month={June},}
@INPROCEEDINGS{5758967, 
author={E. Blondel and C. Monney}, 
booktitle={4th International Telecommunication - Energy special conference}, 
title={Efficient powering of communication and IT equipments using rotating UPS}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Today, the demand for uninterrupted power supply for Telecommunication and Internet services increases drastically. Following the same trend, cooling demand explodes. Supplying sufficient power with extremely high reliability becomes even more challenging. The era of telephone exchanges using 48V power supply and 8 hours of battery backup is past. Telecom Operators are currently migrating from POTS to all IP. In addition, broadband Internet access for everyone using DSL or fibre is already reality. More and more applications like TV on demand, streaming services, online gaming or entertainment are very power hungry. For such large systems, static UPS systems are no more efficient. A better alternative is offered by rotating UPS, also known as "No-break". In addition to requiring less space and no batteries, these systems have a better power efficiency too.}, 
keywords={Uninterruptible power systems;Electromagnetic compatibility;Batteries;Generators;Inductors;Copper}, 
doi={}, 
ISSN={}, 
month={May},}
@ARTICLE{8254315, 
author={A. Johanson and W. Hasselbring}, 
journal={Computing in Science Engineering}, 
title={Software Engineering for Computational Science: Past, Present, Future}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={While the importance of in silico experiments for the scientific discovery process increases, state-of-the-art software engineering practices are rarely adopted in computational science. To understand the underlying causes for this situation and to identify ways for improving the current situation, we conduct a literature survey on software engineering practices in computational science. As a result of our survey, we identified 13 recurring key characteristics of scientific software development that can be divided into three groups: characteristics that results (1) from the nature of scientific challenges, (2) from limitations of computers, and (3) from the cultural environment of scientific software development. Our findings allow us to point out shortcomings of existing approaches for bridging the gap between software engineering and computational science and to provide an outlook on promising research directions that could contribute to improving the current situation.}, 
keywords={Software;Scientific computing;Software engineering;Computational modeling;Computers;Productivity;Object recognition;scientific software development;domain-specific languages;software performance engineering;software testing;requirements engineering}, 
doi={10.1109/MCSE.2018.108162940}, 
ISSN={1521-9615}, 
month={},}
@INPROCEEDINGS{1387329, 
author={S. Kundu and T. M. Mak and R. Galivanche}, 
booktitle={2004 International Conferce on Test}, 
title={Trends in manufacturing test methods and their implications}, 
year={2004}, 
volume={}, 
number={}, 
pages={679-687}, 
abstract={Driven by market applications in the areas of computing, networking, storage, optical, wireless, portable, and consumer electronics, semiconductor chips today are as diverse as ever. Confluence of multiple applications and rapid integration has also driven the heterogeneity of chips. Test methods have evolved with the products. However, the basic goals in testing remain the same: quality of product, recurring and non-recurring costs and time to market. In this paper we try to catalog some commonly used test methods, identify their associated DFT requirements and trends in terms of tester requirements. Given the diversity of semiconductors chips today such as various PLDs, volatile and non-volatile memories, analog, mixed signal, FPGA, ASIC, SOC, MEMs and processors, it is impossible for a paper of this nature to be fully comprehensive. So we limit our focus on processor, ASIC and SOCs.}, 
keywords={design for testability;system-on-chip;microprocessor chips;manufacturing test methods;product quality;recurring cost;nonrecurring cost;time to market;DFT requirements;tester requirements;semiconductors chips;volatile memory;nonvolatile memory;PLD;FPGA;ASIC;SOC;MEM;processors;Manufacturing;Testing;Optical computing;Application specific integrated circuits;Semiconductor device manufacture;Computer networks;Portable computers;Optical fiber networks;Consumer electronics;Costs}, 
doi={10.1109/TEST.2004.1387329}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6676515, 
author={A. S. Nascimento and C. M. F. Rubira and F. Castor}, 
booktitle={2013 IEEE 7th International Conference on Self-Adaptive and Self-Organizing Systems}, 
title={Using CVL to Support Self-Adaptation of Fault-Tolerant Service Compositions}, 
year={2013}, 
volume={}, 
number={}, 
pages={261-262}, 
abstract={We present a dynamic software product line to support fault-tolerant service compositions. Architectural variability is specified and resolved by Common Variability Language (CVL). CVL is a generic variability modeling language that enables the transformation of a product line model into a configured, new product model. At runtime, whenever it is necessary to determine a fault tolerance technique more adapted to the context (i.e. a new product) the correspondent product model is dynamically generated by executing CVL model-to-model transformation. Based on the comparison of the reflection model with the target product model, the adaptation process is fully automated.}, 
keywords={product development;self-adjusting systems;simulation languages;software fault tolerance;software reusability;fault-tolerant service composition self-adaptation;dynamic software product line;architectural variability;Common Variability Language;generic variability modeling language;product line model;runtime;CVL model-to-model transformation;reflection model;Adaptation models;Fault tolerance;Fault tolerant systems;Software;Unified modeling language;Runtime;Quality of service;Fault-tolerant Systems;Self-Adaptation;CVL}, 
doi={10.1109/SASO.2013.34}, 
ISSN={1949-3673}, 
month={Sept},}
@ARTICLE{715185, 
author={}, 
journal={IEEE Spectrum}, 
title={What's ahead for design on the web}, 
year={1998}, 
volume={35}, 
number={9}, 
pages={53-63}, 
abstract={Panel discussion: Experts: the Web offers better design collaboration and a path toward greater tool interoperability.}, 
keywords={Internet;Computer aided engineering;Design engineering;Design automation;Computer science;Circuits;Process design;Web sites;Electrical engineering;Workstations}, 
doi={10.1109/MSPEC.1998.715185}, 
ISSN={0018-9235}, 
month={Sept},}
@INPROCEEDINGS{7107423, 
author={J. Gmeiner and R. Ramler and J. Haslinger}, 
booktitle={2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={Automated testing in the continuous delivery pipeline: A case study of an online company}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Companies running an online business need to be able to frequently push new features and bug fixes from development into production. Successful high-performance online companies deliver code changes often several times per day. Their continuous delivery model supports the business needs of the online world. At the same time, however, such practices increase the risk of introducing quality issues and unwanted side effects. Rigorous test automation is therefore a key success factor for continuous delivery. In this paper we describe how automated testing is used in the continuous delivery pipeline of an Austrian online business company. The paper illustrates the complex technical and organizational challenges involved and summarizes the lessons from more than six years of practical experience in establishing and maintaining an effective continuous delivery pipeline.}, 
keywords={electronic commerce;program debugging;program testing;automated testing;continuous delivery pipeline;Austrian online business company;bug fixing;test automation;Pipelines;Testing;Databases;Companies;Production;Software;automated testing;continuous integration;continuous delivery;continusous deployment;dev ops}, 
doi={10.1109/ICSTW.2015.7107423}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6263963, 
author={A. K. Maji and F. A. Arshad and S. Bagchi and J. S. Rellermeyer}, 
booktitle={IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012)}, 
title={An empirical study of the robustness of Inter-component Communication in Android}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={Over the last three years, Android has established itself as the largest-selling operating system for smartphones. It boasts of a Linux-based robust kernel, a modular framework with multiple components in each application, and a security-conscious design where each application is isolated in its own virtual machine. However, all of these desirable properties would be rendered ineffectual if an application were to deliver erroneous messages to targeted applications and thus cause the target to behave incorrectly. In this paper, we present an empirical evaluation of the robustness of Inter-component Communication (ICC) in Android through fuzz testing methodology, whereby, parameters of the inter-component communication are changed to various incorrect values. We show that not only exception handling is a rarity in Android applications, but also it is possible to crash the Android runtime from unprivileged user processes. Based on our observations, we highlight some of the critical design issues in Android ICC and suggest solutions to alleviate these problems.}, 
keywords={Linux;mobile computing;operating system kernels;program testing;security of data;smart phones;virtual machines;intercomponent communication;Android;operating system;smartphones;Linux-based robust kernel;security-conscious design;virtual machine;ICC;fuzz testing methodology;Androids;Humanoid robots;Smart phones;Robustness;Testing;Runtime;Receivers;android;fuzz;security;smartphone;robustness;exception}, 
doi={10.1109/DSN.2012.6263963}, 
ISSN={2158-3927}, 
month={June},}
@INPROCEEDINGS{6229805, 
author={T. W. Schiller and B. Lucia}, 
booktitle={2012 Second International Workshop on Developing Tools as Plug-Ins (TOPI)}, 
title={Playing cupid: The IDE as a matchmaker for plug-ins}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={We describe a composable, data-driven, plug-in ecosystem for IDEs. Inspired by Unix's and Windows Power-Shell's pipeline communication models, each plug-in declares data-driven capabilities. Developers can then seamlessly mix, match, and combine plug-in capabilities to produce new insight, without modifying the plug-ins. We formalize the architecture using the polymorphic lambda calculus, with special types for source and source locations; the type system prevents nonsensical plug-in combinations, and helps to inform the design of new tools and plug-ins. To illustrate the power of the formalism, we describe several synergies between existing plug-ins (and tools) made possible by the ecosystem.}, 
keywords={ecology;lambda calculus;pipeline processing;software architecture;type theory;Unix;playing cupid;IDE;plug-ins matchmaker;plug-in ecosystem;Unix;Windows;Power-Shell pipeline communication models;data-driven capability;plug-in capability;polymorphic lambda calculus;source locations;type system;nonsensical plug-in combinations;Java;Pipelines;Biological system modeling;Contracts;Cloning;Ecosystems;Debugging}, 
doi={10.1109/TOPI.2012.6229805}, 
ISSN={2327-0748}, 
month={June},}
@ARTICLE{6772080, 
author={T. J. Aprille and D. V. Gupta and P. G. St. Amand}, 
journal={The Bell System Technical Journal}, 
title={D4 Digital Channel Bank Family: Dataport  Channel units for digital data system subrates}, 
year={1982}, 
volume={61}, 
number={9}, 
pages={2721-2740}, 
abstract={The single-channel dataports are a series of D4 channel units that convert the digital signal derived from one T-facility time slot by the D4 common circuits to an appropriate format at speeds of 64, 9.6, 4.8, or 2.4 kb/s for use in the Digital Data System (DDS). They come in two formats, the first being the DDS bipolar format for 64 kb/s and the second, for the remaining three speeds, being an EIA RS-449 format. Their error-correction feature ensures 10-8error-rate performance for a 10-3error-rate transmission channel. Advances in large-scale integration (LSI) technology have allowed the packaging of all the digital circuit functions needed into the space of a single channel unit. An on-board power converter unit generates the additional current required by the dataports over that needed by regular analog channel units. The local loop side of each channel unit uses integrated technology to achieve signal equalization and timing recovery. Standard DDS remote maintenance features are provided. The dataport channel units are easily installed and removed; they supply economical digital transmission.}, 
keywords={}, 
doi={10.1002/j.1538-7305.1982.tb03449.x}, 
ISSN={0005-8580}, 
month={Nov},}
@INPROCEEDINGS{8116418, 
author={I. Jimenez and A. Arpaci-Dusseau and R. Arpaci-Dusseau and J. Lofstead and C. Maltzahn and K. Mohror and R. Ricci}, 
booktitle={2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
title={PopperCI: Automated reproducibility validation}, 
year={2017}, 
volume={}, 
number={}, 
pages={450-455}, 
abstract={This paper introduces PopperCI, a continous integration (CI) service hosted at UC Santa Cruz that allows researchers to automate the end-to-end execution and validation of experiments. PopperCI assumes that experiments follow Popper, a convention for implementing experiments and writing articles following a DevOps approach that has been proposed recently. PopperCI runs experiments on public, private or government-fundend cloud infrastructures in a fully automated way. We describe how PopperCI executes experiments and present a use case that illustrates the usefulness of the service.}, 
keywords={cloud computing;government data processing;private government-fundend cloud;automated reproducibility validation;continous integration service;UC Santa Cruz;end-to-end execution;public government-fundend cloud;PopperCI;Tools;Runtime;Measurement;Conferences;Manuals;Writing}, 
doi={10.1109/INFCOMW.2017.8116418}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6550482, 
author={S. Esnaashari and I. Welch and P. Komisarczuk}, 
booktitle={2013 27th International Conference on Advanced Information Networking and Applications Workshops}, 
title={Determining Home Users' Vulnerability to Universal Plug and Play (UPnP) Attacks}, 
year={2013}, 
volume={}, 
number={}, 
pages={725-729}, 
abstract={Universal Plug and Play (UPnP) technology is used worldwide since it has simplified the installation and management of the devices. As a result, many devices are now equipped with UPnP capabilities. Unfortunately using UPnP in home routers puts routers at risk of abuse. For example, it is easier for hackers to discover the devices and use device vulnerabilities in order to make malicious attacks to cause financial or reputation damage to the users. In this paper, we have analyzed the UPnP protocol and its different vulnerabilities. Furthermore, we have emphasized how common the problem is with the home users' devices. Hence, we suggest a tool to achieve transparency in the health of the Internet by detecting UPnP enabled devices which are likely to be attacked on home networks. The tool will look for UPnP based attacks when people's routers have been compromised. The tool is easy to install and use for novice home users and maintains their privacy too. This project aims not only to implement a tool for a user to determine whether his/her system is vulnerable to a particular attack, but also to measure the prevalence of vulnerabilities at national or global level. Thus a larger framework is required to collect and manage the results from individual users.}, 
keywords={computer crime;computer network management;computer network security;home computing;Internet;protocols;telecommunication network routing;home users vulnerability;universal plug and play attacks;UPnP attacks;UPnP technology;device installation;device management;UPnP capability;home routers;hackers;device vulnerability;malicious attacks;financial damage;reputation damage;UPnP protocol;home users devices;Internet;UPnP enabled devices;home networks;UPnP based attacks;Servers;Protocols;Ports (Computers);Logic gates;Plugs;IP networks;Internet;Security;Network Measurement;UPnP}, 
doi={10.1109/WAINA.2013.225}, 
ISSN={}, 
month={March},}
@INBOOK{5396736, 
author={Liming Xiu}, 
booktitle={VLSI Circuit Design Methodology Demystified: A Conceptual Taxonomy}, 
title={CellBased ASIC Design Methodology}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

What are the major tasks and personnel required in a chip design project?

What are the major steps in ASIC chip construction?

What is the ASIC design flow?

What are the two major aspects of ASIC design flow?

What are the characteristics of good design flow?

What is the role of market research in an ASIC project?

What is the optimal solution of an ASIC project?

What is system-level study of a project?

What are the approaches for verifying design at the system level?

What is register-transfer-level (RTL) system-level description?

What are methods of verifying design at the register-transfer-level?

What is a test bench?

What is code coverage?

What is functional coverage?

What is bug rate convergence?

What is design planning?

What are hard macro and soft macro?

What is hardware description language (HDL)?

What is register-transfer-level (RTL) description of hardware?

What is standard cell? What are the differences among standard cell, gate-array, and sea-of-gate approaches?

What is an ASIC library?

What is logic synthesis?

What are the optimization targets of logic synthesis?

What is schematic or netlist?

What is the gate count of a design?

What is the purpose of test insertion during logic synthesis?

What is the most commonly used model in VLSI circuit testing?

What are controllability and observability in a digital circuit?

What is a testable circuit?

What is the aim of scan insertion?

What is fault coverage? What is defect part per million (DPPM)?

Why is design for testability important for a product's financial success?

What is chip power usage analysis?

What are the major components of CMOS power consumption?

What is power optimization?

What is VLSI physical design?

What are the problems that make VLSI physical design so challenging?

What is floorplanning?

What is the placement process?

What is the routing process?

What is a power network?

What is clock distribution?

What are the key requirements for constructing a clock tree?

What is the difference between time skew and length skew in a clock tree?

What is scan chain?

What is scan chain reordering?

What is parasitic extraction?

What is delay calculation?

What is back annotation?

What kind of signal integrity problems do place and route tools handle?

What is cross-talk delay?

What is cross-talk noise?

What is IR drop?

What are the major netlist formats for design representation?

What is gate-level logic verification before tapeout?

What is equivalence check?

What is timing verification?

What is design constraint?

What is static timing analysis (STA)?

What is simulation approach on timing verification?

What is the logical-effort-based timing closure approach?

What is physical verification?

What are design rule check (DRC), design verification (DV), and geometry verification (GV)?

What is schematic verification (SV) or layout versus schematic (LVS)?

What is automatic test pattern generation (ATPG)?

What is tapeout?

What is yield?

What are the qualities of a good IC implementation designer?

}, 
keywords={}, 
doi={10.1002/9780470199114.ch4}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5396736},}
@INPROCEEDINGS{1607414, 
author={J. White and D. C. Schmidt}, 
booktitle={13th Annual IEEE International Symposium and Workshop on Engineering of Computer-Based Systems (ECBS'06)}, 
title={FireAnt: a tool for reducing enterprise product line architecture deployment, configuration, and testing costs}, 
year={2006}, 
volume={}, 
number={}, 
pages={2 pp.-508}, 
abstract={Product-line architectures (PLA)s are a paradigm for developing software families by customizing and composing reusable artifacts, rather than handcrafting software from scratch. Extensive testing is required to develop reliable PLAs. Each PLA may have hundreds of valid variants that can be constructed from the architecture's components. It is crucial that each of these variants be thoroughly tested to ensure the quality of these applications on multiple OS platforms and hardware configurations. Setting up test environments and running tests can become extremely complex and expensive as the number of variants and the complexity of their deployment and configuration increases. Once a variant is deemed ready for deployment and configuration in a production environment, it is crucial that these activities be done identically to the tested configurations and upholds the assumptions of the component developers. Rapidly setting up numerous distributed test environments and ensuring that they are deployed and configured correctly is hard. This poster paper presents FireAnt, which is a tool for the model-driven development (MDD) of PLA deployment plans}, 
keywords={cost reduction;program testing;software architecture;software reusability;software tools;cost reduction;enterprise product line architecture deployment;enterprise product line architecture configuration;enterprise product line architecture testing;software family development;software reusability;model-driven development;Costs;Programmable logic arrays;Computer architecture;Software reusability;Software testing;Application software;Hardware;Production;Software packages;Packaging}, 
doi={10.1109/ECBS.2006.43}, 
ISSN={}, 
month={March},}
@ARTICLE{4657364, 
author={A. Mattsson and B. Lundell and B. Lings and B. Fitzgerald}, 
journal={IEEE Transactions on Software Engineering}, 
title={Linking Model-Driven Development and Software Architecture: A Case Study}, 
year={2009}, 
volume={35}, 
number={1}, 
pages={83-93}, 
abstract={A basic premise of model driven development (MDD) is to capture all important design information in a set of formal or semi-formal models which are then automatically kept consistent by tools. The concept however is still relatively immature and there is little by way of empirically validated guidelines. In this paper we report on the use of MDD on a significant real-world project over several years. Our research found the MDD approach to be deficient in terms of modelling architectural design rules. Furthermore, the current body of literature does not offer a satisfactory solution as to how architectural design rules should be modelled. As a result developers have to rely on time-consuming and error-prone manual practices to keep a system consistent with its architecture. To realise the full benefits of MDD it is important to find ways of formalizing architectural design rules which then allow automatic enforcement of the architecture on the system model. Without this, architectural enforcement will remain a bottleneck in large MDD projects.}, 
keywords={formal verification;software architecture;systems analysis;model-driven development;software architecture;formal models;semi-formal models;architectural design rules;Joining processes;Software architecture;Computer architecture;Guidelines;Context modeling;Computer industry;Computer errors;Programming;Keyword search;Portals;Software Architecture;Model-Driven Development;Case Study Research;Software Architecture;Model-Driven Development;Case Study Research}, 
doi={10.1109/TSE.2008.87}, 
ISSN={0098-5589}, 
month={Jan},}
@INPROCEEDINGS{781322, 
author={S. P. Harbison}, 
booktitle={Proceedings 1999 Design Automation Conference (Cat. No. 99CH36361)}, 
title={System-level hardware/software trade-offs}, 
year={1999}, 
volume={}, 
number={}, 
pages={258-259}, 
abstract={Operating systems and development tools can impose overly general requirements that prevent an embedded system from achieving its hardware performance entitlement. It is time for embedded processor designers to become more involved with system software and tools.}, 
keywords={microprocessor chips;digital signal processing chips;real-time systems;operating systems (computers);instruction sets;optimising compilers;system-level hardware/software trade-offs;embedded system;instruction set architecture;real-time OS;real-time analysis;debugging;DSP;embedded processors;Hardware;Software systems;System software;Operating systems;Computer architecture;Systolic arrays;Assembly;Real time systems;Embedded software;Digital signal processing}, 
doi={10.1109/DAC.1999.781322}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6614743, 
author={C. Dorn and A. Egyed}, 
booktitle={2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE)}, 
title={Towards collaboration-centric pattern-based software development support}, 
year={2013}, 
volume={}, 
number={}, 
pages={109-112}, 
abstract={Software engineering activities tend to be loosely coupled to allow for flexibly reacting to unforeseen development complexity, requirements changes, and progress delays. This flexibility comes a the price of hidden dependencies among design and code artifacts that make it difficult or even impossible to assess change impact. Incorrect change propagation subsequently results in costly errors. This position paper proposes a novel approach based on monitoring engineering activities for subsequent high-level pattern detection. Patterns of (i) collaboration structures, (ii) temporal action sequences, and (iii) artifact consistency constraints serve as input to recommendation and automatic reconfiguration algorithms for ultimately avoiding and correcting artifact inconsistencies.}, 
keywords={groupware;software development management;collaboration centric pattern-based software development;code artifact;design artifact;engineering activity monitoring;pattern detection;collaboration structure;temporal action sequence;artifact inconsistency constraint;automatic reconfiguration algorithm;Software;Collaboration;Computer architecture;Unified modeling language;Uncertainty;Adaptation models;Software engineering;monitoring;pattern detection;software engineering;recommendation;collaboration structures}, 
doi={10.1109/CHASE.2013.6614743}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{889571, 
author={D. Federici and P. Bisgambiglia and J. -. Santucci}, 
booktitle={Proceedings IEEE International High-Level Design Validation and Test Workshop (Cat. No.PR00786)}, 
title={High level fault simulation: experiments and results on ITC'99 benchmarks}, 
year={2000}, 
volume={}, 
number={}, 
pages={118-123}, 
abstract={In this paper we present our approach for performing Behavioral Fault Simulation (BFS). This approach involves three main steps (i) the definition of an internal modeling of behavioral descriptions, and the determination of a fault model; (ii) the definition of a fault simulation technique; (iii) the implementation of this technique. Finally, this paper deals with experiments conducted on ITC'99 benchmarks in order to validate a VHDL behavioral fault simulator (BFS). The effectiveness of the BFS software is clearly demonstrated through the obtained results.}, 
keywords={fault simulation;high level synthesis;hardware description languages;high level fault simulation;ITC'99 benchmarks;behavioral fault simulation;internal modeling;behavioral descriptions;fault simulation;VHDL;Circuit faults;Circuit simulation;Circuit testing;Benchmark testing;Very large scale integration;Test pattern generators;Electrical fault detection;Fault detection;Data structures;Software tools}, 
doi={10.1109/HLDVT.2000.889571}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4293629, 
author={N. Kitiyakara and J. Graves}, 
booktitle={Agile 2007 (AGILE 2007)}, 
title={Growing a Build Management System from Seed}, 
year={2007}, 
volume={}, 
number={}, 
pages={401-407}, 
abstract={This paper describes the authors' experiences creating a full build management system from a simple version control system. We will explore how the XP values of simplicity, feedback, communication, courage and respect play into making a system that provides the developers, testers and customer with excellent value showing how various XP principles (like baby steps and mutual benefit) come into play. We will also demonstrate how we are able to remain true to our XP values while still achieving ISO 9001- 2001 and CMM Level II certifications. Finally, we compare the build management system with other systems we have encountered that were not developed in accordance with XP values.}, 
keywords={building management systems;configuration management;build management system;version control system;CMM Level II certifications;ISO 9001-2001;System testing;Communication system control;Control systems;Feedback;Pediatrics;ISO standards;Coordinate measuring machines;Certification;Control system synthesis;Writing}, 
doi={10.1109/AGILE.2007.32}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4012600, 
author={D. Ayers}, 
journal={IEEE Internet Computing}, 
title={The Shortest Path to the Future Web}, 
year={2006}, 
volume={10}, 
number={6}, 
pages={76-79}, 
abstract={This column's title could suggest that there is only one best path forward for the Web. The path begins with document metadata and travels through the world of microformats and embedded data. A waypoint is a semantic Web that leverages these approaches, along with those offered by an environment more capable of managing first-class data directly. This is only one path, however, and it probably isn't the shortest. The Internet is a rich environment with billions of active agents. Natural selection, mutation, and genetic breeding of sorts all happen to software systems, together with a significantly higher proportion of "intelligent design" than found in the real world. The net effect is that many different evolutionary paths are being explored simultaneously, and several could lead to a better Web}, 
keywords={document handling;semantic Web;document metadata;semantic Web;Access protocols;HTML;Computer networks;Humans;Semantic Web;Network servers;Web server;Pediatrics;Internet;Electronic mail;Semantic Web;Web 2.0;Web programming}, 
doi={10.1109/MIC.2006.137}, 
ISSN={1089-7801}, 
month={Nov},}
@ARTICLE{1021118, 
author={E. Chou and B. Sheu}, 
journal={IEEE Circuits and Devices Magazine}, 
title={Nanometer mixed-signal system-on-a-chip design}, 
year={2002}, 
volume={18}, 
number={4}, 
pages={7-17}, 
abstract={A mixed-signal system-on-a-chip (SoC) design methodology and the supporting CAD tools are presented. A known tools set is identified for illustration purposes and some alternative tools can equally accomplish the task.}, 
keywords={mixed analogue-digital integrated circuits;integrated circuit layout;circuit layout CAD;hardware-software codesign;product development;design for testability;mixed-signal system-on-a-chip design;CAD tools;integrated-circuit design;design methodology;nanometer system-on-a-chip;design tools;planning stage;R&D prototype stage;product development stage;testability;functional model;floating point system model;front-end design flow;bit-true model;behavioral model;layout optimization;signal integrity effects;top-down design;System-on-a-chip;Integrated circuit modeling;Design methodology;Integrated circuit testing;Circuit testing;Design automation;System testing;Research and development;Design engineering;Prototypes}, 
doi={10.1109/MCD.2002.1021118}, 
ISSN={8755-3996}, 
month={July},}
@INPROCEEDINGS{6113487, 
author={C. Ren and D. Jiang}, 
booktitle={2011 International Conference of Information Technology, Computer Engineering and Management Sciences}, 
title={A New Ontology of Resource Specification for Wireless Sensor Networks}, 
year={2011}, 
volume={2}, 
number={}, 
pages={138-140}, 
abstract={The practical usage of ontology for resource specification is for effective resource control in wireless sensor networks. In order to support geographically and logically distinct resources to be co-scheduled and co-allocated when test beds are federated, we build a new ontology. It provides a simple schema that can be used to present a clear overview of network and the relation between sensing elements. Our ontology has been actively integrated with the common-used control framework. The ontology also provides service abstraction between service providers and users, which defines generalized resource request for users to describe the desired resources on different test beds. It has been shown to be useful for describing heterogeneous networked sensing substrate and be feasible for allocating resources across various test beds.}, 
keywords={formal specification;ontologies (artificial intelligence);resource allocation;wireless sensor networks;ontology;resource specification;resource control;wireless sensor networks;coscheduling;common used control framework;service abstraction;service providers;generalized resource request;networked sensing substrate;resource allocation;Ontologies;Sensors;Substrates;XML;Wireless sensor networks;Semantics;Wireless communication;Wireless Senor Networks;Ontology;Virtulization}, 
doi={10.1109/ICM.2011.282}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6958413, 
author={J. S. Kracht and J. Z. Petrovic and K. R. Walcott-Justice}, 
booktitle={2014 14th International Conference on Quality Software}, 
title={Empirically Evaluating the Quality of Automatically Generated and Manually Written Test Suites}, 
year={2014}, 
volume={}, 
number={}, 
pages={256-265}, 
abstract={The creation, execution, and maintenance of tests are some of the most expensive tasks in software development. To help reduce the cost, automated test generation tools can be used to assist and guide developers in creating test cases. Yet, the tests that automated tools produce range from simple skeletons to fully executable test suites, hence their complexity and quality vary. This paper compares the complexity and quality of test suites created by sophisticated automated test generation tools to that of developer-written test suites. The empirical study in this paper examines ten real-world programs with existing test suites and applies two state-of-the-art automated test generation tools. The study measures the resulting test suite quality in terms of code coverage and fault-finding capability. On average, manual tests covered 31.5% of the branches while the automated tools covered 31.8% of the branches. In terms of mutation score, the tests generated by automated tools had an average mutation score of 39.8% compared to the average mutation score of 42.1% for manually written tests. Even though automatically created tests often contain more lines of source code than those written by developers, this paper's empirical results reveal that test generation tools can provide value by creating high quality test suites while reducing the cost and effort needed for testing.}, 
keywords={program testing;software maintenance;software metrics;software quality;source code (software);empirical software quality evaluation;manually written test suites;automatically generated test suites;software test maintenance;software test execution;software test creation;software development;cost reduction;automated test generation tools;test suite quality;test suite complexity;real-world programs;code coverage;fault-finding capability;average mutation score;Manuals;Complexity theory;Software;Testing;Writing;Standards;Java}, 
doi={10.1109/QSIC.2014.33}, 
ISSN={1550-6002}, 
month={Oct},}
@INPROCEEDINGS{7601515, 
author={M. Sroka and D. Fisch and R. Nagy}, 
booktitle={2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)}, 
title={Impact of crossover and mutation on reproduction in evolutionary test model learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={39-44}, 
abstract={Automation in the software test design process has a significant impact on the software testing process and therefore also on the overall software development in the industry. The focus of this paper is on the automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is briefly described. This paper further investigates the impact of reproduction configuration on the evolutionary learning method.}, 
keywords={automatic test software;automotive engineering;embedded systems;evolutionary computation;learning (artificial intelligence);program testing;crossover impact;mutation impact;evolutionary test model learning;software test design process automation;software development;test case design automation;model-based testing;automotive embedded software;evolutionary algorithm;reproduction configuration impact;Biological cells;Sociology;Statistics;Software;Testing;Software algorithms;Evolutionary computation}, 
doi={10.1109/SISY.2016.7601515}, 
ISSN={1949-0488}, 
month={Aug},}
@INBOOK{5732852, 
author={Kamran Etemad and Ming-Yee Lai}, 
booktitle={WiMAX Technology and Network Evolution}, 
title={Overview of WiMAX Network Architecture and Evolution}, 
year={2010}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Introduction

WiMAX Basic Network Reference Model

WiMAX Network Roadmap: Release 1.0, 1.5, 1.6, and 2.0

Overview of Major Features in Release 1.0

Overview of Major Features in Release 1.5

Major Features in Network Release 1.6

Comparison of Mobile WiMAX and 3GPP/SAE Network Architecture

Summary

}, 
keywords={WiMAX;network architecture;service provider's working group;network working group;residential gateways}, 
doi={10.1002/9780470633021.ch6}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5732852},}
@INPROCEEDINGS{4384854, 
author={J. Ferreira and A. Carvalho and J. Pimentel and M. Guedes and F. Furini and N. Silva}, 
booktitle={2007 5th IEEE International Conference on Industrial Informatics}, 
title={Modeling Engineering and Manufacturing Activity in Vehicle Development Process}, 
year={2007}, 
volume={2}, 
number={}, 
pages={675-680}, 
abstract={Designing and consequent assembly of a new vehicle is a complex process as it requires close coordination and inputs from a number of disciplines in developing a number of systems and sub-systems in the vehicle that should fit within the confined vehicle space, function and provide the customers an acceptable combination of all relevant vehicle attributes. Understanding how these processes interact and how they are aligned with other while they should support the tasks involved in the conception of a new vehicle at a minimum time and cost. The first step to achieve this goal is the definition of a new UML profile (called VDML, vehicle development modeling language) based on the extension mechanics of UML (industry standard language) to assist business process description and consequent improvements achieved by the high level vision. To show the benefits of this new language applied to this specific business we model the engineer and manufacturing activity process using VDML.}, 
keywords={automobile industry;manufacturing data processing;Unified Modeling Language;vehicles;engineering activity modeling;manufacturing activity modeling;vehicle development process;UML profile;VDML;vehicle development modeling language;business process description;automotive industry;Automotive engineering;Manufacturing processes;Virtual manufacturing;Space vehicles;Unified modeling language;Process design;Assembly systems;Costs;Manufacturing industries;Standards development}, 
doi={10.1109/INDIN.2007.4384854}, 
ISSN={1935-4576}, 
month={June},}
@ARTICLE{4042539, 
author={B. Beckert and T. Hoare and R. Hahnle and D. R. Smith and C. Green and S. Ranise and C. Tinelli and T. Ball and S. K. Rajamani}, 
journal={IEEE Intelligent Systems}, 
title={Intelligent Systems and Formal Methods in Software Engineering}, 
year={2006}, 
volume={21}, 
number={6}, 
pages={71-81}, 
abstract={Over the last few years, technologies for the formal description, construction, analysis, and validation of software - based mostly on logics and formal reasoning - have matured. We can expect them to complement and partly replace traditional software engineering methods in the future. Formal methods in software engineering are an increasingly important application area for intelligent systems. The field has outgrown the area of academic case studies, and industry is showing serious interest. We convincingly argue that we've reached the point where we can solve the problem of how to formally verify industrial-scale software. We propose program verification as a computer science Grand Challenge. Deductive software verification is a core technology of formal methods. We describe recent dramatic changes in the way it's perceived and used. Another important base technique of formal methods, besides software verification, is synthesizing software that's correct by construction because it's formally derived from its specification. We discuss recent developments and trends in this area. Surprisingly efficient decision procedures for the satisfiability modulo theories problem have recently emerged. We explain these techniques and why they're important for all formal-methods tools. We look at formal methods from an industry perspective. We explain the success of Microsoft Research's SLAM project, which has developed a verification tool for device drivers}, 
keywords={formal logic;formal specification;inference mechanisms;knowledge based systems;program verification;software tools;intelligent system;formal method;software engineering;formal software description;software construction;software analysis;software validation;formal logic;formal reasoning;industrial-scale formal software verification;program verification;deductive software verification;software synthesis;software specification;decision procedure;satisfiability modulo theories problem;Microsoft Research SLAM project;verification tool;device driver;Intelligent systems;Software engineering;Costs;Application software;Programming profession;Computer science;Computer errors;Humans;Energy management;Financial management;formal methods;software engineering;program verification;deductive software verification;satisfiability modulo theories;software synthesis}, 
doi={10.1109/MIS.2006.117}, 
ISSN={1541-1672}, 
month={Nov},}
@INPROCEEDINGS{6912285, 
author={R. Wohlrab and T. de Gooijer and A. Koziolek and S. Becker}, 
booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)}, 
title={Experience of pragmatically combining RE methods for performance requirements in industry}, 
year={2014}, 
volume={}, 
number={}, 
pages={344-353}, 
abstract={To meet end-user performance expectations, precise performance requirements are needed during development and testing, e.g., to conduct detailed performance and load tests. However, in practice, several factors complicate performance requirements elicitation: lacking skills in performance requirements engineering, outdated or unavailable functional specifications and architecture models, the specification of the system's context, lack of experience to collect good performance requirements in an industrial setting with very limited time, etc. From the small set of available non-functional requirements engineering methods, no method exists that alone leads to precise and complete performance requirements with feasible effort and which has been reported to work in an industrial setting. In this paper, we present our experiences in combining existing requirements engineering methods into a performance requirements method called PROPRE. It has been designed to require no up-to-date system documentation and to be applicable with limited time and effort. We have successfully applied PROPRE in an industrial case study from the process automation domain. Our lessons learned show that the stakeholders gathered good performance requirements which now improve performance testing.}, 
keywords={formal specification;program testing;software architecture;RE methods;end-user performance expectations;software development;performance requirements elicitation;performance requirements engineering;functional specifications;architecture models;system context specification;industrial setting;nonfunctional requirements engineering methods;PROPRE;process automation domain;performance testing;Measurement;Context;Time factors;Documentation;Adaptation models;Throughput;Testing}, 
doi={10.1109/RE.2014.6912285}, 
ISSN={1090-705X}, 
month={Aug},}
@INPROCEEDINGS{5771265, 
author={A. Madhavapeddy and S. Singh}, 
booktitle={2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines}, 
title={Reconfigurable Data Processing for Clouds}, 
year={2011}, 
volume={}, 
number={}, 
pages={141-145}, 
abstract={Reconfigurable computing in the cloud helps to solve many practical problems relating to scaling out data-centers where computation is limited by energy consumption or latency. However, for reconfigurable computing in the cloud to become practical several research challenges have to be addressed. This paper identifies some of the perquisites for reconfigurable computing systems in the cloud and picks out several scenarios made possible with immense cloud-based computing capability.}, 
keywords={cloud computing;reconfigurable architectures;reconfigurable data processing;reconfigurable computing;cloud-based computing;Field programmable gate arrays;Cloud computing;USA Councils;Hardware;Computational modeling;Programming;Operating systems;reconfigurable computing;cloud computing}, 
doi={10.1109/FCCM.2011.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7133545, 
author={J. Cheng and Y. Zhu and T. Zhang and C. Zhu and W. Zhou}, 
booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
title={Mobile Compatibility Testing Using Multi-objective Genetic Algorithm}, 
year={2015}, 
volume={}, 
number={}, 
pages={302-307}, 
abstract={Mobile compatibility testing has been identified as one urgent and challenging issue. Mobile apps are expected to work on thousand kinds of mobile devices with diverse device features and mobile platforms. So mobile compatibility testing is complex and costly, it is impossible to test mobile apps on all mobile devices and in all environments with limited test resources. Then the question is how to select test devices in cost-effective mobile app compatibility testing. This paper proposes a novel test device selection approach using multi-objective genetic algorithm. Using the proposed approach, the minimum number of mobile devices is selected, and the multiple test coverage requirements are met simultaneously. Furthermore, the case study results have successfully demonstrated that the proposed approach is effective for mobile compatibility testing.}, 
keywords={genetic algorithms;mobile computing;program testing;cost-effective mobile app compatibility testing;multiobjective genetic algorithm;mobile devices;diverse device features;mobile platforms;limited test resources;test device selection approach;multiple test coverage requirements;Mobile communication;Testing;Mobile handsets;Biological cells;Genetic algorithms;Sociology;Statistics;software testing;mobile testing;compatibility testing;clustering algorithm;test coverage}, 
doi={10.1109/SOSE.2015.36}, 
ISSN={}, 
month={March},}
@ARTICLE{4814954, 
author={P. Liggesmeyer and M. Trapp}, 
journal={IEEE Software}, 
title={Trends in Embedded Software Engineering}, 
year={2009}, 
volume={26}, 
number={3}, 
pages={19-25}, 
abstract={Software's importance in the development of embedded systems has been growing rapidly over the last 20 years. Because of current embedded systems' complexity, they require sophisticated engineering methods for systematically developing high-quality software. Embedded software development differs from IT system development in several ways. For example, IT systems developers can use standard hardware and software platforms and don't face the resource requirements that embedded systems developers must take into account. To meet embedded software's extrafunctional requirements, embedded systems development is shifting from programming to model-driven development. Another important trend is the emphasis on the quality assurance of safety-related systems.}, 
keywords={embedded systems;software quality;embedded software engineering;embedded systems;embedded software development;IT system development;safety-related systems;quality assurance;Embedded software;Object oriented modeling;Mathematical model;Embedded system;Hardware;IEC standards;Costs;Automotive engineering;Computer languages;Operating systems;embedded systems development;model-driven development;embedded software;quality assurance;safety-critical systems}, 
doi={10.1109/MS.2009.80}, 
ISSN={0740-7459}, 
month={May},}
@INPROCEEDINGS{843835, 
author={A. Bommireddy and J. Khare and S. Shaikh and S. -. Su}, 
booktitle={Proceedings 18th IEEE VLSI Test Symposium}, 
title={Test and debug of networking SoCs-a case study}, 
year={2000}, 
volume={}, 
number={}, 
pages={121-126}, 
abstract={This paper describes the test challenges faced and testability features implemented on Level One's networking System on Chip (SoC), IXE2000. The IXE2000 SoC is a 20+ million transistor Layer 2/3/4 Switch with 24 10/100 Mbps and 2 1000 Mbps Ethernet ports, and a predominantly IP-based design. The chip had constraints in terms of both design time and total system costs, which added an extra burden on test. The paper discusses how these constraints led to the current testability solutions and debug features on the chip.}, 
keywords={application specific integrated circuits;design for testability;local area networks;integrated circuit testing;built-in self test;logic testing;automatic testing;networking SoCs;testability features;Level One;IXE2000;Layer 2/3/4 Switch;Ethernet ports;IP-based design;design time;total system costs;testability solutions;debug features;10 to 1000 Mbit/s;Computer aided software engineering;Switches;Clocks;System testing;Ethernet networks;System-on-a-chip;Costs;Business communication;Design for manufacture;IP networks}, 
doi={10.1109/VTEST.2000.843835}, 
ISSN={1093-0167}, 
month={April},}
@INPROCEEDINGS{8257807, 
author={P. Perera and R. Silva and I. Perera}, 
booktitle={2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
title={Improve software quality through practicing DevOps}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={DevOps is extended from certain agile practices with a mix of patterns intended to improve collaboration between development and operation teams. The main purpose of this paper is to conduct a study on how DevOps practice has impacted to software quality. The secondary objective is to find how to improve quality efficiently. A literature survey has carried out to explore about current DevOps practices in industry. According to the literature survey, the conceptual research model was developed and five hypotheses were derived. Research objectives were accomplished by testing hypotheses using Pearson correlation. A linear model is derived based on the linear regression analysis. An online questionnaire was used to collect quantitative data whereas interviews with experts on DevOps and Quality assurance have been used to identify how to improve the quality of software by practicing DevOps. Recommendations are given based on interview feedback, hypotheses testing with regression analysis. According to the quantitative study, researchers have identified that quality of the software gets improved when practice DevOps by following CAMS (Culture, Automation, Measurement, Sharing) framework. Automation is the most critical factor to improve the software quality. As per the results of multiple regression analysis, it has proved culture, automation, measurement and sharing are important factors to consider to improve quality of the software. In conclusion it can be recommended to use DevOps to achieve high quality software.}, 
keywords={regression analysis;software development management;software prototyping;software quality;agile practices;operation teams;DevOps practice;conceptual research model;research objectives;linear regression analysis;Quality assurance;practice DevOps;high quality software;software quality;quantitative data collection;Companies;Automation;Software quality;Testing;Software measurement;DevOps;CAMS Framework;Quality;ISO 9126;Automation}, 
doi={10.1109/ICTER.2017.8257807}, 
ISSN={2472-7598}, 
month={Sept},}
@INPROCEEDINGS{7589821, 
author={C. M. Tang and J. Keung and Y. T. Yu and W. K. Chan}, 
booktitle={2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
title={DFL: Dual-Service Fault Localization}, 
year={2016}, 
volume={}, 
number={}, 
pages={412-422}, 
abstract={In engineering a service, software developers often construct and deploy a newer (forthcoming) version of the service to replace the current version. A forthcoming version is often placed online for users to consume and report feedback. In the case of observed failures, the forthcoming version should be debugged and further evolved. In this paper, we propose the model of dual-service fault localization (DFL) to aid this evolution process. Many prior research studies on spectrum-based fault localization (SBFL) consider each version separately. The DFL model correlates the dynamic execution spectra of the current and the forthcoming versions of the same service placed for live test of the forthcoming version, and dynamically generates an adaptive fault localization formula to estimate the code regions in the forthcoming service responsible for the observed failures. We report an experiment in which we initialized the DFL model into six instances, each using an ensemble technique dynamically composed from 11 existing SBFL formulas, and applied the model to four benchmarks. The results show that DFL is feasible and multiple instances are statistically more effective than, if not as effective as, the best of these individual SBFL formulas on each benchmark.}, 
keywords={program debugging;software maintenance;dual-service fault localization;evolution process;DFL model;dynamic execution spectra;adaptive fault localization;code region estimation;ensemble technique;SBFL formulas;spectrum-based fault localization;Software;Debugging;Computer bugs;Adaptation models;Benchmark testing;Production;Companies;debugging;spectrum-based fault localization;ensemble techniques;dual-service fault localization}, 
doi={10.1109/QRS.2016.53}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{794352, 
author={B. Smith and W. Millar and J. Dunphy and Yu-Wen Tung and P. Nayak and E. Gamble and M. Clark}, 
booktitle={1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403)}, 
title={Validation and verification of the remote agent for spacecraft autonomy}, 
year={1999}, 
volume={1}, 
number={}, 
pages={449-468 vol.1}, 
abstract={The six-day Remote Agent Experiment (RAX) on the Deep Space 1 mission will be the first time that an artificially intelligent agent will control a NASA spacecraft. Successful completion of this experiment will open the way for AI-based autonomy technology on future missions. An important validation objective for RAX is implementation of a credible validation and verification strategy for RAX that also "scales up" to missions that make full use of spacecraft autonomy. Autonomous flight software presents novel and difficult testing challenges that traditional flight software (FSW) does not face. Since autonomous software must respond robustly in an immense number of situations, the all-paths testing approaches used for traditional FSW is not feasible. Instead, we advocate a combination of scenario-based testing and model-based validation. This paper describes the testing challenges faced by autonomous spacecraft commanding software, discusses the testing strategies and model-validation methods that we found effective for RAX, and argues that these methods will "scale up" to missions that make full use of spacecraft autonomy. Among the key challenges for validating autonomous systems such as the RAX are ensuring adequate coverage for scenario-based tests, developing methods for specifying the expected behavior, and developing automated tools for verifying the observed behavior against those specifications. Another challenge, also faced by traditional FSW, is the scarcity of high-fidelity test-beds. The test plan must be designed to take advantage of lower-fidelity test-beds without compromising test effectiveness.}, 
keywords={aerospace computing;program testing;program verification;automatic test software;space research;aerospace expert systems;knowledge verification;software architecture;remote agent;spacecraft autonomy;Deep Space 1 mission;artificially intelligent agent;NASA spacecraft;verification;autonomous flight software;autonomous software;scenario-based testing;model-based validation;test effectiveness;Space vehicles;Space technology;Software testing;Automatic testing;Space missions;Propulsion;Laboratories;Intelligent agent;NASA;Robustness}, 
doi={10.1109/AERO.1999.794352}, 
ISSN={}, 
month={March},}
@INBOOK{5238155, 
author={Mostafa Hashem Sherif}, 
booktitle={Managing Projects in Telecommunication Services}, 
title={Standards and Innovation in Telecommunication Services}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

The Two Dimensions of Telecommunication Projects

Innovation in Telecommunication Services

Phasic Relation Between Equipment and Services

Standardization for Telecommunication Services

Summary

}, 
keywords={}, 
doi={10.1002/9780470047682.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5238155},}
@ARTICLE{182763, 
author={}, 
journal={IEEE Std 610}, 
title={IEEE Standard Computer Dictionary: A Compilation of IEEE Standard Computer Glossaries}, 
year={1991}, 
volume={}, 
number={}, 
pages={1-217}, 
abstract={Identifies terms currently in use in the computer field. Standard definitions for thoseterms are established. Compilation of IEEE Stds IEEE Std 1084, IEEE Std 610.2, IEEE Std 610.3, IEEE Std 610.4, IEEE Std 610.5 and IEEE Std 610.12}, 
keywords={glossaries;dictionary;glossary;computer field;definitions;Terminology;terminology;computer;applications;glossary;definitions;dictionary;610}, 
doi={10.1109/IEEESTD.1991.106963}, 
ISSN={}, 
month={Jan},}
@INBOOK{8040815, 
author={Sauming Pang}, 
booktitle={Successful Service Design for Telecommunications: A comprehensive guide to design and implementation}, 
title={Glossary}, 
year={2009}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={10.1002/9780470741207.gloss}, 
ISSN={}, 
publisher={Wiley}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8040815},}
@INPROCEEDINGS{217601, 
author={M. R. Lyu and J. -. Chen and A. Avizienis}, 
booktitle={[1992] Proceedings. The Sixteenth Annual International Computer Software and Applications Conference}, 
title={Software diversity metrics and measurements}, 
year={1992}, 
volume={}, 
number={}, 
pages={69-78}, 
abstract={The authors define and formalize the concept of software diversity which characterizes N-Version software (NVS) from four different points of view that are designated as structural diversity, fault diversity, tough-spot diversity, and failure diversity. The goals are to find a way to quantify software diversity and to investigate the measurements which can be applied during the life cycle of NVS to gain confidence that operation will be dependable when NVS is actually used. The versions from a six-language N-Version programming project for fault-tolerant flight control software were used in the software diversity measurement.<<ETX>>}, 
keywords={software metrics;software reliability;software diversity metrics;N-Version software;structural diversity;fault diversity;tough-spot diversity;failure diversity;fault-tolerant flight control software;Software measurement;Fault tolerant systems;Aerospace control;Fault tolerance;Gain measurement;Functional programming;Error correction;Delay;Software algorithms;Quality control}, 
doi={10.1109/CMPSAC.1992.217601}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{878373, 
author={J. E. Polk and R. Y. Kakuda and J. R. Anderson and J. R. Brophy and V. K. Rawlin and J. Sovey and J. Hamley}, 
booktitle={2000 IEEE Aerospace Conference. Proceedings (Cat. No.00TH8484)}, 
title={In-flight performance of the NSTAR ion propulsion system on the Deep Space One mission}, 
year={2000}, 
volume={4}, 
number={}, 
pages={123-148 vol.4}, 
abstract={Deep Space 1 is the first interplanetary spacecraft to use an ion propulsion system for the primary delta-v maneuvers. The purpose of the mission is to validate a number of technologies, including ion propulsion and a high degree of spacecraft autonomy, on a flyby of an asteroid and two comets. The ion propulsion system has operated for a total of 3500 hours at engine power levels ranging from 0.48 to 1.94 kW and has completed the encounter with the asteroid 1992KD and the first set of deterministic burns required for a 2001 encounter with comet Wilson-Harrington. The system has worked extremely well after an initial grid short was cleared after launch. Operation during this primary mission phase has demonstrated all ion propulsion system and autonomous navigation functions. All propulsion system operating parameters are very close to the expected values with the exception of the thrust at higher power levels, which is about 2 percent lower than that calculated from the electrical parameters. This paper provides an overview of the system and presents the first flight validation data on an ion propulsion system in interplanetary space.}, 
keywords={aerospace propulsion;ion engines;space vehicles;navigation;aerospace control;NSTAR ion propulsion system;in-flight performance;Deep Space 1 mission;interplanetary spacecraft;primary delta-v maneuvers;spacecraft autonomy;asteroid flyby;comets flyby;deterministic burns;primary mission phase;autonomous navigation function;operating parameters;flight validation data;ion thruster;throttle table;decontamination;3500 hour;0.48 to 1.94 kW;Propulsion;Space missions;Space technology;Space vehicles;Engines;NASA;Xenon;Instruments;Control systems;Feeds}, 
doi={10.1109/AERO.2000.878373}, 
ISSN={1095-323X}, 
month={March},}
@INPROCEEDINGS{7755285, 
author={M. Jain and D. Gopalani}, 
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, 
title={Testing application security with aspects}, 
year={2016}, 
volume={}, 
number={}, 
pages={3161-3165}, 
abstract={For the purpose of security of the computer systems, organizations now a days plan a lot of things like firewalls, network scanning tools, secure sockets layer (SSL) etc. However security bugs present at the application layer (code level) caused by unawareness or mistakes of the developers are usually ignored. Such security bugs can lead to unauthorized privileges on a computer system. For example most web applications connect back to databases which contain sensitive information. Malicious input can allow the attacker to alter the flow of the web application and provide unauthorized access to the confidential database. Hence proper security tests are required to be conducted in order to assess the security of applications. In this paper, we propose the use of Aspect Oriented Programming (AOP) for the purpose of security testing of Java applications. With the examples of fuzz testing and servlet testing using aspects, we will show how AOP can be used for detection of security bugs in Java applications by creeping inside the program without making any changes to the source code.}, 
keywords={aspect-oriented programming;authorisation;Internet;Java;program debugging;program testing;source code (software);source code;Java application security bug detection;servlet testing;fuzz testing;AOP;aspect oriented programming;confidential database;unauthorized access;Web applications;code level security bugs;application layer security bugs;SSL;secure socket layer;network scanning tools;firewalls;computer system security;application security testing;Security;Testing;Java;Databases;Computer bugs;HTML;Programming;Aspect Oriented Programming;Security Testing;Software Testing;Aspects;AOP}, 
doi={10.1109/ICEEOT.2016.7755285}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7546497, 
author={G. Argyros and I. Stais and A. Kiayias and A. D. Keromytis}, 
booktitle={2016 IEEE Symposium on Security and Privacy (SP)}, 
title={Back in Black: Towards Formal, Black Box Analysis of Sanitizers and Filters}, 
year={2016}, 
volume={}, 
number={}, 
pages={91-109}, 
abstract={We tackle the problem of analyzing filter and sanitizer programs remotely, i.e. given only the ability to query the targeted program and observe the output. We focus on two important and widely used program classes: regular expression (RE) filters and string sanitizers. We demonstrate that existing tools from machine learning that are available for analyzing RE filters, namely automata learning algorithms, require a very large number of queries in order to infer real life RE filters. Motivated by this, we develop the first algorithm that infers symbolic representations of automata in the standard membership/equivalence query model. We show that our algorithm provides an improvement of x15 times in the number of queries required to learn real life XSS and SQL filters of popular web application firewall systems such as mod-security and PHPIDS. % Active learning algorithms require the usage of an equivalence oracle, i.e. an oracle that tests the equivalence of a hypothesis with the target machine. We show that when the goal is to audit a target filter with respect to a set of attack strings from a context free grammar, i.e. find an attack or infer that none exists, we can use the attack grammar to implement the equivalence oracle with a single query to the filter. Our construction finds on average 90% of the target filter states when no attack exists and is very effective in finding attacks when they are present. For the case of string sanitizers, we show that existing algorithms for inferring sanitizers modelled as Mealy Machines are not only inefficient, but lack the expressive power to be able to infer real life sanitizers. We design two novel extensions to existing algorithms that allow one to infer sanitizers represented as single-valued transducers. Our algorithms are able to infer many common sanitizer functions such as HTML encoders and decoders. Furthermore, we design an algorithm to convert the inferred models into BEK programs, which allows for further applications such as cross checking different sanitizer implementations and cross compiling sanitizers into different languages supported by the BEK backend. We showcase the power of our techniques by utilizing our black-box inference algorithms to perform an equivalence checking between different HTML encoders including the encoders from Twitter, Facebook and Microsoft Outlook email, for which no implementation is publicly available.}, 
keywords={automata theory;context-free grammars;firewalls;hypermedia markup languages;information filtering;Internet;learning (artificial intelligence);query processing;social networking (online);SQL;black box analysis;filter programs;sanitizer programs;regular expression filters;RE filters;string sanitizers;machine learning;automata learning algorithms;membership/equivalence query model;real life XSS;SQL filters;Web application;firewall systems;mod-security;PHPIDS;context free grammar;Mealy machines;equivalence checking;HTML encoders;Twitter;Facebook;Microsoft Outlook email;Algorithm design and analysis;Machine learning algorithms;Learning automata;Transducers;Grammar;HTML;Inference algorithms;sanitizers;filters;automata;learning;web security}, 
doi={10.1109/SP.2016.14}, 
ISSN={2375-1207}, 
month={May},}
@INPROCEEDINGS{1541840, 
author={M. Kajko-Mattsson and P. Meyer}, 
booktitle={2005 International Symposium on Empirical Software Engineering, 2005.}, 
title={Evaluating the acceptor side of EM/sup 3/: release management at SAS}, 
year={2005}, 
volume={}, 
number={}, 
pages={10 pp.-}, 
abstract={Today, there are no detailed standard process models encompassing the overall release management activities. To remedy this, we have created an individual release management process model, called EM/sup 3/: release management. In this paper, we evaluate its acceptor side against an industrial release management process performed at Scandinavian Airline Systems (SAS). We have observed some similarities and differences. Some of the observed differences provide feedback for the improvement and further extension of EM/sup 3/: release management.}, 
keywords={software maintenance;travel industry;software development management;EM/sup 3/ release management;acceptor side evaluation;Scandinavian Airline Systems;standard process model;industrial release management process;software maintenance;Synthetic aperture sonar;Technology management;Control systems;Software maintenance;Laboratories;Software standards;Performance evaluation;Feedback;Software systems;Lead}, 
doi={10.1109/ISESE.2005.1541840}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{1498464, 
author={J. Ott and D. Kutscher}, 
booktitle={Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Communications Societies.}, 
title={A disconnection-tolerant transport for drive-thru Internet environments}, 
year={2005}, 
volume={3}, 
number={}, 
pages={1849-1862 vol. 3}, 
abstract={Today's mobile, wireless, and ad-hoc communications often exhibit extreme characteristics challenging assumptions underlying the traditional way of end-to-end communication protocol design in the Internet. One specific scenario is Internet access from moving vehicles on the road as we are researching in the drive-thru Internet project. Using wireless LAN as a broadly available access technology leads to intermittent - largely unpredictable and usually short-lived - connectivity, yet providing high performance while available. To allow Internet applications to deal reasonably well with such intermittent connectivity patterns, we have introduced a supportive drive-thru architecture. A key component is a "session" protocol offering persistent end-to-end communications even in the presence of interruptions. In this paper, we present the design of the persistent connectivity management protocol (PCMP) and report on findings from our implementation.}, 
keywords={Internet;mobile radio;protocols;computer network management;mobile computing;disconnection-tolerant transport;drive-thru Internet;ad-hoc communication;wireless communication;mobile communication;end-to-end communication protocol;wireless LAN;connectivity pattern;session protocol;persistent connectivity management protocol;Internet;Access protocols;Mobile communication;Transport protocols;Road vehicles;Wireless LAN;Wireless networks;Performance loss;Wireless application protocol;Vehicle driving}, 
doi={10.1109/INFCOM.2005.1498464}, 
ISSN={0743-166X}, 
month={March},}
@INPROCEEDINGS{7980399, 
author={A. Contan and L. Miclea and C. Dehelean}, 
booktitle={2017 14th International Conference on Engineering of Modern Electric Systems (EMES)}, 
title={Automated testing framework development based on social interaction and communication principles}, 
year={2017}, 
volume={}, 
number={}, 
pages={136-139}, 
abstract={The speed of development of the IT industry as well as the computational power which are increasing exponentially, create great competitiveness in the process of development but also in the launching of software products on the market. Automated testing comes to help with these challenges by trying to increase the speed of development by offering fast feedback and trustworthy quality by means of repeated runs of the implemented tests. This isn't a problem just on a technical level, but also on a social level, especially in the area of communication and understanding the requirements of the client. This work presents the implementation of an automated testing framework which also addresses the social problems. BDD or Behavior Driven Development includes an approach which would like to line up the area of client requests to the technical area, offering a uniform platform of collaboration and development. The implementation of this principle is applied in an MVP (Minimum Viable Product) type project which is meant to demonstrate the technical solution which may draw together, both socially and communication wise, the business teams and the technical implementation teams.}, 
keywords={program testing;software development management;software quality;automated testing framework development;social interaction;communication principles;IT industry;computational power;software products;trustworthy quality;fast feedback;social level;BDD;automated testing framework;behavior driven development;MVP type project;minimum viable product;business teams;technical implementation teams;Testing;Software;Business;Documentation;Collaboration;Automation;Libraries;testing process;BDD;automated testing;Gherkin language}, 
doi={10.1109/EMES.2017.7980399}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1677379, 
author={C. Smidts}, 
booktitle={RAMS '06. Annual Reliability and Maintainability Symposium, 2006.}, 
title={Research in software reliability engineering}, 
year={2006}, 
volume={}, 
number={}, 
pages={228-233}, 
abstract={Our research has focused on development of an approach to predicting software reliability based on a systematic identification of software process failure modes and their likelihoods. A direct consequence of the approach and its supporting data collection efforts is the identification of weak areas in the software development process. A Bayes framework for the quantification of software process failure mode probabilities can be useful since it allows use of historical data that are only partially relevant to the software at hand. The approach has been applied in the context of a waterfall life-cycle and for failure modes related to the requirements phase}, 
keywords={failure analysis;software development management;software reliability;software reliability engineering;systematic identification;software process failure modes;software development process;Bayes framework;waterfall life-cycle;Software reliability;Reliability engineering;Programming;Risk management;Predictive models;Software tools;Software testing;Mechanical engineering;Computer science education;Educational programs}, 
doi={10.1109/RAMS.2006.1677379}, 
ISSN={0149-144X}, 
month={Jan},}
@INPROCEEDINGS{879426, 
author={Yu-Wen Tung and W. S. Aldiwan}, 
booktitle={2000 IEEE Aerospace Conference. Proceedings (Cat. No.00TH8484)}, 
title={Automating test case generation for the new generation mission software system}, 
year={2000}, 
volume={1}, 
number={}, 
pages={431-437 vol.1}, 
abstract={The significant expansion of autonomous control and information processing capabilities in the coming generation of mission software systems results in a qualitatively larger space of behaviors that needs to be "covered" during testing, not only at the system level but also at subsystem and unit levels. A major challenge in this area is to automatically generate a relatively small set of test cases that, collectively, guarantees a selected degree of coverage of the behavior space. This paper describes an algorithm for a parametric test case generation tool that applies a combinatorial design approach to the selection of candidate test cases. Evaluation of this algorithm on test parameters from the Deep Space One mission reveals a valuable reduction in the number of test cases, when compared to an earlier home-brewed generator.}, 
keywords={automatic test pattern generation;aerospace expert systems;program testing;software tools;software agents;combinatorial mathematics;new generation mission software system;test case generation automation;autonomous control;selected degree of coverage;behavior space;parametric test case generation tool;algorithm;combinatorial design approach;Deep Space One mission reveals;software testing;remote agent experiment;unit testing harness;reusable software component;seed cases;Automatic testing;Space missions;Automatic generation control;Process control;Control systems;Information processing;Software systems;Software testing;System testing;Algorithm design and analysis}, 
doi={10.1109/AERO.2000.879426}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{1541696, 
author={F. Karayannis and J. Serrat-Fernandez and J. Baliosian and J. Rubio-Loyola and K. G. Vaxevanakis and G. Pagomenos and T. B. Zahariadis}, 
journal={IEEE Communications Magazine}, 
title={In-field evaluation of a managed IP/MPLS over WDM provisioning solution}, 
year={2005}, 
volume={43}, 
number={11}, 
pages={S26-S33}, 
abstract={This article demonstrates results and experiences gained in the area of multilayer internetworking, with emphasis on bandwidth on-demand provisioning as well as resource and restoration management. Behavioral characteristics and numerical results were obtained from a management. system prototype implemented and tested in an appropriately adapted commercial WDM environment enhanced with multivendor gigabit IP routers. The management solution, the testbed environment, and a representative evaluation scenario are presented as a means of explaining in detail the results that finally allow a global system assessment.}, 
keywords={computer network management;IP networks;multiprotocol label switching;wavelength division multiplexing;optical fibre networks;bandwidth allocation;telecommunication network routing;managed IP-MPLS;WDM provisioning solution;multilayer internetworking;bandwidth on-demand provisioning;restoration management;multivendor Gigabit IP routers;Multiprotocol label switching;Wavelength division multiplexing;Telecommunication network management;Testing;Optical scattering;Biomedical optical imaging;Asynchronous transfer mode;Information management;Inventory management;Synchronous digital hierarchy}, 
doi={10.1109/MCOM.2005.1541696}, 
ISSN={0163-6804}, 
month={Nov},}
@ARTICLE{1075437, 
author={K. Maxham and J. Dugan and M. McDonald and C. Hogge}, 
journal={Journal of Lightwave Technology}, 
title={1.13-Gbit/lightwave transmission system}, 
year={1987}, 
volume={5}, 
number={10}, 
pages={1510-1517}, 
abstract={A new high capacity lightwave transmission system has been developed using GaAs semicustom logic arrays and a DFB single-mode laser, and is presently in production. The architecture of this product is designed for in-service upgrade of a 565-Mbit/s product. This paper reviews the technical characteristics and design considerations of the Rockwell LTS-21130 lightwave transmission system.}, 
keywords={Distributed feedback (DFB) lasers;Logic arrays;Optical fiber communication;Optical fiber transmitters, lasers;Optical transmitters;Protection;Gallium arsenide;Multiplexing;Circuits;Condition monitoring;Logic arrays;Production systems;High speed optical techniques;Optical receivers}, 
doi={10.1109/JLT.1987.1075437}, 
ISSN={0733-8724}, 
month={October},}
@INPROCEEDINGS{6912254, 
author={P. Pruski and S. Lohar and R. Aquanette and G. Ott and S. Amornborvornwong and A. Rasin and J. Cleland-Huang}, 
booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)}, 
title={TiQi: Towards natural language trace queries}, 
year={2014}, 
volume={}, 
number={}, 
pages={123-132}, 
abstract={One of the surprising observations of traceability in practice is the under-utilization of existing trace links. Organizations often create links in order to meet compliance requirements, but then fail to capitalize on the potential benefits of those links to provide support for activities such as impact analysis, test regression selection, and coverage analysis. One of the major adoption barriers is caused by the lack of accessibility to the underlying trace data and the lack of skills many project stakeholders have for formulating complex trace queries. To address these challenges we introduce TiQi, a natural language approach, which allows users to write or speak trace queries in their own words. TiQi includes a vocabulary and associated grammar learned from analyzing NL queries collected from trace practitioners. It is evaluated against trace queries gathered from trace practitioners for two different project environments.}, 
keywords={grammars;natural language processing;query processing;regression analysis;natural language trace queries;TiQi;trace links;organizations;compliance requirements;impact analysis;test regression selection;coverage analysis;trace data;natural language approach;associated grammar;Unified modeling language;Databases;Vocabulary;Natural languages;Software;Speech;Hazards;Traceability;Queries;Speech Recognition;Natural Language Processing}, 
doi={10.1109/RE.2014.6912254}, 
ISSN={1090-705X}, 
month={Aug},}
@INPROCEEDINGS{4626838, 
author={R. Lutz}, 
booktitle={2008 12th International Software Product Line Conference}, 
title={Enabling Verifiable Conformance for Product Lines}, 
year={2008}, 
volume={}, 
number={}, 
pages={35-44}, 
abstract={NASA is, with the rest of industry, turning to product-line engineering to reduce costs and improve quality by effectively managing reuse. Experience in industry has shown that it is the verifiable conformance of each system to the product-line specifications that makes or breaks the product-line practice. Verification that the software for each project satisfies its intended product-line constraints is thus essential. This paper reports early results from aneffort to assemble from previous, industrial experience a set of enablers of verifiable conformance for use in the application engineering of NASA product lines. Lessons learned may be useful for developers of safety-critical, long-lived, or highly autonomous productlines, as well as for companies that integrate product line subsystems developed by multiple contractors.}, 
keywords={aerospace computing;conformance testing;formal verification;safety-critical software;software reusability;verifiable conformance;product-line engineering;cost reduction;managing reuse;product-line specifications;product-line practice;software verification;product-line constraints;NASA product lines;safety-critical productlines;long-lived productlines;highly autonomous productlines;product line subsystems;NASA;Software;Computer architecture;Organizations;Industries;Evolution (biology);Safety;software product line;application engineering;verifying conformance;experience}, 
doi={10.1109/SPLC.2008.12}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{404523, 
author={E. L. Parrella and Sin-Min Chang}, 
booktitle={Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit}, 
title={Four channel DS1 framer}, 
year={1994}, 
volume={}, 
number={}, 
pages={445-448}, 
abstract={A four channel DS1 framer chip has been developed for deployment in multichannel T1 systems, SONET add-drop multiplexers, T3 multiplexes, and ATM over T1 applications. Area reduction was realized through the use of a high speed clock, permitting use of shared resources and construction of simple arbiters for single port RAM. For further gate reduction, a state-machine based framing algorithm utilizing RAM as next state memory was developed.<<ETX>>}, 
keywords={multiplexing equipment;SONET;asynchronous transfer mode;digital signal processing chips;application specific integrated circuits;synchronisation;four channel DS1 framer chip;multichannel T1 systems;SONET add-drop multiplexers;T3 multiplexes;ATM over T1 applications;chip area reduction;high speed clock;shared resources;arbiters;single port RAM;gate reduction;state-machine based framing algorithm;next state memory;Clocks;SONET;Costs;Buffer storage;Switches;Jitter;Application specific integrated circuits;Read-write memory;Add-drop multiplexers;Asynchronous transfer mode}, 
doi={10.1109/ASIC.1994.404523}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{1687602, 
author={F. Caruso and D. Milham and S. Orobec}, 
booktitle={2006 IEEE/IFIP Network Operations and Management Symposium NOMS 2006}, 
title={Emerging industry standard for managing next generation transport networks: TMF MTOSI}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-15}, 
abstract={There are enormous business benefits to being able to separate the business logic from the massive technical complexity at the network level. One of the greatest challenges to being able to achieve this abstraction at the OS level has been the requirement to communicate and manage many different sets of vendor technologies. The main inhibitor to overcoming this challenge has been the lack of standards supporting both the interface from an OS to an EMS and also that between OSs. Building upon the successful multi technology network management (MTNM) CORBA/IDL interface, MTOSI has extended MTNM work to support XML/Web service interactions between various types of operations systems. MTOSI bridged the standard gap by defining a methodology and a framework to map the domain specific business activities into well defined TMF NGOSS contracts according to the service oriented architecture principles. This session introduces the key aspects of MTOSI and presents a real use case of MTOSI in BT}, 
keywords={computer network management;distributed object management;Internet;IP networks;operating systems (computers);telecommunication standards;XML;emerging industry standard;next generation transport network management;TMF MTOSI;business benefits;business logic;massive technical complexity;OS level;vendor technologies;EMS;multi technology network management;CORBA-IDL interface;XML-Web service interactions;operations systems;domain specific business activities;TMF NGOSS contracts;service oriented architecture principles;Next generation networking;SOA;MDA;Web Services;TMF NGOSS;MTOSI;MTNM}, 
doi={10.1109/NOMS.2006.1687602}, 
ISSN={1542-1201}, 
month={April},}
@ARTICLE{4392504, 
author={}, 
journal={IEEE Unapproved Draft Std P2600/D30b, Nov 2007}, 
title={IEEE Draft Standard for Information Technology: Hardcopy Device and System Security}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{7492282, 
author={S. Datta and S. Sarkar and A. S. M. Sajeev}, 
journal={IEEE Transactions on Big Data}, 
title={How Long Will This Live? Discovering the Lifespans of Software Engineering Ideas}, 
year={2016}, 
volume={2}, 
number={2}, 
pages={124-137}, 
abstract={We all want to be associated with long lasting ideas; as originators, or at least, expositors. For a tyro researcher or a seasoned veteran, knowing how long an idea will remain interesting in the community is critical in choosing and pursuing research threads. In the physical sciences, the notion of half-life is often evoked to quantify decaying intensity. In this paper, we study a corpus of 19,000+ papers written by 21,000+ authors across 16 software engineering publication venues from 1975 to 2010, to empirically determine the half-life of software engineering research topics. In the absence of any consistent and well-accepted methodology for associating research topics to a publication, we have used natural language processing techniques to semi-automatically identify and associate a set of topics with a paper. We adapted measures of half-life already existing in the bibliometric context for our study, and also defined a new measure based on publication and citation counts. We find evidence that some of the identified research topics show a mean half-life of close to 15 years, and there are topics with sustaining interest in the community. We report the methodology of our study in this paper, as well as the implications and utility of our results.}, 
keywords={citation analysis;software engineering;software engineering ideas;software engineering publication;software engineering research topics;citation counts;publication counts;Software engineering;Big data;Measurement;Collaboration;Context;Special issues and sections;Software;Big data;software engineering;research;half-life}, 
doi={10.1109/TBDATA.2016.2580541}, 
ISSN={2332-7790}, 
month={June},}
@INPROCEEDINGS{6614319, 
author={M. Saadatmand and M. Sjdin}, 
booktitle={2013 10th International Conference on Information Technology: New Generations}, 
title={On Combining Model-Based Analysis and Testing}, 
year={2013}, 
volume={}, 
number={}, 
pages={260-266}, 
abstract={Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.}, 
keywords={program diagnostics;systems analysis;model-based analysis;model-based testing;computer system testing;static analysis;model-based development;nonfunctional requirements;model-guided testing;trade-off analysis;Unified modeling language;Testing;Analytical models;Software;Security;Timing;Batteries;Model-based development;static analysis;model-based testing;non-functional requirements;test-case prioritization}, 
doi={10.1109/ITNG.2013.42}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7338271, 
author={S. Ali and T. Yue}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={Formalizing the ISO/IEC/IEEE 29119 Software Testing Standard}, 
year={2015}, 
volume={}, 
number={}, 
pages={396-405}, 
abstract={Model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in the literature and in the practice. However, all of the techniques have been developed using their own concepts and terminology of MBT, which are very often different than other techniques and at times have conflicting semantics. Moreover, while working on MBT projects with our industrial partners in the last several years, we were unable to find a unified way of defining MBT techniques based on standard terminology. To precisely define MBT concepts with the aim of providing common understanding of MBT terminology across techniques, we formalize a small subset of the recently released ISO/IEC/IEEE 29119 Software Testing Standard as a conceptual model (UML class diagrams) together with OCL constraints. The conceptual model captures all the necessary concepts based on the standard terminology that are mandatory or optional in the context of MBT techniques and can be used to define new MBT tools and techniques. To validate the conceptual model, we instantiated its concepts for various MBT techniques previously developed in the context of our industrial partners. Such instantiation automatically enforces the specified OCL constraints. This type of validation provided us feedback to further refine the conceptual model. Finally, we also provide our experiences and lessons learnt for such formalization and validation.}, 
keywords={IEC standards;IEEE standards;ISO standards;program testing;software standards;Unified Modeling Language;ISO/IEC/IEEE 29119 software testing standard;model-based testing;UML class diagram;OCL constraint;Unified modeling language;Concrete;Testing;Terminology;Data models;ISO Standards;Model-Based Testing;ISO/IEC/IEEE 29119;UML;Test Case Generation;Modeling Methodology}, 
doi={10.1109/MODELS.2015.7338271}, 
ISSN={}, 
month={Sept},}
@ARTICLE{8383962, 
author={F. Basciani and M. D'Emidio and D. Di Ruscio and D. Frigioni and L. Iovino and A. Pierantonio}, 
journal={IEEE Transactions on Software Engineering}, 
title={Automated Selection of Optimal Model Transformation Chains via Shortest-Path Algorithms}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Conventional wisdom on model transformations in Model-Driven Engineering (MDE) suggests that they are crucial components in modeling environments to achieve superior automation, whether it be refactoring, simulation, or code generation. While their relevance is well-accepted, model transformations are challenging to design, implement, and verify because of the inherent complexity that they must encode. Thus, defining transformations by chaining existing ones is key to success for enhancing their reusability. This paper proposes an approach, based on well-established algorithms, to support modellers when multiple transformation chains are available to bridge a source metamodel with a target one. The all-important goal of selecting the optimal chain has been based on the quality criteria of coverage and information loss. The feasibility of the approach has been demonstrated by means of experiments operated on chains obtained from transformations borrowed from a publicly available repository.}, 
keywords={Unified modeling language;Adaptation models;Bridges;Analytical models;Model driven engineering;Ecosystems;Model-driven engineering;Model Transformation Composition;Graph Algorithms;Shortest Paths}, 
doi={10.1109/TSE.2018.2846223}, 
ISSN={0098-5589}, 
month={},}
@ARTICLE{1653662, 
author={Pei Hsia and Petry}, 
journal={Computer}, 
title={A Systematic Approach to Interactive Programming}, 
year={1980}, 
volume={13}, 
number={6}, 
pages={27-34}, 
abstract={Designed for program development in an interactive environment, this framework can help create a new generation of programmers with an invaluable disciplined approach to software development.}, 
keywords={Programming profession;Software engineering;Software design;Automatic programming;Programming environments;Production systems;Software quality}, 
doi={10.1109/MC.1980.1653662}, 
ISSN={0018-9162}, 
month={June},}
@INPROCEEDINGS{6984580, 
author={P. Oliveira and M. Souza and R. Braga and R. Britto and R. L. Rablo and P. S. Neto}, 
booktitle={2014 IEEE 26th International Conference on Tools with Artificial Intelligence}, 
title={Athena: A Visual Tool to Support the Development of Computational Intelligence Systems}, 
year={2014}, 
volume={}, 
number={}, 
pages={950-959}, 
abstract={Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.}, 
keywords={artificial intelligence;software product lines;visual programming;Athena;Computational Intelligence Systems;software product line;test case selection;CI techniques;trivial activity;drag-and-drop approach;CI-as-a-Service;CIaaS;visual programming;prioritization;visual tool;Visualization;Productivity;Remuneration;Computational modeling;Algorithm design and analysis;Computational intelligence;Resource management;Computational Intelligence;Artificial Intelligence;Visual Programming;Tool;Service}, 
doi={10.1109/ICTAI.2014.144}, 
ISSN={1082-3409}, 
month={Nov},}
@INPROCEEDINGS{4702753, 
author={T. Schavey and S. Duba}, 
booktitle={2008 IEEE/AIAA 27th Digital Avionics Systems Conference}, 
title={Streamlining IMA integration through model-driven methodologies}, 
year={2008}, 
volume={}, 
number={}, 
pages={1.B.3-1-1.B.3-5}, 
abstract={Avionics systems integration is an inherently complex undertaking. In addition to ensuring that basic functionality is satisfied, the systems integrator must maximize the systempsilas flexibility and reliability while minimizing weight and cost of change. With the introduction of integrated modular architectures (IMA) based on open standards, many traditional integration issues have been greatly improved. However, additional integration responsibilities arise due to having a large number of functions developed by independent suppliers all sharing the same physical resources. This paper describes the benefits of incorporating model-driven methodologies and associated tools into the systems integration process through Model Driven Integration (MDI). It describes how a system model can not only streamline the additional responsibilities but also establish a highly productive systems development environment and allow for virtual integration. In addition, this paper discusses a number of side- benefits that grow out of having a modeling tool platform such as enhanced team communication and automation opportunities. The analysis of avionics architectures and the use of model-driven methodologies to increase IMA manageability is based upon the authors' experience in developing platform computing systems at GE Aviation. GE Aviation has developed open system IMA architectures for both commercial aircraft and military aircraft.}, 
keywords={aerospace computing;avionics;computer architecture;avionics systems integration;integrated modular architectures;IMA;Aerospace electronics;Hardware;Cost function;Computer architecture;Humans;Biological system modeling;Manufacturing;Military aircraft;Muscles;Automotive engineering}, 
doi={10.1109/DASC.2008.4702753}, 
ISSN={2155-7195}, 
month={Oct},}
@INBOOK{8044359, 
author={William A. Flanagan}, 
booktitle={VoIP and Unified Communications: Internet Telephony and the Future Voice Network}, 
title={Index}, 
year={2011}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={10.1002/9781118166048.index}, 
ISSN={}, 
publisher={Wiley}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8044359},}
@INPROCEEDINGS{7102628, 
author={E. Rodrigues and M. Bernardino and L. Costa and A. Zorzo and F. Oliveira}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={PLeTsPerf - A Model-Based Performance Testing Tool}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Performance testing is a highly specialized task, since it requires that a performance engineer knows the application to be tested, its usage profile, and the infrastructure where it will execute. Moreover, it requires that testing teams expend a considerable effort and time on its automation. In this paper, we present the PLeTsPerf, a model-based performance testing tool to support the automatic generation of scenarios and scripts from application models. PLetsPerf is a mature tool, developed in collaboration with an IT company, which has been used in several works, experimental studies and pilot studies. We present an example of use to demonstrate the process of generating test scripts and scenarios from UML models to test a Web application. We also present the lessons learned and discuss our conclusions about the use of the tool.}, 
keywords={Internet;program testing;PLeTsPerf tool;model-based performance testing tool;scenario generation;script generation;UML model;Unified Modeling Language;Web application;Unified modeling language;Testing;Load modeling;Software;Companies;Generators;Visualization}, 
doi={10.1109/ICST.2015.7102628}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{7546576, 
author={M. Jain and D. Gopalani}, 
booktitle={2016 Second International Conference on Computational Intelligence Communication Technology (CICT)}, 
title={Aspect Oriented Programming and Types of Software Testing}, 
year={2016}, 
volume={}, 
number={}, 
pages={64-69}, 
abstract={Software testing is a process to determine that a software product satisfies the specified requirements. Software testing spans over all phases of the Software Development Life Cycle namely, requirement specification, analysis, designing, development, deployment and maintenance of a software. Software testing is important to point out the defects in the software and to ensure that the developed software works fine in the real environment with different operating systems, devices, browsers and concurrent users. Further software testing can be classified into various types based on the objective of testing, level at which the testing is performed, knowledge of the system or the degree of automation. In this paper, we examine the suitability of Aspect Oriented Programming (AOP) for the purpose of performing various types of software testing. AOP is a programming paradigm which modularizes the crosscutting concerns into units called aspects and separates them from the modules implementing the primary business logic. This leads to a system that is easier to understand and simpler to maintain. The basis of the idea behind using AOP for software testing is that aspects in AOP can be used to capture execution points within the program's modules and thus we can test components where we suspect bugs without even modifying the source code.}, 
keywords={aspect-oriented programming;formal specification;program debugging;program testing;software maintenance;aspect oriented programming;AOP;software testing;software development life cycle;requirement specification;software analysis;software design;software deployment;software maintenance;bugs;Computational intelligence;Communications technology;Aspect Oriented Programming;Types of Software Testing;Aspects;AOP}, 
doi={10.1109/CICT.2016.22}, 
ISSN={}, 
month={Feb},}
@ARTICLE{4067154, 
author={}, 
journal={IEEE Std P1175.2/D12.2, Jul 2006}, 
title={IEEE Draft Recommended Practice for Case Tool Interconnection-Characterization of Interconnections}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{6823865, 
author={H. Lackner and M. Thomas and F. Wartenberg and S. Weileder}, 
booktitle={2014 IEEE Seventh International Conference on Software Testing, Verification and Validation}, 
title={Model-Based Test Design of Product Lines: Raising Test Design to the Product Line Level}, 
year={2014}, 
volume={}, 
number={}, 
pages={51-60}, 
abstract={System quality assurance techniques like testing are important for high-quality products and processes. The effort for applying them is usually high, but can be reduced using automation. Automated test design is possible by using models to specify test-relevant aspects and by generating tests on this basis. Testing multiple variants of a system like, e.g., a product line of a German car manufacturer, results in a significant, additional effort. In this paper, we deal with model-based testing of product lines. We combine feature models that are used to describe product lines and models that are used for automated model-based test design. Our main contribution is the definition of a test generation approach on the product line level, i.e., that does not depend on resolving single product variants. Furthermore, we compare our approach to other test generation approaches and evaluate it using our tool chain SPLTestbench for some product line examples.}, 
keywords={program testing;quality assurance;software product lines;software quality;model-based test design;product line level design;system quality assurance techniques;high-quality products;automated model-based test design;test generation approach;chain SPLTestbench tool;Unified modeling language;Testing;Biological system modeling;Standards;Security;Quality assurance;Credit cards;Software Product Lines;Quality Assurance;Software Testing;Model-Based Testing;Software Reuse;Domain Level Testing}, 
doi={10.1109/ICST.2014.16}, 
ISSN={2159-4848}, 
month={March},}
@INBOOK{5238094, 
author={Mostafa Hashem Sherif}, 
booktitle={Managing Projects in Telecommunication Services}, 
title={Index}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={
No abstract.

}, 
keywords={Indexes}, 
doi={10.1002/9780470047682.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5238094},}
@INBOOK{5273158, 
author={Steven M. Kaplan}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={A}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={
}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch1}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273158},}
@INPROCEEDINGS{4259241, 
author={R. Van Roijen and C. Collins and J. Ayala and K. Barker and H. Boiselle and S. Catlett and K. Dezfulian and R. Logan and J. Maxson and R. Ramachandran and B. Rawlins and S. Ruegsegger and T. Rust and J. Shepard and R. Singh}, 
booktitle={2007 IEEE/SEMI Advanced Semiconductor Manufacturing Conference}, 
title={Reducing Time-to-Respond in a Modern Manufacturing Environment}, 
year={2007}, 
volume={}, 
number={}, 
pages={29-33}, 
abstract={The complexity of modern manufacturing processes has sharply increased the number of steps affecting device and circuit performance. We discuss a number of critical steps, their control methodology and how to minimize the time to detect. Product test results and data-mining are used to identify critical steps and to determine which inline signals require most attention. The last section is devoted to optimizing the analysis of inline electrical signals and their application to tool control.}, 
keywords={data mining;process control;semiconductor device manufacture;semiconductor device testing;manufacturing process control;product testing;data-mining;semiconductor device manufacture;time-to-respond;Implants;Manufacturing processes;Temperature control;Manufacturing automation;Rapid thermal annealing;Microelectronics;Circuit optimization;Circuit testing;Signal processing;Signal analysis;300mm manufacturing;SOI;Process control;Manufacturing automation}, 
doi={10.1109/ASMC.2007.375075}, 
ISSN={1078-8743}, 
month={June},}
@INPROCEEDINGS{5463662, 
author={M. Palviainen}, 
booktitle={2010 Third International Conference on Software Testing, Verification, and Validation Workshops}, 
title={A Dynamic Behaviour and Reliability Evaluation Method for Applications That Are Based on Asynchronous Processing Nodes}, 
year={2010}, 
volume={}, 
number={}, 
pages={309-318}, 
abstract={Many embedded and distributed applications are based on processing nodes that perform parallel processing tasks. Unfortunately, it is difficult to evaluate the overall behaviour of this kind of applications because the overall behaviour consists of 1) the execution-paths of asynchronous processing nodes and of 2) messages that either activate or deactivate processing nodes to perform parallel processing tasks. In order to facilitate behaviour and reliability evaluation of applications doing parallel processing, we developed a method that: 1) is capable of composing an overall representation for parallel behaviours and recognizing both the defined use cases and undetermined behaviours from this representation and 2) supports calculation of use case-specific reliability values for components. In this paper, we describe the method, present a ComponentBee tool that implements the method and supports behaviour and reliability evaluation of multithreaded Java applications, and finally demonstrate the use of the method with a case study.}, 
keywords={Java;multi-threading;parallel processing;software reliability;software tools;dynamic behaviour;reliability evaluation;asynchronous processing node;parallel processing;parallel behaviour;defined use case;case-specific reliability value;ComponentBee tool;multithreaded Java application;Parallel processing;Application software;Software measurement;Java;Software systems;Software testing;Performance evaluation;Concurrent computing;Predictive models;behaviour evaluation;reliability evaluation;ComponentBee;parallel processing}, 
doi={10.1109/ICSTW.2010.45}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8432197, 
author={R. Rana and T. Lagercrantz and M. Staron}, 
booktitle={2018 IEEE International Conference on Software Architecture Companion (ICSA-C)}, 
title={Building an Effective Software Issues Scorecard: An Action Research Report from the Automotive Domain}, 
year={2018}, 
volume={}, 
number={}, 
pages={136-143}, 
abstract={A large number of mature software companies use data and analytic for status monitoring of their projects and to help improve their decision making at different levels within the organization. Dashboards or scorecards also provide common platform for different stakeholders to access information they need for tracking the status of projects of their interest. Further data from software issues database can provide real and observable indicators to track the quality of given product during its development and testing. The study presented here reports on distinct and evolution of information needs of different stakeholder groups interested in tracking such data. The action research report documents the evolution of software issues scorecard as it is extended to meet information need of specific user groups. A roadmap for future into how such scorecard can be made more effective is also presented.}, 
keywords={decision making;insurance data processing;software development management;action research report documents;observable indicators;software issues database;access information;common platform;decision making;status monitoring;mature software companies;automotive domain;effective software issues scorecard;Software;Testing;Stakeholders;Monitoring;Automotive engineering;Companies;Automobiles;software issue;scorecard;software development;action research;information need;defect database}, 
doi={10.1109/ICSA-C.2018.00042}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6934560, 
author={S. Agnelli and P. Feltz and P. Griffiths and D. Roth}, 
booktitle={2014 7th Advanced Satellite Multimedia Systems Conference and the 13th Signal Processing for Space Communications Workshop (ASMS/SPSC)}, 
title={Satellite's role in the penetration of broadband connectivity within the European Union}, 
year={2014}, 
volume={}, 
number={}, 
pages={306-311}, 
abstract={The European Commission (EC) has recently acknowledged that broadband coverage for all - the 2013 target of the Digital Agenda for Europe (DAE) - has been achieved thanks to satellite broadband services, such as Tooway, the consumer-grade Internet access at 20 Mbps via the Eutelsat KA-SAT satellite. However, broadband take-up in the European Union (EU) is still far from being satisfactory, notably in rural and remote areas where satellite solutions are ideally suited.}, 
keywords={Internet;satellite communication;satellite role;broadband connectivity penetration;European union;European commission;EC;broadband coverage;digital agenda for Europe;satellite broadband services;Internet access;Eutelsat KA-SAT satellite;Broadband communication;Satellites;Europe;Internet;Investment;Multimedia systems;Signal processing;Broadband Coverage;Broadband Take-Up;Digital Divide;Digital Agenda for Europe;European Union;KA-SAT;SABER Project;Voucher Schemes}, 
doi={10.1109/ASMS-SPSC.2014.6934560}, 
ISSN={2329-7093}, 
month={Sept},}
@ARTICLE{5389542, 
author={T. B. Brodnax and R. V. Billings and S. C. Glenn and P. T. Patel}, 
journal={IBM Journal of Research and Development}, 
title={Implementation of the PowerPC 601 microprocessor}, 
year={1994}, 
volume={38}, 
number={5}, 
pages={621-632}, 
abstract={To produce a marketable PowerPC microprocessor on a short development schedule, the logic had to be designed in a manner flexible enough to allow quick modifications without sacrificing high performance and density when customized cells were required. This was accomplished for the PowerPC 601 microprocessor (601) with a high-level design-language description, which was synthesized for a gate-level implementation and simulated for functional verification. In a similar way, the physical design strategy for the 601 struck an attractive balance between a highly automated, flexible floorplan and the additional density that had to be available for limited, well-conceived manual placements. Finally, a rigorous test strategy was implemented, which has proved very useful in analyzing the processor and in assembling 601-based systems. Careful adherence to this methodology led to a successful first-pass physical implementation, leaving the second iteration for additional customer requests.}, 
keywords={}, 
doi={10.1147/rd.385.0621}, 
ISSN={0018-8646}, 
month={Sept},}
@INPROCEEDINGS{7473044, 
author={P. Zawistowski}, 
booktitle={2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)}, 
title={The Method of Measurement and Control Systems Design and Validation with Use of BRMS Systems}, 
year={2016}, 
volume={}, 
number={}, 
pages={324-332}, 
abstract={Quality of software has always been a problem in every area of software use. Lack of software can cause problems during software execution and lead to different failures. Measurement and control systems (MCS) are such a group of software which use laboratory devices to obtain measurement data or to control e.g. production processes. Bugs in software can lead to variety of problems, from incorrect measurement data to device damages. For this reason it is important to deliver software of good quality. Software development should be considered a process which consists of a sequence of steps and is supported by a tool or set of tools that interoperate to make the work with the software easier. The problem is that there is no such approach nor tools for MCS systems. For this reason, a suitable method of design and validation of this group of systems has been developed. Moreover, the proper solution for supporting the proposed method has been implemented and tested. The solution consists of Business Rule Management System (BMRS) used for validating software execution data in an efficient way.}, 
keywords={formal verification;program debugging;software quality;BRMS system;software quality;measurement-and-control system;software development;business rule management system;Software;Software measurement;Standards;Software engineering;Computer languages;Nickel;Control systems;BRMS;Drools;LabVIEW;measurement and control systems;software design;software quality assurance;software runtime validation}, 
doi={10.1109/SOSE.2016.61}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7313460, 
author={C. Berger and D. Block and C. Hons and S. Khnel and A. Leschke and D. Plotnikov and B. Rumpe}, 
booktitle={2015 IEEE 18th International Conference on Intelligent Transportation Systems}, 
title={Large-Scale Evaluation of an Active Safety Algorithm with EuroNCAP and US NCAP Scenarios in a Virtual Test Environment -- An Industrial Case Study}, 
year={2015}, 
volume={}, 
number={}, 
pages={2280-2286}, 
abstract={Context: Recently, test protocols from organizations like European New Car Assessment Programme (EuroNCAP) were extended to also cover active safety systems. Objective: The official EuroNCAP test protocol for Autonomous Emergency Braking (AEB)/Forward Collision Warning (FCW) systems explicitly defines to what extent a Vehicle-Under-Test (VUT) is allowed to vary in its lateral position. In addition, the United States New Car Assessment Programme (US NCAP) test protocol has broader tolerance ranges. The goal for automotive OEMs is to understand the impact of such allowed variations on a the overall vehicle's performance. Method: A simulation-based approach is outlined that allows systematic, large-scale analysis of such influences to effectively plan time-consuming and resource-intense real-world vehicle tests. Our models allow a profound analysis of an AEB algorithm by modeling and conducting more than 3,000 simulation runs with EuroNCAP's dynamic CCRm and CCRb scenarios including those with adopted USNCAP parameters. Results: Our structured analysis of such test procedures involving dynamic actors is the first of its kind in a relevant industrial setting. Several anomalies were unveiled under US NCAP conditions to support real-world test runs. Hence, we could show that the proposed method supports all possible scenarios in AEB consumer tests and scales as we had to timely process approx. 7.7GB of simulation data. Conclusion: To achieve the expected performance and to study a system's behavior in potential misuse cases from a functional point of view, large scale, model-based simulations complement traditional testing on proving ground.}, 
keywords={automobile industry;automobiles;automotive engineering;braking;digital simulation;driver information systems;mechanical testing;road safety;large-scale evaluation;active safety algorithm;US NCAP Scenario;virtual test environment;industrial case study;European New Car Assessment Programme;active safety system;official EuroNCAP test protocol;autonomous emergency braking;forward collision warning;FCW system;vehicle-under-test;VUT;lateral position;United States New Car Assessment Programme test protocol;US NCAP test protocol;automotive OEM;vehicle performance;simulation-based approach;systematic large-scale analysis;vehicle test;USNCAP parameters;test procedure;AEB consumer test;system behavior;potential misuse cases;model-based simulation;Vehicles;Safety;Data models;Protocols;Vehicle dynamics;Trajectory;Systematics}, 
doi={10.1109/ITSC.2015.368}, 
ISSN={2153-0017}, 
month={Sept},}
@INPROCEEDINGS{4221614, 
author={A. Bertolino}, 
booktitle={Future of Software Engineering (FOSE '07)}, 
title={Software Testing Research: Achievements, Challenges, Dreams}, 
year={2007}, 
volume={}, 
number={}, 
pages={85-103}, 
abstract={Software engineering comprehends several disciplines devoted to prevent and remedy malfunctions and to warrant adequate behaviour. Testing, the subject of this paper, is a widespread validation approach in industry, but it is still largely ad hoc, expensive, and unpredictably effective. Indeed, software testing is a broad term encompassing a variety of activities along the development cycle and beyond, aimed at different goals. Hence, software testing research faces a collection of challenges. A consistent roadmap of the most relevant challenges to be addressed is here proposed. In it, the starting point is constituted by some important past achievements, while the destination consists of four identified goals to which research ultimately tends, but which remain as unreachable as dreams. The routes from the achievements to the dreams are paved by the outstanding research challenges, which are discussed in the paper along with interesting ongoing work.}, 
keywords={program testing;program verification;software testing;software engineering;widespread validation approach;Software testing;Software engineering;Laboratories;Software systems;Software quality;Councils;Computer industry;Quality assurance;Feedback;State estimation}, 
doi={10.1109/FOSE.2007.25}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5070969, 
author={A. Jaaskelainen and M. Katara and A. Kervinen and M. Maunumaa and T. Paakkonen and T. Takala and H. Virtanen}, 
booktitle={2009 31st International Conference on Software Engineering - Companion Volume}, 
title={Automatic GUI test generation for smartphone applications - an evaluation}, 
year={2009}, 
volume={}, 
number={}, 
pages={112-122}, 
abstract={We present the results of an evaluation where we studied the effectiveness of automatic test generation for graphical user interface (GUI) testing of smartphone applications. To describe the context of our evaluation, the tools and the test model library we have developed for the evaluation are also presented. The library contains test models for basic S60 applications, such as camera, contacts, etc. The tools include an on-line test generator that produces sequences of so called keywords to be executed on the test targets. In our evaluation, we managed to find over 20 defects from applications that had been on the market for several months. We also describe the problems we faced during the evaluation.}, 
keywords={automatic testing;graphical user interfaces;program testing;smartphone applications;automatic GUI test generation;graphical user interface;test model library;S60 applications;Graphical user interfaces;Automatic testing;Software testing;System testing;Application software;Software systems;Libraries;Cameras;Optical character recognition software;Context modeling}, 
doi={10.1109/ICSE-COMPANION.2009.5070969}, 
ISSN={}, 
month={May},}
@ARTICLE{7997723, 
author={C. Raibulet and F. Arcelli Fontana and M. Zanoni}, 
journal={IEEE Access}, 
title={Model-Driven Reverse Engineering Approaches: A Systematic Literature Review}, 
year={2017}, 
volume={5}, 
number={}, 
pages={14516-14542}, 
abstract={This paper explores and describes the state of the art for what concerns the model-driven approaches proposed in the literature to support reverse engineering. We conducted a systematic literature review on this topic with the aim to answer three research questions. We focus on various solutions developed for model-driven reverse engineering, outlining in particular the models they use and the transformations applied to the models. We also consider the tools used for model definition, extraction, and transformation and the level of automation reached by the available tools. The model-driven reverse engineering approaches are also analyzed based on various features such as genericity, extensibility, automation of the reverse engineering process, and coverage of the full or partial source artifacts. We describe in detail and compare fifteen approaches applying model-driven reverse engineering. Based on this analysis, we identify and indicate some hints on choosing a model-driven reverse engineering approach from the available ones, and we outline open issues concerning the model-driven reverse engineering approaches.}, 
keywords={reverse engineering;model-driven reverse engineering;model definition;model extraction;model transformation;genericity;extensibility;Reverse engineering;Object oriented modeling;Tools;Systematics;Analytical models;Bibliographies;Search engines;Models;reverse engineering;model-driven reverse engineering;model transformation;legacy system}, 
doi={10.1109/ACCESS.2017.2733518}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8115704, 
author={C. Pietsch and M. Ohrndorf and U. Kelter and T. Kehrer}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Incrementally slicing editable submodels}, 
year={2017}, 
volume={}, 
number={}, 
pages={913-918}, 
abstract={Model slicers are tools which provide two services: (a) finding parts of interest in a model and (b) displaying these parts somehow or extract these parts as a new, autonomous model, which is referred to as slice or sub-model. This paper focuses on the creation of editable slices, which can be processed by model editors, analysis tools, model management tools etc. Slices are useful if, e.g., only a part of a large model shall be analyzed, compared or processed by time-consuming algorithms, or if sub-models shall be modified independently. We present a new generic incremental slicer which can slice models of arbitrary type and which creates slices which are consistent in the sense that they are editable by standard editors. It is built on top of a model differencing framework and does not require additional configuration data beyond those available in the differencing framework. The slicer can incrementally extend or reduce an existing slice if model elements shall be added or removed, even if the slice has been edited meanwhile. We demonstrate the usefulness of our slicer in several scenarios using a large UML model. A screencast of the demonstrated scenarios is provided at http://pi.informatik.uni-siegen.de/projects/SiLift/ase2017.}, 
keywords={formal specification;object-oriented programming;program slicing;program visualisation;Unified Modeling Language;UML model;autonomous model;editable slices;model editors;analysis tools;model management tools;generic incremental slicer;model differencing framework;model elements;incrementally sliced editable submodels;Unified modeling language;Adaptation models;Tools;Servers;Load modeling;Computational modeling;Data models}, 
doi={10.1109/ASE.2017.8115704}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8452099, 
author={A. Steffens and H. Lichter and J. S. Dring}, 
booktitle={2018 IEEE/ACM 4th International Workshop on Rapid Continuous Software Engineering (RCoSE)}, 
title={Designing a Next-Generation Continuous Software Delivery System: Concepts and Architecture}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Continuous Integration and Continuous Delivery are established practices in modern agile software development. The DevOps movement adapted theses practices and places the deployment pipeline at its heart as one of the main requirements to automate the software development process and to deliver and operate software in a more robust way with higher quality. Over the time a lot of systems and tools has been developed to implement the deployment pipeline and to support continuous delivery. But software development is complex, its process even more and due to the individual organization of software vendors no real all-in-one solution for CD exists. Literature identified a lot of challenges when adopting CD and DevOps in an organization. This paper presents a conceptual model and fundamental design decisions for a new generation of software delivery systems tackling some of these issues. Our approach focuses on two specific challenges for adopting CD. The first is the lack of flexibility and maintainability of software delivery systems. The second is the insufficient user support to model and manage delivery processes and pipelines. We introduce an automated mechanism to ease the effort for developers and other stakeholders. Based on these results this paper introduces an architectural proposal for a next-generation continuous software delivery system."}, 
keywords={Software;Pipelines;Logic gates;Computer architecture;Tools;Computational modeling;Next generation networking;Continuous Delivery;Continuous Integration;DevOps;Software Architecture;Microservices;Domain Driven Design;Domain Model;Software Delivery;Continuous Software Engineering;Software Design}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4148978, 
author={J. Liu and J. Dehlinger and H. Sun and R. Lutz}, 
booktitle={14th Annual IEEE International Conference and Workshops on the Engineering of Computer-Based Systems (ECBS'07)}, 
title={State-Based Modeling to Support the Evolution and Maintenance of Safety-Critical Software Product Lines}, 
year={2007}, 
volume={}, 
number={}, 
pages={596-608}, 
abstract={Changes to safety-critical product lines can jeopardize the safety properties that they must ensure. Thus, evolving software product lines must consider the impact that changes to requirements may have on the existing systems and their safety. The contribution of this work is a systematic, tool-supported technique to support safe evolution of product-line requirements using a model-based approach. We show how the potential feature interactions that need to be modeled are scoped and identified with the aid of product-line software fault tree analysis. Further, we show how reuse of the state-based models is effectively exploited in the evolution phase of product-line engineering. To illustrate this approach, we apply our technique to the evolution of a safety-critical cardiac pacemaker product line}, 
keywords={formal specification;safety-critical software;software maintenance;software prototyping;state-based modeling;safety-critical software product line evolution;safety-critical software product line maintenance;product-line requirements;product-line engineering;Software maintenance;Software safety;Product safety;Fault trees;Pacemakers;Software tools;Systems engineering and theory;Costs;Reliability engineering;Computer science}, 
doi={10.1109/ECBS.2007.66}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5979373, 
author={S. Wang and Y. Wu and M. Lu and H. Li}, 
booktitle={The Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety}, 
title={Software reliability modeling based on test coverage}, 
year={2011}, 
volume={}, 
number={}, 
pages={665-671}, 
abstract={In order to incorporate the effect of test coverage, two novel software reliability growth models (SRGMs) are proposed in this paper using failure data and test coverage simultaneously. One is continuous using testing time, and the other is discrete with respect to the number of executed test cases instead of testing time. Since one of the most important factors of the coverage-based SRGMs is the test coverage function (TCF), we first discuss a discrete TCF based on Beta function. Then we develop mean value functions (MVF) of the two models integrating test coverage and imperfect debugging. Finally the proposed TCF and MVFs are evaluated and validated on actual software reliability data collected from real software development projects. The results demonstrate clearly that both the proposed TCF and SRGMs provide better estimation and fitting for the data sets under comparisons.}, 
keywords={program debugging;program testing;software fault tolerance;software reliability;software reliability growth models;test coverage function;beta function;mean value functions;software debugging;software development projects;Software reliability;Testing;Software;Mathematical model;Fault detection;Equations;Software reliability growth model;test coverage function;beta function;non-homogeneous poisson process;mean value function}, 
doi={10.1109/ICRMS.2011.5979373}, 
ISSN={}, 
month={June},}
@ARTICLE{1166666, 
author={R. Baines and D. Pulley}, 
journal={IEEE Communications Magazine}, 
title={A total cost approach to evaluating different reconfigurable architectures for baseband processing in wireless receivers}, 
year={2003}, 
volume={41}, 
number={1}, 
pages={105-113}, 
abstract={There is growing interest in the use of flexible digital signal processors for wireless systems, driven by the demands of time to market, cost pressure, the requirement for flexibility to cope with evolving standards, and rapidly increasing processing needs. Much of the discussion of these techniques involves terms like "efficient" or "cost-effective" without necessarily quantifying the terms. This article considers the various architectures applicable to a wideband CDMA node-B base station (ASIC, FPGA, traditional DSP, and two varieties of flexible DSP) and builds a quantitative total cost approach to evaluating them, including benchmarked performance data.}, 
keywords={field programmable gate arrays;application specific integrated circuits;reconfigurable architectures;radio receivers;3G mobile communication;digital signal processing chips;broadband networks;code division multiple access;total cost approach;reconfigurable architectures;baseband processing;wireless receivers;flexible digital signal processors;wideband CDMA node-B base station;ASIC;FPGA;flexible DSP;3G wideband code-division multiple access base station;WCDMA;Costs;Reconfigurable architectures;Baseband;Digital signal processing;Digital signal processors;Time to market;Wideband;Multiaccess communication;Base stations;Application specific integrated circuits}, 
doi={10.1109/MCOM.2003.1166666}, 
ISSN={0163-6804}, 
month={Jan},}
@INPROCEEDINGS{6166261, 
author={J. Edmondson and A. Gokhale and S. Neema}, 
booktitle={2011 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)}, 
title={Automating testing of service-oriented mobile applications with distributed knowledge and reasoning}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Automated testing of distributed, service-oriented applications, particularly mobile applications, is a hard problem due to challenges testers often must deal with, such as (1) heterogeneous platforms, (2) difficulty in introducing additional resources or backups of resources that fail during testing, and (3) lack of fine-grained control over test sequencing. To address these challenges, this paper describes an approach that combines portable operating system libraries with knowledge and reasoning, which together leverage the best features of centralized and decentralized testing infrastructures to support both heterogeneous systems and distributed control by reasoning on distributed testing events.}, 
keywords={automatic test software;distributed control;inference mechanisms;mobile computing;operating systems (computers);program testing;service-oriented architecture;software libraries;automated service-oriented mobile application testing;distributed knowledge;distributed reasoning;resource backups;fine-grained control;test sequencing;portable operating system libraries;decentralized testing infrastructure;centralized testing infrastructure;heterogeneous systems;distributed control;distributed testing events;Testing;Cognition;Servers;Engines;Knowledge engineering;Real time systems;Conferences;test sequencing;distributed control in testing;portability;knowledge dissemination}, 
doi={10.1109/SOCA.2011.6166261}, 
ISSN={2163-2871}, 
month={Dec},}
@INPROCEEDINGS{7107547, 
author={M. Binas}, 
booktitle={2014 IEEE 12th IEEE International Conference on Emerging eLearning Technologies and Applications (ICETA)}, 
title={Identifying web services for automatic assessments of programming assignments}, 
year={2014}, 
volume={}, 
number={}, 
pages={45-50}, 
abstract={The main objective of this article is to verify the assumption, if the web services can be used in the process of automatic assessment of programming assignments. It tries to identify general services for such processes and presents the experimental part by creating platform based on set of web services.}, 
keywords={computer science education;programming;Web services;automatic programming assignment assessment;Web services;Web services;Programming;Testing;Electronic mail;Plagiarism;Uniform resource locators;Electronic learning}, 
doi={10.1109/ICETA.2014.7107547}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{5679044, 
author={S. Gallant and C. Gaughan}, 
booktitle={Proceedings of the 2010 Winter Simulation Conference}, 
title={Systems engineering for distributed live, virtual, and constructive (LVC) simulation}, 
year={2010}, 
volume={}, 
number={}, 
pages={1501-1511}, 
abstract={Designing a distributed simulation environment across multiple domains that typically have disparate middleware transport protocols, data exchange formats and applications increases the difficulty of capturing and linking system design decisions to the resultant implementation. Systems engineering efforts for distributed simulation environments are typically based on the middleware transport used, the applications available and the constraints placed on the technical team including network, computer and personnel limitations. To facilitate community re-use, systems engineering should focus on integrated operational function decomposition. This links data elements produced within the simulation to the functional capabilities required by the user. The system design should be captured at a functional level and subsequently linked to the technical design. Doing this within a data-driven systems engineering infrastructure allows generative programming techniques to assist accurate, flexible and rapid architecture development. This paper describes the MATREX program systems engineering process, infrastructure and path forward.}, 
keywords={digital simulation;electronic data interchange;middleware;military computing;systems engineering;transport protocols;data driven system engineering;live virtual constructive simulation;LVC simulation;distributed simulation environment;middleware transport protocol;data exchange format;system design;generative programming technique;MATREX program;Computer architecture;System analysis and design;Data models;Testing;Middleware}, 
doi={10.1109/WSC.2010.5679044}, 
ISSN={1558-4305}, 
month={Dec},}
@INPROCEEDINGS{1214600, 
author={I. Gohar and A. Mirza}, 
booktitle={IEEE Students Conference, ISCON '02. Proceedings.}, 
title={Voice over asynchronous transfer modc(ATM)}, 
year={2002}, 
volume={2}, 
number={}, 
pages={11-12}, 
abstract={}, 
keywords={Asynchronous transfer mode;Software testing;Software performance;Transportation;Educational institutions;Shape;System testing;Fault detection;Speech analysis;Spine}, 
doi={10.1109/ISCON.2002.1214600}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{4641444, 
author={A. Gargantini and E. Riccobene and P. Scandurra and A. Carioni}, 
booktitle={2008 Forum on Specification, Verification and Design Languages}, 
title={Scenario-based validation of embedded systems}, 
year={2008}, 
volume={}, 
number={}, 
pages={191-196}, 
abstract={This paper describes a scenario-based methodology for system-level design validation based on the Abstract State Machines formal method. This scenario-based approach complements an existing model-driven design methodology for embedded systems based on the SystemC UML profile. It allows the designer to functionally validate system components from SystemC UML designs early at high levels of abstraction and without requiring strong skills and expertise on formal methods. A validation tool integrated into an existing model-driven co-design environment to support the proposed scenario-based validation flow is also presented.}, 
keywords={C++ language;embedded systems;finite automata;object-oriented programming;program compilers;Unified Modeling Language;scenario-based validation;embedded systems;system-level design validation;abstract state machines formal method;model-driven design;SystemC UML profile;system components;Unified modeling language;Biological system modeling;Modeling;Embedded system;Analytical models;Libraries;Design methodology}, 
doi={10.1109/FDL.2008.4641444}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{5767210, 
author={R. Mohammed and R. Sahan and Y. Xia and Y. Pang}, 
booktitle={2011 27th Annual IEEE Semiconductor Thermal Measurement and Management Symposium}, 
title={High performance air-cooled temperature margining thermal tools for silicon validation}, 
year={2011}, 
volume={}, 
number={}, 
pages={265-271}, 
abstract={Thermal tools provide temperature margining capability by varying the case temperature at silicon thermal design power (TDP). They are used for process, voltage, temperature and frequency (PVTF) testing by Intel's post-silicon validation customers across servers, desktops, mobile and graphics segments. Thermal margining tools are widely used in silicon debug validation by varying the case temperature over a wide operating range of specifications of the Silicon to i) validate the silicon, ii) accelerate fault detection, and iii) reduce escapes and identify bugs. Thermal tool is controlled by a thermal controller to provide a temperature set-point based on the device under test's (DUT's) case or junction diode temperature. Air cooled thermal tool (AC-TT) employs a controller card to achieve the margining capability by running the tool's thermoelectric cooler (TEC), a Peltier device, within the optimal temperature range. AC-TT has an active heat sink design to remove the heat dissipated by the TEC and the silicon. Although AC-TT is expected to provide narrower range of margining capability due to the limitations of air cooling, they still can be an excellent solution for some specific thermal margining applications. Therefore, a new line of AC-TTs were developed for validation customers whose needs can be addressed without requiring costly controllers and noisy chillers while enhancing the user-experience. This paper presents the design improvement strategies implemented for developing the new line of CPU, Chipset and ASIC AC-TTs. Improved designs provide wider margining capability by using i) high performance active heat sink designs, ii) high power thermo-electric cooler (TEC), iii) cold plate designs compatible to keep out volume (KOV), iv) new choice of thermal interface material (TIM), and v) new retention design. This paper discusses the details of the design process and how multiple design strategies are implemented to finalize the design and to achieve the overall performance improvement while keeping the cost of the AC-TT low. The new line of AC-TT designs have performance improvement of 44% (~25C) for 130W CPU TT compared to existing CPU AC-TT, of 32% (~19C) for 60W chipset compared to existing chipset AC-TT, and of 41% (~8C) compared to existing 15W PCH (Peripheral Component Hub) AC-TT. Design strategies provided here can be easily adapted to develop future generation of low-cost CPU, chipset, and ASIC AC-TTs with a wider margining capability.}, 
keywords={cooling;heat sinks;Peltier effect;temperature measurement;thermoelectric devices;high performance air-cooled temperature margining thermal tools;silicon validation;thermal design power;thermal controller;junction diode temperature;thermoelectric cooler;Peltier device;thermal interface material;Heat sinks;Silicon;Cold plates;Heating;Heat transfer;Thermal resistance;Thermal tool;CPU;chipset;ASIC;retention design;CFD;TEC;air cooling;TIM;temperature margining}, 
doi={10.1109/STHERM.2011.5767210}, 
ISSN={1065-2221}, 
month={March},}
@INPROCEEDINGS{7927983, 
author={C. Artho and Q. Gros and G. Rousset and K. Banzai and L. Ma and T. Kitamura and M. Hagiya and Y. Tanabe and M. Yamamoto}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Model-Based API Testing of Apache ZooKeeper}, 
year={2017}, 
volume={}, 
number={}, 
pages={288-298}, 
abstract={Apache ZooKeeper is a distributed data storage that is highly concurrent and asynchronous due to network communication, testing such a system is very challenging. Our solution using the tool "Modbat" generates test cases for concurrent client sessions, and processes results from synchronous and asynchronous callbacks. We use an embedded model checker to compute the test oracle for non-deterministic outcomes, the oracle model evolves dynamically with each new test step. Our work has detected multiple previously unknown defects in ZooKeeper. Finally, a thorough coverage evaluation of the core classes show how code and branch coverage strongly relate to feature coverage in the model, and hence modeling effort.}, 
keywords={application program interfaces;distributed databases;formal verification;program testing;storage management;model-based API testing;Apache ZooKeeper;distributed data storage;network communication;Modbat;test cases;concurrent client sessions;synchronous callbacks;asynchronous callbacks;embedded model checker;nondeterministic outcomes;coverage evaluation;core classes;branch coverage;feature coverage;modeling effort;Servers;Testing;Electronic mail;Computational modeling;Computer science;Tools;Complexity theory;Model-based testing;concurrency;asynchronous systems;networked systems;test oracle;Apache ZooKeeper}, 
doi={10.1109/ICST.2017.33}, 
ISSN={}, 
month={March},}
@INBOOK{6542510, 
author={G. Keith Cambron}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Front Matter}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={

The prelims comprise:

Half-Title Page

Title Page

Copyright Page

Dedication Page

Table of Contents

List of Figures

About the Author

Foreword

Preface

Acknowledgments

List of Acronyms

]]>}, 
keywords={}, 
doi={10.1002/9781118394519.fmatter}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6542510},}
@INPROCEEDINGS{7966874, 
author={R. Souza and A. Oliveira}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER)}, 
title={GuideAutomator: Continuous Delivery of End User Documentation}, 
year={2017}, 
volume={}, 
number={}, 
pages={31-34}, 
abstract={User guides, also known as user manuals, are a type of documentation aimed at helping a user operate a specific system. For software systems, user guides usually include screenshots that show users how to interact with the user interface. Because creating such screenshots is a slow, manual process, keeping the user guide up-to-date with changes in the user interface is challenging. We propose an approach in which the documentation writer interleaves the user guide text with source code that automates screen capturing. As a result, screenshots always reflect the latest software version, which makes the approach suitable for a project that uses continuous delivery. The approach was implemented as a prototype, called GuideAutomator.}, 
keywords={software tools;source code (software);system documentation;user interfaces;user manuals;GuideAutomator;continuous delivery;end user documentation;user guide text;software systems;user interface;source code;command-line tool;Documentation;Tools;Software;Browsers;Cascading style sheets;Programming;software documentation;automated documentation generator;literate programming;continuous delivery}, 
doi={10.1109/ICSE-NIER.2017.10}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1203468, 
author={P. De and A. Neogi and T. -. Chiueh}, 
booktitle={23rd International Conference on Distributed Computing Systems, 2003. Proceedings.}, 
title={VirtualWire: a fault injection and analysis tool for network protocols}, 
year={2003}, 
volume={}, 
number={}, 
pages={214-221}, 
abstract={The prevailing practice for testing protocol implementations is direct code instrumentation to trigger specific states in the code. This leaves very little scope for reuse of the test cases. In this paper, we present the design, implementation, and evaluation of VirtualWire, a network fault injection and analysis system designed to facilitate the process of testing network protocol implementations. VirtualWire injects user-specified network faults and matches network events against anticipated responses based on high-level specifications written in a declarative scripting language. With VirtualWire, testing requires no code instrumentation and fault specifications can be reused across versions of a protocol implementation. We illustrate the effectiveness of VirtualWire with examples drawn from testing Linux's TCP implementation and a real-time Ethernet protocol called Rether. In each case, 10 to 20 lines of script is sufficient to specify the test scenario. VirtualWire is completely transparent to the protocols under test, and additional overhead in protocol processing latency it introduces is below 10% of the normal.}, 
keywords={transport protocols;formal specification;formal verification;fault tolerant computing;local area networks;VirtualWire;network protocol;code instrumentation;fault specification;Linux TCP implementation testing;Ethernet protocol;Rether;Instruments;Access protocols;Ethernet networks;Kernel;System testing;Filters;Laboratories;Computer science;Delay;Formal verification}, 
doi={10.1109/ICDCS.2003.1203468}, 
ISSN={1063-6927}, 
month={May},}
@INPROCEEDINGS{6130706, 
author={A. Svendsen and . Haugen and B. Moller-Pedersen}, 
booktitle={2011 18th Asia-Pacific Software Engineering Conference}, 
title={Using Variability Models to Reduce Verification Effort of Train Station Models}, 
year={2011}, 
volume={}, 
number={}, 
pages={348-356}, 
abstract={We show how the effort needed to verify a transformed base model can be reduced by analyzing the definition of the modification. The Common Variability Language (CVL) is a generic language for modeling variability, where a CVL model describes the increment from one base model to another (transformed) base model. Assuming that a property of the base model has been verified, we use the CVL model to reduce the effort needed to verify the property of the transformed model. Based on the CVL model, we narrow down the set of traces required to be verified, including the increment and the cascading effects. We apply CVL to several models of the Train Control Language (TCL) to illustrate how the effort of verifying safety properties of transformed train station models can be reduced.}, 
keywords={formal verification;railway engineering;railway safety;railways;safety-critical software;specification languages;variability models;verification effort;transformed base model;common variability language;generic language;variability modeling;CVL model;transformed model;cascading effects;train control language;TCL;safety property verification;transformed train station models;Safety;Biological system modeling;Analytical models;Metals;Switches;Semantics;Mathematical model;analysis;variability;safety property;Common Variability Language;Train Control Language}, 
doi={10.1109/APSEC.2011.21}, 
ISSN={1530-1362}, 
month={Dec},}
@INPROCEEDINGS{7338242, 
author={A. Kusel and J. Etzlstorfer and E. Kapsammer and W. Retschitzegger and W. Schwinger and J. Schnbck}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={Consistent co-evolution of models and transformations}, 
year={2015}, 
volume={}, 
number={}, 
pages={116-125}, 
abstract={Evolving metamodels are in the center of Model-Driven Engineering, necessitating the co-evolution of dependent artifacts like models and transformations. While model co-evolution has been extensively studied, transformation co-evolution has received less attention up to now. Current approaches for transformation co-evolution provide a fixed, restricted set of metamodel (MM) changes, only. Furthermore, composite changes are treated as monolithic units, which may lead to inconsistent co-evolution for overlapping atomic changes and prohibits extensibility. Finally, transformation co-evolution is considered in isolation, possibly inducing inconsistencies between model and transformation co-evolution. To overcome these limitations, we propose a complete set of atomic MM changes being able to describe arbitrary MM evolutions. Reusability and extensibility are supported by means of change composition, ensuring an intra-artifact consistent co-evolution. Furthermore, each change provides resolution actions for both, models and transformations, ensuring an inter-artifact consistent co-evolution. Based on our conceptual approach, a prototypical implementation is presented.}, 
keywords={software reusability;evolving metamodels;model-driven engineering;model coevolution;transformation coevolution;atomic MM changes;reusability;extensibility;change composition;intraartifact consistent coevolution;Biological system modeling;Semantics;Systematics;Feature extraction;Software;Syntactics;Companies}, 
doi={10.1109/MODELS.2015.7338242}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7430116, 
author={A. Marroquin and D. Gonzalez and S. Maag}, 
booktitle={2015 7th IEEE Latin-American Conference on Communications (LATINCOM)}, 
title={Testing distributed systems with test cases dependencies architecture}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In this work, we present a novel distributed testing architecture based on a formal definition of test cases dependencies to test the conformance of distributed systems in a black box context. Utilizing the European Telecommunication Standards Institute, Test Description Language standard, we apply our approach to a real Internet Multimedia Subsystem (IMS)/ SIP (Session Initiation Protocol) test bed and perform the tests through two use cases. This crucial activity belongs to the conformance testing context. Which aims at stimulating the communication system under test (SUT) to detect errors and unexpected behaviors with regards to the standards. When handling distributed systems, a major difficulty arises when testing these, due to the joint and linked stimulation of distributed entities. The main reason for it is the correlation of verdicts obtained from these entities.}, 
keywords={conformance testing;Internet;multimedia systems;signalling protocols;telecommunication standards;distributed systems testing;test case dependency architecture;black box context;European telecommunication standards institute;test description language standard;Internet multimedia subsystem;IMS;SIP;session initiation protocol;system under test;SUT;error detection;Testing;Protocols;Standards;Context;Synchronization;Correlation;Conformance Testing;Distributed Systems;Test Cases;SIP;TDL}, 
doi={10.1109/LATINCOM.2015.7430116}, 
ISSN={}, 
month={Nov},}
@INBOOK{5396935, 
author={}, 
booktitle={A Guide to the Wireless Engineering Body of Knowledge (WEBOK)}, 
title={Fundamental Knowledge}, 
year={2009}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Introduction

Contents

Electrical Engineering Basics for Wireless Communications

Signal Processing and Communication Systems

RF Engineering

Instruments And Measurements [Wit02]

Communication Networks

Other Communication Systems

General Engineering Management and Economics

Key References

}, 
keywords={}, 
doi={10.1002/9780470439128.ch7}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5396935},}
@INBOOK{7827467, 
author={Tyson T. Brooks}, 
booktitle={Cyber-Assurance for the Internet of Things}, 
title={Cyber-Assurance Through Embedded Security for the Internet of Things}, 
year={2017}, 
volume={}, 
number={}, 
pages={}, 
abstract={The Internet of Things (IoT) comprises billions of Internet-connected devices (ICD) or "things", each of which can sense, communicate, compute, and potentially actuate and can have intelligence, multimodal interfaces, physical/virtual identities, and attributes. Cyber-assurance is the justified confidence that networked systems are adequately secure to meet operational needs, even in the presence of attacks, failures, accidents, and unexpected events. The cyber-assurance recognition strategy is to define only the service-level interfaces and leave out domain-specific implementation details. Once the recognition of a cyber-attack has been identified from the recognition process, the fortification process takes place. Reestablishment is a means to return the ICDs to its operational condition after the cyber-attack through remapping to a different route since the ICD was under attack. When the IoT technologies are used as part of mission critical systems, the IoT services should be survivable in order to support the important missions.}, 
keywords={Embedded systems;Sensors;Protocols;Wireless sensor networks;Internet of Things;Authentication}, 
doi={10.1002/9781119193784.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7827467},}
@INPROCEEDINGS{6606719, 
author={V. Guana}, 
booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 
title={Supporting maintenance tasks on transformational code generation environments}, 
year={2013}, 
volume={}, 
number={}, 
pages={1369-1372}, 
abstract={At the core of model-driven software development, model-transformation compositions enable automatic generation of executable artifacts from models. Although the advantages of transformational software development have been explored by numerous academics and industry practitioners, adoption of the paradigm continues to be slow, and limited to specific domains. The main challenge to adoption is the fact that maintenance tasks, such as analysis and management of model-transformation compositions and reflecting code changes to model transformations, are still largely unsupported by tools. My dissertation aims at enhancing the field's understanding around the maintenance issues in transformational software development, and at supporting the tasks involved in the synchronization of evolving system features with their generation environments. This paper discusses the three main aspects of the envisioned thesis: (a) complexity analysis of model-transformation compositions, (b) system feature localization and tracking in model-transformation compositions, and (c) refactoring of transformation compositions to improve their qualities.}, 
keywords={software maintenance;software metrics;transformational code generation environments;maintenance tasks support;model-driven software development;model-transformation compositions;transformational software development;reflecting code management;software quality;Maintenance engineering;Object oriented modeling;Analytical models;Complexity theory;Software;Games;Semantics;software maintenance;transformation composition;transformation complexity;transformation refactoring}, 
doi={10.1109/ICSE.2013.6606719}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{6549304, 
author={R. Iyer and B. Chandramouleeswaran}, 
booktitle={International Conference on Software Engineering and Mobile Application Modelling and Development (ICSEMA 2012)}, 
title={Best practices and case study for open source middleware migration: Egate to apache camel migration}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Many legacy systems use message oriented middleware to communicate between themselves. Message oriented middleware are considered to be the most effective technology for enterprise integration. There are a lot of proprietary middleware solutions in the market that involve huge licensing costs, difficult maintenance procedures and niche skill sets. The usage of open source middleware to replace these proprietary solutions in a cost effective manner is an idea that can now bear fruition due to the relative maturity of such solutions. The use of open source reduces licensing cost, enables the developers to have greater insight into the working of the system and avail of the wide spread community support for such systems.}, 
keywords={middleware;public domain software;software maintenance;open source middleware migration;apache camel migration;legacy systems;message oriented middleware;enterprise integration;proprietary middleware solutions;Middleware;open source;Apache Camel;Egate;enterprise migration}, 
doi={10.1049/ic.2012.0140}, 
ISSN={}, 
month={Dec},}
@ARTICLE{5389507, 
author={P. Bose and S. Surya}, 
journal={IBM Journal of Research and Development}, 
title={Architectural timing verification of CMOS RISC processors}, 
year={1995}, 
volume={39}, 
number={1.2}, 
pages={113-129}, 
abstract={We consider the problem of verification and testing of architectural timing models ("timers") coded to predict cycles-per-instruction (CPI) performance of advanced CMOS superscalar (RISC) processors. Such timers are used for pre-hardware performance analysis and prediction. As such, these software models play a vital role in processor performance tuning as well as application-based competitive analysis, years before actual product availability. One of the key problems facing a designer, modeler, or application analyst who uses such a tool is to understand how accurate the model is, in terms of the actual design. In contrast to functional simulators, there is no direct way of testing timers in the classical sense, since the correct execution time (in cycles) of a program on the machine model under test is not directly known or computable from equations, truth tables, or other formal specifications. Ultimate validation (or invalidation) of such models can be achieved under actual hardware availability, by direct comparisons against measured performance. However, deferring validation solely to that stage would do little to achieve the overall purpose of accurate pre-hardware analysis, tuning, and projection. We describe a multilevel validation method which has been used successfully to transform evolving timers into highly accurate pre-hardware models. In this paper, we focus primarily on the following aspects of the methodology: a) establishment of cause-effect relationships in terms of model defects and the associated fault signatures; b) derivation of application-based test loop kernels to verify steady-state (periodic) behavior of pipeline flow, against analytically predicted signatures; and c) derivation of synthetic test cases to verify the core parameters characterizing the pipeline-level machine organization as implemented in the timer model. The basic tenets of the theory and its application are described in the context of an example processor, comparable in complexity to an advanced member of the PowerPC 6XX processor family.}, 
keywords={}, 
doi={10.1147/rd.391.0113}, 
ISSN={0018-8646}, 
month={Jan},}
@INPROCEEDINGS{5476763, 
author={B. Mathieu and P. Paris and G. L. Guelvouit and S. Rouibia}, 
booktitle={2010 Fifth International Conference on Internet and Web Applications and Services}, 
title={A Secure and Legal Network-Aware P2P VoD System}, 
year={2010}, 
volume={}, 
number={}, 
pages={194-199}, 
abstract={File sharing applications using Peer-to-Peer (P2P) networks such as Bittorrent or eDonkey rapidly attracted a lot of people and proved the efficiency and interest of this P2P technology. Distribution of video and of live contents also experienced the P2P mechanisms with success. PPLive, UUSee and others have many of customers, hundreds of channels and thousands of concurrent users. However, major content providers are reluctant to use this technology because no solution to ensure the distribution of only legal contents is provided. In the same way, network operators do not really push towards P2P content distribution because bad organization of the overlay can lead to overload the network and consume a lot of networks resources. In this paper, a secure and legal network-aware P2P video system is introduced, which aims at overcoming those two drawbacks. The design of the system and the evaluation of a prototype showed good results and let us be optimistic about a possible deployment of P2P systems for video delivery, having the support of content providers as well as network operators.}, 
keywords={computer network security;content management;peer-to-peer computing;video on demand;video streaming;P2P VoD system;file sharing;peer-to-peer network;content distribution;P2P video system;video delivery;network operator;Law;Legal factors;Peer to peer computing;Video sharing;Prototypes;Watermarking;Computer architecture;IP networks;Web and internet services;Design optimization;P2P Video Streaming;network-awareness;secure distribution;legal contents;watermarking}, 
doi={10.1109/ICIW.2010.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5381646, 
author={S. Wieczorek and A. Stefanescu and I. Schieferdecker}, 
booktitle={2009 Testing: Academic and Industrial Conference - Practice and Research Techniques}, 
title={Model-Based Integration Testing of Enterprise Services}, 
year={2009}, 
volume={}, 
number={}, 
pages={56-60}, 
abstract={The success of service-oriented architectures (SOA) depends on faultless and seamless service integration. Formal modeling of global communication protocols between services enables a model-based integration testing (MBIT) approach. In this paper we present an MBIT approach based on SAP proprietary choreography models called message choreography models (MCM). We explain how MBIT fits into the SAP testing methodology for SOA and give some insights into the experience we gained from the work.}, 
keywords={enterprise resource planning;program testing;software architecture;model-based integration testing;enterprise services;service-oriented architectures;seamless service integration;formal modeling;global communication protocols;SAP proprietary choreography models;message choreography models;enterprise resource planning;Service oriented architecture;Enterprise resource planning;Automatic testing;Software testing;Protocols;Application software;Global communication;Unified modeling language;Java;Fault detection;Model-based Testing;Integration Testing;Service Oriented Architecture}, 
doi={10.1109/TAICPART.2009.11}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6227107, 
author={N. Devos and C. Ponsard and J. Deprez and R. Bauvin and B. Moriau and G. Anckaerts}, 
booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
title={Efficient reuse of domain-specific test knowledge: An industrial case in the smart card domain}, 
year={2012}, 
volume={}, 
number={}, 
pages={1123-1132}, 
abstract={While testing is heavily used and largely automated in software development projects, the reuse of test practices across similar projects in a given domain is seldom systematized and supported by adequate methods and tools. This paper presents a practical approach that emerged from a concrete industrial case in the smart card domain at STMicroelectronics Belgium in order to better address this kind of challenge. The central concept is a test knowledge repository organized as a collection of specific patterns named QPatterns. A systematic process was followed, first to gather, structure and abstract the test practices, then to produce and validate an initial repository, and finally to make it evolve later on Testers can then rely on this repository to produce high quality test plans identifying all the functional and nonfunctional aspects that have to be addressed, as well as the concrete tests that have to be developed within the context of a new project. A tool support was also developed and integrated in a traceable way into the existing industrial test environment. The approach was validated and is currently under deployment at STMicroelectronics Belgium.}, 
keywords={program testing;project management;smart cards;software management;domain specific test knowledge;industrial case;smart card domain;software development projects;STMicroelectronics Belgium;industrial test environment;Smart cards;Testing;Libraries;Software;Security;Concrete;patterns;test;generation;smartcard}, 
doi={10.1109/ICSE.2012.6227107}, 
ISSN={1558-1225}, 
month={June},}
@INPROCEEDINGS{1321646, 
author={Hau Lam}, 
booktitle={IEEE/CPMT/SEMI 29th International Electronics Manufacturing Technology Symposium (IEEE Cat. No.04CH37585)}, 
title={New design-to-test software strategies accelerate time-to-market}, 
year={2004}, 
volume={}, 
number={}, 
pages={140-143}, 
abstract={Today's growing device complexity and new manufacturing requirements have presented significant challenges for manufacturers looking to speed time-to-market. One such challenge is the need to contain test costs, of which a major component is the time and resources required for test program development. Some test development tools that exist today can translate a device's functional events and scan patterns into test programs for targeted ATE. Identification and specification of critical timing parameters that require conversion into cycle-based ATE formats have become an increasing cost factor, which can also significantly impact test accuracy. Traditionally, timing specifications from microprocessor and IP cores, multiple bus types, and other device components can be established via published timing specifications and by a manageable, iterative process between design and test engineering. Likewise, automatic test pattern generation tools for structural test can address simple timing, and are capable of generating cycle-based timing. Today's complex SoC may consist of over 60 IP cores made more complicated by increased challenges from high-speed serial bus technology and multiple-time domain designs. Further complicating test program development is the need for compatibility with multiple ATE platforms to accommodate global manufacturing strategies. Next generation design-to-test software tools have to address these factors to help reduce the ever growing cost-of-test. Tools must support standard industry test languages such as standard test interface language (STIL), support both functional events and scan patterns, and validate outputs to ensure first-pass success of test programs pre- and post silicon, across multiple ATE platforms.}, 
keywords={electronic design automation;automatic test software;design for testability;automatic test pattern generation;built-in self test;system-on-chip;design-to-test software strategies;time-to-market;device complexity;manufacturing requirements;test program development;test development tools;timing parameters;cost factor;microprocessor;IP cores;timing specifications;iterative process;design engineering;test engineering;automatic test pattern generation tools;structural test;SoC;serial bus technology;multiple-time domain designs;ATE platforms;global manufacturing strategies;cost-of-test;standard test interface language;scan patterns;silicon;Software design;Acceleration;Time to market;Testing;Timing;Manufacturing;Costs;Automatic test pattern generation;Microprocessors;Engineering management}, 
doi={10.1109/IEMT.2004.1321646}, 
ISSN={1089-8190}, 
month={July},}
@INPROCEEDINGS{5457806, 
author={M. Bakera and C. Wagner and T. Margaria and E. Vassev and M. Hinchey and B. Steffen}, 
booktitle={2010 Seventh IEEE International Conference and Workshops on Engineering of Autonomic and Autonomous Systems}, 
title={Extracting Component-Oriented Behaviour for Self-Healing Enabling}, 
year={2010}, 
volume={}, 
number={}, 
pages={152-161}, 
abstract={Rich and multifaceted domain specific specification languages like the Autonomic System Specification Language (ASSL) help to design reliable systems with self-healing capabilities. The GEAR game-based Model Checker has been used successfully to investigate in depth properties of the ESA ExoMars Rover. We show here how to enable GEAR's game-based verification techniques for ASSL via systematic model extraction from a behavioral subset of the language, and illustrate it on a description of the Voyager II space mission. This way, we close the gap between the design-time and the run-time techniques provided in the SHADOWS platform for self-healing of concurrency, performance, and functional issues.}, 
keywords={computer games;formal verification;learning (artificial intelligence);object-oriented programming;software fault tolerance;specification languages;self healing enabling;autonomic system specification language;ASSL;GEAR game based model checker;gamebased verification;SHADOWS platform;component oriented behaviour extraction;Gears;Specification languages;Runtime;Concurrent computing;Software systems;Aerospace industry;Aerospace electronics;Conferences;Design engineering;Software engineering}, 
doi={10.1109/EASe.2010.23}, 
ISSN={2168-1864}, 
month={March},}
@ARTICLE{4152663, 
author={}, 
journal={IEEE Unapproved Std P487/D7 Feb 2007}, 
title={Unapproved IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{6805151, 
author={Y. Jarraya and T. Madi and M. Debbabi}, 
journal={IEEE Communications Surveys Tutorials}, 
title={A Survey and a Layered Taxonomy of Software-Defined Networking}, 
year={2014}, 
volume={16}, 
number={4}, 
pages={1955-1980}, 
abstract={Software-defined networking (SDN) has recently gained unprecedented attention from industry and research communities, and it seems unlikely that this will be attenuated in the near future. The ideas brought by SDN, although often described as a revolutionary paradigm shift in networking, are not completely new since they have their foundations in programmable networks and control-data plane separation projects. SDN promises simplified network management by enabling network automation, fostering innovation through programmability, and decreasing CAPEX and OPEX by reducing costs and power consumption. In this paper, we aim at analyzing and categorizing a number of relevant research works toward realizing SDN promises. We first provide an overview on SDN roots and then describe the architecture underlying SDN and its main components. Thereafter, we present existing SDN-related taxonomies and propose a taxonomy that classifies the reviewed research works and brings relevant research directions into focus. We dedicate the second part of this paper to studying and comparing the current SDN-related research initiatives and describe the main issues that may arise due to the adoption of SDN. Furthermore, we review several domains where the use of SDN shows promising results. We also summarize some foreseeable future research challenges.}, 
keywords={power consumption;software radio;telecommunication network management;telecommunication power management;software defined networking;SDN;layered taxonomy;revolutionary paradigm shift;programmable networks;control-data plane separation projects;simplified network management;network automation;fostering innovation;programmability;CAPEX;OPEX;power consumption;Control systems;Taxonomy;Software defined networking;Ports (Computers);Network operating systems;Software-defined networking;OpenFlow;programmable networks;controller;management;virtualization;flow}, 
doi={10.1109/COMST.2014.2320094}, 
ISSN={1553-877X}, 
month={Fourthquarter},}
@INPROCEEDINGS{7066751, 
author={A. Malini and N. Venkatesh and K. Sundarakantham and S. Mercyshalinie}, 
booktitle={International Conference on Computing and Communication Technologies}, 
title={Mobile application testing on smart devices using MTAAS framework in cloud}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Testing of mobile applications which run on smart devices is more complex due to diversity of mobile devices and computational resources. Mobile testing using emulators which doesn't include real network traffic and testing mobile applications in more than one portable device in single system was also not possible in normal testing. In order to overcome the draw backs of normal testing, in this paper we deployed a Mobile Testing as a Service (MTAAS) framework in cloud environment. By using MTAAS framework many mobile applications can be tested in different portable devices and different mobile platforms. Testing of mobile applications using MTAAS provides most realistic results since it includes real network speed. Finally, we conducted an experiment in MTAAS framework and testing results shows that MTAAS can effectively reduce the complexity of mobile testing on different smart devices.}, 
keywords={cloud computing;diversity reception;mobile radio;telecommunication traffic;mobile application testing;smart devices;mobile testing as a service;MTAAS framework;mobile device diversity;computational resources;real network traffic;cloud environment;mobile applications;real network speed;Testing;Mobile communication;Mobile computing;Performance evaluation;Smart phones;Load modeling;Cloud Testing;Mobile Application Testing;Testing as a service}, 
doi={10.1109/ICCCT2.2014.7066751}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7503833, 
author={R. Joo and R. Marinheiro and P. Assuno and L. Cruz}, 
booktitle={2016 IEEE International Conference on Communications Workshops (ICC)}, 
title={A flexible monitor for assessing 3D video QoE in real-time}, 
year={2016}, 
volume={}, 
number={}, 
pages={480-485}, 
abstract={With the evolution of 3D technology, 3D IPTV services may prove to be a common service widely distributed by operators. So it is important that they have the necessary means to easily and inexpensively monitor the Quality of Experience (QoE) of this new service. Deployment of 3D video QoE monitors anywhere in the network will enable operators to adapt their service and network infrastructure in order to guarantee a desired QoE level, e.g., in scenarios where 3D IPTV streaming is offered to users with multi-homed equipment and simultaneous access to the network by means of heterogeneous smartcells in the customer premises.}, 
keywords={data encapsulation;IPTV;quality of experience;video coding;network impairment simulation;ITU-T G.1050 recommendation;network embedded device;encapsulation scheme;coding standard;multiple use-case scenario;network infrastructure;quality of experience flexible monitoring;3D IPTV services;real-time 3D video QoE assessment;Monitoring;Streaming media;Three-dimensional displays;Real-time systems;Mathematical model;Artificial neural networks;Computational modeling;3D video;G.1050;monitor;video-plus-depth;MVC}, 
doi={10.1109/ICCW.2016.7503833}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4670300, 
author={N. Guelfi and B. Ries}, 
booktitle={Testing: Academic Industrial Conference - Practice and Research Techniques (taic part 2008)}, 
title={Selection, Evaluation and Generation of Test Cases in an Industrial Setting: A Process and a Tool}, 
year={2008}, 
volume={}, 
number={}, 
pages={47-51}, 
abstract={The test phase in safety-critical systems industry is a crucial phase of the development process. Some companies of these industries have their own test methods which do not reuse the notions available in the theory of software testing or model driven engineering. This paper reports on an experience in a testing process improvement made inside a safety-critical systems company in order to improve the quality of the test phase improvement. We present the initial situation, the objectives, the proposed process and the tools that are used to support it. In particular, we show that the most efficient improvements were achieved concerning the test process definition and in allowing a tailored and precise delimitation of the systempsilas elements to be tested.}, 
keywords={program testing;test cases;industrial setting;safety-critical systems industry;software testing;model driven engineering;System testing;Software testing;Computer industry;Embedded software;Performance evaluation;Laboratories;Software systems;Model driven engineering;Software performance;Software engineering;test selection;model-driven testing;industrial;tool-support;process}, 
doi={10.1109/TAIC-PART.2008.12}, 
ISSN={}, 
month={Aug},}
@INBOOK{6078760, 
author={Stuart Jacobs}, 
booktitle={Engineering Information Security: The Application of Systems Engineering Concepts to Achieve Information Assurance}, 
title={Index}, 
year={2011}, 
volume={}, 
number={}, 
pages={}, 
abstract={
No abstract.

}, 
keywords={}, 
doi={10.1002/9780470947913.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6078760},}
@INPROCEEDINGS{4382117, 
author={S. Ramanathan and C. Lac}, 
booktitle={2007 IEEE International Symposium on Consumer Electronics}, 
title={Use of fault tree analysis to improve residential gateway testing}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={A residential gateway, heart of the strategy of most Telcos, is a centralized intelligent device between the operator's access network and the home's network. It terminates all external access networks and enables residential services to be delivered to the consumer. Besides a plethora of useful services, the growth in market depends upon the reputation of its resilience (availability, reliability and security). This emphasizes a near zero fault design and efficient testing should be taken care before its launch into the market. This paper deals with the analysis of failures, both from test and field data, aiming to increase the efficiency of laboratory testing. Using fault tree analysis, we study the faults that have passed through the testing phase and created failures in the customer premises. With the help of defined specifications, we have identified the zones in which testing in the laboratory needs to be improved.}, 
keywords={broadband networks;fault trees;reliability;fault tree analysis;residential gateway testing;availability;reliability;security;Fault trees;Testing;Failure analysis;Laboratories;Heart;Intelligent networks;Home automation;Resilience;Availability;Data security}, 
doi={10.1109/ISCE.2007.4382117}, 
ISSN={0747-668X}, 
month={June},}
@INBOOK{5273129, 
author={Steven M. Kaplan}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={C}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={
}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch3}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273129},}
@INPROCEEDINGS{8327233, 
author={S. Chodarev and M. Backov}, 
booktitle={2017 IEEE 14th International Scientific Conference on Informatics}, 
title={Development of Oberon-0 using YAJCo}, 
year={2017}, 
volume={}, 
number={}, 
pages={122-127}, 
abstract={YAJCo is a tool for the development of software languages based on an annotated language model. The model is represented by Java classes with annotations defining its mapping to concrete syntax. This approach to language definition enables the abstract syntax to be central point of the development process, instead of concrete syntax. In this paper a case study of Oberon-0 programming language development is presented. The study is based on the LTDA Tool Challenge and showcases details of abstract and concrete syntax definition using YAJCo, as well as implementation of name resolution, type checking, model transformation and code generation.}, 
keywords={Java;program compilers;program verification;programming languages;specification languages;YAJCo;software languages;annotated language model;Java classes;annotations;concrete syntax;language definition;abstract syntax;Oberon-0 programming language development;LTDA Tool Challenge;model transformation;code generation;name resolution;type checking;Syntactics;Tools;Grammar;Java;Generators;Analytical models;Abstract syntax;experience report;language development;Oberon-0;parser generator;YAJCo}, 
doi={10.1109/INFORMATICS.2017.8327233}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{493448, 
author={R. B. Kieburtz and L. McKinney and J. M. Bell and J. Hook and A. Kotov and J. Lewis and D. P. Oliva and T. Sheard and I. Smith and L. Walton}, 
booktitle={Proceedings of IEEE 18th International Conference on Software Engineering}, 
title={A software engineering experiment in software component generation}, 
year={1996}, 
volume={}, 
number={}, 
pages={542-552}, 
abstract={The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.}, 
keywords={automatic programming;software reusability;military computing;human resource management;specification languages;program verification;program interpreters;command and control systems;formal specification;software engineering experiment;software component generation;program generator construction;domain-specific specification languages;reuse technology;reusable Ada program templates;message translation modules;message validation modules;military command control communications and information systems;personnel;independent contractor;message specifications;Air Force systems;productivity;software modules;average performance;confidence levels;Software engineering;Paper technology;Automatic programming;System testing;Specification languages;Military communication;Communication system control;Control systems;Information systems;Personnel}, 
doi={10.1109/ICSE.1996.493448}, 
ISSN={0270-5257}, 
month={March},}
@INPROCEEDINGS{7338996, 
author={M. Borek and K. Stenzel and K. Katkalov and W. Reif}, 
booktitle={2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
title={Abstracting security-critical applications for model checking in a model-driven approach}, 
year={2015}, 
volume={}, 
number={}, 
pages={11-14}, 
abstract={Model checking at the design level makes it possible to find protocol flaws in security-critical applications automatically. But depending on the size of the application and especially on the abstraction of the application model, model checking may need a lot of resources, primarily time. To reduce the complexity, the application models are usually highly abstracted. But in a model-driven approach with automatic generation of runnable applications the application models need to be detailed and are often too complex to check in reasonable time. In this paper we describe an approach to handle this problem by using additional UML models to restrict the protocol runs, the attacker abilities and the numbers of participants. This makes model checking of large applications in our model-driven approach called SecureMDD possible without manual abstraction of the generated specifications. For model checking we use AVANTSSAR and show how the restrictions modeled within UML are translated. We demonstrate our approach with a smart card based electronic ticketing example.}, 
keywords={program diagnostics;program verification;protocols;safety-critical software;Unified Modeling Language;security-critical application abstraction;model-driven approach;design level model checking;application model abstraction;automatic application generation;UML models;SecureMDD;AVANTSSAR;Unified modeling language;Model checking;Security;Protocols;Complexity theory;Manuals;Smart cards;UML;model checking;security-critical systems;model-driven development;transformations;SecureMDD}, 
doi={10.1109/ICSESS.2015.7338996}, 
ISSN={2327-0594}, 
month={Sept},}
@ARTICLE{4040128, 
author={}, 
journal={IEEE Std P1175.2/D11.2}, 
title={Unapproved IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections (Superseded by P1175.2_D12.2)}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{5567088, 
author={Y. Afek and U. Drepper and P. Felber and C. Fetzer and V. Gramoli and M. Hohmuth and E. Riviere and P. Stenstrom and O. Unsal and W. M. Moreira and D. Harmanci and P. Marlier and S. Diestelhorst and M. Pohlack and A. Cristal and I. Hur and A. Dragojevic and R. Guerraoui and M. Kapalka and S. Tomic and G. Korland and N. Shavit and M. Nowack and T. Riegel}, 
journal={IEEE Micro}, 
title={The Velox Transactional Memory Stack}, 
year={2010}, 
volume={30}, 
number={5}, 
pages={76-87}, 
abstract={The adoption of multi- and many-core architectures for mainstream computing undoubtedly brings profound changes in the way software is developed. In particular, the use of fine grained locking as the multi-core programmer's coordination methodology is considered by more and more experts as a dead-end. The transactional memory (TM) programming paradigm is a strong contender to become the approach of choice for replacing locks and implementing atomic operations in concurrent programming. Combining sequences of concurrent operations into atomic transactions allows a great reduction in the complexity of both programming and verification, by making parts of the code appear to execute sequentially without the need to program using fine-grained locking. Transactions remove from the programmer the burden of figuring out the interaction among concurrent operations that happen to conflict when accessing the same locations in memory. The EU-funded FP7 VELOX project designs, implements and evaluates an integrated TM stack, spanning from programming language to the hardware support, and including runtime and libraries, compilers, and application environments. This paper presents an overview of the VELOX TM stack and its associated challenges and contributions.}, 
keywords={concurrency control;multiprocessing systems;parallel architectures;parallel programming;Velox transactional memory stack;multicore architecture;many-core architecture;mainstream computing;fine grained locking;transactional memory programming;concurrent programming;atomic transaction;Velox project design;integrated TM stack;Velox TM stack;parallel programming;Libraries;Runtime;Hardware;Java;Programming;Program processors;concurrent programming;software transactional memory;hardware transactional memory;compilers;language extensions}, 
doi={10.1109/MM.2010.80}, 
ISSN={0272-1732}, 
month={Sept},}
@INPROCEEDINGS{7964337, 
author={J. O. Ringert and B. Rumpe and C. Schulze and A. Wortmann}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET)}, 
title={Teaching agile model-driven engineering for cyber-physical systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={127-136}, 
abstract={Agile development methods, model-driven engineering, and cyber-physical systems are important topics in software engineering education. It is not obvious how to teach their combination while respecting individual challenges posed to students and educators. We have devised a software project class for teaching the agile MDE for CPS. The project class was held in three different semesters. In this paper, we report on the setup of our exploratory study and its goals for teaching. We base our evaluation and insights on interviews and questionnaires. Our results show the feasibility of combination of agile MDE for CPS but also the challenges this combination poses to students and educators.}, 
keywords={computer science education;cyber-physical systems;software engineering;software prototyping;teaching;agile model-driven engineering teaching;cyber-physical systems;agile development method;software engineering education;software project class;agile MDE;CPS;Education;Unified modeling language;Educational robots;Software engineering;Model driven engineering;teaching;model-driven engineering;cyber-physical systems;case study}, 
doi={10.1109/ICSE-SEET.2017.16}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{386480, 
author={S. R. Rao and Bi-Yu Pan and J. R. Armstrong}, 
booktitle={1993 European Conference on Design Automation with the European Event in ASIC Design}, 
title={Hierarchical test generation for VHDL behavioral models}, 
year={1993}, 
volume={}, 
number={}, 
pages={175-182}, 
abstract={In this method, the VHDL model to be tested is represented by its process model graph (PMG). Test sets for individual processes of the model are precomputed and stored in the design library. The Hierarchical Behavioral Test Generator (HBTG) algorithm accepts the PMG and the precomputed tests as inputs, from which it hierarchically constructs a test sequence that tests the functionality of the model. Such an automatic test generation process relieves the modeler of the time-consuming task of developing test-benches. The test sequence generated by HBTG is then used for simulation of the model. Experimental results indicate that the tests generated exercise the model thoroughly.<<ETX>>}, 
keywords={hardware description languages;high level synthesis;design for testability;logic testing;automatic testing;hierarchical test generation;high level testability;VHDL behavioral models;process model graph;algorithm;functionality;automatic test generation process;simulation;Circuit testing;Automatic testing;Circuit faults;Libraries;Computational modeling;Circuit simulation;Computer simulation;Computer industry;Hardware;Design engineering}, 
doi={10.1109/EDAC.1993.386480}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{5541042, 
author={Yan Huan and Miao Changyun and Wu Zhigang}, 
booktitle={2010 International Conference On Computer Design and Applications}, 
title={The design of IP telephony media gateway which is based on soft-switching technology}, 
year={2010}, 
volume={5}, 
number={}, 
pages={V5-379-V5-381}, 
abstract={This paper presents a new type of design method for IP telephony gateway which is base on the soft-switch technology. It designs the hardware circuit with TMS320C5402 as its core and develops a telephone communication protocol which is based on UDP / IP. The Trunk Gateway supports MGCP protocol standards. Besides, it can fulfill lots of functions including audio processing, voice codec, and signaling tone generation and detection.}, 
keywords={access protocols;Internet telephony;IP telephony media gateway;soft-switching technology;hardware circuit;telephone communication protocol;audio processing;voice codec;signaling tone generation;Telephony;Protocols;Design methodology;Hardware;Circuits;Communication standards;Standards development;Codecs;Signal processing;Signal generators;soft-switching;IP phones gateways;Media Gateway Controller (MGC);Media Gateway (MG);signaling;protocol}, 
doi={10.1109/ICCDA.2010.5541042}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{554009, 
author={O. Tanir}, 
booktitle={Wescon/96}, 
title={Specification driven design of complex systems}, 
year={1996}, 
volume={}, 
number={}, 
pages={327-332}, 
abstract={This paper examines the benefits of applying a specification driven approach and presents a framework for environments that can support the related design activities. The paper also outlines interrelated advanced topics such as intermediate architecture languages, model libraries and model verification issues.}, 
keywords={design engineering;CAD;specification driven design;complex system;intermediate architecture language;model library;model verification;Costs;Hardware;Design engineering;Audio systems;Switches;Software performance;Software design;Libraries;Testing}, 
doi={10.1109/WESCON.1996.554009}, 
ISSN={1095-791X}, 
month={Oct},}
@INPROCEEDINGS{4700654, 
author={A. Khoche and P. Burlison and J. Rowe and G. Plowman}, 
booktitle={2008 IEEE International Test Conference}, 
title={A Tutorial on STDF Fail Datalog Standard}, 
year={2008}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Advances in technology are making it imperative to collect detailed structural IC fail data during manufacturing test to improve yield. However, there is currently no standard format for communicating and storing such structural fail data efficiently. This leads to ad-hoc tool-specific solutions, which do not offer interoperability required in a typical multi-tool, multi-vendor customer environment. These ad-hoc solutions result in unnecessary investment in development of point-to-point interface solutions that are ultimately still not integrated with the traditional data collection for a unified yield analysis data format. Expanding an established datalogging standard to accommodate the new requirements solves these issues. Standard Test Data Format (STDF) is the predominant format used today for traditional failure datalogging storage, but in its current form falls far short in handling the new high-volume structural failures for yield learning. A group of more than 20 companies from ATE, EDA, Semiconductor and Yield Management companies has been working on a new enhanced STDF standard that addresses the new requirements. This paper provides the overview of the new enhanced standard.}, 
keywords={data loggers;failure analysis;integrated circuit manufacture;integrated circuit reliability;production engineering computing;system monitoring;STDF fail datalog standard;IC fail data;standard test data format;failure datalogging storage;Tutorial;Electronic design automation and methodology;Production;Manufacturing;Silicon;Integrated circuit testing;Communication standards;Investments;Data analysis;Test pattern generators}, 
doi={10.1109/TEST.2008.4700654}, 
ISSN={1089-3539}, 
month={Oct},}
@INPROCEEDINGS{7352573, 
author={Y. Bandung and L. B. Subekti and Y. S. Gondokaryono}, 
booktitle={2015 International Conference on Electrical Engineering and Informatics (ICEEI)}, 
title={A design of teacher portal for supporting teacher's internet literacy web based solution for teacher learning}, 
year={2015}, 
volume={}, 
number={}, 
pages={618-623}, 
abstract={There are a lot of valuable materials available in the Internet to support learning and self-improvement. But in some developing countries like Indonesia, Internet penetration rate is still low and people still don't comprehend on how to get benefit from it. In this research, a web based solution to support teacher's Internet literacy is designed. This solution is a teacher portal system which provides featured contents and files management system for teacher. Some application modules have been developed to support this system those are web-based portal, file management system, chatting system, and content aggregator module as the main component of the system builder.}, 
keywords={computer aided instruction;computer literacy;file organisation;portals;teacher training;teacher portal design;teacher Internet literacy;learning improvement;self-improvement;developing countries;Indonesia;Internet penetration rate;content management system;Web-based portal;file management system;chatting system;content aggregator module;system builder;Servers;Internet;Portals;Synchronization;Feature extraction;Computer architecture;Education;Internet literacy;teacher portal;files management;information and communication technology}, 
doi={10.1109/ICEEI.2015.7352573}, 
ISSN={2155-6830}, 
month={Aug},}
@INPROCEEDINGS{6693140, 
author={A. Gambi and W. Hummer and S. Dustdar}, 
booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Automated testing of cloud-based elastic systems with AUToCLES}, 
year={2013}, 
volume={}, 
number={}, 
pages={714-717}, 
abstract={Cloud-based elastic computing systems dynamically change their resources allocation to provide consistent quality of service and minimal usage of resources in the face of workload fluctuations. As elastic systems are increasingly adopted to implement business critical functions in a cost-efficient way, their reliability is becoming a key concern for developers. Without proper testing, cloud-based systems might fail to provide the required functionalities with the expected service level and costs. Using system testing techniques, developers can expose problems that escaped the previous quality assurance activities and have a last chance to fix bugs before releasing the system in production. System testing of cloud-based systems accounts for a series of complex and time demanding activities, from the deployment and configuration of the elastic system, to the execution of synthetic clients, and the collection and persistence of execution data. Furthermore, clouds enable parallel executions of the same elastic system that can reduce the overall test execution time. However, manually managing the concurrent testing of multiple system instances might quickly overwhelm developers' capabilities, and automatic support for test generation, system test execution, and management of execution data is needed. In this demo we showcase AUToCLES, our tool for automatic testing of cloud-based elastic systems. Given specifications of the test suite and the system under test, AUToCLES implements testing as a service (TaaS): It automatically instantiates the SUT, configures the testing scaffoldings, and automatically executes test suites. If required, AUToCLES can generate new test inputs. Designers can inspect executions both during and after the tests.}, 
keywords={cloud computing;program testing;automated testing;AUToCLES;cloud-based elastic computing system;resources allocation;quality of service;business critical function;system testing technique;quality assurance;concurrent testing;test generation;system test execution;testing as a service;TaaS;SUT;Elasticity;Monitoring;Standards;System testing;Cloud computing}, 
doi={10.1109/ASE.2013.6693140}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7733581, 
author={R. Awad and G. Heppner and A. Roennau and M. Bordignon}, 
booktitle={2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={ROS engineering workbench based on semantically enriched app models for improved reusability}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this work, the ReApp Engineering Workbench and its underlying semantically enriched app models are presented. The usage of a model, which describes the apps functionality, interfaces and other attributes, allows the utilization of engineering tools for code generation and automated testing. Further, it ensures the compatibility of the generated interfaces, which in turn enhances the reusability of the developed apps in larger applications.}, 
keywords={program compilers;software reusability;ROS engineering workbench;app models;reusability;ReApp engineering workbench;apps functionality;engineering tools;code generation;automated testing;Biological system modeling;Unified modeling language;Hardware;Model driven engineering;Robot kinematics}, 
doi={10.1109/ETFA.2016.7733581}, 
ISSN={}, 
month={Sept},}
@ARTICLE{6179576, 
author={Y. A. Katsigiannis and P. S. Georgilakis and E. S. Karapidakis}, 
journal={IEEE Transactions on Sustainable Energy}, 
title={Hybrid Simulated AnnealingTabu Search Method for Optimal Sizing of Autonomous Power Systems With Renewables}, 
year={2012}, 
volume={3}, 
number={3}, 
pages={330-338}, 
abstract={Small autonomous power systems (SAPS) that include renewable energy sources are a promising option for isolated power generation at remote locations. The optimal sizing problem of SAPS is a challenging combinatorial optimization problem, and its solution may prove a very time-consuming process. This paper initially investigates the performance of two popular metaheuristic methods, namely, simulated annealing (SA) and tabu search (TS), for the solution of SAPS optimal sizing problem. Moreover, this paper proposes a hybrid SA-TS method that combines the advantages of each one of the above-mentioned metaheuristic methods. The proposed method has been successfully applied to design an SAPS in Chania region, Greece. In the study, the objective function is the minimization of SAPS cost of energy (/kWh), and the design variables are: 1) wind turbines size, 2) photovoltaics size, 3) diesel generator size, 4) biodiesel generator size, 5) fuel cells size, 6) batteries size, 7) converter size, and 8) dispatch strategy. The performance of the proposed hybrid optimization methodology is studied for a large number of alternative scenarios via sensitivity analysis, and the conclusion is that the proposed hybrid SA-TS improves the obtained solutions, in terms of quality and convergence, compared to the solutions provided by individual SA or individual TS methods.}, 
keywords={combinatorial mathematics;power generation dispatch;renewable energy sources;search problems;simulated annealing;wind turbines;hybrid simulated annealing-Tabu search method;small autonomous power systems;renewable energy sources;isolated power generation;remote locations;combinatorial optimization problem;time-consuming process;SA;SAPS optimal sizing problem;hybrid SA-TS method;above-mentioned metaheuristic methods;Chania region;Greece;wind turbines;photovoltaics size;diesel generator size;biodiesel generator size;fuel cell size;battery size;converter size;dispatch strategy;hybrid optimization methodology;Generators;Batteries;Fuels;Power systems;Simulated annealing;Renewable energy resources;Hybrid power systems;optimal sizing;optimization methods;power generation dispatch;renewable energy sources;simulated annealing (SA);small autonomous power systems (SAPS);solar energy;tabu search (TS);wind energy}, 
doi={10.1109/TSTE.2012.2184840}, 
ISSN={1949-3029}, 
month={July},}
@INPROCEEDINGS{1552877, 
author={Y. Fan and E. A. Kendall}, 
booktitle={IEEE International Conference on e-Business Engineering (ICEBE'05)}, 
title={A hybrid dialogue strategy for speech-enabled mobile commerce}, 
year={2005}, 
volume={}, 
number={}, 
pages={110-117}, 
abstract={Designing a dialogue strategy for speech-enabled mobile commerce is a significant challenge due to the context. This paper introduces a hybrid dialogue strategy to overcome the inflexibility of application-directed interactions while avoiding the significant recognition difficulty of a full mixed-initiative style. The system uses N-gram grammars to govern the recognition at the request segment of a dialogue, and employs an application-directed strategy at the clarification discourse segment. The paper also details generating a corpus for the N-gram grammar through a case-based reasoning approach, and constructing application-directed dialogues with decision trees. Our preliminary testing indicates the strategy is a feasible and effective solution for voice-enabling mobile commerce applications}, 
keywords={case-based reasoning;decision trees;electronic commerce;interactive systems;mobile computing;speech-based user interfaces;hybrid dialogue strategy;speech-enabled mobile commerce;application-directed interaction;N-gram grammar;case-based reasoning approach;decision trees;voice-enabling mobile commerce;Business;Natural languages;Computer networks;Speech;Cities and towns;Mobile computing;Decision trees;Testing;Standards development;Control systems}, 
doi={10.1109/ICEBE.2005.6}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4101803, 
author={S. J. Ring}, 
journal={IEEE Transactions on Aerospace and Electronic Systems}, 
title={A Distributed Intelligence Automatic Test System for PATRIOT}, 
year={1977}, 
volume={AES-13}, 
number={3}, 
pages={264-272}, 
abstract={An automatic test system supporting high volume production testing of diverse state-of-the-art electronic assemblies is described. The test complex consists of a centralized computer system communicating to a network of satellite stations, each structured as "Intelligent Test Centers" dedicated to a particular family of assemblies (e.g., analog, digital, microwave). Allocation of resources and tasks have been distributed for optimum efficiency of production testing. This paper describes the organization and characteristics of the test system. Test center operation is explained with emphasis given to unique man-machine interactive features designed for on-line generation, examination, and maintenance of Unit-Under-Test (UUT) programs. Details are presented of the test language, RATEL, used for UUT programming. Other aspects that are discussed include test data and UUT program characteristics.}, 
keywords={Intelligent systems;Automatic testing;System testing;Electronic equipment testing;Assembly systems;Production systems;Analog computers;Computer networks;Artificial satellites;Intelligent networks}, 
doi={10.1109/TAES.1977.308394}, 
ISSN={0018-9251}, 
month={May},}
@INPROCEEDINGS{6389024, 
author={P. Gananchchelvi and Jiao Yu and M. S. Pukish}, 
booktitle={IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society}, 
title={Current trends in in-vehicle electrical engineering applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={6268-6273}, 
abstract={Novel electrical engineering applications play a major role in in-vehicle technology. Today's automobile industry is transforming from mechanically driven functions to electronic and software driven functions. We can see the impact of electrical engineering technology in many parts of vehicle and in all of its operations such as control, power conversion, navigation, communication, entertainment, safety and security. In addition, today's automobile industry is showing a renewed interest in electric transportation. This paper presents a review on the recent developments in the in-vehicle electrical engineering applications classified into two sections, the embedded systems and the power and energy system, both of which can strongly influence the automobile industry in the future.}, 
keywords={automobile industry;electric vehicles;electrical engineering;embedded systems;transportation;in-vehicle electrical engineering applications;electrical engineering applications;in-vehicle technology;automobile industry;mechanically driven functions;software driven functions;electrical engineering technology;power conversion;navigation;communication;entertainment;safety;security;electric transportation;in-vehicle electrical engineering;embedded systems;energy system;power system;Field programmable gate arrays;Supercapacitors;Lead;Batteries;Digital signal processing;Frequency conversion;Frequency estimation;in-vehicle electronics;embedded systems;ECU;FPGA;EV;ultracapacitors}, 
doi={10.1109/IECON.2012.6389024}, 
ISSN={1553-572X}, 
month={Oct},}
@ARTICLE{50774, 
author={W. K. Ehrlich and S. K. Lee and R. H. Molisani}, 
journal={IEEE Software}, 
title={Applying reliability measurement: a case study}, 
year={1990}, 
volume={7}, 
number={2}, 
pages={56-64}, 
abstract={The problem of knowing when to stop testing software is considered, focusing on the strategy of stopping when a reliability level or rate of failure occurrence acceptable to the customer is reached. The system's reliability is monitored throughout the system test, and the system is released to the field only when the measured reliability is at or above this objective. This approach was applied to test-failure data collected on Remote Measurement System-Digital 1, a large telecommunications testing system that had already gone through system test and been released to the field. The RMS-D1 failure data, which consisted of command-response errors versus commands executed, had been routinely collected by the system-test organization during testing. The testing phase analyzed, the load test, was an operational-profile-driven test in which a controlled load was imposed on the system reflective of the system's busy-hour usage pattern. It was found to be feasible to apply the reliability-measurement approach in real time, to systems actually undergoing system test, given a controlled load-test environment.<<ETX>>}, 
keywords={software reliability;software reliability measurement;rate of failure occurrence;Remote Measurement System-Digital 1;telecommunications testing system;controlled load-test environment;System testing;Control systems;Software testing;Reliability;Condition monitoring;Remote monitoring;Pattern analysis;System buses;Real time systems}, 
doi={10.1109/52.50774}, 
ISSN={0740-7459}, 
month={March},}
@INPROCEEDINGS{5431981, 
author={H. Esquivel and A. Akella and T. Mori}, 
booktitle={2010 Second International Conference on COMmunication Systems and NETworks (COMSNETS 2010)}, 
title={On the effectiveness of IP reputation for spam filtering}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Modern SMTP servers apply a variety of mechanisms to stem the volume of spam delivered to users. These techniques can be broadly classified into two categories: pre-acceptance approaches, which apply prior to a message being accepted (e.g. IP reputation), and post-acceptance techniques which apply after a message has been accepted (e.g. content based signatures). We argue that the effectiveness of these measures varies based on the SMTP sender type. This paper focuses on the most light-weight pre-acceptance filtering mechanism-IP reputation. We first classify SMTP senders into three main categories: legitimate servers, end-hosts, and spam gangs, and empirically study the limits of effectiveness regarding IP reputation filtering for each category. Next, we develop new techniques that build custom IP reputation lists, which significantly improve the performance of existing IP reputation lists. In compiling these lists, we leverage a somewhat surprising fact that both legitimate domains and spam domains often use the DNS Sender Policy Framework (SPF) in an attempt to pass simple authentication checks. That is, good/bad IP addresses can be systematically compiled by collecting good/bad domains and looking up their SPF resource records. We also evaluate the effectiveness of these lists over time. Finally, we aim to understand the characteristics of the three categories of email senders in depth. Overall, we find that it is possible to construct IP reputation lists that can identify roughly 90% of all spam and legitimate mail, but some of the lists, i.e. the lists for spam gangs, must be updated on a constant basis to maintain this high level of accuracy.}, 
keywords={e-mail filters;information filtering;Internet;IP networks;unsolicited e-mail;IP reputation;spam filtering;legitimate servers;end-hosts;spam gangs;SMTP servers;DNS sender policy framework;email senders;spam mail;legitimate mail;postacceptance techniques;lightweight preacceptance filtering mechanism;Filtering;Testing;Optical filters;Authentication;Protocols;Optical character recognition software;Unsolicited electronic mail;Laboratories;Postal services;Optical recording}, 
doi={10.1109/COMSNETS.2010.5431981}, 
ISSN={2155-2487}, 
month={Jan},}
@INBOOK{7406343, 
author={Stuart Jacobs}, 
booktitle={Engineering Information Security: The Application of Systems Engineering Concepts to Achieve Information Assurance}, 
title={Index}, 
year={2016}, 
volume={}, 
number={}, 
pages={}, 
abstract={

No abstract.

}, 
keywords={}, 
doi={10.1002/9781119104728.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7406343},}
@INPROCEEDINGS{8449448, 
author={C. Krher and S. El-Sharkawy and K. Schmid}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={KernelHaven  An Experimentation Workbench for Analyzing Software Product Lines}, 
year={2018}, 
volume={}, 
number={}, 
pages={73-76}, 
abstract={Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem. KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.}, 
keywords={Data mining;Pipelines;Data models;Feature extraction;Tools;Analytical models;Software engineering;Software product line analysis;variability extraction;static analysis;empirical software engineering}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@INPROCEEDINGS{8369576, 
author={G. Hains and A. Jakobsson and Y. Khmelevsky}, 
booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
title={Towards formal methods and software engineering for deep learning: Security, safety and productivity for dl systems development}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Deep Learning (DL) techniques are now widespread and being integrated into many important systems. Their classification and recognition abilities ensure their relevance for multiple application domains far beyond pure signal processing. As a machine-learning technique that relies on training instead of explicit algorithm programming they offer a high degree of productivity. But recent research has shown that they can be vulnerable to attacks and the verification of their correctness is only just emerging as a scientific and engineering possibility. Moreover DL tools are not integrated into classical software engineering so software tools to specify, modify and verify them would make them even more mainstream as software-hardware systems. This paper surveys recent work and proposes research directions and methodologies for this purpose.}, 
keywords={formal verification;learning (artificial intelligence);pattern classification;security of data;software engineering;software tools;formal methods;recognition abilities;multiple application;machine-learning technique;classical software engineering;software tools;software-hardware systems;systems development;DL techniques;classification abilities;signal processing;Deep Learning;Artificial neural networks;Safety;Testing;Machine learning;Tools;Security;Training;deep-learning systems;neural networks;vulnerability of deep learning;security;verification;software engineering}, 
doi={10.1109/SYSCON.2018.8369576}, 
ISSN={2472-9647}, 
month={April},}
@INPROCEEDINGS{8088307, 
author={S. Grning and C. Rosas and C. Wietfeld}, 
booktitle={2017 IEEE International Systems Engineering Symposium (ISSE)}, 
title={Validating electric vehicle to grid communication systems based on model checking assisted test case generation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In last decades software development processes changed in order to address increasing complexity within decreasing implementation time. Hence, new practices like Kanban, Extreme Programming or Agile Software Development emerged. Model-based development is one potential option, which is more and more used to cope these new demands. However, adapting testing processes to the needs is still an open topic. This paper describes how model checking assisted test case generation can be used to integrate testing in new software development processes, focusing on a protocol implementation for electric vehicle charging communication as a case study. Therefore, it describes certain extensions made in the COMmunication Protocol vaLidation Toolchain COMFLgTg in order to enable test case generation in TTCN-3 core language using counterexamples of SPIN model checker.}, 
keywords={electric vehicle charging;electric vehicles;formal specification;power engineering computing;power grids;program testing;protocols;software prototyping;implementation time;Agile Software Development;SPIN model checker;grid communication systems;software development processes;testing processes;model checking assisted test case generation;Kanban;extreme programming;model-based development;electric vehicle vehicle charging communication;COMmunication Protocol vaLidation Toolchain;COMFLgTg;TTCN-3 core language;Unified modeling language;Protocols;Machine-to-machine communications;Model checking;Adaptation models;Grammar}, 
doi={10.1109/SysEng.2017.8088307}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4267608, 
author={P. G. Bassett}, 
journal={IEEE Software}, 
title={The Case for Frame-Based Software Engineering}, 
year={2007}, 
volume={24}, 
number={4}, 
pages={90-99}, 
abstract={Frame technology adapts generic components into custom information structures. Its facility for maximizing reuse and minimizing redundancy has demonstrated dramatic improvements across software's life cycle.}, 
keywords={object-oriented programming;software reusability;software engineering;frame technology;generic components;custom information structures;reuse maximation;redundancy minimization;software life cycle;Software engineering;Programming profession;Software maintenance;Redundancy;Software systems;Software quality;Computer industry;Humans;Statistics;Organizing;reuse models;automatic programming;evolutionary programming;software engineering process}, 
doi={10.1109/MS.2007.119}, 
ISSN={0740-7459}, 
month={July},}
@ARTICLE{6781158, 
author={P. A. Battaglia and C. C. Byers and L. A. Guth and A. Holliday and C. Spinelli and J. J. Tong}, 
journal={Bell Labs Technical Journal}, 
title={Modular platform vision and strategy}, 
year={2004}, 
volume={9}, 
number={1}, 
pages={121-142}, 
abstract={A platform is a set of hardware, software, and process building blocks that can form the basis of many different products. The goals of platforms are to reuse and/or share assets wherever possible, to save on development costs, to spread fixed development and production costs across the largest volumes, and to offer highly integrated solutions, all while maintaining critical differentiation of products. Using platforms correctly can produce substantial life-cycle cost benefits and can lead to enhanced supplier management and customer satisfaction. This paper will begin by describing the general concept of platforms. It will then consider their economic benefits to development teams, suppliers, and customers. Next, it will discuss the role of industry standards in forming the basis of a platform offer. It will consider the changing architectures of telecommunications networks, along with the contribution of platforms to these changes. Finally, the paper will outline an example set of platform building blocks and their requirements, along with some case studies of how to combine these building blocks into products.}, 
keywords={}, 
doi={10.1002/bltj.20009}, 
ISSN={1538-7305}, 
month={},}
@INPROCEEDINGS{7781923, 
author={E. Klein and A. Gschwend and A. C. Neuroni}, 
booktitle={2016 Conference for E-Democracy and Open Government (CeDEM)}, 
title={Towards a Linked Data Publishing Methodology}, 
year={2016}, 
volume={}, 
number={}, 
pages={188-196}, 
abstract={Linked open government data (LOGD) can be a catalyst in the development of value-added services and products. The vision of many Linked Open Data (LOD) projects is to make publishing and reuse of linked data as easy as possible for the end user thanks to a thriving marketplace with data publishers, developers, and consumers along the value chain. In the large scale LOD project "Fusepool P3", tourism-related applications and software components were developed that support data owners and open data enthusiasts in transforming legacy data to linked data. Based on experiences from this project, we present reflections and discuss pitfalls in drawing a linked data publishing methodology. An integrated view on all phases of the publishing process has not been described so far, for the technical phases linked data life-cycles have been identified only. The methodology developed enables stakeholders to transfer the lessons learned to other use cases and application contexts. This allows for better estimation of efforts and skills for future LOD projects.}, 
keywords={government data processing;Linked Data;publishing;Linked Data publishing methodology;linked open government data;LOGD;value-added service development;product development;Linked Data reusability;Fusepool P3 large scale LOD project;tourism-related applications;software components;legacy data;Linked Data life-cycles;Publishing;Stakeholders;Context;Government;Data models;Software;Portals;linked open data;data publishing;linked data life-cycle;publishing methodology;linked data platform}, 
doi={10.1109/CeDEM.2016.12}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{114002, 
author={L. Bonet and J. Ganger and J. Girardeau and C. Greaves and M. Pendleton and D. Yatim}, 
booktitle={Proceedings. International Test Conference 1990}, 
title={Test features of the MC145472 ISDN U-transceivers}, 
year={1990}, 
volume={}, 
number={}, 
pages={68-79}, 
abstract={The design of a single-chip implementation of a 2B1Q ISDN (integrated services digital network) U transceiver that meets the ANSI T1.601 standards has been completed. The MC145472 was designed with testability in mind and to be consistent with Motorola's design-for-manufacturability goals. The authors describe in detail the design-for-testability techniques specifically intended for the IC manufacturer production test and other ad hoc test/diagnostic structures for the customer to use in evaluating system performance. A global test strategy for testing the ISDN U transceiver is presented. The test features have been used extensively not only for testing the device in the production environment but also for conducting evaluations and design verification experiments during the chip debugging phase. The test features described are well integrated with the architecture of the chip, thus minimizing incremental cost.<<ETX>>}, 
keywords={automatic testing;electronic equipment testing;ISDN;production testing;transceivers;Motorola;MC145472 ISDN U-transceivers;2B1Q ISDN;integrated services digital network;design-for-manufacturability;design-for-testability;IC manufacturer production test;design verification;chip debugging;cost;ISDN;Transceivers;Integrated circuit testing;System testing;ANSI standards;Manufacturing;Production systems;System performance;Debugging;Costs}, 
doi={10.1109/TEST.1990.114002}, 
ISSN={}, 
month={Sept},}
@ARTICLE{5389336, 
author={B. Wile and M. P. Mullen and C. Hanson and D. G. Bair and K. M. Lasko and P. J. Duffy and E. J. Kaminski and T. E. Gilbert and S. M. Licker and R. G. Sheldon and W. D. Wollyung and W. J. Lewis and R. J. Adkins}, 
journal={IBM Journal of Research and Development}, 
title={Functional verification of the CMOS S/390 Parallel Enterprise Server G4 system}, 
year={1997}, 
volume={41}, 
number={4.5}, 
pages={549-566}, 
abstract={Verification of the S/390 Parallel Enterprise Server G4 processor and level 2 cache (L2) chips was performed using a different approach than previously. This paper describes the methods employed by our functional verification team to demonstrate that its logical system complied with the S/390 architecture while staying within the changing cost structure and time-to-market constraints. Verification proceeded at four basic levels defined by the breadth of logic being tested. The lowest level, designer macro verification, contained a single designer's hardware description language (in VHDL). Unit-level verification consisted of a logical portion of function that generally contained four or five designers' logic. The third level of verification was the chip level, in which the processor or L2 chips were individually tested. Finally, system-level verification was performed on symmetric multiprocessor (SMP) configurations that included bus-switching network (BSN) chips and I/O connection chips, designated as memory bus adaptors (MBAs), along with multiple copies of the processor and L2 chips.}, 
keywords={}, 
doi={10.1147/rd.414.0549}, 
ISSN={0018-8646}, 
month={July},}
@INPROCEEDINGS{8502653, 
author={A. S. Nezhad and J. J. Lukkien and R. H. Mak}, 
booktitle={2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Behavior-driven Development for Real-time Embedded Systems}, 
year={2018}, 
volume={1}, 
number={}, 
pages={59-66}, 
abstract={Embedded systems are a class of computer systems that are typically characterized by a tight interaction with the physical environment. Various methodologies have been adopted for the development of such systems, ranging from traditional waterfall to modern agile techniques. One of the agile techniques that has recently attracted increasing attention is Behavior-Driven Development (BDD). BDD promotes the engagement of all stakeholders in every development iteration to minimize the misunderstanding between technical and non-technical stakeholders and, consequently, to speed up the development process and lower the costs. In this paper, we investigate the application of BDD to the development of embedded systems, especially focusing on the testing of timing requirements for real-time embedded software. In particular, we extend BDD with time-related concepts and propose an approach to generate test code for the verification of timing behavior of real-time embedded systems. Our approach offers more automation for the development of test code compared to existing BDD tools, thus minimizing the risk of timing faults and reducing development costs and time-to-market.}, 
keywords={Logic gates;Testing;Timing;Embedded systems;Real-time systems;Stakeholders}, 
doi={10.1109/ETFA.2018.8502653}, 
ISSN={1946-0759}, 
month={Sept},}
@INPROCEEDINGS{7588753, 
author={L. Chen and P. James and D. Kirkwood and H. N. Nguyen and G. L. Nicholson and M. Roggenbach}, 
booktitle={2016 IEEE International Conference on Intelligent Rail Transportation (ICIRT)}, 
title={Towards integrated simulation and formal verification of rail yard designs - an experience report based on the UK East Coast Main Line}, 
year={2016}, 
volume={}, 
number={}, 
pages={347-355}, 
abstract={The development of railway systems is often supported by a range of tools, each addressing individual, but overlapping concerns such as, e.g., performance or safety analysis. However, it is a challenge for users to organise work-flows; results are often in different, non-aligning data formats; furthermore, tools work on different levels of abstraction from macro to microscopic. Thus, tool integration would be beneficial, and also allow for more playful, experimental prototyping and design. This paper reports on lessons learned from the integration of BRaVE - the Birmingham Railway Virtual Environment - and OnTrack from Swansea University. BRaVE is an easy-to-use railway simulation software for development, modelling and flow analysis. OnTrack allows for the automatic verification of scheme plans against a number of safety properties via different formal methods. We present an approach that bridges the gap that occurs from varying details in data sources through automated transformations. This integration provides a first step towards a seamless environment for prototyping, concept development, and safety analysis under one roof. We demonstrate the usefulness of our approach by giving integrated simulation and verification results for the UK East Coast Main Line. This work is part of the wider RSSB's Future Traffic Regulation Optimisation research programme.}, 
keywords={formal verification;railway engineering;software engineering;rail yard designs;formal verification;UK East Coast Main Line;railway system development;nonaligning data formats;tool integration;BRaVE;Birmingham Railway Virtual Environment;Swansea University;railway simulation software;safety properties;RSSB future traffic regulation optimisation research programme;Rail transportation;Solid modeling;Analytical models;Data models;Computational modeling;Vehicles}, 
doi={10.1109/ICIRT.2016.7588753}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{4031222, 
author={J. P. Almeida and P. V. Eck and M. Iacob}, 
booktitle={2006 10th IEEE International Enterprise Distributed Object Computing Conference (EDOC'06)}, 
title={Requirements Traceability and Transformation Conformance in Model-Driven Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={355-366}, 
abstract={The variety of design artefacts (models) produced in a model-driven design process results in an intricate relationship between requirements and the various models. This paper proposes a methodological framework that simplifies management of this relationship. This framework is a basis for tracing requirements, assessing the quality of model transformation specifications, metamodels, models and realizations. We propose a notion of conformance between application models which reduces the effort needed for assessment activities. We discuss how this notion of conformance can be integrated with model transformations}, 
keywords={formal specification;systems analysis;requirements traceability;model-driven design;transformation conformance;model-driven development;model transformation specification;Process design;Application software;Testing;Design engineering;Telematics;Information technology;Buildings;Distributed computing;requirements traceability;assessment;con-formance;model transformation;model-driven design}, 
doi={10.1109/EDOC.2006.45}, 
ISSN={1541-7719}, 
month={Oct},}
@ARTICLE{7992926, 
author={B. Garcia and F. Gortazar and L. Lopez-Fernandez and M. Gallego and M. Paris}, 
journal={IEEE Communications Standards Magazine}, 
title={WebRTC Testing: Challenges and Practical Solutions}, 
year={2017}, 
volume={1}, 
number={2}, 
pages={36-42}, 
abstract={WebRTC comprises a set of novel technologies and standards that provide Real-Time Communication on Web browsers. WebRTC makes simple the embedding of voice and video communications in all types of applications. However, releasing those applications to production is still very challenging due to the complexity of their testing. Validating a WebRTC service requires assessing many functional (e.g. signaling logic, media connectivity, etc.) and non-functional (e.g. quality of experience, interoperability, scalability, etc.) properties on large, complex, distributed and heterogeneous systems that spawn across client devices, networks and cloud infrastructures. In this article, we present a novel methodology and an associated tool for doing it at scale and in an automated way. Our strategy is based on a blackbox end-to-end approach through which we use an automated containerized cloud environment for instrumenting Web browser clients, which benchmark the SUT (system under test), and fake clients, that load it. Through these benchmarks, we obtain, in a reliable and statistically significant way, both network-dependent QoS (Quality of Service) metrics and media-dependent QoE (Quality of Experience) indicators. These are fed, at a second stage, to a number of testing assertions that validate the appropriateness of the functional and non-functional properties of the SUT under controlled and configurable load and fail conditions. To finish, we illustrate our experiences using such tool and methodology in the context of the Kurento open source software project and conclude that they are suitable for validating large and complex WebRTC systems at scale.}, 
keywords={cloud computing;online front-ends;WebRTC testing;real-time communication;voice communications;video communications;WebRTC service;distributed systems;heterogeneous systems;client devices;cloud infrastructures;automated containerized cloud environment;Web browser clients;SUT;system under test;fake clients;network-dependent QoS metrics;Quality of Service metrics;media-dependent QoE indicators;Quality of Experience indicators;Kurento open source software project;complex WebRTC systems;WebRTC;Browsers;Telecommunication traffic;Media;Real-time systems;Quality of service;Internet}, 
doi={10.1109/MCOMSTD.2017.1700005}, 
ISSN={2471-2825}, 
month={},}
@INPROCEEDINGS{8109427, 
author={L. Nobach and J. Blendin and H. Kolbe and G. Schyguda and D. Hausheer}, 
booktitle={2017 IEEE 42nd Conference on Local Computer Networks (LCN)}, 
title={Bare-Metal Switches and Their Customization and Usability in a Carrier-Grade Environment}, 
year={2017}, 
volume={}, 
number={}, 
pages={649-657}, 
abstract={The current ecosystem of network elements, such as switches and appliances, is largely dominated by devices supplied and sold with a bundled operating system, and software dedicated to manage the device's forwarding hardware, however, these platforms are not open-source and cannot be arbitrarily customized, and there is no cost transparency or flexibility in choosing software different to the bundled components.,,,,In this paper, we explore the capabilities of bare-metal switches, which are equipped with commodity switching hardware components, but shipped without an operating system. We evaluate the feasibility of these commonly lower-cost devices to meet the requirements of a customized, carrier-grade network function. Therefore, we have implemented a prototype on generic hardware, re-using as much open-source software as possible. Our Broadband Remote Access Server (BRAS) prototype can lower the cost compared to proprietary network appliances, and, known to have a hardware backplane capacity of 720 Gbps, the merchant-silicon / ASIC approach can highly outperform the state of the art of current x86-based virtualized network functions, while implementing the most important BRAS features.}, 
keywords={application specific integrated circuits;broadband networks;computer centres;computer network management;Internet;network servers;operating systems (computers);public domain software;virtualisation;lower-cost devices;customized carrier-grade network function;generic hardware;open-source software;proprietary network appliances;hardware backplane capacity;virtualized network functions;bare-metal switches;customization;usability;carrier-grade environment;current ecosystem;network elements;bundled operating system;cost transparency;bundled components;commodity switching hardware components;broadband remote access server prototype;Hardware;Software;Servers;Linux;Ports (Computers);Switches;Bare-Metal Switching;Dataplanes;Network Functions;Middleboxes;Sofware-Defined Networking;Cost-Efficiency}, 
doi={10.1109/LCN.2017.104}, 
ISSN={0742-1303}, 
month={Oct},}
@INPROCEEDINGS{6030046, 
author={S. Hutchesson and J. McDermid}, 
booktitle={2011 15th International Software Product Line Conference}, 
title={Towards Cost-Effective High-Assurance Software Product Lines: The Need for Property-Preserving Transformations}, 
year={2011}, 
volume={}, 
number={}, 
pages={55-64}, 
abstract={Generative programming and model transformation techniques are becoming widely used for the development of software components for product lines. The ability to develop components with identified common and variable parts, and rapidly instantiate product-specific versions is key to many software product line approaches. However if this approach is to be truly cost effective for high assurance applications, the instantiation process must be property-preserving, any verification evidence acquired on the product-line component must be demonstrably applicable to the instantiated component. In this paper we outline an approach that uses static analysis techniques and the SPARK language that can potentially demonstrate the correctness of model transformations.}, 
keywords={software cost estimation;software reliability;cost effective high assurance software product line component;property preserving transformation;generative programming;model transformation technique;software component;product-specific version;high assurance application;instantiation process;verification evidence;static analysis technique;SPARK language;Software;Unified modeling language;Sparks;Programming;Ignition;Contracts;UML;SPARK;M2M;Safety Critical;High Integrity;Software Product Lines;Verification;Static Analysis;DO-178B/ED-12B}, 
doi={10.1109/SPLC.2011.32}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8250775, 
author={J. Johnson and A. E. Jai}, 
booktitle={2017 International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
title={Netact based test-automation framework development for IMS CMREPO}, 
year={2017}, 
volume={}, 
number={}, 
pages={518-522}, 
abstract={IP Multimedia Subsystem (IMS) is an architectural framework to provide VoLte and other multimedia services. It is based on ETSI standards like SIP for interfaces between architectural elements.[9] IMS(IP Multimedia subsysytem) was originally designed by the wireless standards body 3rd Generation Partnership Project (3GPP). IMS(IP Multimedia subsysytem) is the key element in the 3G architecture that makes it possible to provide cellular access to all the services that the Internet provides. It is considered as a bridge between[9] cellular network and internet. The introduction of Configuration Management (CM) Repository Server (CMRepo Server) is an important prerequisite for the mass roll out of network elements (NEs), such as, Call Session Control Function (CSCF) and Home Subscriber Server (HSS). It is also required to pre-administer important parameters of the NEs that are required for IMS functionality. Thus, CMRepo Server provides the central CM system to manage multiple NEs. NetAct functions as the central CM system for managing online changeable parameters (class D and class E parameters). Management of other parameters (class A, B, and C parameters) is done through the customization procedure, which is time consuming as well as complex. Management of these parameters in simplified with the introduction of CMRepo Server. NetAct is an OSS Platform. This environment giving access to statistic, performance monitoring, configuration management, user management, fault management and all OSS aspect for the overall subsystem. All tools which is available in NetAct mostly developed by Java platform. Using the Robot framework tool automating NetAct test cases in IMS(IP Multimedia subsysytem).}, 
keywords={3G mobile communication;cellular radio;computer network management;fault diagnosis;Internet;IP networks;Java;multimedia communication;network servers;telecommunication standards;netact based test-automation framework development;pre-administer important parameters;IP multimedia subsystem;configuration management repository server;ETSI standards;SIP;wireless standards;3rd Generation Partnership Project;3GPP;Internet;cellular access network;network element;NE;call session control function;CSCF;home subscriber server;HSS;OSS platform;performance monitoring;user management;fault management;Java platform;Robot framework tool;IMS CMRepo server;Robots;Servers;Testing;Protocols;Multimedia communication;Logic gates;IMS;Robot framework;Netact;VOLTE;CMRepo}, 
doi={10.1109/ICCONS.2017.8250775}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1390775, 
author={B. Hicks}, 
booktitle={The 23rd Digital Avionics Systems Conference (IEEE Cat. No.04CH37576)}, 
title={Transforming avionics architectures to support network centric warfare}, 
year={2004}, 
volume={2}, 
number={}, 
pages={8.E.3-81}, 
abstract={Network centric warfare was applied to different layers in the military force structure to enable commanders and direct combatants to monopolize information to increase lethality and survivability. The flow of information, the amount, type, and other attributes to be discussed, heavily impact the aviation sector of military operations and acquisition. This work concentrates on the impact of NCW on avionics architectures and provides insight to the changes required of aircraft systems to fully utilize the NCW tenets. The NCW concepts are described along with the properties of information necessary for network centric operations.}, 
keywords={military avionics;military aircraft;electronic warfare;military communication;avionics architectures;network centric warfare;military force structure;aviation sector;military operations;military acquisition;aircraft systems;military communication;Aerospace electronics;Information systems;Military aircraft;Privacy;Information security;Logistics;Business;Information resources;Radar;Electrooptic devices}, 
doi={10.1109/DASC.2004.1390775}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6759193, 
author={T. Clark and V. Kulkarni and B. Barn and R. France and U. Frank and D. Turk}, 
booktitle={2014 47th Hawaii International Conference on System Sciences}, 
title={Towards the Model Driven Organization}, 
year={2014}, 
volume={}, 
number={}, 
pages={4817-4826}, 
abstract={Modern organizations are faced with the need to rapidly respond to frequent changes arising from external business pressures. The effect of such continuous evolution eventually leads to organizational misalignment, that is, situations in which sub-optimal configurations of underlying systems significantly reduce an organization's ability to meet its strategic goals. Ensuring alignment of an organization's systems and its goals has been a concern of researchers and practitioners in the enterprise architecture (EA) domain. Unfortunately, current approaches do not adequately address alignment problems that modern organizations face. In this paper we propose that alignment concerns can be better addressed by making models the primary entities that stakeholders within and outside of an organization use to interact with the organization. We call an organization that maintains and uses an integrated set of models to manage alignment concerns a Model Driven Organization (MDO). In this paper we characterize the alignment problem, discuss the shortcomings of current alignment management approaches and present our MDO vision.}, 
keywords={business data processing;organisational aspects;model driven organization;external business pressures;organizational misalignment;suboptimal configurations;enterprise architecture domain;EA;alignment problems;MDO;alignment management approaches;Organizations;Analytical models;Context modeling;Adaptation models;Electronic mail;Context;Enterprise Architecture;Enterprise Modelling;Simulation}, 
doi={10.1109/HICSS.2014.591}, 
ISSN={1530-1605}, 
month={Jan},}
@INPROCEEDINGS{6602477, 
author={P. Morrison and C. Holmgreen and A. Massey and L. Williams}, 
booktitle={2013 5th International Workshop on Software Engineering in Health Care (SEHC)}, 
title={Proposing regulatory-driven automated test suites for electronic health record systems}, 
year={2013}, 
volume={}, 
number={}, 
pages={46-49}, 
abstract={In regulated domains such as finance and health care, failure to comply with regulation can lead to financial, civil and criminal penalties. While systems vary from organization to organization, regulations apply across organizations. We propose the use of Behavior-Driven-Development (BDD) scenarios as the basis of an automated compliance test suite for standards such as regulation and interoperability. Such test suites could become a shared asset for use by all systems subject to these regulations and standards. Each system, then, need only create their own system-specific test driver code to automate their compliance checks. The goal of this research is to enable organizations to compare their systems to regulation in a repeatable and traceable way through the use of BDD. To evaluate our proposal, we developed an abbreviated HIPAA test suite and applied it to three open-source electronic health record systems. The scenarios covered all security behavior defined by the selected regulation. The system-specific test driver code covered all security behavior defined in the scenarios, and identified where the tested system lacked such behavior.}, 
keywords={automatic testing;conformance testing;health care;medical information systems;open systems;program testing;security of data;regulatory-driven automated test suites;health care;financial penalties;civil penalties;criminal penalties;organization;regulations;behavior-driven-development;BDD scenarios;automated compliance test suite;interoperability;system-specific test driver code;compliance checks;HIPAA test suite;open-source electronic health record systems;security behavior;Data structures;Boolean functions;NIST;Certification;Behavior-Driven-Development Healthcare IT;Regulatory Compliance;Security;Software Engineering;Software Testing}, 
doi={10.1109/SEHC.2013.6602477}, 
ISSN={}, 
month={May},}
@ARTICLE{7206603, 
author={M. Jamro}, 
journal={IEEE Transactions on Industrial Informatics}, 
title={POU-Oriented Unit Testing of IEC 61131-3 Control Software}, 
year={2015}, 
volume={11}, 
number={5}, 
pages={1119-1129}, 
abstract={Software testing is an important part of project development. Depending on system type and size, it is performed variously. Unit testing is one of the available approaches that is used to ensure that behavior of small software parts is consistent with requirements. It allows to improve software quality and decrease overall costs. Despite the fact that such an approach is commonly judged as a vital concept, it is not usual in control software. In this paper, the comprehensive approach to test the IEC 61131-3 software using unit tests is presented. It supports to create tests in two ways-either in textual and graphical IEC 61131-3 languages or in the CPTest+ dedicated test definition language. The latter is equipped with many advanced features, such as test fixtures and inclusions, parameterized and analog signal extensions, mock objects, as well as a few kinds of suites. The overall solution runs on the developer and testing station; hence, it does not have significant impact on performance of the control program and tests are more reliable and repeatable. To explain the concept, the simple running example is presented in this paper. The described solution has been introduced in the CPDev engineering environment for programming controllers.}, 
keywords={control engineering computing;program testing;software quality;POU-oriented unit testing;IEC 61131-3 control software;software testing;software quality;textual IEC 61131-3 languages;graphical IEC 61131-3 languages;CPTest+ dedicated test definition language;CPDev engineering environment;Testing;Software;IEC Standards;Informatics;Automation;Control systems;control software;IEC 61131-3;testing;unit test;Control software;IEC 61131-3;testing;unit test}, 
doi={10.1109/TII.2015.2469257}, 
ISSN={1551-3203}, 
month={Oct},}
@INPROCEEDINGS{5501495, 
author={S. Chatterjee}, 
booktitle={2010 Seventh International Conference on Information Technology: New Generations}, 
title={Modeling, Debugging, and Tuning QoE Issues in Live Stream-Based Applications - A Case Study with VoIP}, 
year={2010}, 
volume={}, 
number={}, 
pages={1044-1050}, 
abstract={End-user acceptance criteria of live-stream based, interactive applications are different from traditional B2B or B2C applications. For example, if users sense disruptions in audio or video stream quality, they may quickly form a negative opinion. The Quality of Experience (QoE) in such live-stream applications is, thus, based on perception, and is open to subjective interpretations. QoE can be affected by hundreds of possible variables. QoE problems (QoE bugs), however, require an objective solution (fix in the product's code or tuning of product parameters). Understanding and debugging QoE bugs in such scenarios starts with designing relevant metrics and analysis tools. Thereafter, smart test-designs and strategies are required to gain insights into bottlenecks. This paper builds a discussion around these thoughts, and through a case study (sample QoE problem in a Voice over Internet Protocol (VoIP) application), collates some generic guidelines to investigate QoE bugs in live-streaming scenarios.}, 
keywords={computer debugging;Internet telephony;media streaming;program debugging;modeling QoE issues;debugging QoE issues;tuning QoE issues;live stream-based applications;VoIP;end-user acceptance criteria;audio stream quality;video stream quality;quality of experience;analysis tools;smart test-designs;voice over Internet protocol;Debugging;Streaming media;Computer bugs;Collaboration;Delay;Information technology;Videoconference;Product codes;Testing;Internet telephony;VOIP;QoE;QoS;Media Streaming;VOIP Metric;Modeling}, 
doi={10.1109/ITNG.2010.44}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{4293588, 
author={M. Fletcher and W. Bereza and M. Karlesky and G. Williams}, 
booktitle={Agile 2007 (AGILE 2007)}, 
title={Evolving into Embedded Develop}, 
year={2007}, 
volume={}, 
number={}, 
pages={150-155}, 
abstract={In late 2005 we had the opportunity to start our first embedded development project. We apply agile practices to a variety of domains from web development to desktop applications to factory floor test equipment. The challenge for this new work was not learning the environment and technology. Our challenge was applying the practices of the agile world to the small and complex world of embedded systems. The hurdles were numerous: we battled the archaic state of many embedded tool sets, the lack of integration with tools like Rake that provide easy automation, and poor support for object oriented design. We've overcome each of these difficulties. This report is about our yearlong experience in introducing our development practices to embedded development.}, 
keywords={embedded systems;object-oriented programming;program testing;embedded system;Web development;object oriented design;factory floor test equipment;agile practices;Velocity control;Microprogramming;Vehicle safety;Automatic testing;Embedded system;Vehicle driving;Automation;System testing;Sampling methods;Production facilities}, 
doi={10.1109/AGILE.2007.25}, 
ISSN={}, 
month={Aug},}
@ARTICLE{5388069, 
author={C. E. Walston and C. P. Felix}, 
journal={IBM Systems Journal}, 
title={A method of programming measurement and estimation}, 
year={1977}, 
volume={16}, 
number={1}, 
pages={54-73}, 
abstract={The present approach to productivity estimation, although useful, is far from being optimized. Based on the results of the variable analysis described in this paper, and supplemented by the results of the continued investigation of additional variables related to productivity, an experimental regression model has been developed. Preliminary results indicate that the model reduces the scatter. Further work is being done to determine the potential of regression as an estimating tool, as well as to extend the analyses of the areas of computer usage, documentation volume, duration, and staffing.}, 
keywords={}, 
doi={10.1147/sj.161.0054}, 
ISSN={0018-8670}, 
month={},}
@INPROCEEDINGS{7073242, 
author={E. Biliri and M. Petychakis and I. Alvertis and F. Lampathaki and S. Koussouris and D. Askounis}, 
booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
title={Infusing social data analytics into Future Internet applications for manufacturing}, 
year={2014}, 
volume={}, 
number={}, 
pages={515-522}, 
abstract={Today, a new age of engagement and collaboration has emerged with the proliferation of usergenerated content in social networks and generally the Web 2.0, rendering it particularly difficult for enterprises to monitor and act upon all content following conventional data mining methodologies. In this paper, we present our approach for a Future Internet enabler (FITMAN Anlzer) that provides automated, social data analytics and aims at assisting enterprises in becoming more tuned to their customer needs and gaining insights into current and future trends to early embed them into product design. The FITMAN Anlzer implementation is domainindependent and allows any manufacturer to effectively train it based on his needs and create personalized reports to timely capture the right information. Our methodology includes trend analytics, polarity detection through machine learning, data querying through flexible reports and finally informative charts to visualize the results in order to help companies in their decision making procedures.}, 
keywords={data analysis;data mining;data visualisation;decision making;groupware;Internet;learning (artificial intelligence);product design;production engineering computing;query processing;social networking (online);social data analytics infusion;Future Internet applications;manufacturing;collaboration;user generated content;social networks;Web 2.0;data mining methodologies;Future Internet enabler;FITMAN Anlzer;product design;trend analytics;polarity detection;machine learning;data querying;informative charts;result visualization;decision making procedures;Media;Sentiment analysis;Market research;Facebook;Twitter;Data mining;Context;social media monitoring;trend analysis;opinion mining;natural language processing;sentiment analysis;social data analytics}, 
doi={10.1109/AICCSA.2014.7073242}, 
ISSN={2161-5330}, 
month={Nov},}
@ARTICLE{4346539, 
author={Y. Chen and D. Bindel and H. H. Song and R. H. Katz}, 
journal={IEEE/ACM Transactions on Networking}, 
title={Algebra-Based Scalable Overlay Network Monitoring: Algorithms, Evaluation, and Applications}, 
year={2007}, 
volume={15}, 
number={5}, 
pages={1084-1097}, 
abstract={Overlay network monitoring enables distributed Internet applications to detect and recover from path outages and periods of degraded performance within seconds. For an overlay network with end hosts, existing systems either require measurements, and thus lack scalability, or can only estimate the latency but not congestion or failures. Our earlier extended abstract [Y. Chen, D. Bindel, and R. H. Katz, ldquoTomography-based overlay network monitoring,rdquo Proceedings of the ACM SIGCOMM Internet Measurement Conference (IMC), 2003] briefly proposes an algebraic approach that selectively monitors linearly independent paths that can fully describe all the paths. The loss rates and latency of these paths can be used to estimate the loss rates and latency of all other paths. Our scheme only assumes knowledge of the underlying IP topology, with links dynamically varying between lossy and normal. In this paper, we improve, implement, and extensively evaluate such a monitoring system. We further make the following contributions: i) scalability analysis indicating that for reasonably large n (e.g., 100), the growth of k is bounded as O(n log n), ii) efficient adaptation algorithms for topology changes, such as the addition or removal of end hosts and routing changes, iii) measurement load balancing schemes, iv) topology measurement error handling, and v) design and implementation of an adaptive streaming media system as a representative application. Both simulation and Internet experiments demonstrate we obtain highly accurate path loss rate estimation while adapting to topology changes within seconds and handling topology errors.}, 
keywords={algebra;computational complexity;computer network management;Internet;resource allocation;algebra-based scalable overlay network monitoring;load balancing;adaptive streaming media system;Internet architecture;Topology;Delay;Condition monitoring;IP networks;Scalability;Algorithm design and analysis;Degradation;Routing;Load management;Measurement errors;Dynamics;load balancing;network measurement and monitoring;numerical linear algebra;overlay;scalability}, 
doi={10.1109/TNET.2007.896251}, 
ISSN={1063-6692}, 
month={Oct},}
@INPROCEEDINGS{4839257, 
author={D. Dechev and B. Stroustrup}, 
booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems}, 
title={Model-Based Product-Oriented Certification}, 
year={2009}, 
volume={}, 
number={}, 
pages={295-304}, 
abstract={Future space missions such as the Mars Science Laboratory and Project Constellation suggest the engineering of some of the most complex man-rated software systems. The present process-oriented certification methodologies employed by NASA are becoming prohibitively expensive when applied to systems of such complexity. The process of software certification establishes the level of confidence in a software system in the context of its functional and safety requirements. Providing such certification evidence may require the application of a number of software development, analysis, and validation techniques. We define product-oriented certification as the process of measuring the system's reliability and efficiency based on the analysis of its design (expressed in models) and implementation (expressed in source code). In this work we introduce a framework for model-based product-oriented certification founded on the concept of source code enhancement and analysis. We describe a classification of the certification artifact types, the development and validation tools and techniques, the application domain-specific factors, and the levels of abstraction. We demonstrate the application of our certification platform by analyzing the process of model-based development of the parallel autonomic goals network, a critical component of the Jet Propulsion Laboratory's Mission Data System (MDS). We describe how we identify and satisfy seven critical certification artifacts in the process of model-driven development and validation of the MDS goal network. In the analysis of this process, we establish the relationship among the seven certification artifacts, the applied development and validation techniques and tools, and the level of abstraction of system design and development.}, 
keywords={aerospace computing;parallel processing;program verification;software reliability;software tools;model-based product-oriented certification;space missions;Mars Science Laboratory;Project Constellation;complex man-rated software systems;certification methodologies;software certification evidence;software development;validation techniques;product-oriented certification;source code enhancement;validation tools;application domain-specific factors;model-based development;Jet Propulsion Laboratory Mission Data System;Certification;Laboratories;Software systems;Software safety;Space missions;Mars;NASA;Application software;Programming;Reliability;product-oriented certification;nonblocking synchronization;semantic enhancement;concurrent real-time systems}, 
doi={10.1109/ECBS.2009.15}, 
ISSN={}, 
month={April},}
@ARTICLE{8016712, 
author={}, 
journal={ISO/IEC/IEEE 24765:2017(E)}, 
title={ISO/IEC/IEEE International Standard - Systems and software engineering--Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-541}, 
abstract={This document provides a common vocabulary applicable to all systems and software engineering work. It was prepared to collect and standardize terminology. This document is intended to serve as a useful reference for those in the information technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute. This document includes references to the active source standards for definitions so that systems and software engineering concepts and requirements can be further explored.}, 
keywords={IEC standards;IEEE standards;ISO standards;project management;software engineering;systems engineering;vocabulary;ISO/IEC/IEEE international standard;systems engineering;vocabulary;terminology standardization;information technology;ISO standards;IEEE Computer Society and Project Management Institute;software engineering concepts;software requirements;IEEE Standards;ISO Standards;IEC Standards;Informatino technology;Software engineering;Systems engineering and theoryt;Terminology;computer;dictionary;information technology;software engineering;systems engineering;24765}, 
doi={10.1109/IEEESTD.2017.8016712}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{1471346, 
author={C. Verdonck and D. Vandenameele}, 
booktitle={Proceedings of the 27th European Solid-State Circuits Conference}, 
title={A single chip configurable network processor with built in ADSL-modem in 0.18 amp;#181;m CMOS}, 
year={2001}, 
volume={}, 
number={}, 
pages={109-112}, 
abstract={The SEA ASIC integrates a complete Discrete Multi-Tone (DMT) ADSL modem with an Asynchronous Transfer Mode (ATM) switch, an IEEE 802.3 Ethernet based packet switch and an ARM microcontroller into a single 0.18 &#181;m CMOS chip. The device is a cost-effective platform for a complete range of Alcatel Customer Premises ADSL Equipment.}, 
keywords={Intelligent networks;CMOS process;Hardware;Application specific integrated circuits;Local area networks;Modems;Wide area networks;Switches;Ethernet networks;Protocols}, 
doi={}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7357239, 
author={S. Barnett and I. Avazpour and R. Vasa and J. Grundy}, 
booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={A multi-view framework for generating mobile apps}, 
year={2015}, 
volume={}, 
number={}, 
pages={305-306}, 
abstract={This paper demonstrates a multi-view framework for Rapid APPlication Tool (RAPPT). RAPPT enables rapid development of mobile applications. It employs a multilevel approach to mobile application development: a Domain Specific Visual Language to define the high level structure of mobile apps, a Domain Specific Textual Language to define behavioural concepts, and concrete source code for fine grained improvements.}, 
keywords={mobile computing;software engineering;source code (software);visual languages;mobile apps generation;multiview framework;Rapid Application Tool;RAPPT;mobile applications development;Domain Specific Visual Language;Domain Specific Textual Language;source code;high level structure;behavioural concepts;Navigation}, 
doi={10.1109/VLHCC.2015.7357239}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{5770632, 
author={P. L. M. Navarro and G. M. Prez and D. S. Ruiz}, 
booktitle={2011 Fourth IEEE International Conference on Software Testing, Verification and Validation}, 
title={Towards Software Quality and User Satisfaction through User Interfaces}, 
year={2011}, 
volume={}, 
number={}, 
pages={415-418}, 
abstract={With this work we expect to provide the community and the industry with a solid basis for the development, integration, and deployment of software testing tools. As a solid basis we mean, on one hand, a set of guidelines, recommendations, and clues to better comprehend, analyze, and perform software testing processes, and on the other hand, a set of robust software frameworks that serve as a starting point for the development of future testing tools.}, 
keywords={program testing;software quality;user interfaces;software quality;user satisfaction;user interfaces;software testing;Graphical user interfaces;Software testing;Usability;Open source software;Computer architecture;software testing;GUI testing;automatic test case generation;usability evaluation;user experience evaluation;GUI-data verification}, 
doi={10.1109/ICST.2011.13}, 
ISSN={2159-4848}, 
month={March},}
@ARTICLE{4197854, 
author={}, 
journal={IEEE Unapproved Draft Std P487/D8, Apr 2007}, 
title={Unapproved IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations (Revision of IEEE Std 487-2000)}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{8317991, 
author={A. Johanson and W. Hasselbring}, 
journal={Computing in Science Engineering}, 
title={Software Engineering for Computational Science: Past, Present, Future}, 
year={2018}, 
volume={20}, 
number={2}, 
pages={90-109}, 
abstract={Despite the increasing importance of in silico experiments to the scientific discovery process, state-of-the-art software engineering practices are rarely adopted in computational science. To understand the underlying causes for this situation and to identify ways to improve it, the authors conducted a literature survey on software engineering practices in computational science. They identified 13 recurring key characteristics of scientific software development that are the result of the nature of scientific challenges, the limitations of computers, and the cultural environment of scientific software development. Their findings allow them to point out shortcomings of existing approaches for bridging the gap between software engineering and computational science and to provide an outlook on promising research directions that could contribute to improving the current situation.}, 
keywords={natural sciences computing;software engineering;computational science;scientific discovery process;scientific software development;software engineering practices;Scientific computing;Software engineering;Computational modeling;Software development management;survey;software engineering;computational science;software development;history of computing}, 
doi={10.1109/MCSE.2018.021651343}, 
ISSN={1521-9615}, 
month={Mar},}
@INPROCEEDINGS{6005491, 
author={T. D. Hellmann and F. Maurer}, 
booktitle={2011 Agile Conference}, 
title={Rule-Based Exploratory Testing of Graphical User Interfaces}, 
year={2011}, 
volume={}, 
number={}, 
pages={107-116}, 
abstract={This paper introduces rule-based exploratory testing, an approach to GUI testing that combines aspects of manual exploratory testing with rule-based test automation. This approach uses short, automated rules to increase the bug-detection capability of recorded exploratory test sessions. A preliminary evaluation found that this approach can be used to detect both general and application-specific bugs, but that rules for general bugs are easier to transfer between applications. Also, despite the advantages of keyword-based testing, it interferes with the transfer of rules between applications.}, 
keywords={graphical user interfaces;program debugging;program testing;rule based exploratory testing;graphical user interfaces;GUI;rule based test automation;bug detection;keyword based testing;Testing;Graphical user interfaces;Computer bugs;Humans;Manuals;Security;Fires;GUI testing;rule-based testing;exploratory testing}, 
doi={10.1109/AGILE.2011.23}, 
ISSN={}, 
month={Aug},}
@INBOOK{5273677, 
author={P. K. Bhatnagar}, 
booktitle={Engineering Networks for Synchronization, CCS 7, and ISDN: Standards, Protocols, Planning and Testing}, 
title={Testing in the ISDN}, 
year={1997}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Introduction

Layer 1 Tests

Layer 2 and Layer 3 Tests

Maintenance of ISDN Access

This chapter contains sections titled:

References

}, 
keywords={}, 
doi={10.1109/9780470544570.ch14}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273677},}
@INPROCEEDINGS{8491148, 
author={D. N. Jorge and P. D. L. Machado and E. L. G. Alves and W. L. Andrade}, 
booktitle={2018 IEEE 26th International Requirements Engineering Conference (RE)}, 
title={Integrating Requirements Specification and Model-Based Testing in Agile Development}, 
year={2018}, 
volume={}, 
number={}, 
pages={336-346}, 
abstract={In agile development, Requirements Engineering (RE) and testing have to cope with a number of challenges such as continuous requirement changes and the need for minimal and manageable documentation. In this sense, extensive research has been conducted to automatically generate test cases from (structured) natural language documents using Model-Based Testing (MBT). However, the imposed structure may impair agile practices or test case generation. In this paper, inspired by cooperation with industry partners, we propose CLARET, a notation that allows the creation of use case specifications using natural language to be used as central artifacts for both RE and MBT practices. A tool set supports CLARET specification by checking syntax of use cases structure as well as providing visualization of flows for use case revisions. We also present exploratory studies on the use of CLARET to create RE documents as well as on their use as part of a system testing process based on MBT. Results show that, with CLARET, we can document use cases in a cost-effective way. Moreover, a survey with professional developers shows that CLARET use cases are easy to read and write. Furthermore, CLARET has been successfully applied during specification, development and testing of industrial applications.}, 
keywords={Testing;Industries;Requirements engineering;Tools;Natural languages;Manuals;Syntactics;Agile Development;Requirements Engineering;Model-Based Testing;Use case specification}, 
doi={10.1109/RE.2018.00041}, 
ISSN={2332-6441}, 
month={Aug},}
@ARTICLE{7548916, 
author={D. Lbke and T. van Lessen}, 
journal={IEEE Software}, 
title={Modeling Test Cases in BPMN for Behavior-Driven Development}, 
year={2016}, 
volume={33}, 
number={5}, 
pages={15-21}, 
abstract={Testing large-scale process integration solutions is complex and cumbersome. To tackle this problem, researchers employed behavior-driven development. They used the Business Process Model and Notation language to model domain-specific test cases. These test cases can be understood by both developers and business stakeholders and can be executed automatically.}, 
keywords={business data processing;program testing;software engineering;test case modelling;Business Process Model and Notation;BPMN;behavior-driven development;Simple object access protocol;Business process management;Modeling;Testing;Software engineering;Behaviorial sciences;business processes;Business Process Model and Notation;BPMN;behavior-driven development;BDD;test-driven development;TDD;software testing;software development;software engineering}, 
doi={10.1109/MS.2016.117}, 
ISSN={0740-7459}, 
month={Sept},}
@ARTICLE{278309, 
author={}, 
journal={IEEE Std 610.13-1993}, 
title={IEEE Standard Glossary of Computer Languages}, 
year={1993}, 
volume={}, 
number={}, 
pages={i-}, 
abstract={}, 
keywords={}, 
doi={10.1109/IEEESTD.1993.119224}, 
ISSN={}, 
month={},}
@ARTICLE{7752755, 
author={}, 
journal={ISO/IEC/IEEE FDIS P24765 FDIS, October 2016}, 
title={ISO/IEC/IEEE Draft Systems and Software Engineering - Vocabulary}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-554}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;ISO Standards;IEC Standards;Software engineering;Terminology;Dictionaries}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{6972271, 
author={A. Tokmakoff and B. Sparrow and D. Turner and A. Lowe}, 
booktitle={2014 IEEE 10th International Conference on e-Science}, 
title={AusPlots Rangelands Field Data Collection and Publication: Infrastructure for Ecological Monitoring}, 
year={2014}, 
volume={1}, 
number={}, 
pages={249-255}, 
abstract={The TERN AusPlots Rangelands field data collection system has been developed to facilitate simple and efficient data collection by ecologists operating in the Australian outback. The infrastructure provides tooling for 'clean' data collection on mobile (tablet) devices, associated data storage in a cloud-based server infrastructure, facilities for data curation and management and interfaces with the Australian Ecological Knowledge and Observation System (KOS) data repository for long-term data management and semantic enrichment. In this paper, we introduce the AusPlots Rangelands field data collection solution, providing a systems-level view and motivating its development through the discussion of key functional requirements. We provide an outline of the KOS data repository and demonstrate that the combined system provides a unique end-to-end data collection, curation, archiving and publishing mechanism for ecological data.}, 
keywords={cloud computing;data handling;ecology;environmental science computing;mobile computing;ecological data publishing;ecological data archiving;ecological data curation;end-to-end ecological data collection;KOS data repository;Australian Ecological Knowledge and Observation System;data management;cloud-based server infrastructure;data storage;mobile devices;ecological monitoring;TERN AusPlots Rangelands field data publication;TERN AusPlots Rangelands field data collection;Data collection;Vegetation;Databases;Servers;Soil;Protocols;Vegetation mapping;ecological data;mobile;data collection;data publishing}, 
doi={10.1109/eScience.2014.55}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7811570, 
author={L. C. Hoyos and C. E. Rothenberg}, 
booktitle={2016 8th IEEE Latin-American Conference on Communications (LATINCOM)}, 
title={NOn: Network function virtualization ontology towards semantic service implementation}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={A hazard of ongoing Network Function Virtualization (NFV) realizations is the lack of a common understanding in support of development, deployment and operation tasks related to Virtual Function Networks (VNFs), NFV components and interfaces. In the current state of affairs, NFV stakeholders commonly create their own terminology to define and describe NFV components, following going the specifications led by European Telecommunications Standard Institute but also adopting telecommunication- and software-centric definitions. As a consequence, portability and interoperability goals of NFV get compromised since NFV technology providers have hard times in understanding and using definitions and descriptions across different domains. Furthermore, VNF data models of operational systems and deployment configuration software need to be re-defined, re-coded, and re-compiled to make them work over different NFV platforms. In this work, we present the design and implementation of our proposed NFV Ontology (NOn) enabling Semantic nFV Services (SnS) to reduce manual intervention during the integration process of heterogeneous NFV domains and effectively overcome the costly re-work hazards of current NFV implementation approaches. We present the proof of concept implementation of a Generic Client leveraging SnS/NOn to create and consume dynamic workflows in an open source testbed based on OpenStack and OpenBaton.}, 
keywords={ontologies (artificial intelligence);open systems;semantic Web;virtualisation;network function virtualization ontology;semantic service implementation;NOn;NFV component;virtual function network;VNF data model;operational system;NFV interface;European Telecommunications Standard Institute;interoperability;portability;deployment configuration software;manual intervention reduction;open source testbed;OpenStack testbed;OpenBaton testbed;Semantics;Ontologies;Interoperability;Software;Network function virtualization;Manuals;Engines;Network Function Virtualization;NFV;Semantic Services;Ontology}, 
doi={10.1109/LATINCOM.2016.7811570}, 
ISSN={}, 
month={Nov},}
@ARTICLE{8451922, 
author={M. Sayagh and N. Kerzazi and B. Adams and F. Petrillo}, 
journal={IEEE Transactions on Software Engineering}, 
title={Software Configuration Engineering in Practice: Interviews, Survey, and Systematic Literature Review}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Modern software applications are adapted to different situations (e.g., memory limits, enabling/disabling features) by only changing few configuration option values without any source code modifications. According to several studies, this flexibility is expensive. Indeed, configuration errors represent one of the largest percentage of software errors, they are hard to debug and resolve, while comprehension of the code also is hampered by sprinkling conditional checks of configuration options. Although researchers have proposed various approaches to help debug or prevent configuration errors, especially from the end users' perspective, this paper takes a step back to understand the activities required by practitioners to engineer the software configuration options in their source code, the challenges they experience as well as best practices that they have or could adopt. By interviewing 14 software engineering experts, followed by a large survey on 229 software engineers, we identified 9 major activities related to configuration engineering, 22 challenges faced by developers, and 25 expert recommendations to improve software configuration quality. We complemented this study by a systematic literature review to enrich the experts' recommendations, and to identify possible solutions for the developers' challenges discussed and evaluated by the research community.}, 
keywords={Software systems;Interviews;Systematics;Facebook;Bibliographies;Software algorithms}, 
doi={10.1109/TSE.2018.2867847}, 
ISSN={0098-5589}, 
month={},}
@INPROCEEDINGS{7965258, 
author={D. Mazinanian and N. Tsantalis}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
title={CSSDev: Refactoring Duplication in Cascading Style Sheets}, 
year={2017}, 
volume={}, 
number={}, 
pages={63-66}, 
abstract={Cascading Style Sheets (CSS) is a widely-used language for defining the presentation of structured documents and user interfaces. Despite its popularity, CSS still lacks adequate tool support for everyday maintenance tasks, such as debugging and refactoring. In this paper, we present CSSDEV, a tool suite for analyzing CSS code to detect refactoring opportunities.(https://youtu.be/lu3oITi1XrQ).}, 
keywords={document handling;program diagnostics;software maintenance;source code (software);user interfaces;CSSDev;duplication refactoring;cascading style sheets;structured documents;user interfaces;CSS code analysis;Cascading style sheets;Tools;HTML;Browsers;Crawlers;Maintenance engineering;Runtime;Cascading Style Sheets;Preprocessors;Refactoring}, 
doi={10.1109/ICSE-C.2017.7}, 
ISSN={}, 
month={May},}
@INBOOK{8044562, 
author={Moray Rumney}, 
booktitle={LTE and the Evolution to 4G Wireless: Design and Measurement Challenges}, 
title={List of Acronyms}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={10.1002/9781118799475.oth1}, 
ISSN={}, 
publisher={Wiley}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8044562},}
@INPROCEEDINGS{5614873, 
author={F. Abbors and D. Truscan}, 
booktitle={2010 Second International Conference on Advances in System Testing and Validation Lifecycle}, 
title={Approaching Performance Testing from a Model-Based Testing Perspective}, 
year={2010}, 
volume={}, 
number={}, 
pages={125-128}, 
abstract={The paper introduces the concept of model-based performance testing, which we plan to pursue in our research. The underlying idea is to describe various performance aspects as well as functional aspects of a software system using modeling languages like UML, and from the resulting models to automatically design tests that can be used for performance testing. In our research, we also plan to focus on how the modeling and traceability of performance requirements can be achieved across the testing process.}, 
keywords={model-based reasoning;program testing;simulation languages;software performance evaluation;performance testing;model based testing perspective;software system;modeling language;UML;Unified modeling language;Testing;Adaptation model;Analytical models;Software systems;Load modeling;Model-Based Testing;Model Validation;Requirements Traceability}, 
doi={10.1109/VALID.2010.22}, 
ISSN={}, 
month={Aug},}
@INBOOK{8045719, 
author={Henning Schulzrinne and Hannes Tschofenig}, 
booktitle={Internet Protocol-based Emergency Services}, 
title={Architectures}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={Emergency services;IP networks;3GPP;Protocols;Internet;WiMAX}, 
doi={10.1002/9781119993858.ch3}, 
ISSN={}, 
publisher={Wiley}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8045719},}
@INPROCEEDINGS{7905631, 
author={F. Fourati and M. T. Bhiri and R. Robbana}, 
booktitle={2016 5th International Conference on Multimedia Computing and Systems (ICMCS)}, 
title={Verification and validation of PDDL descriptions using Event-B formal method}, 
year={2016}, 
volume={}, 
number={}, 
pages={770-776}, 
abstract={The automatic planning community of Artificial Intelligence AI have developed a de facto standard language for PDDL, producing formal modeling of Planning problems. Equally it have conceived and produced tools called planners to automatically generate plans for PDDL descriptions. But the verification and validation of PDDL descriptions is little treated topic. In this paper, we shall treat this issue through the Event-B formal method. We illustrate the contribution of the static analysis tools associated with Event-B (provers, model checker, animator, and simulator) for verification and validation of PDDL descriptions.}, 
keywords={planning (artificial intelligence);program diagnostics;program verification;PDDL description validation;PDDL description verification;automatic planning community;artificial intelligence AI;de facto standard language;formal modeling;planners;automatic plan generation;static analysis tools;Event-B formal method;model checker;provers;animator;simulator;Planning;Context;Boats;Context modeling;Poles and towers;Syntactics;Electronic mail;Artificial Intelligence;Planning and Scheduling;PDDL;Verification and Validation;Event-B;Transformation;Static and Dynamic analysis}, 
doi={10.1109/ICMCS.2016.7905631}, 
ISSN={2472-7652}, 
month={Sept},}
@INPROCEEDINGS{4766483, 
author={S. Sarkar and A. Panayappan}, 
booktitle={TENCON 2008 - 2008 IEEE Region 10 Conference}, 
title={Formal architecture modeling of business application- software maintenance case study}, 
year={2008}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Maintenance of complex business applications is challenging for software services industry. The maintenance team inherits the software with little design and implementation knowledge. The client-facing team gathers an ad-hoc architectural description of some sort and communicates the same to the geographically distributed maintenance team through informal box and line diagrams. This information is poorly understood, and the underlying architectural constraints are never enforced. This paper proposes a type system to model the architecture of a complex enterprise IT system using Acme architecture description language and reports a modeling approach to capture various architectural design decisions architects perform as a part of the architecture review. An initial field-study to evaluate the usefulness of such modeling has been encouraging.}, 
keywords={DP industry;software architecture;software development management;software maintenance;specification languages;formal architecture modeling;business application;software maintenance;software service industry;type system;complex enterprise IT system;Acme architecture description language;software development management;Computer architecture;Software maintenance;Application software;Computer industry;Architecture description languages;Software design;Documentation;Logic design;Software development management;Programming}, 
doi={10.1109/TENCON.2008.4766483}, 
ISSN={2159-3442}, 
month={Nov},}
@INPROCEEDINGS{8071326, 
author={D. Mordvinov and Y. Litvinov and T. Bryksin}, 
booktitle={2017 20th Conference of Open Innovations Association (FRUCT)}, 
title={TRIK studio: Technical introduction}, 
year={2017}, 
volume={}, 
number={}, 
pages={296-308}, 
abstract={This paper presents TRIK Studio - an environment for visual (and textual) programming of robotic kits, which is used in educational organizations across Russia and Europe. First part of the article provides overview of the system - its purpose, features, differences from similar programming environments, general difficulties of robot programming and solutions proposed by TRIK Studio. Second part presents implementation details of TRIK Studio and its most interesting components. This article combines five fields of study: robotics, domain-specific visual modeling, education, formal methods and methods of program analysis. Main contribution of this article is detailed technical description of TRIK Studio as complex and successful open-source cross-platform robot programming environment written in C++/Qt, and first part of the article can also be interesting for teachers as it provides an overview of existing robot programming tools and related problems.}, 
keywords={humanoid robots;mobile robots;robot programming;similar programming environments;TRIK Studio;domain-specific visual modeling;complex source cross-platform robot programming environment;robot programming tools;TRIK studio;visual programming;open-source cross-platform robot programming environment}, 
doi={10.23919/FRUCT.2017.8071326}, 
ISSN={2305-7254}, 
month={April},}
@INPROCEEDINGS{186190, 
author={F. Godon and D. Al-Khalili and R. Inkol}, 
booktitle={Third Annual IEEE Proceedings on ASIC Seminar and Exhibit}, 
title={Multi circular buffer controller chip for advanced ESM system}, 
year={1990}, 
volume={}, 
number={}, 
pages={P14/5.1-P14/5.4}, 
abstract={A 90 K transistor 1.5 mu m CMOS integrated circuit that operates at a data transfer rate of 20 MHz and implements an array of variable size circular buffers mapped into a high-speed RAM through physical and virtual addressing techniques is discussed. The device is fully programmable with the capability of single and block data transfers. The target application is an advanced multiprocessor ESM system.<<ETX>>}, 
keywords={buffer storage;CMOS integrated circuits;electronic warfare;storage management chips;physical addressing;advanced ESM system;CMOS integrated circuit;data transfer rate;variable size circular buffers;high-speed RAM;virtual addressing;block data transfers;multiprocessor;1.5 micron;Control systems;Buffer storage;Random access memory;Read-write memory;Pulse measurements;Counting circuits;Very large scale integration;Computer architecture;Registers;Logic arrays}, 
doi={10.1109/ASIC.1990.186190}, 
ISSN={}, 
month={Sept},}
@INBOOK{5273145, 
author={Steven M. Kaplan}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={R}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={
}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch18}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273145},}
@INPROCEEDINGS{8247727, 
author={S. Pfrang and D. Meier and V. Kautz}, 
booktitle={2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Towards a modular security testing framework for industrial automation and control systems: ISuTest}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Industrial automation and control systems (IACS) play a key role in modern production facilities. On the one hand, they provide real-time functionality to the connected field devices. On the other hand, they get more and more connected to local networks and the internet in order to facilitate use cases promoted by Industry 4.0. This makes IACS susceptible to cyber-attacks which exploit vulnerabilities, for example in order to interrupt the automation process. Security testing targets at discovering those vulnerabilities before they are exploited. In order to enable IACS manufacturers and integrators to perform security testing for their devices, we present ISuTest, a modular security testing framework for IACS. ISuTest is designed to be extendable regarding all kinds of automation protocols, different connection paths as well as evaluating arbitrary outputs of the tested devices. This paper describes the fundamental ideas behind ISuTest, its design and a basic evaluation in which the ISuTest framework was able to discover a vulnerability in a programmable logic controller (PLC). The paper concludes with a broad overview of the planned future work.}, 
keywords={Internet;production control;production engineering computing;production facilities;program testing;programmable controllers;security of data;security testing targets;vulnerability;integrators;modular security testing framework;automation protocols;different connection paths;tested devices;ISuTest framework;programmable logic controller;modern production facilities;connected field devices;automation process;IACS;industrial automation and control systems;Industry 4.0;cyber-attacks;PLC;Testing;Security;Automation;Protocols;Control systems;Software;Hardware}, 
doi={10.1109/ETFA.2017.8247727}, 
ISSN={1946-0759}, 
month={Sept},}
@ARTICLE{5621965, 
author={M. Schwartz}, 
journal={IEEE Communications Magazine}, 
title={X.25 Virtual Circuits - TRANSPAC IN France - Pre-Internet Data Networking [History of communications]}, 
year={2010}, 
volume={48}, 
number={11}, 
pages={40-46}, 
abstract={The following article , by Remi Despres, is the second on the history of X.25 systems to appear in this column. As noted by Dr. Despres, the previous article focused on the Canadian Datapac system. Earlier articles on packet switching in this column have included one on the history of the Arpanet/Internet and one on early British packet switching systems. What makes this article particularly distinctive, aside, of course, from the fact that it focuses on the major contributions of French engineers to the development of packet switching as well as to X.25 standardization, is that it carefully outlines the reasons for the choice of connection-oriented virtual circuits for the Transpac network, as contrasted with datagram-based packet switching adopted for Arpanet. Interestingly, Dr. Despres notes that the idea of using virtual-circuit connection-oriented packet switching in the Transpac development came from the British packet switching activity. It is to be noted that early commercial packet switching networks in the United States, such as Tymnet and Telenet, also adopted the virtual circuit paradigm.}, 
keywords={packet switching;protocols;wide area networks;communications history;X.25 systems;Canadian Datapac system;packet switching;X.25 standardization;connection-oriented virtual circuits;Transpac network;Packet switching;Virtual circuits;Protocols;Internet;History;Software}, 
doi={10.1109/MCOM.2010.5621965}, 
ISSN={0163-6804}, 
month={November},}
@INPROCEEDINGS{6075019, 
author={D. Balaretnaraja and S. Weerawarana}, 
booktitle={2011 International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
title={A framework for managing persistence in distributed systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={9-13}, 
abstract={Enterprise applications today have acquired the need to be distributed due various demanding reasons. Such systems are developed with focus on distributed concerns than on the application logic. This diverted the developers from the functional requirement of the system and burdened them with the responsibility of developing and maintaining code related to distributed concerns. The main intention of this research is to facilitate development of distributed systems without any consideration for distributed concerns. We suggest a way where the application is initially designed without them and later enabled by integrating the framework proposed in this research. We confine our interest in separating persistence and replication among other distributed concerns. The motivation for this research comes by recognizing the fact that such a framework drastically reduces the code and complexity involved to make a distributed application resilient to failures and thereby to minimize the effort necessary to debug, deploy and maintain.}, 
keywords={distributed processing;software fault tolerance;software maintenance;software management;persistence management;distributed system;enterprise application;functional requirement;code maintenance;failure resilience;debugging;Peer to peer computing;Robustness;Indexing;Servers;distributed computing;group communication framework;peer to peer overlay;JGroups;FreePastry}, 
doi={10.1109/ICTer.2011.6075019}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7332467, 
author={J. Kim and D. Batory and D. Dig}, 
booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Scripting parametric refactorings in Java to retrofit design patterns}, 
year={2015}, 
volume={}, 
number={}, 
pages={211-220}, 
abstract={Retrofitting design patterns into a program by hand is tedious and error-prone. A programmer must distinguish refactorings that are provided by an Integrated Development Environment (IDE) from those that must be realized manually, determine a precise sequence of refactorings to apply, and perform this sequence repetitively to a laborious degree. We designed, implemented, and evaluated Reflective Refactoring (R<sup>2</sup>), a Java package to automate the creation of classical design patterns (Visitor, Abstract Factory, etc.), their inverses, and variants. We encoded 18 out of 23 Gang-of-Four design patterns as R<sup>2</sup> scripts and explain why the remaining are inappropriate for refactoring engines. We evaluate the productivity and scalability of R<sup>2</sup> with a case study of 6 real-world applications. In one case, R<sup>2</sup> automatically created a Visitor with 276 visit methods by invoking 554 Eclipse refactorings in 10 minutes - an achievement that could not be done manually. R<sup>2</sup> also sheds light on why refactoring correctness, expressiveness, and speed are critical issues for scripting in next-generation refactoring engines.}, 
keywords={Java;object-oriented methods;programming environments;software maintenance;parametric refactoring scripting;Java;design pattern retrofitting;integrated development environment;IDE;reflective refactoring;R2;visitor;abstract factory;Eclipse refactoring;refactoring correctness;refactoring expressiveness;refactoring speed;next-generation refactoring engine;time 10 min;Manuals;Graphics;DVD}, 
doi={10.1109/ICSM.2015.7332467}, 
ISSN={}, 
month={Sept},}
@ARTICLE{4519427, 
author={}, 
journal={IEEE Unapproved Draft Std P11073-00101/D03, Sep 2007}, 
title={Health Informatics - Point-Of-Care Medical Device Communication - Technical Report - Guidelines for the Use of RF Wireless Technology}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={The following Guidance document addresses the use of radio frequency 1 (RF) wireless technology for the transport of medical data both to and from point-of-care (PoC) medical devices. The context of such wireless medical data transport can range from home- or mobile-based healthcare to in hospital ambulatory and stationary situations. The intent of the guidance document is to be global with respect to wireless spectrum and equipment, although working group participation and expertise have favored detail of scenarios from the US. At the time of this Guidance document several applicable RF wireless technologies exist with a range of capabilities and characteristics, and in different stages of maturity, standardization, and adoption in healthcare. It is recognized that RF technologies are rapidly evolving, and new options may become available (or sufficiently established) after the publication of this Guidance document. The recommendations, therefore, avoid being overly prescriptive and instead attempt to assist medical device manufacturers, wireless equipment manufacturers, healthcare providers, government agencies and any other end-user of this document to make reasonable judgments regarding performance and practical implementation of wireless solutions. The Guidance document defines specific use cases to estimate, compare, and contrast performance of known technologies operating on wireless personal area (WPAN), wireless local area (WLAN), wireless metropolitan area (WMAN), and wireless wide area (WWAN) networks. Major considerations are 1) the quality-of-service (QoS) requirements (reliability, latency, priority, bandwidth) associated with the data being transported, 2) the expected performance (power, link range, throughput, link establishment and maintenance) of the wireless technology, and 3) the specific needs and resources of the end user. Related issues include network architecture, EMI/EMC, coexistence with other data streams, security, cost, power consumption, and technology configurability. Performance summaries for specific wireless technologies that support defined use cases are not intended as an endorsement of optimal solution because different needs, resources, sizes, and environments cannot be comprehensively addressed. This overview document is meant to be a foundation and reference for several follow-on IEEE 11073.3.5.x standards that will profile specific classes of off-the-shelf RF wireless technologies for medical data transport. Importantly, this guidance document is not envisioned to be periodically updated, but instead will act as a source of information for the follow-on IEEE 11073-0305.x standards that will supplant it. Periodic updates will be performed on the IEEE 11073- 0305.x standards only.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{577990, 
author={S. M. Kransner and D. E. Bernard}, 
booktitle={1997 IEEE Aerospace Conference}, 
title={Integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium}, 
year={1997}, 
volume={2}, 
number={}, 
pages={409-420 vol.2}, 
abstract={Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process.}, 
keywords={aerospace computing;special purpose computers;software engineering;real-time systems;computerised navigation;computerised monitoring;autonomy technologies;embedded spacecraft system-flight software;Deep Space 1;deep-space mission;NASA;DS1 flight software;spacecraft activities;Smart Executive;mode identification;reconfiguration engine;autonomous navigation;beacon monitoring;spacecraft flight software architecture;real-time;rapid prototype;Space technology;Space vehicles;Hardware;Scheduling;Trajectory;Space missions;Image resolution;Timing;Engines;Radio navigation}, 
doi={10.1109/AERO.1997.577990}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8453176, 
author={O. Semerth and A. S. Nagy and D. Varr}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)}, 
title={A Graph Solver for the Automated Generation of Consistent Domain-Specific Models}, 
year={2018}, 
volume={}, 
number={}, 
pages={969-980}, 
abstract={Many testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy.}, 
keywords={formal logic;formal specification;formal verification;graph theory;query processing;specification languages;consistent domain-specific models;automated model generation;software engineering;graph models;domain-specific instance models;mapping-based approach;incremental graph query evaluation;graph solver framework;back-end logic solvers;first-order logic specification;safety standards;tool qualification;systematic generation;systems engineering;Analytical models;Object oriented modeling;Tools;Biological system modeling;IP networks;Testing;Graph generation;Test generation;Domain Specific Modeling Languages;Logic Solver;Graph Solver}, 
doi={}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{5985928, 
author={S. Siegl and K. Hielscher and R. German and C. Berger}, 
booktitle={2011 12th Latin American Test Workshop (LATW)}, 
title={Automated testing of embedded automotive systems from requirement specification models}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Embedded software for modern automotive and avionic systems is increasingly complex. In early design phases, even when there is still uncertainty about the feasibility of the requirements, valuable information can be gained from models that describe the expected usage and the desired system reaction. The generation of test cases from these models indicates the feasibility of the intended solution and helps to identify scenarios for which the realization is hardly feasible or the intended system behavior is not properly defined. In this paper we present the formalization of requirements by models to simulate the expected field usage of a system. These so called usage models can be enriched by information about the desired system reaction. Thus, they are the basis for all subsequent testing activities: First, they can be used to verify the first implementation models and design decisions w.r.t. the fulfillment of requirements and second, test cases can be derived in a random or statistic manner. The generation can be controlled with operational profiles that describe different classes of field usage. We have applied our approach at a large German car manufacturer in the early development phase of active safety functionalities. Test cases were generated from the usage models to assess the implementation models in MATLAB/Simulink. The parametrization of the systems could be optimized and a faulty transition in the implementation models was revealed. These design and implementation faults had not been discovered with the established test method.}, 
keywords={automotive engineering;avionics;embedded systems;formal specification;program testing;software fault tolerance;automated testing;embedded automotive system;requirement specification model;embedded software;avionic system;early design phase;intended system;requirement formalization;German car manufacturer;safety functionality;test case;MATLAB-Simulink;faulty transition;Safety;Testing;Timing;MATLAB;Belts;Clocks}, 
doi={10.1109/LATW.2011.5985928}, 
ISSN={2373-0862}, 
month={March},}
@INPROCEEDINGS{8109258, 
author={C. Duffau and B. Grabiec and M. Blay-Fornarino}, 
booktitle={2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={Towards Embedded System Agile Development Challenging Verification, Validation and Accreditation: Application in a Healthcare Company}, 
year={2017}, 
volume={}, 
number={}, 
pages={82-85}, 
abstract={When Agile development meets critical embedded systems, verification, validation and accreditation activities are impacted. Challenges such as tests increase or accreditation documents production have to be managed in terms of time and resources. In this paper, we highlight these challenges and present a continuous integration ecosystem that aims to tackle these issues. We report on how this approach has been applied in a research and development healthcare company named AXONIC.}, 
keywords={accreditation;embedded systems;formal verification;health care;safety-critical software;software prototyping;critical embedded systems;accreditation activities;embedded system agile development;verification activities;validation activities;research and development healthcare company;continuous integration ecosystem;AXONIC;Accreditation;Testing;Embedded systems;Hardware;Companies;Ecosystems;agile development;embedded systems;justification;VV&A;continuous integration}, 
doi={10.1109/ISSREW.2017.8}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7018532, 
author={A. Ulrich and S. Jell and A. Votintseva and A. Kull}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={The ETSI Test Description Language TDL and its application}, 
year={2014}, 
volume={}, 
number={}, 
pages={601-608}, 
abstract={The wide-scale introduction of model-based testing techniques in an industrial context faces many obstacles. One of the obstacles is the existing methodology gap between informally described test purposes and formally defined test descriptions used as the starting point for test automation. The provision of an explicit test description becomes increasingly essential when integrating complex, distributed systems and providing support for conformance and interoperability tests of such systems. The upcoming ETSI standard on the Test Definition Language (TDL) covers this gap. It allows describing scenarios on a higher abstraction level than programming or scripting languages. Furthermore, TDL can be used as an intermediate representation of tests generated from other sources, e.g. simulators, test case generators, or logs from previous test runs. TDL is based on a meta-modelling approach that expresses its abstract syntax. Deploying this design approach, individual concrete syntaxes of TDL can be designed for different application domains. The paper provides an overview of TDL and discusses its application on a use case from the rail domain.}, 
keywords={program testing;specification languages;ETSI test description language;TDL language;model-based testing techniques;test purpose;test description;test automation;conformance test;interoperability test;test representation;meta-modelling approach;abstract syntax;application domain;rail domain;Testing;Unified modeling language;Concrete;Telecommunication standards;Syntactics;Semantics;Abstracts;Model-based Testing;Domain-Specific Languages;Meta-modelling;Rail Application}, 
doi={}, 
ISSN={}, 
month={Jan},}
@ARTICLE{7820199, 
author={M. Fleck and J. Troya and M. Kessentini and M. Wimmer and B. Alkhazi}, 
journal={IEEE Transactions on Software Engineering}, 
title={Model Transformation Modularization as a Many-Objective Optimization Problem}, 
year={2017}, 
volume={43}, 
number={11}, 
pages={1009-1032}, 
abstract={Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.}, 
keywords={genetic algorithms;program debugging;search problems;software maintenance;software quality;model transformations modularization;model transformation modularization;model transformation programs;transformation languages;higher-order transformations;in-place transformation engine;concrete transformation language;ATL transformations;bug fixing;transformation rules;many-objective optimization problem;metamodels version;monolithic artifacts;automated search-based approach;NSGA-III;statistical analysis;maintenance activities;Unified modeling language;Object oriented modeling;Adaptation models;Measurement;Algorithm design and analysis;Software engineering;Computer bugs;Model transformation;modularization;ATL;NSGA-III;MDE;SBSE}, 
doi={10.1109/TSE.2017.2654255}, 
ISSN={0098-5589}, 
month={Nov},}
@ARTICLE{7361678, 
author={}, 
journal={IEEE Std 487-2015 (Revision of IEEE Std 487-2007) - Redline}, 
title={IEEE Standard for the Electrical Protection of Communications Facilities Serving Electric Supply Locations -- General Considerations - Redline}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-324}, 
abstract={General considerations are presented for the electrical protection of telecommunications facilities serving electric supply locations. This standard contains material that is common to the IEEE 487(TM) family of standards (i.e., dot-series) including fundamental protection theory; basic electrical protection philosophy, concepts, and designs; protection apparatus; service types; reliability; service performance objective (SPO) classifications; and transmission considerations. In general, special protective measures, handling procedures, and administrative procedures are necessary to provide electrical protection against damage to telecommunications facilities and equipment, maintain reliability of service, and ensure the safety of personnel.}, 
keywords={IEEE Standards;Electricity supply industry;Voltage control;Power transmission lines;Power stations;electric supply locations;high-voltage tower;IEEE 487(TM);power stations;protection;wire-line telecommunications}, 
doi={}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7372061, 
author={A. Wlfl and N. Siegmund and S. Apel and H. Kosch and J. Krautlager and G. Weber-Urbina}, 
booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Generating Qualifiable Avionics Software: An Experience Report (E)}, 
year={2015}, 
volume={}, 
number={}, 
pages={726-736}, 
abstract={We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.}, 
keywords={avionics;helicopters;program compilers;software engineering;qualifiable avionics software;data-management component;NH90 helicopter;Airbus helicopter;legally-binding certification process;model-based technology;product-line technology;software repository analysis;Aerospace electronics;Helicopters;System software;Interviews;Hardware;Encoding}, 
doi={10.1109/ASE.2015.35}, 
ISSN={}, 
month={Nov},}
@INBOOK{5273147, 
author={Steven M. Kaplan}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={B}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={

}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273147},}
@INPROCEEDINGS{6597883, 
author={G. Fan and H. Yu and L. Chen and D. Liu}, 
booktitle={2013 International Symposium on Theoretical Aspects of Software Engineering}, 
title={Aspect Orientation Based Test Case Selection Strategy for Service Composition}, 
year={2013}, 
volume={}, 
number={}, 
pages={95-104}, 
abstract={Software testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may select part of their test cases so that those that are more important are run earlier in the testing process. However, the methods that can be used to select test cases for service composition and its analysis are still lacking at present. This paper proposes an aspect orientation based test case selection strategy for service composition. Aspect-orientation is used to weave testing crosscutting concerns of service composition, which includes component testing concern and testing concern of service composition, the weaving mechanism dynamically integrates these schemas into a testing enforcement model. Based on this, the test cases selection strategy for service composition is given, and abstract it as a crosscutting concern to weave into testing model, the corresponding enforcement algorithm is also given, the operation semantics and related theories of Petri nets help prove its effectiveness and feasibility. A case study explains the testing process of service composition, and a series of experiments are done to explain that the use of aspects for testing Web service is more efficient than conventional techniques, which can improve the testing quality and efficiency.}, 
keywords={aspect-oriented programming;Petri nets;program testing;programming language semantics;software maintenance;Web services;aspect orientation based test case selection strategy;service composition;software testing;software maintenance;component testing;weaving mechanism;testing enforcement model;crosscutting concern;operation semantics;Petri nets;Web service;testing quality;testing efficiency;Testing;Weaving;Computational modeling;Analytical models;Semantics;Web services;Service composition; testing model; test case; aspect orientation; Petri nets}, 
doi={10.1109/TASE.2013.21}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8051361, 
author={M. Terber}, 
booktitle={2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
title={Function-Oriented Decomposition for Reactive Embedded Software}, 
year={2017}, 
volume={}, 
number={}, 
pages={288-295}, 
abstract={Due to C's overwhelming dominance in industry, reactive embedded applications usually rely on conventional sequential programming. Adopted approaches favor event-driven paradigms which prevent function-oriented code decomposition in particular. This encourages the violation of fundamental software engineering principles. The reactive programming paradigm is proposed as a general solution. However, most reactive languages cannot keep up with C's practical advantages. It appears, that the subfamily of synchronous languages provides promising features but real-world deployments and evaluations are rarely reported in literature. On this account, we make two major contributions in this paper. First, we elaborate how the lack of function-oriented software decomposition manifests in a real-life industrial application. Second, we provide a corresponding re-implementation which illustrates the deployment and discusses the gained engineering benefits provided by the third-party, synchronous-reactive programming language Cu. We believe that our work generally reveals a practicable way of improving embedded software quality in industrial applications.}, 
keywords={C language;embedded systems;programming languages;software quality;function-oriented decomposition;reactive embedded software;sequential programming;event-driven paradigms;software engineering principles;reactive programming paradigm;reactive languages;C practical advantages;synchronous languages;function-oriented software decomposition;synchronous-reactive programming language;embedded software quality;Programming;Logic gates;Software;Software engineering;Heating systems;Mirrors;Switches;synchronous reactive programming;Cu;software decomposition;software quality}, 
doi={10.1109/SEAA.2017.42}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7323164, 
author={T. C. De}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Applying model-driven development to environment monitoring System}, 
year={2015}, 
volume={}, 
number={}, 
pages={577-584}, 
abstract={Environmental monitoring is critical in understanding whether the quality of our environment is getting better or worse. Information gathered by using an environmental monitoring system is important to make decisions. Vietnam is a vulnerable country of climate change. Specially, in the South of Vietnam, the Mekong delta is known as the region getting the most impact of sea level rise in Vietnam. That leads to a lot of problems making the worst effects to residents in the area, who are mainly still very poor. On the other hand, Vietnam is going on industrialization process that makes a strong effect on the environment. To deal with these challenges, different projects of environment management have been proposed and implemented and many monitoring systems have been built in those projects. Those systems are basically sensor networks with high cost in developing and maintaining. They are related to modern technology such as cloud, communication mobile and wireless. They provide the data for large community for different purposes. Therefore, building such a system is normally a long term project that requires an incremental and modular development for a complex system. This paper, on one hand, represents some common characteristics of an environment monitoring system that requires more study to develop a formal model and a methodology for their specifications, implementations and verification. On the other hand, we would like to adapt the formal model approach proposed for Intelligent Transport Systems (ITS) to an environmental monitoring system. The framework of Baobab is also introduced as an example for transformation from model to code.}, 
keywords={environmental monitoring (geophysics);environmental science computing;environment monitoring system;applying model driven development;vulnerable country;South of Vietnam;industrialization process;environment management;intelligent transport systems;ITS;Unified modeling language;Cities and towns;Environmental monitoring;Hardware;Mobile communication;Adaptation models;Environment Management;Environment Monitoring System;Formal Method;Model-Driven Development;Sensor System}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7872763, 
author={S. R. Akbar and W. Kurniawan and M. H. H. Ichsan and I. Arwani and M. T. Handono}, 
booktitle={2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, 
title={Pervasive device and service discovery protocol in XBee sensor network}, 
year={2016}, 
volume={}, 
number={}, 
pages={79-84}, 
abstract={Internet of Things is a novel paradigm that combined microcomputer and wireless communication technology. IoT device will be considered as a pervasive and ubiquitous device that able to interact with it user and environment autonomously with minimum human intervention. At present, wireless technology already has pervasive features in the link and the network layer. They able to do the dynamic addressing, finding a neighbor and do the routing task such as in Xbee technology. In the future, pervasive sensing will support adaptive context-aware services that are not provided by the wireless sensor network protocol in the link and the network layer. Our research proposed pervasive device and service discovery protocol at the application level by creating a protocol in the smart sensor device and the smart sensor gateway. By implementing our protocol, the sensor network gateway is able to find each of the sensor network device and service descriptions and request the on-demand service to the smart sensor device. The protocol is implemented in two Arduino Uno integrated with XBee transceiver as the smart sensor device and the raspberry pi as the smart sensor gateway. Result shows, the gateway was able to find both device and service description in the smart sensor network with 4.13 seconds average time. The average round trip time for request and response data from the gateway is 0.201 seconds.}, 
keywords={intelligent sensors;Internet of Things;internetworking;protocols;wireless sensor networks;service discovery protocol;XBee sensor network;Internet of Things;microcomputer technology;wireless communication technology;ubiquitous device;wireless technology;network layer;dynamic addressing;routing task;Xbee technology;pervasive sensing;adaptive context-aware services;wireless sensor network protocol;pervasive device discovery protocol;smart sensor device;smart sensor gateway;service description;on-demand service;Arduino Uno;XBee transceiver;Raspberry Pi;smart sensor network;round trip time;request data;response data;Logic gates;Intelligent sensors;Protocols;Machine-to-machine communications;Temperature sensors;Humidity;Wireless sensor networks;Internet of Things;pervasive discovery;sensor network}, 
doi={10.1109/ICACSIS.2016.7872763}, 
ISSN={}, 
month={Oct},}
@INBOOK{6544996, 
author={G. Keith Cambron}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Integration and Innovation}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={

This chapter contains sections titled:

Technology Integration

Lifecycle Support

Invention and Innovation

Summary

References

]]>}, 
keywords={Benchmark testing}, 
doi={10.1002/9781118394519.ch13}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6544996},}
@INPROCEEDINGS{6227131, 
author={S. Thummalapenta and S. Sinha and N. Singhania and S. Chandra}, 
booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
title={Automating test automation}, 
year={2012}, 
volume={}, 
number={}, 
pages={881-891}, 
abstract={Mention test case, and it conjures up the image of a script or a program that exercises a system under test. In industrial practice, however, test cases often start out as steps described in natural language. These are essentially directions a human tester needs to follow to interact with an application, exercising a given scenario. Since tests need to be executed repeatedly, such manual tests then have to go through test automation to create scripts or programs out of them. Test automation can be expensive in programmer time. We describe a technique to automate test automation. The input to our technique is a sequence of steps written in natural language, and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention. The technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets, except that there can be ambiguity in identifying segments, in identifying the action in a segment, as well as in the specification of the target of the action. The technique resolves this ambiguity by backtracking, until it can synthesize a successful sequence of calls. We present an evaluation of our technique on professionally created manual test cases for two open-source web applications as well as a proprietary enterprise application. Our technique could automate over 82% of the steps contained in these test cases with no human intervention, indicating that the technique can reduce the cost of test automation quite effectively.}, 
keywords={natural languages;program testing;automating test automation;test case;natural language;programmer time;human intervention;Manuals;Automation;Optimization;Humans;Natural languages;Programming profession}, 
doi={10.1109/ICSE.2012.6227131}, 
ISSN={1558-1225}, 
month={June},}
@INPROCEEDINGS{1617602, 
author={Mingjing Chen and H. Haggag and A. Orailoglu}, 
booktitle={24th IEEE VLSI Test Symposium}, 
title={Decision tree based mismatch diagnosis in analog circuits}, 
year={2006}, 
volume={}, 
number={}, 
pages={6 pp.-285}, 
abstract={Mismatch is a critical consideration in analog circuit design. Knowledge of mismatch locations and an understanding of their impact on circuit performance are crucial for design optimization and process improvement. We present a circuit level mismatch diagnosis methodology in this paper. The functional parameters with abnormal values are measured as manifestations of mismatch, from which reverse tracing is employed to determine the mismatch source. The methodology is implemented on a representative benchmark and its efficiency confirmed by simulation results}, 
keywords={analogue circuits;decision trees;fault diagnosis;decision tree;mismatch diagnosis;analog circuit design;design optimization;process improvement;reverse tracing;mismatch source;Decision trees;Analog circuits;Design optimization;Circuit simulation;Fabrication;Degradation;Design automation;Predictive models;Circuit testing;Circuit optimization}, 
doi={10.1109/VTS.2006.26}, 
ISSN={1093-0167}, 
month={April},}
@INPROCEEDINGS{6139107, 
author={F. Belli and A. T. Endo and M. Linschulte and A. Simao}, 
booktitle={Proceedings of 2011 IEEE 6th International Symposium on Service Oriented System (SOSE)}, 
title={Model-based testing of web service compositions}, 
year={2011}, 
volume={}, 
number={}, 
pages={181-192}, 
abstract={The use of web services integrated in different applications, especially the composition of services, brings challenges for testing due to their complex interactions. In this paper, we propose an event-based approach to test web service compositions. The approach is based on event sequence graphs which we extend by facilities to consider the specific features of web service compositions. An enterprise service bus component supports the test case execution. A case study, based on a commercial web application, demonstrates the feasibility of the approach and analyzes its characteristics. The results of empirical work suggest that the approach is a promising candidate to reach a high level of confidence and reliability.}, 
keywords={program testing;Web services;model-based testing;Web service compositions;event sequence graphs;Testing;Service oriented architecture;Business;Data models;Monitoring;enterprise service bus;event sequence graphs;model-based testing;service composition testing}, 
doi={10.1109/SOSE.2011.6139107}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6908678, 
author={M. F. Granda and N. Condori-Fernndez and T. E. J. Vos and O. Pastor}, 
booktitle={2014 IEEE 1st International Workshop on Requirements Engineering and Testing (RET)}, 
title={Towards the automated generation of abstract test cases from requirements models}, 
year={2014}, 
volume={}, 
number={}, 
pages={39-46}, 
abstract={In a testing process, the design, selection, creation and execution of test cases is a very time-consuming and error-prone task when done manually, since suitable and effective test cases must be obtained from the requirements. This paper presents a model-driven testing approach for conceptual schemas that automatically generates a set of abstract test cases, from requirements models. In this way, tests and requirements are linked together to find defects as soon as possible, which can considerably reduce the risk of defects and project reworking. The authors propose a generation strategy which consists of: two meta-models, a set of transformations rules which are used to generate a Test Model, and the Abstract Test Cases from an existing approach to communication-oriented Requirements Engineering; and an algorithm based on Breadth-First Search. A practical application of our approach is included.}, 
keywords={formal specification;program testing;tree searching;automated abstract test cases generation;requirements models;model-driven testing approach;project reworking;transformations rules;communication-oriented requirements engineering;breadth-first search;Unified modeling language;Testing;Abstracts;Object oriented modeling;Analytical models;Concrete;Business;Requirements-based testing;Communication Analysis;Model-driven testing;Conceptual Schema Testing;Test Model Generation;Test Case Generation}, 
doi={10.1109/RET.2014.6908678}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{5280052, 
author={H. Krapfenbauer and D. Ertl and A. Zoitl and F. Kupzog}, 
booktitle={2009 Fourth International Multi-Conference on Computing in the Global Information Technology}, 
title={Improving Component Testing of Industrial Automation Software}, 
year={2009}, 
volume={}, 
number={}, 
pages={259-262}, 
abstract={Industrial automation systems are tested nowadays mainly via system tests at a very late stage of development. These tests are conducted manually, are time-consuming and cost-intensive. Earlier testing of automation software, e.g., component testing, is therefore desired in order to reduce the effort for system testing by detecting errors sooner. In this paper we present an improved concept for a test environment that enables developers of industrial control electronics to test the functionality of IEC 61499 software components. Components can be tested on any hardware with an IEC 61499 runtime environment, even on the target hardware. There is no need to change the automation software for testing. We propose using dynamically typed languages to implement tests because such languages have inherent properties that are useful for this task. We provide example code of a typical test case.}, 
keywords={control engineering computing;IEC standards;object-oriented programming;program testing;software engineering;industrial automation software;system testing;component testing improvement;industrial automation systems;industrial control electronics;test environment;IEC 61499 software components;target hardware;dynamically typed languages;Automatic testing;Software testing;Computer industry;Automation;System testing;Electronic equipment testing;IEC standards;Hardware;Industrial control;Industrial electronics;Industrial Automation Software;IEC 61499;Component Testing;Dynamically Typed Languages}, 
doi={10.1109/ICCGI.2009.46}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4068343, 
author={}, 
journal={IEEE Std 1175.2-2006}, 
title={IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-45}, 
abstract={This recommended practice describes interconnections that need to be understood and evaluated when buying, building, testing, or using computer-aided software engineering (CASE) tools. CASE tools are developed for use in creating computing systems. By assisting users to reach a clear understanding of the context of operation for a computing system tool, this recommended practice contributes to the effective implementation and application of computing system tools. This recommended practice does not describe the processes of evaluating, acquiring, or adopting CASE tools. This recommended practice is limited to the technical aspects of CASE tools. It does not include issues in the management, marketing, or training domains.}, 
keywords={computer aided software engineering;software tools;CASE tool interconnection;computer-aided software engineering;computing system tool;IEEE standards;Context;Computer aided software engineering;Trademarks;Standards Board;Patents;Computer-Aided Software Engineering (CASE) tools;tool communications;tool interconnections}, 
doi={10.1109/IEEESTD.2007.288641}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{4610718, 
author={M. Nicolae and R. Dobrescu and M. Dobrescu and D. Popescu}, 
booktitle={2008 6th International Symposium on Communication Systems, Networks and Digital Signal Processing}, 
title={Embedded node around a DSP core for mobile sensor networks over 802.11 infrastructure}, 
year={2008}, 
volume={}, 
number={}, 
pages={643-646}, 
abstract={Digital signal processors (DSPs) are very efficient devices to implement algorithms for signal processing and analyzing. Endowing sensorial nodes from a sensor network with such processing cores it could lead to high performance because of the possibility of parallel and distribute processing and thus reducing the quantity of information spread into the network (network load). If such a node uses for communication a mature infrastructure like 802.11 standard, will be obtained a solution for implementing mobile sensor networks with high performance end cost efficient. In the same way of obtaining high performances with low costs we suggest that on the information chain from the physical quantity to the numerical result, the acquisition part to be done by an audio codec. In this way, the entire network may look like a Voice over IP (VoIP) mobile network, yet the information exchanged will not be voice but measures and commands with quality of service (QoS) inherited from VoIP.}, 
keywords={digital signal processing chips;embedded systems;Internet telephony;parallel processing;quality of service;wireless LAN;wireless sensor networks;mobile sensor networks;digital signal processors;IEEE 802.11;parallel processing;distributed processing;voice over IP;quality of service;Digital signal processing;Computer architecture;Wireless sensor networks;Codecs;Mobile communication;Mobile computing;Hardware}, 
doi={10.1109/CSNDSP.2008.4610718}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7000095, 
author={M. Staron and R. Rana and W. Meding and M. Nilsson}, 
booktitle={2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement}, 
title={Consequences of Mispredictions of Software Reliability: A Model and its Industrial Evaluation}, 
year={2014}, 
volume={}, 
number={}, 
pages={157-162}, 
abstract={Predicting reliability of software under development is an important part of estimations in software engineering projects. In many organizations as the goal is that software products are released with no known defects, the process of finding and removing defects correlates with the effort for software projects. Software development projects estimate the resources needed to design, develop, test and release software products, and the number of defects which have to be handled. In this paper we present a model for consequence analysis of inaccurate predictions of quality in software projects. The model is a result of multiple case studies and is evaluated at two companies. The model recognizes the most common mispredictions - e.g. Over- and under-prediction, early- and late-predictions - and the combination of theses. The results from the industrial evaluation show that the consequences can be grouped according to under- and over-predictions and that the late- and early-predictions have the same consequences. The results show also that mispredicting the shape of the reliability curve has a significant consequence with regard to assessment of release readiness and resource planning.}, 
keywords={project management;software management;software reliability;software reliability;industrial evaluation;software engineering project;software product;software development project;consequence analysis;software project;reliability curve;release readiness;resource planning;Software;Testing;Software reliability;Shape;Organizations;Predictive models;Software Reliability;SRGMs;Consequence;Mispredictions;Software;Forecasting}, 
doi={10.1109/IWSM.Mensura.2014.16}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6297159, 
author={L. Nagowah and K. Doorgah}, 
booktitle={2012 International Conference on Computer Information Science (ICCIS)}, 
title={Improving test data management in record and playback testing tools}, 
year={2012}, 
volume={2}, 
number={}, 
pages={931-937}, 
abstract={It is almost impossible to prevent requirement change in the web development life cycle. Selenium despite being a widely used open source automated tool for testing web application, has its limitation when it concerns test data management. Frequent changes in requirement result in changes in the user interface which in turn requires additional effort to re-record the test script. Eventually keeping track of test data used for each test script becomes very problematic for the tester. In this paper, we analyse existing tools and provide a design of an automated testing tool, Kishanium that also manages the set of test data. A prototype was created during experimentation phase to prove the concept of the underlying ideas of the proposed tool. The prototype has been implemented based on the core technologies of DomDocument, XPath and Curl. The testing carried out proves that Kishanium is a useful automated tool that can be used on its own or in conjunction with Selenium. With a very systematic approach it automatically searches input and button objects, allows testers to add new test data, edit existing test data and delete previous test data in order to respond to frequent requirement changes. The power of Kishanium is that it is able to re-use existing test data even if there are a number of changes in the user interface. It also automatically runs the tests with the appropriate set of test data using its Poster Component. Moreover the Kishanium automated tool provides additional features such as Data generator, Spylink and Snapshot.}, 
keywords={formal specification;Internet;program testing;user interfaces;test data management;record testing tool;playback testing tool;requirement change;Web development life cycle;user interface;test script;automated testing tool;Kishanium;DomDocument;XPath;Curl;Selenium;poster component;data generator;Spylink;Snapshot;Presses;Fires;Manuals;Libraries;record and playback problem;automated testing;test data management}, 
doi={10.1109/ICCISci.2012.6297159}, 
ISSN={}, 
month={June},}
@INBOOK{6542353, 
author={G. Keith Cambron}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Disasters and Outages}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Disasters

Outages

The Vicious Cycle

Summary

References

]]>}, 
keywords={Tornadoes;Earthquakes;Copper;Storms;Power cables;Bandwidth;Organizations}, 
doi={10.1002/9781118394519.ch14}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6542353},}
@INPROCEEDINGS{7321627, 
author={Z. Bicevska and J. Bicevskis and I. Oditis}, 
booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
title={Smart technologies for improved software maintenance}, 
year={2015}, 
volume={}, 
number={}, 
pages={1533-1538}, 
abstract={Steadily increasing complexity of software systems makes them difficult to configure and use without special IT knowledge. One of the solutions is to improve software systems making them smarter, i.e. to supplement software systems with features of self-management, at least partially. This paper describes several software components known as smart technologies, which facilitate software use and maintenance. As to date smart technologies incorporate version updating, execution environment testing, self-testing, runtime verification and business process execution. The proposed approach has been successfully applied in several software projects.}, 
keywords={software maintenance;smart technology;improved software maintenance;software systems;software components;business process execution;software projects;execution environment testing;runtime verification;Software;Information systems;Business;Built-in self-test;Runtime;Complexity theory;Autonomic computing;smart technologies;self-managing systems;software maintenance}, 
doi={10.15439/2015F170}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7878375, 
author={H. Fangohr and M. Albert and M. Franchin}, 
booktitle={2016 IEEE/ACM International Workshop on Software Engineering for Science (SE4Science)}, 
title={Nmag Micromagnetic Simulation Tool  Software Engineering Lessons Learned}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={We review design and development decisions and their impact for the open source code Nmag from a software engineering in computational science point of view. We summarise lessons learned and recommendations for future computational science projects. Key lessons include that encapsulating the simulation functionality in a library of a general purpose language, here Python, provides great flexibility in using the software. The choice of Python for the top-level user interface was very well received by users from the science and engineering community. The from-source installation in which required external libraries and dependencies are compiled from a tarball was remarkably robust. In places, the code is a lot more ambitious than necessary, which introduces unnecessary complexity and reduces main- tainability. Tests distributed with the package are useful, although more unit tests and continuous integration would have been desirable. The detailed documentation, together with a tutorial for the usage of the system, was perceived as one of its main strengths by the community.}, 
keywords={digital simulation;graphical user interfaces;micromagnetics;physics computing;public domain software;software libraries;software packages;Nmag micromagnetic simulation tool;software engineering;open source code;general purpose language library;Python;top-level user interface;external libraries;tarball;software package;unit tests;Libraries;Software;Mathematical model;Computational modeling;Software engineering;Micromagnetics;Magnetization;Nmag;Computational Science Software Engineering;Python;Finite Elements}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6949287, 
author={R. Angmo and M. Sharma}, 
booktitle={2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)}, 
title={Performance evaluation of web based automation testing tools}, 
year={2014}, 
volume={}, 
number={}, 
pages={731-735}, 
abstract={In today's 21<sup>st</sup> century era countless software applications are written as a web based application which runs in a web browsers. With new technologies and commercialization of I.T. sector, the web based system has undergoes frequent and rapid changes. Today Softwares are coded as a web based application, which help to access data from any part of the globe. Even the economic relevance of web based enhances the control and quality of software. The quality assurance of any system depends on its test. But to do manually testing in most of the cases is time consuming, expensive and hectic. For the better business purpose and to save time and money automation testing is required. There are variety of tools are available in the market for this. One of the best known tool is selenium suite which is a combination of different automation testing tool. In this paper we will discuss about the selenium suite. It provides testers with different framework for different test cases. The main objective of this paper is to find the best tool in selenium suite and then compare it with some other tool for same task. For this purpose, performance evaluation is done on the basis of some criteria.}, 
keywords={Internet;program testing;software performance evaluation;software quality;software tools;selenium suite;software quality;Web browsers;Web based application;software applications;Web based automation testing tools;performance evaluation;Automation;Performance evaluation;Browsers;Software;Software testing;Information technology;Web applications;Selenium;Performance;Watir-webdriver;Test case;Automation testing}, 
doi={10.1109/CONFLUENCE.2014.6949287}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{1559629, 
author={J. Noll and R. Steel}, 
booktitle={2005 IEEE Aerospace Conference}, 
title={EKLOPS: An Adaptive Approach to a Mission Planning System}, 
year={2005}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={EKLOPS is the Enhanced Kernel Library for Operational Planning and Scheduling. This paper discusses the area of mission planning, and present EKLOPS as a generic mission planning solution proposed by the Mission Planning Group of Anite Systems GmbH. EKLOPS has evolved from mission planning systems that were developed under contracts of the European Space Agency. It implements an adaptive object model architecture to integrate the common elements of mission planning systems. The model of a specific satellite mission is expressed as metadata, which configure the MPS. Rules implement functions of the planning process for which a number of specific roles can be identified. The paper presents a language that has so far been utilized to express constraint-checking rules. The experience made with EKLOPS is shown using the examples of the ENVISAT and Mars Express missions. The generic nature of EKLOPS facilitates an extension of its usage outside the field of spacecraft operations planning}, 
keywords={aerospace computing;artificial satellites;planning;scheduling;space research;EKLOPS;mission planning system;enhanced kernel library for operational planning and scheduling;European Space Agency;adaptive object model architecture;satellite mission;metadata;planning process;constraint-checking rules;ENVISAT;Mars Express missions;spacecraft operations planning;Satellites;Space missions;Process planning;Programmable control;Testing;Hardware;Adaptive systems;Steel;Kernel;Libraries}, 
doi={10.1109/AERO.2005.1559629}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{4040327, 
author={}, 
journal={IEEE Std P487/Draft6}, 
title={IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={This recommended practice presents engineering design practices for special high-voltage protection systems intended to protect wire-line telecommunication facilities serving electric supply locations. The following topics are included in this document: a) A description of the electric supply locations environment, i.e., ground potential rise (GPR), induced voltages, lightning, and switching transients; b)A discussion of special high-voltage protection devices; c)Definitions of service types and service performance objectives for electric supply locations telecommunication services; d)Special protection theory and philosophy; e)Special protection system design guidelines; f)Personnel safety considerations; g)Grounding; h)Cables with metallic members. Other telecommunication alternatives such as radio and optica fiber systems are excluded from this document.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{7128891, 
author={F. Wanderley and A. Silva and J. Arajo}, 
booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
title={Evaluation of BehaviorMap: A user-centered behavior language}, 
year={2015}, 
volume={}, 
number={}, 
pages={309-320}, 
abstract={In the software development process, one of the recurring problems is to ensure that the expectations of stakeholders are being met. These expectations must match the system's behavior and be present in the requirements specifications and models. The Requirements Engineering discipline studies how to capture, specify, validate and manage requirements. However, recent empirical studies show that stakeholders do not usually understand traditional requirements models. This paper focuses on the cognitive evaluation of a user-centered language called BehaviorMap that aims to specify behavioral user scenarios in a cognitive way, based on mind map modelling. This paper describes an experimental evaluation to verify the understandability of the BehaviorMap scenarios compared to the textual ones. The experiment gathered data from 15 individuals (nave-users), with different backgrounds, that had to analyze 8 scenarios, being 4 graphical and 4 textual. To assess the participants' cognitive effort, it was used questionnaires. Also, the time effort to perform the tasks was measured. This experiment showed promising results for the BehaviorMap scenarios.}, 
keywords={cognition;formal specification;software prototyping;user centred design;BehaviorMap evaluation;user-centered behavior language;software development process;stakeholder expectation;system behavior;requirements specifications;requirements engineering;empirical analysis;cognitive evaluation;behavioral user scenarios;mind map modelling;graphical analysis;textual analysis;Data structures;Boolean functions;Visualization;Software;Atmospheric measurements;Particle measurements;Agile Requirements;Mind Map Modelling;Behavior-Driven Design;User-Centred Requirements;Cognitive Effort}, 
doi={10.1109/RCIS.2015.7128891}, 
ISSN={2151-1349}, 
month={May},}
@INPROCEEDINGS{687914, 
author={D. E. Bernard and G. A. Dorais and C. Fry and E. B. Gamble and B. Kanefsky and J. Kurien and W. Millar and N. Muscettola and P. P. Nayak and B. Pell and K. Rajan and N. Rouquette and B. Smith and B. C. Williams}, 
booktitle={1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)}, 
title={Design of the Remote Agent experiment for spacecraft autonomy}, 
year={1998}, 
volume={2}, 
number={}, 
pages={259-281 vol.2}, 
abstract={This paper describes the Remote Agent flight experiment for spacecraft commanding and control. In the Remote Agent approach, the operational rules and constraints are encoded in the flight software. The software may be considered to be an autonomous "remote agent" of the spacecraft operators in the sense that the operators rely on the agent to achieve particular goals. The experiment will be executed during the flight of NASA's Deep Space One technology validation mission. During the experiment, the spacecraft will not be given the usual detailed sequence of commands to execute. Instead, the spacecraft will be given a list of goals to achieve during the experiment. In flight, the Remote Agent flight software will generate a plan to accomplish the goals and then execute the plan in a robust manner while keeping track of how well the plan is being accomplished. During plan execution, the Remote Agent stays on the lookout for any hardware faults that might require recovery actions or replanning. In addition to describing the design of the remote agent, this paper discusses technology-insertion challenges and the approach used in the Remote Agent approach to address these challenges. The experiment integrates several spacecraft autonomy technologies developed at NASA Ames and the Jet Propulsion Laboratory: on-board planning, a robust multi threaded executive, and model-based failure diagnosis and recovery.}, 
keywords={aerospace control;space vehicles;robust control;software agents;Remote Agent experiment;spacecraft autonomy;spacecraft control;operational rules;flight software;Deep Space One technology;validation mission;robust manner;recovery actions;technology-insertion challenges;NASA;on-board planning;multi threaded executive;model-based failure diagnosis;Space vehicles;Space technology;Robustness;NASA;Humans;Propulsion;Laboratories;Space missions;Orbital robotics;Paper technology}, 
doi={10.1109/AERO.1998.687914}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{4040044, 
author={}, 
journal={IEEE Std P1073.0.1.1/D01J}, 
title={Unapproved IEEE Draft Guide for Health Informaticspoint-Of-Care Medical Device Communicationtechnical Reportguidelines for the Use of RF Wireless Technology}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{7886987, 
author={J. Kim and D. Batory and D. Dig and M. Azanza}, 
booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)}, 
title={Improving Refactoring Speed by 10X}, 
year={2016}, 
volume={}, 
number={}, 
pages={1145-1156}, 
abstract={Refactoring engines are standard tools in today's Integrated Development Environments (IDEs). They allow programmers to perform one refactoring at a time, but programmers need more. Most design patterns in the Gang-of-Four text can be written as a refactoring script - a programmatic sequence of refactorings. In this paper, we present R3, a new Java refactoring engine that supports refactoring scripts. It builds a main-memory, non-persistent database to encode Java entity declarations (e.g., packages, classes, methods), their containment relationships, and language features such as inheritance and modifiers. Unlike classical refactoring engines that modify Abstract Syntax Trees (ASTs), R3 refactorings modify only the database; refactored code is produced only when pretty-printing ASTs that reference database changes. R3 performs comparable precondition checks to those of the Eclipse Java Development Tools (JDT) but R3's codebase is about half the size of the JDT refactoring engine and runs an order of magnitude faster. Further, a user study shows that R3 improved the success rate of retrofitting design patterns by 25% up to 50%.}, 
keywords={Java;software maintenance;refactoring speed;integrated development environment;IDE;Gang-of-Four text;refactoring script;refactoring sequence;Java refactoring engine;Java entity declarations;abstract syntax trees;AST;Eclipse Java development tools;JDT;retrofitting design patterns;Java;Engines;Databases;Graphics;Computer bugs;Graphical user interfaces;Maintenance engineering}, 
doi={10.1145/2884781.2884802}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{4577708, 
author={A. Gargantini and E. Riccobene and P. Scandurra}, 
booktitle={2008 International Symposium on Industrial Embedded Systems}, 
title={A model-driven validation amp; verification environment for embedded systems}, 
year={2008}, 
volume={}, 
number={}, 
pages={241-244}, 
abstract={This paper presents a validation and verification tool component, based on the abstract state machine formal method, that we are developing to support high level formal analysis of embedded system model-driven design. This component is integrated into a model-driven environment for HW/SW co-design that provides a graphical high-level representation of HW and SW components by means of UML profiles for SystemC/multi-thread C, and allows C/C++/SystemC code generation/back-annotation from/to graphical UML models.}, 
keywords={C++ language;embedded systems;formal verification;hardware-software codesign;multi-threading;program compilers;Unified Modeling Language;model-driven validation;model-driven verification;embedded system;abstract state machine;formal method;formal analysis;model-driven design;model-driven environment;HW/SW codesign;graphical high-level representation;SystemC;multithread C language;C++ language;graphical UML model;Embedded system;Unified modeling language;Hardware;Diffusion tensor imaging;Application software;Computer architecture;Industrial control;Standardization;Job shop scheduling;Timing}, 
doi={10.1109/SIES.2008.4577708}, 
ISSN={2150-3109}, 
month={June},}
@INPROCEEDINGS{7784207, 
author={M. Sroka and D. Fisch and R. Nagy}, 
booktitle={2016 6th International Conference on Information Communication and Management (ICICM)}, 
title={Localised mutation in evolutionary test model learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={13-18}, 
abstract={The focus of this paper is on automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is described. In an experiment the impact of localised mutational changes on the evolutionary learning method is investigated.}, 
keywords={automotive engineering;evolutionary computation;learning (artificial intelligence);program testing;localised mutation;evolutionary test model learning;test case design automation;model-based testing;automotive embedded software;evolutionary algorithm;localised mutational changes;Biological cells;Sociology;Statistics;Testing;Software;Algorithm design and analysis;Software algorithms;model-based testing;evolutionary test model learning;localised mutation;reproduction operator}, 
doi={10.1109/INFOCOMAN.2016.7784207}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6575271, 
author={N. Mulkey and B. Liu and A. Medda}, 
booktitle={2013 8th International Conference on System of Systems Engineering}, 
title={The Integrated Blast Effects Sensor Suite: A rapidly developed, complex, system of systems}, 
year={2013}, 
volume={}, 
number={}, 
pages={224-228}, 
abstract={The need for rapid development of tactical system of systems solutions for military applications requires the use of system modeling techniques and simulation and validation methods to be applied throughout the lifecycle of the system. This combined approach of development and verification is preferred to traditional approaches for risk mitigation and cost effectiveness. This paper examines the Integrated Blast Effects Sensor Suite developed at the Georgia Tech Research Institute and its architecture as a complex system of systems.}, 
keywords={explosions;military equipment;risk management;sensors;systems engineering;integrated blast effect sensor suite;complex system of systems;tactical system of systems solutions;system modeling techniques;validation methods;simulation methods;risk mitigation;Georgia Tech Research Institute;Vehicles;Systems engineering and theory;Computer architecture;Databases;Complexity theory;Explosives;Data collection;System of System;SoS lifecycle;Fast Development;High Complexity}, 
doi={10.1109/SYSoSE.2013.6575271}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6511809, 
author={F. Wanderley and D. S. da Silveria}, 
booktitle={2012 Eighth International Conference on the Quality of Information and Communications Technology}, 
title={A Framework to Diminish the Gap between the Business Specialist and the Software Designer}, 
year={2012}, 
volume={}, 
number={}, 
pages={199-204}, 
abstract={Requirements Engineering establishes the process for defining requirements as one in which elicitation, modeling and analysis are tasks which must be carried out. This process should involve different stakeholders and their different viewpoints. Among these stakeholders, there is the software designer, responsible for creating models based on the information gathered by business specialists. However, this communication channel may create some "noise" that leads to information being lost. This loss produces a semantic gap between what is desired and what will be developed. The semantic gap is characterized by inconsistencies in the requirements represented by scenarios -- user stories in a behavior-driven context -- and by the conceptual model. This paper presents an interactive approach to the agile requirements modeling, thus fostering greater consistency between the artifacts of the scenarios and the conceptual model. This consistency is ensured by using a mind model specification which will serve as a basis for transforming the definitions of the scenario and generating a conceptual model represented by a UML class diagram. The mind model represents the main role of this approach, and functions as a bond that represents the business entities, thus enabling the requirements to be more consistent with the reality of the business.}, 
keywords={formal specification;software prototyping;Unified Modeling Language;business specialist;software designer;requirements engineering;semantic gap;behavior-driven context;interactive approach;agile requirements modeling;UML class diagram;mind model specification;Agile Modeling Requirements;Behaviour Driven Development;UML;Mind Map Modeling;Domain Model}, 
doi={10.1109/QUATIC.2012.9}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6115389, 
author={T. Cruz and P. Simes and J. Almeida and J. Rodrigues and E. Monteiro and F. Bastos and A. Laranjeira}, 
booktitle={2011 IEEE 36th Conference on Local Computer Networks}, 
title={How to provision and manage off-the-shelf SIP phones in domestic and SOHO environments}, 
year={2011}, 
volume={}, 
number={}, 
pages={42-49}, 
abstract={Integrated services delivered over broadband connections are becoming the norm in domestic households, as it is the case with triple-play bundles which offer combined Voice, Television and Data services delivered using IP-based technologies and protocols. As a result, the usage of SIP-based (Session Initiation Protocol) VoIP devices has known a significant growth in domestic environments, either in the form of standalone (e.g. SIP telephones) or embedded devices (as it happens with some domestic gateways, which embed analog-to-SIP adaptors). For Internet Service Providers (ISPs), the provisioning and management of those devices is a challenge - especially standalone SIP phones, since most of them were exclusively designed for corporate LAN usage, not supporting adequate mechanisms for remote management over broadband access networks. In this paper we propose a framework which allows the integration of off-the-shelf SIP phones with the CWMP protocol suite, the prevailing standard for remote management of Customer Premises Devices (CPEs) in broadband access networks. This integration framework supports the vast majority of commercially available SIP phones whilst maintaining full compatibility with the original CWMP specification - thus allowing ISPs to reuse their CWMP management infrastructure to configure and provision off-the-shelf SIP telephones.}, 
keywords={broadband networks;Internet telephony;local area networks;protocols;telecommunication network management;session initiation protocol phones;domestic environments;SOHO environments;integrated services;domestic households;triple-play bundles;voice services;television services;data services;IP-based technologies;protocols;VoIP devices;Internet Service Providers;ISP;LAN;remote management over broadband access networks;CWMP protocol suite;customer premises devices remote management;Data models;Logic gates;Servers;Protocols;Broadband communication;Local area networks;Runtime;CWMP;VoIP;SIP;Home Networks}, 
doi={10.1109/LCN.2011.6115389}, 
ISSN={0742-1303}, 
month={Oct},}
@ARTICLE{1638205, 
author={}, 
journal={IEEE Std 1100-2005 (Revision of IEEE Std 1100-1999)}, 
title={IEEE Recommended Practice for Powering and Grounding Electronic Equipment}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-703}, 
abstract={The IEEE Emerald Book(TM) presents a collection of consensus best practices for thepowering and grounding of electronic equipment used in commercial and industrial applications.The main objective is to provide consensus recommended practices in an area where conflictinginformation and conflicting design philosophies have dominated. The recommended practicesdescribed are intended to enhance equipment performance while maintaining a safe installation. Adescription of the nature and origin of power disturbances is provided, followed by theory on thevarious parameters that impact power quality. Information on quantifying and resolving power andgrounding related concerns using measurement and diagnostic instrumentation and standardizedinvestigative procedures are included. Recommended power protection equipment and wiring andgrounding system design practices are presented. Information on telecommunications systempower protection as well as grounding, industrial system grounding, and noise control is included.Finally a selection of case studies are presented to support the recommended practices presentedthroughout the book.}, 
keywords={IEEE Standards;Commercialization;Electronic equipment;Industry applications;Power conditioning;Power system quality;Monitoring;Grounding;commercial applications;electrical power;electronic equipment;grounding;industrialapplications;power conditioning;power disturbance;power monitor;power quality}, 
doi={10.1109/IEEESTD.2006.216391}, 
ISSN={}, 
month={May},}
@ARTICLE{7891866, 
author={}, 
journal={ISO/IEC/IEEE P24765/D3:2017}, 
title={ISO/IEC/IEEE Approved Draft International Standard - Systems and Software Engineering - Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-570}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;IEC Standards;ISO Standards;Terminology;Dictionaries;Software engineering}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{4400351, 
author={L. Lengyel and T. Levendovszky and G. Mezei and T. Vajk and H. Charaf}, 
booktitle={EUROCON 2007 - The International Conference on "Computer as a Tool"}, 
title={Practical Uses of Validated Model Transformation}, 
year={2007}, 
volume={}, 
number={}, 
pages={2200-2207}, 
abstract={Model-based approaches in development are widely recognized as a potential way of increasing productivity in software engineering. Model-based development is driven by model transformations that attempt to bridge the large semantic gaps between high-level models and low-level languages. There is a demand for researching the ways in which model transformation can become more flexible, efficient, highly-configurable as well as validated. This paper addresses issues of visually defined metamodel-based model transformations that support validated model transformations. We introduce our model transformation framework, visual modeling and transformation system (VMTS), and a list of applications realized with VMTS on metamodel-based model transformation basis. Furthermore, a comprehensive comparison is given related to other model transformation approaches.}, 
keywords={data models;meta data;software engineering;validated model transformation;metamodel-based model transformations;visual modeling and transformation system;VMTS;Algorithm design and analysis;Application software;Programming;Microwave integrated circuits;Productivity;Software engineering;System analysis and design;Automation;Informatics;Electronic mail;metamodel-based model transformation;graph rewriting;validated model transformation}, 
doi={10.1109/EURCON.2007.4400351}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{232773, 
author={E. Simeu and A. Puissochet and J. L. Rainard and A. M. Tagant and M. Poize}, 
booktitle={Digest of Papers. 1992 IEEE VLSI Test Symposium}, 
title={A new tool for random testability evaluation using simulation and formal proof}, 
year={1992}, 
volume={}, 
number={}, 
pages={321-326}, 
abstract={A set of tools is described, allowing one to compute random testability measurement for combinational circuits, based on a black box worst case hypothesis. These tools provide enough information to allow circuit modification, in order to meet a prescribed testability value. The efficiency of these tools is due to the use of a statistical method combined with formal proof mechanisms. The random testability of the complete ISCAS benchmark of combinational circuits is computed. For the least testable circuits, a few modifications, guided by the testability measurements, are shown to be sufficient to make them randomly testable.<<ETX>>}, 
keywords={combinatorial circuits;logic testing;simulation;statistical analysis;random testability evaluation;simulation;combinational circuits;statistical method;ISCAS benchmark;Circuit testing;Circuit faults;Computational modeling;Circuit simulation;Switching circuits;Combinational circuits;Benchmark testing;Built-in self-test;Telecommunication computing;Semiconductor device modeling}, 
doi={10.1109/VTEST.1992.232773}, 
ISSN={}, 
month={April},}
@ARTICLE{7140715, 
author={}, 
journal={IEEE Std 2030.2-2015}, 
title={IEEE Guide for the Interoperability of Energy Storage Systems Integrated with the Electric Power Infrastructure}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-138}, 
abstract={This guide applies the smart grid interoperability reference model (SGIRM) process (IEEE Std 2030-2011) to energy storage by highlighting the information relevant to energy=storage system (ESS) interoperability with the energy power system (EPS). The process can be applied to ESS applications located on customer premises, at the distribution level, and on the transmission level (i.e., bulk storage). This guide provides useful industry-derived definitions for ESS characteristics, applications, and terminology that, in turn, simplify the task of defining system information and communications technology (ICT) requirements. As a result. these requirements can be communicated more clearly and consistently in project specifications. This guide also presents a methodology that can be used for most common ESS projects to describe the power system, communications, and information technology (IT) perspectives based on the IEEE 2030 definitions. From this framework, a seemingly complex system can be more clearly understood by all project stakeholders. Emerging cybersecurity requirements can also be incorporated into the framework as appropriate. Additionally, this guide provides the templates that can be used to develop requirements for an ESS project and goes through several real-world ESS project examples step by step.}, 
keywords={energy storage;IEEE standards;inductive power transmission;smart power grids;IEEE guide;energy storage system interoperability;electric power infrastructure;smart grid interoperability reference model;SGIRM process;IEEE Std 2030-2011;ESS interoperability;energy power system;EPS;distribution level;transmission level;system information and communications technology;ICT;cybersecurity requirements;IEEE Std 2030.2-2015;IEEE Standards;Energy storage;Batteries;Smart grids;Electric power systems;Power system reliability;battery;communications technology;electric power system;energy storage system;IEEE 2030.2(TM);information technology;interoperability;power system;Smart Grid}, 
doi={10.1109/IEEESTD.2015.7140715}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7962325, 
author={O. Liechti and J. Pasquier and R. Reis}, 
booktitle={2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST)}, 
title={Supporting Agile Teams with a Test Analytics Platform: A Case Study}, 
year={2017}, 
volume={}, 
number={}, 
pages={9-15}, 
abstract={Continuous improvement, feedback mechanisms and automated testing are cornerstones of agile methods. We introduce the concept of test analytics, which brings these three practices together. We illustrate the concept with an industrial case study and describe the experiments run by a team who had set a goal for itself to get better at testing. Beyond technical aspects, we explain how these experiments have changed the mindset and the behaviour of the team members. We then present an open source test analytics platform, later developed to share the positive learnings with the community. We describe the platform features and architecture and explain how it can be easily put to use. Before the conclusions, we explain how test analytics fits in the broader context of software analytics and present our ideas for future work.}, 
keywords={feedback;program diagnostics;program testing;public domain software;software prototyping;agile teams;feedback mechanisms;automated testing;team member behaviour;open source test analytics platform;software analytics;Testing;Software;Companies;Context;Collaboration;agile development;automated testing;gamification;feedback channels}, 
doi={10.1109/AST.2017.3}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1317496, 
author={D. Batory}, 
booktitle={Proceedings. 26th International Conference on Software Engineering}, 
title={Feature-oriented programming and the AHEAD tool suite}, 
year={2004}, 
volume={}, 
number={}, 
pages={702-703}, 
abstract={Feature oriented programming (FOP) is an emerging paradigm for application synthesis, analysis, and optimization. A target application is specified declaratively as a set of features, like many consumer products (e.g., personal computers, automobiles). FOP technology translates such declarative specifications into efficient programs.}, 
keywords={software tools;object-oriented programming;feature-oriented programming;AHEAD tool suite;application synthesis;application analysis;application optimization;target application;FOP technology;declarative specifications;Automatic programming;Algebra;Application software;Java;Software engineering;Design optimization;Query processing;Large-scale systems;Domain specific languages;Prototypes}, 
doi={10.1109/ICSE.2004.1317496}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{685775, 
author={}, 
booktitle={Proceedings. Fifth International Conference on Software Reuse (Cat. No.98TB100203)}, 
title={Subject index}, 
year={1998}, 
volume={}, 
number={}, 
pages={377-388}, 
abstract={The index contains an entry for all items that appeared in this publication.}, 
keywords={}, 
doi={10.1109/ICSR.1998.685775}, 
ISSN={1085-9098}, 
month={June},}
@INPROCEEDINGS{6821183, 
author={S. Meyer and P. Healy and T. Lynn and J. Morrison}, 
booktitle={2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing}, 
title={Quality Assurance for Open Source Software Configuration Management}, 
year={2013}, 
volume={}, 
number={}, 
pages={454-461}, 
abstract={Commonly used open source configuration management systems, such as Puppet, Chef and CFEngine, allow for system configurations to be expressed as scripts. A number of quality issues that may arise when executing these scripts are identified. An automated quality assurance service is proposed that identifies the presence of these issues by automatically executing scripts across a range of environments. Test results are automatically published to a format capable of being consumed by script catalogues and social coding sites. This would serve as an independent signal of script trustworthiness and quality to script consumers and would allow developers to be made quickly aware of quality issues. As a result, potential consumers of scripts can be assured that a script is likely to work when applied to their particular environment. Script developers can be notified of compatibility issues and take steps to address them.}, 
keywords={program compilers;public domain software;software quality;automated quality assurance service;social coding sites;script catalogues;script trustworthiness;independent signal;script consumers;system configurations;open source software configuration management;Servers;Operating systems;Communities;Linux;Quality assurance;Testing;Automated configuration;configuration management;continuous integration;automated deployment;service orchestration;assurance}, 
doi={10.1109/SYNASC.2013.66}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6616325, 
author={P. Zech and M. Felderer and M. Farwick and R. Breu}, 
booktitle={2013 IEEE Seventh International Conference on Software Security and Reliability Companion}, 
title={A Concept for Language-Oriented Security Testing}, 
year={2013}, 
volume={}, 
number={}, 
pages={53-62}, 
abstract={Today's ongoing trend towards intense usage of web service based applications in daily business and everybody's daily life poses new challenges for security testing. Additionally, such applications mostly not execute in their own runtime environment but instead are deployed in some data center, run alongside multiple other applications, and serve different purposes for sundry user domains with diverging security requirements. As a consequence, security testing also has to adapt to be able to meet the necessary requirements for each application in its domain and its specific security requirements. In addition, security testing needs to be feasible for both service providers and consumers. In our paper we identify drawbacks of existing security testing approaches and provide directions for meeting emerging challenges in future security testing approaches. We also introduce and describe the idea of language-oriented security testing, a novel testing approach building upon domain-specific languages and domain knowledge to meet future requirements in security testing.}, 
keywords={high level languages;program testing;security of data;Web services;language-oriented security testing;Web service based applications;domain-specific languages;domain knowledge;software testing;Testing;Security;Business;Cloud computing;Automation;Security Testing;Domainspecific Language;Languageoriented Programming;Servicecentric Systems}, 
doi={10.1109/SERE-C.2013.16}, 
ISSN={}, 
month={June},}
@ARTICLE{8011815, 
author={M. Bernardino and E. M. Rodrigues and A. F. Zorzo and L. Marchezan}, 
journal={IET Software}, 
title={Systematic mapping study on MBT: tools and models}, 
year={2017}, 
volume={11}, 
number={4}, 
pages={141-155}, 
abstract={Every year several contributions to the model-based testing (MBT) field are published. Therefore, to follow the evolution and trends of several tools and models available is difficult. Moreover, since the variety of models and tools that became available in recent years, choosing an approach to support the MBT process is a challenging activity. The main objective of this study is to provide an overview on MBT tools and models used by those tools. Furthermore, the authors' study can help academic researchers and companies to understand the topics involving MBT. Therefore, a systematic mapping study was conducted in which 1197 distinct papers were evaluated. At the end, 87 primary studies were selected to be analysed in a quantitative and qualitative way. As a result, they classified the tools and models that are currently used to support MBT. Moreover, they identified 70 MBT tools, as well as different domains in which MBT is already applied to. Therefore, there are some evidence that MBT continues to be a broad and `alive' research field since every year a significant number of papers presenting different kinds of contributions are published.}, 
keywords={program testing;software engineering;systematic mapping;model-based testing;MBT process;software modelling;software development}, 
doi={10.1049/iet-sen.2015.0154}, 
ISSN={1751-8806}, 
month={},}
@INPROCEEDINGS{7140503, 
author={M. de Bayser and L. G. Azevedo and R. Cerqueira}, 
booktitle={2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
title={ResearchOps: The case for DevOps in scientific applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={1398-1404}, 
abstract={DevOps (a portmanteau of development and operations) is a software development method that extends the agile philosophy to rapidly produce software products and services and to improve operations performance and quality assurance. It was born to accelerate the delivery of Web-based systems and quickly bring new value to users. Many Web-based systems evolve according to usage trends without a clear long-term goal. Before the widespread use of Web services, most software with a clear goal were delivered as packages that users installed on their own system. New versions were delivered with a much lower frequency, with periods in between versions ranging from months to years. Development cycles were divided into large design, coding and testing phases culminating in the release of a new stable version. In software development in the context of applied science, even when the goal is clear, the process to attain it is not. Hence, working releases that capture the current software state must be released frequently in order to reduce the risks for all stakeholders and to make it possible to assess the current state of a project and steer it in the right direction. This paper explores the usefulness of DevOps concepts to improve the development of software that supports scientific projects. We establish the similarities and differences between scientific projects and Web applications development, and discuss where the related methodologies need to be extended. Unique challenges are discussed herewith developed solutions, and still open questions. Lessons learned are highlighted as best practices to be followed in research projects. This discussion is rooted in our experience in real-life projects at the IBM Research Brazil Lab, which just as well apply to other research institutions.}, 
keywords={program testing;project management;research and development;scientific information systems;software development management;software product lines;software prototyping;software quality;Web services;ResearchOps;DevOps;scientific applications;software development method;software products;software services;operation performance improvement;quality assurance improvement;agile software;Web-based system delivery;Web services;software packages;software development cycles;software design phase;software coding phase;software testing phase;risks reduction;project state assessent;research projects;IBM Research Brazil Lab;Software;Testing;Prototypes;Conferences;Libraries;Servers;Production}, 
doi={10.1109/INM.2015.7140503}, 
ISSN={1573-0077}, 
month={May},}
@ARTICLE{8405633, 
author={V. Garousi and M. Felderer and . M. Karapiak and U. Yilmaz}, 
journal={IEEE Software}, 
title={What We Know about Testing Embedded Software}, 
year={2018}, 
volume={35}, 
number={4}, 
pages={62-69}, 
abstract={To cost-effectively test embedded software, practitioners and researchers have proposed many test techniques, approaches, tools, and frameworks. However, obtaining an overview of the state of the art and state of the practice in this area is challenging for practitioners or new researchers. In addition, owing to an inadequate overview of what already exists in this area, some companies often reinvent the wheel by designing a test approach that's new to them but already exists. To address these problems, the authors conducted a systematic literature review of this area that covered the testing topics, testing activities, test artifacts, and industries on which the studies focused. The results can benefit both practitioners and researchers by serving as an index to the vast body of knowledge in this important, fast-growing area.}, 
keywords={embedded systems;program testing;test techniques;test approach;testing topics;test artifacts;embedded software testing;testing activities;Testing;Unified modeling language;Automation;Automotive engineering;Embedded software;software testing;embedded systems;embedded software;systematic literature mapping;systematic literature review;software engineering;software development}, 
doi={10.1109/MS.2018.2801541}, 
ISSN={0740-7459}, 
month={July},}
@INPROCEEDINGS{687918, 
author={A. S. Aljabri and D. E. Bernard and D. L. Dvorak and G. K. Man and B. Pell and T. W. Starbird}, 
booktitle={1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)}, 
title={Infusion of autonomy technology into space missions: DS1 lessons learned}, 
year={1998}, 
volume={2}, 
number={}, 
pages={315-329 vol.2}, 
abstract={The impact of infusing breakthrough autonomy technology into a flight project was a big surprise. Valuable technical and cultural lessons, many of general applicability when introducing system-level autonomy, have been learned by infusing the Remote Agent (RA) into NASA's Deep Space 1 (DS1) spacecraft. The RA's architecture embodies system-level autonomy in three major components: planning and scheduling, execution, and fault diagnosis and reconfiguration. Lessons learned include: the architecture was confirmed; active participation by nonautonomy personnel in the development is essential; communication of new concepts is essential, difficult, and hampered by differences in terminology; giving a spacecraft system-level autonomy changes organizational roles in operating the spacecraft after launch, and hence changes roles during development; software models supporting functions traditionally handled on the ground must be developed early enough to get on-board; shortfalls in planned features must be technically and developmentally accomodatable, in particular not to threaten the launch schedule; traditional commanding must be supported; testing must be emphasized. These lessons and others, on incremental system releases and use of autocode generation, are based on 16 months of spiral development from start of project through the project's decision to reduce the role of the RA from full-time control of the spacecraft to a separable experiment.}, 
keywords={space vehicles;aerospace control;software agents;scheduling;fault diagnosis;autonomy technology;space missions;DS1;Remote Agent;NASA;Deep Space 1;system-level autonomy;planning;scheduling;fault diagnosis;spacecraft system-level autonomy;software models;launch schedule;incremental system releases;autocode generation;full-time control;Space technology;Space missions;Space vehicles;Scheduling;Cultural differences;Fault diagnosis;Computer architecture;Personnel;Terminology;Communication system software}, 
doi={10.1109/AERO.1998.687918}, 
ISSN={1095-323X}, 
month={March},}
@INPROCEEDINGS{8432020, 
author={B. Eberhardinger and H. Ponsar and G. Siegert and W. Reif}, 
booktitle={2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={Case Study: Adaptive Test Automation for Testing an Adaptive Hadoop Resource Manager}, 
year={2018}, 
volume={}, 
number={}, 
pages={513-518}, 
abstract={Coping with adaptive software systems is one of the key challenges testing is currently faced with. In our previous work, we proposed to enable the test system itself to be adaptive to the system under test as a solution. The adaptation is built up on the concepts of a self-aware test automation enabling to use this information to sequence, instantiate, or update the test suite to the current situation. In our test framework the modeling language S# allows to use a run-time model to do so in a model-based testing approach. In this paper, we demonstrate how our concepts of adaptive, self-aware test automation are applied to a real world scenario: testing an adaptive resource manager of Hadoop. We show the steps necessary to implement the approach and discuss our experiences in this case study paper.}, 
keywords={data handling;parallel processing;program testing;adaptive Hadoop resource manager;adaptive software systems;self-aware test automation;test framework;run-time model;adaptive test automation;Adaptation models;Automation;Adaptive systems;Testing;Software systems;Yarn;Task analysis;Software Testing;Hadoop;Test Automation;Adaptive Testing;Adaptive Systems}, 
doi={10.1109/QRS-C.2018.00092}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7018469, 
author={O. Badreddin and A. Forward and T. C. Lethbridge}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={A test-driven approach for developing software languages}, 
year={2014}, 
volume={}, 
number={}, 
pages={225-234}, 
abstract={Test-Driven Development (TDD) is the practice of attempting to use the software you intend to write, before you write it. The premise is straightforward, but the specifics of applying it in different domains can be complex. In this paper, we provide aTDD approach for language development. The essence is to apply TDD at each of four levels of language processing, hence we call our approach Multi-Level TDD, or MLTDD. MLTDD can be applied to programming languages, preprocessors, domain specific languages, and transformation engines. MLTDD was used to build Umple, a model-oriented programming language available for Java, Ruby, and PHP. We present two case studies where this approach was implemented to develop two other domain specific languages.}, 
keywords={Testing;Syntactics;Semantics;Java;Generators;Unified modeling language;Software;Test Driven Development;Model Oriented Programming Language;UML}, 
doi={}, 
ISSN={}, 
month={Jan},}
@ARTICLE{7839172, 
author={}, 
journal={ISO/IEC/IEEE FDIS P24765:2016(E), January 2017}, 
title={ISO/IEC/IEEE Draft Systems and Software Engineering - Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-568}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;IEC Standards;ISO Standards;Dictionaries;Terminology;Software engineering}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{4730478, 
author={S. Wieczorek and A. Roth and A. Stefanescu and A. Charfi}, 
booktitle={2008 IEEE International Symposium on Service-Oriented System Engineering}, 
title={Precise Steps for Choreography Modeling for SOA Validation and Verification}, 
year={2008}, 
volume={}, 
number={}, 
pages={148-153}, 
abstract={Service-oriented architecture (SOA) enables organizations to transform their existing IT infrastructure into a more flexible business process platform. In this architecture, decoupled components that provide standard services can be composed to form individually configured and highly flexible applications. When building such applications it is important to have a formal specification of the interaction protocols between the composed services not only because such a specification provides an accurate and unambiguous description of the interactions and their ordering but also to enable automated verification and validation. In this paper, we present a case study from the SAP context showing the interactions between two SAP service components and use that case study to derive a set of modeling requirements. This motivates a discussion about applicable techniques for service choreography modeling and whether existing choreography languages cover the identified needs.}, 
keywords={program verification;software architecture;choreography modeling;SOA validation;SOA verification;service-oriented architecture;Semiconductor optical amplifiers;Service oriented architecture;Enterprise resource planning;Marketing and sales;Software systems;Systems engineering and theory;Buildings;Formal specifications;Protocols;Context-aware services;SOA;Choreography;Service;Modeling}, 
doi={10.1109/SOSE.2008.43}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8116506, 
author={I. Jimenez and S. Hamedian and J. Lofstead and C. Maltzahn and K. Mohror and R. Arpaci-Dusseau and A. Arpaci-Dusseau and R. Ricci}, 
booktitle={2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
title={Demo abstract: PopperCI: Automated reproducibility validation}, 
year={2017}, 
volume={}, 
number={}, 
pages={952-953}, 
abstract={In this demo we illustrate the usage of PopperCI [1], a continous integration (CI) service for experiments hosted at UC Santa Cruz that allows researchers to automate the end-to-end execution and validation of experiments. PopperCI assumes that experiments follow Popper [2], a convention for implementing experiments and writing articles following a DevOps approach that has been proposed recently.}, 
keywords={cloud computing;software engineering;end-to-end execution;continous integration service;automated reproducibility validation;DevOps approach;PopperCI;Tools;Measurement;Runtime;Software;Conferences;Laboratories;Guidelines}, 
doi={10.1109/INFCOMW.2017.8116506}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7363910, 
author={R. Richardet and J. Chappelier and S. Tripathy and S. Hill}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={Agile text mining with Sherlok}, 
year={2015}, 
volume={}, 
number={}, 
pages={1479-1484}, 
abstract={The successful development of an intelligent text mining application requires the collaboration of two main stakeholders: subject matter experts and text miners. In this paper, we describe a new methodology, agile text mining to improve that collaboration. Agile text mining is characterized by short development cycles, frequent tasks redefinition and continuous performance monitoring through integration tests. We introduce Sherlok, a system supporting the development of agile text mining applications and present an application to extract mention of neurons from a very large corpus of scientific articles. The resulting code and models are publicly available.}, 
keywords={data integration;data mining;scientific information systems;text analysis;Sherlok;intelligent text mining application;subject matter experts;text miners;agile text mining;frequent tasks redefinition;continuous performance monitoring;integration tests;agile text mining applications;scientific articles;Text mining;Pipelines;Ontologies;Engines;Proteins;Collaboration;natural language processing;text mining;big data;UIMA;agile data science}, 
doi={10.1109/BigData.2015.7363910}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{4031279, 
author={B. Langlois and D. Exertier and S. Bonnet}, 
booktitle={2006 10th IEEE International Enterprise Distributed Object Computing Conference Workshops (EDOCW'06)}, 
title={Performance Improvement of MDD Tools}, 
year={2006}, 
volume={}, 
number={}, 
pages={19-19}, 
abstract={From first to mature versions of Model-Driven Development (MDD) tools, there is a gap, as for any other software applications. All functional requirements must be met, including qualities of services, at the risk of seeing MDD tools rejected by users. In this paper, we focus on performance, especially for large-scale developments. After an overview of methodological elements, we give a list of reusable practices on performance. We conclude by a set of observations and stakes in order to understand where efforts must be applied during the development process.}, 
keywords={Model driven engineering;Software tools;Scalability;Unified modeling language;Aircraft;Bridges;Software performance;Application software;Quality of service;Large-scale systems}, 
doi={10.1109/EDOCW.2006.54}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4152660, 
author={}, 
journal={IEEE Unapproved Std P11073-00101/D02J, Feb 2007}, 
title={Unapproved IEEE Draft Guide for Health Informatics-Point-Of-Care Medical Device Communication-Technical Report-Guidelines for the Use of RF Wireless Technology}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{5655567, 
author={S. Wieczorek and A. Stefanescu and A. Roth}, 
booktitle={2010 Seventh International Conference on the Quality of Information and Communications Technology}, 
title={Model-Driven Service Integration Testing - A Case Study}, 
year={2010}, 
volume={}, 
number={}, 
pages={292-297}, 
abstract={This paper presents a case study for the modeling and model-based testing (MBT) of enterprise service choreographies. Our proposed MBT approach uses proprietary models called Message Choreography Models (MCM) as test models. The case study illustrates how MCM-based service integration testing allows to formalize design decisions and enables full integration into an existing industrial test infrastructure by using the concepts of domain specific languages and model transformations. Further, the MBT tools integrated into the testing framework have been compared based on one concrete use case.}, 
keywords={formal specification;program testing;model-driven service integration testing;model-based testing;enterprise service choreographies;message choreography models;industrial test infrastructure;domain specific languages;model transformations;testing framework;Testing;Business;Service oriented architecture;Generators;Unified modeling language;Data models;Context;Model-based Testing;Enterprise Systems;Service-oriented Architecture;Case Study;Service Choreographies}, 
doi={10.1109/QUATIC.2010.49}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6151484, 
author={A. Jabeen and S. Tariq and Q. Farooq and Z. I. Malik}, 
booktitle={2011 IEEE 14th International Multitopic Conference}, 
title={A lightweight aspect modelling approach for BPMN}, 
year={2011}, 
volume={}, 
number={}, 
pages={255-260}, 
abstract={Aspectual Business Process Modelling is not a new concept in business process based development to support the separation of the cross cutting concerns. Most of the researchers use the concept of the heavyweight extensions of the business processes to incorporate the aspects. This requires changes in the meta-models of the languages and the tool infrastructures, which is not a feasible option. Some of the researchers also provide lightweight extensions in the form of profiles, but these are mostly incomplete and do not provide solutions for modelling some important aspectual concepts like Pointcuts effectively. To overcome this issue, in this paper, we provide a lightweight extension of the business processes expressed in BPMN to incorporate the aspect specific concepts in it. We propose a profile ABPMN which uses the existing notations of the BPMN models for expressing aspectual concepts. Further, we developed a language PCDL to express the pointcuts in an effective way. The language is implemented using XText in Eclipse. To evaluate the applicability of our approach, we applied it on a case study of the E-Bidding system.}, 
keywords={business data processing;corporate modelling;simulation languages;lightweight aspect modelling approach;BPMN;aspectual business process modelling;cross cutting concerns;tool infrastructures;languages meta models;Pointcuts;ABPMN;PCDL;XText;Eclipse;e-bidding system;Weaving;Aspect-oriented Modelling;BPMN;Xtext;Metamodel;ABPMN}, 
doi={10.1109/INMIC.2011.6151484}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7943664, 
author={D. Bhatt and B. Hall and A. Murugesan and D. Oglesby and E. Bush and E. Engstrom and J. Mueller and M. Pelican}, 
booktitle={2017 IEEE Aerospace Conference}, 
title={Opportunities and challenges for formal methods tools in the certification of avionics software}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-20}, 
abstract={Formal methods tools, whose underlying principles are based on mathematics and formal logic, are considered one of the most effective and rigorous means of verifying system properties and assuring the absence of undesirable system behavior. The use of such tools seem to squarely fit the needs of those aiming to develop and certify avionic software as per the DO-178C standard, the set of objectives laid out by FAA to achieve a high level of confidence on the systems. However, our recent work on a NASA-funded research project revealed that there are practical considerations and additional complexities involved in using formal method tools to provide the level of assurance as exemplified by the DO-178C. In this paper we discuss one of the key concerns with formal tools: its soundness - the characteristic of a tool to never permit the verified system property be declared true when it is actually not true. We explored two major classes of formal methods tools - namely model checkers and static analyzers - and observed several threats to their soundness such as tool fallacies and failure modes that could lead to misplaced confidence in the verified system. We present various strategies to mitigate them, including an assurance case framework to verify that potential risks are all mitigated. The intent of this paper is not to discourage but encourage scrupulous use of formal tools to certify critical avionic software by being wary of the subtle but serious issues that may be overlooked.}, 
keywords={avionics;certification;formal verification;program diagnostics;safety-critical software;formal methods tools;avionics software;formal logic;static analyzers;model checkers;DO-178C standard;Tools;Software;Analytical models;Model checking;Aerospace electronics;Safety;Algorithm design and analysis}, 
doi={10.1109/AERO.2017.7943664}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6037562, 
author={M. K. Agaram and C. Liu}, 
booktitle={2011 IEEE 15th International Enterprise Distributed Object Computing Conference}, 
title={An Engine-Independent Framework for Business Rules Development}, 
year={2011}, 
volume={}, 
number={}, 
pages={75-84}, 
abstract={There is a compelling need for highly customized Domain Specific Languages and Business Vocabulary in certain industries such as insurance, mortgage, and finance to enable Knowledge Workers to articulate and to automate complex rules pertinent to their areas of function within their companies. Rule Engine vendors attempt to provide a solution to the problem by selling an integrated Rules Engine and Business Rules Management System. Usually, the BRMS's provided by vendors need to be customized and integrated into the overall Enterprise Architecture. This results in the Enterprise Architecture to be tightly coupled with the vendor's rule offering. Moreover, it poses a significant risk to the Enterprise as vendor solutions change between releases. The Enterprise Architecture needs a way to insulate itself from such impacts. This paper describes a framework that delivers the core BRMS functions of authoring and representation in a vendor neutral fashion. In addition, the paper sheds light on specific areas of the framework that can be standardized.}, 
keywords={business data processing;insurance data processing;knowledge based systems;mortgage processing;specification languages;vocabulary;engine-independent framework;business rules development;domain specific languages;business vocabulary;insurance;mortgage;finance;rules engine;business rules management system;enterprise architecture;BRMS functions;Vocabulary;Business;Engines;Production;Dentistry;Computer architecture;Syntactics;Business Rules;Business Rules Languages;Business Rule Components;Business Vocabulary;Domain Specific Languages}, 
doi={10.1109/EDOC.2011.20}, 
ISSN={1541-7719}, 
month={Aug},}
@INPROCEEDINGS{7329720, 
author={M. Sroka and R. Nagy and D. Fisch}, 
booktitle={2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)}, 
title={Impact of mutation intensity on evolutionary test model learning}, 
year={2015}, 
volume={}, 
number={}, 
pages={271-276}, 
abstract={Automation in the software testing process has significant impact on the overall software development in industry. The focus of this paper is on automation of test case design via model-based testing for automotive embedded software. A new method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information was designed and this paper investigates the impact of mutation intensity on the evolutionary learning process.}, 
keywords={automobiles;evolutionary computation;program testing;software engineering;traffic engineering computing;mutation intensity;evolutionary test model learning;software testing process;software development;automotive embedded software;evolutionary algorithm;evolutionary learning process;Biological cells;Sociology;Statistics;Software;Testing;Evolutionary computation;Software algorithms}, 
doi={10.1109/INES.2015.7329720}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{738516, 
author={K. H. Bennett}, 
booktitle={Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272)}, 
title={Do program transformations help reverse engineering?}, 
year={1998}, 
volume={}, 
number={}, 
pages={247-254}, 
abstract={Program transformations have been advocated as a method for accomplishing reverse engineering. The hypothesis is that the original source code can be progressively transformed into alternative forms, but with the same semantics. At the end of the process, an equivalent program is acquired, but one which is much easier to understand and more maintainable. We have been undertaking an extensive programme of research over twelve years into the design and development of transformations for the support of software maintenance. The paper very briefly explains the theory, practice and tool support for transformational systems, but does not present new theoretical results. The main results are on an analysis of the strengths and weaknesses of the approach, based on experience with case studies and industrial applications. The evaluation framework used (called DERE) is that presented in Bennett and Munro (1998). It is hoped that the results will be of benefit to industry, who might be considering using the technology; and to other researchers, interested in addressing the open problems. The overall conclusion is that transformations can help in the bottom-up analysis and manipulation of source code at approximately the 3GL level, and have proved successful in code migration, but need to be complemented by other top-down techniques to be useful at higher levels of abstraction or in more ambitious re-engineering projects.}, 
keywords={reverse engineering;software maintenance;reverse engineering;program transformations;software maintenance;evaluation framework;DERE;3GL level;top-down techniques;Reverse engineering;Libraries;Computer science;Electronic mail;Electrical capacitance tomography;Computer languages;Read only memory}, 
doi={10.1109/ICSM.1998.738516}, 
ISSN={1063-6773}, 
month={Nov},}
@INPROCEEDINGS{7813841, 
author={S. Kumar and Rajkumar}, 
booktitle={2016 International Conference on Computing, Communication and Automation (ICCCA)}, 
title={Test case prioritization techniques for software product line: A survey}, 
year={2016}, 
volume={}, 
number={}, 
pages={884-889}, 
abstract={Software product line (SPL) testing is a tougher work than testing of single systems. Still testing of each individual SPL product would be perfect but it is too costly in practice. In fact, when the number of features increases then the number of possible products also increases exponentially usually derived from a feature model. Number of features is leading to thousands of different products. Due to cost and time constraints, it is infeasible or large number of effort to run all the test cases in an existing test suite. To decrease the cost of testing, various techniques have been proposed. One of them is test case prioritization (TCP) techniques. Here we presented a survey for TCP techniques for software SPL.}, 
keywords={program testing;software product lines;software product line;SPL product;feature model;test suite;test case prioritization;TCP;Software;Software product lines;Testing;Frequency modulation;Fault detection;Automation;Libraries;Software product lines;Test Case Prioritization;Variability;Commonality;Feature Model}, 
doi={10.1109/CCAA.2016.7813841}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6209957, 
author={T. Kanstrn and O. Puolitaival and V. Rytky and A. Saarela and J. S. Kernen}, 
booktitle={2012 IEEE International Conference on Industrial Technology}, 
title={Experiences in setting up domain-specific model-based testing}, 
year={2012}, 
volume={}, 
number={}, 
pages={319-324}, 
abstract={Model-based testing is a technique for generating test cases based on a model of the system under test. Typically the model is expressed in a specific notation of the test tool, using a generic notation intended to describe any system under test. In this paper we present experiences in using a domain-specific modeling layer on top of the specific model-based testing tools. This allows for easier change of the used testing tool, while providing a more familiar modeling notation in terms of the domain concepts familiar to the user. Our experiences show how this can significantly help in adopting the model-based testing approach and provide improved test results.}, 
keywords={formal specification;program testing;software tools;domain-specific model-based testing;test case generation;system under test;test tool;generic notation;domain-specific modeling layer;specific model-based testing tools;modeling notation;Generators;Encoding;Testing}, 
doi={10.1109/ICIT.2012.6209957}, 
ISSN={}, 
month={March},}
@ARTICLE{7389278, 
author={A. Bellucci and M. Romano and I. Aedo and P. Daz}, 
journal={IEEE Pervasive Computing}, 
title={Software Support for Multitouch Interaction: The End-User Programming Perspective}, 
year={2016}, 
volume={15}, 
number={1}, 
pages={78-86}, 
abstract={The hardware development of the past years favored the widespread diffusion of multitouch devices (such as smartphones, tablets, and interactive tabletops) to such an extent that a wide variety of users are now exploiting them to perform different activities on a daily basis. In the heterogeneous and manifold context of modern computation, it is impossible to predict, at design time, all the possible configurations of such technologies, and especially the way users will be willing to interact with them. Therefore, empowering end users with tools for developing multitouch interaction is a promising step toward the materialization of ubiquitous computing. The aim of this survey is to frame the state of the art of existing multitouch software development tools from an end-user programming (EUP) perspective.}, 
keywords={haptic interfaces;software tools;ubiquitous computing;EUP perspective;multitouch software development tools;ubiquitous computing;materialization;daily basis;multitouch devices;hardware development;end-user programming perspective;multitouch interaction;software support;Software tools;Programming profession;Graphics;Complexity theory;Sensors;coding tools and techniques;ubiquitous computing;end-user programming;pervasive computing;software engineering;graphics;mobile}, 
doi={10.1109/MPRV.2016.3}, 
ISSN={1536-1268}, 
month={Jan},}
@INPROCEEDINGS{6135869, 
author={D. F. Ferguson and E. Hadar}, 
booktitle={2011 8th International Conference Expo on Emerging Technologies for a Smarter World}, 
title={Optimizing the IT business supply chain utilizing cloud computing}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Information technology applications and systems are essential to businesses and enterprises as they implement business components of the enterprise. In some cases, IT is the business, such as with financial services. Optimizing Return-on-Investment (ROI) in the IT area is essential to the business performance. Reducing cost is one component of ROI, however the predominant value is increasing revenue. IT is essential to the enterprise agilely to exploit new business opportunities. Cloud computing is emerging as a technology for optimizing IT costs and supporting agility. Enterprises are incrementally moving to cloud computing in an exploratory, ad hoc manner. Since, enterprises think in terms of IT Services that IT provides to the business, and an IT service is interconnecting hardware and software resources, the management aspects are conceptually similar to a manufacture or retail supply chain. As a result, exploiting cloud computing is a supply chain management problem for IT services using cloud computing. This paper describes the architecture requirements and implementation of a set of components for optimizing the IT supply chain using cloud computing.}, 
keywords={business data processing;cloud computing;cost reduction;investment;supply chain management;IT business supply chain optimisation;cloud computing;information technology application;financial services;return-on-investment optimisation;cost reduction;supply chain management problem;IT services;architecture requirements;Business;Optimization;Servers;Monitoring;Cloud computing;Hardware;Automation;enterprise IT;cloud archtiecture;supply chain management;composite IT system}, 
doi={10.1109/CEWIT.2011.6135869}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4599483, 
author={P. King and C. Smith}, 
booktitle={Agile 2008 Conference}, 
title={Technical lessons Learned Turning the Agile Dials to Eleven!}, 
year={2008}, 
volume={}, 
number={}, 
pages={233-238}, 
abstract={This report outlines technical lessons learnt by about 20 of Australiapsilas most experienced agile specialists over several years across several projects within an organization which aggressively applied the agile practices with much success. In these projects the agile dials were cranked to eleven to achieve very high levels of quality. Most of the specialists involved believe that they produced the highest quality software of their careers with some of the highest productivity they have ever experienced.}, 
keywords={agile manufacturing;software quality;agile dials;agile practices;software quality;Production;Radiation detectors;Complexity theory;Testing;Libraries;Productivity;Writing;Agile;Coverage;Mocking;TDD;Pair-programming}, 
doi={10.1109/Agile.2008.15}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4040125, 
author={}, 
journal={IEEE Std P1175.2/D8.0}, 
title={Unapproved IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections (Superseded by P1175.2_D11.2)}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INBOOK{6354029, 
author={Andrzej Jajszczyk}, 
booktitle={A Guide to the Wireless Engineering Body of Knowledge (WEBOK)}, 
title={Fundamental Knowledge}, 
year={2012}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Introduction

Electrical and RF Engineering

Communication Engineering

Engineering Management

]]>}, 
keywords={Microwave filters;Radio frequency;RLC circuits;Filtering theory;Wireless communication;Band-pass filters;Microwave circuits}, 
doi={10.1002/9781118444221.ch7}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6354029},}
@INPROCEEDINGS{5533639, 
author={G. Piho and J. Tepandi and M. Parman and D. Perkins}, 
booktitle={The 33rd International Convention MIPRO}, 
title={From archetypes-based domain model of clinical laboratory to LIMS software}, 
year={2010}, 
volume={}, 
number={}, 
pages={1179-1184}, 
abstract={We present our approach for developing a laboratory information management system (LIMS) software by combining Bjrners software triptych methodology (from domain models via requirements to software) with Arlow and Neustadt archetypes and archetype patterns based initiative. The fundamental hypothesis is that through this Archetypes Based Development (ABD) approach to domains, requirements and software, it is possible to improve the software development process as well as to develop more dependable software. We use ADB in developing LIMS software for the Clinical and Biomedical Proteomics Group (CBPG), University of Leeds.}, 
keywords={information management;medical information systems;clinical laboratory;LIMS software;laboratory information management system;Bjorners software triptych methodology;archetypes based development;Clinical and Biomedical Proteomics Group;Leeds University;Laboratories;Abstracts;Programming;Production facilities;Software engineering;Proteomics;Information management;Software systems;Application software;Customer relationship management}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4813952, 
author={F. Borchardt}, 
booktitle={2009 IET Smart Metering - Making It Happen}, 
title={Meeting the operational and logistical challenges of smart meter roll-out - the European experience (It's not just hanging meters on walls)}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-20}, 
abstract={A collection of slides on meeting the operational and logistical challenges of smart meter roll-out by Frank Borchardt, was given.}, 
keywords={automatic meter reading;logistics;power system management;operational challenge;logistical challenge;smart meter roll out;smart metering;business process}, 
doi={}, 
ISSN={0537-9989}, 
month={Feb},}
@ARTICLE{8352077, 
author={K. Rubinov and L. Baresi}, 
journal={Computer}, 
title={What Are We Missing When Testing Our Android Apps?}, 
year={2018}, 
volume={51}, 
number={4}, 
pages={60-68}, 
abstract={Android's broad adoption drives the development of millions of new apps. Apps on this OS are not just trivial games; many of them handle sensitive information, exhibit complex structure, and require high reliability and trustworthiness. The authors discuss the problem of testing Android apps-the results achieved with current approaches, and what is still missing and requires fresh solutions.}, 
keywords={mobile computing;program testing;sensitive information;trustworthiness;reliability;Android apps testing;Android adoption;Software testing;Androids;Humanoid robots;Software development;Graphical user interfaces;Analytical models;Runtime;Computer applications;Android;mobile;software testing;mobile computing;mobile applications;debugging}, 
doi={10.1109/MC.2018.2141024}, 
ISSN={0018-9162}, 
month={April},}
@INPROCEEDINGS{6405436, 
author={J. Lauret and H. Waeselynck and J. Fabre}, 
booktitle={2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops}, 
title={Detection of Interferences in Aspect-Oriented Programs Using Executable Assertions}, 
year={2012}, 
volume={}, 
number={}, 
pages={165-170}, 
abstract={Aspect-oriented programming (AOP) is a technique that promotes separation of concerns. Unfortunately, it still suffers from well-known composition issues, in particular from undesirable interferences when multiple concerns are applied at the same join point. In this paper we propose an approach to detect interferences side effect using executable assertions. The assertions are inserted in the aspect chain to detect various types of interferences. The implementation is based on the AIRIA resolver construct, recently introduced to better control conflicting aspects in AspectJ. Resolvers add observation points that were lacking in AspectJ. We propose to take advantage of this to implement automated detection of interferences at execution time. We study the feasibility of this approach and demonstrate it on artificial examples.}, 
keywords={aspect-oriented programming;executable assertions;aspect-oriented programming technique;AOP technique;aspect chain;AIRIA resolver construct;AspectJ;observation points;automatic interference detection;execution time;Interference;Monitoring;Instruments;Weaving;Encryption;Data structures;Programming;Aspect interference;executable assertions;verification}, 
doi={10.1109/ISSREW.2012.34}, 
ISSN={}, 
month={Nov},}
@ARTICLE{8438923, 
author={P. Parra and O. R. Polo and J. Fernandez and A. Da Silva and S. Sanchez Prieto and A. Martinez}, 
journal={IEEE Transactions on Emerging Topics in Computing}, 
title={A Platform-Aware Model-Driven Embedded Software Engineering Process Based on Annotated Analysis Models}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={In this work a platform-aware model-driven engineering process for building component-based embedded software systems using annotated analysis models is described. The process is supported by a framework, called MICOBS, that allows working with different component technologies and integrating different tools that, independently of the component technology, enable the analysis of non-functional properties based on the principles of composability and compositionality. An actor, called Framework Architect, is responsible for this integration. Three other actors take a relevant part in the analysis process. The Component Provider supplies the components, while the Component Tester is in charge of their validation. The latter also feeds MICOBS with the annotated analysis models that characterize the extra-functional properties of the components for the different platforms on which they can be deployed. The Application Architect uses these components to build new systems, performing the trade-off between different alternatives. At this stage, and in order to verify that the final system meets the extra-functional requirements, the Application Architect uses the reports generated by the integrated analysis tools. This process has been used to support the validation and verification of the on-board application software for the Instrument Control Unit of the Energetic Particle Detector of the Solar Orbiter mission.}, 
keywords={Analytical models;Tools;Biological system modeling;Computational modeling;Embedded software;Measurement;Component-based Software Engineering;Model-Driven Development;On-board Software;Schedulability Analysis}, 
doi={10.1109/TETC.2018.2866024}, 
ISSN={2168-6750}, 
month={},}
@INPROCEEDINGS{6114166, 
author={C. Ioannides and G. Barrett and K. Eder}, 
booktitle={2011 IEEE International High Level Design Validation and Test Workshop}, 
title={Introducing XCS to Coverage Directed test Generation}, 
year={2011}, 
volume={}, 
number={}, 
pages={57-64}, 
abstract={Coverage Directed test Generation (CDG) is rife with challenges and problems, despite the relative successes of machine learning methodologies over the years in automating it. This paper introduces the use of the eXtended Classifier System (XCS) in simulation-based digital design verification. It argues for the use of this novel genetics-based machine learning technique to perform effective CDG by learning the full mapping between coverage results and test generator directives. Using the resulting production rules, efficient test suites can be constructed, and inference on the validity of the verification environment can be made. There is great potential in using XCS for design verification and this paper forms an initial attempt to highlight the associated advantages. The technique requires no domain knowledge to setup and satisfies important CDG requirements. Once matured, it is expected to be utilized seamlessly in any industrial level simulation-based verification process.}, 
keywords={formal verification;learning (artificial intelligence);pattern classification;XCS;coverage directed test generation;machine learning methodologies;extended classifier system;simulation-based digital design verification;genetics-based machine learning technique;CDG requirements;industrial level simulation-based verification process;Generators;Fires;Pipelines;Genetic algorithms;Machine learning;Measurement;Learning systems;Electronic Design Automation and Methodology;Digital Simulation;Learning Systems;Learning Classifier Systems;XCS}, 
doi={10.1109/HLDVT.2011.6114166}, 
ISSN={1552-6674}, 
month={Nov},}
@ARTICLE{5525316, 
author={T. Schavey and S. Duba}, 
journal={IEEE Aerospace and Electronic Systems Magazine}, 
title={Streamlining IMA integration through model-driven methodologies}, 
year={2010}, 
volume={25}, 
number={6}, 
pages={21-24}, 
abstract={Avionics systems integration is an inherently complex undertaking. In addition to ensuring that basic functionality is satisfied, the systems integrator must maximize the system's flexibility and reliability while minimizing weight and cost of change. With the introduction of integrated modular architectures (IMA) based on open standards, many traditional integration issues have been greatly improved. However, additional integration responsibilities arise due to having a large number of functions developed by independent suppliers all sharing the same physical resources.}, 
keywords={aerospace computing;avionics;computer architecture;military aircraft;avionics systems integration;integrated modular architectures;integration issues;model-driven integration;streamline;virtual integration;IMA;commercial aircraft;military aircraft;Aerospace electronics;Cost function}, 
doi={10.1109/MAES.2010.5525316}, 
ISSN={0885-8985}, 
month={June},}
@INPROCEEDINGS{6581637, 
author={H. Neufeldt and S. Stanzel}, 
booktitle={2013 14th International Radar Symposium (IRS)}, 
title={An operational WAM in frankfurt airspace}, 
year={2013}, 
volume={2}, 
number={}, 
pages={561-566}, 
abstract={Extending the capacity of Frankfurt airport with an additional runway to the north of the existing runway system, the surveillance capability in Frankfurt Terminal Maneuvering Area (TMA) had to be adapted as well. Based on their surveillance strategy, the DFS invested in multilateration technology to establish a Precision Approach Monitor (PAM) system and integrate it into the existing surveillance infrastructure. After a phase of thorough planning and preparation and an open tender process, Thales was contracted to implement the PAM FRA system which is now going into operation. This paper reports the successful implementation and testing of an operational WAM system in one of the most congested airspaces of the world. Transponder anomalies found as well as methods and strategies to achieve required performances are presented.}, 
keywords={airports;distributed sensors;surveillance;transponder anomaly;Precision Approach Monitor system;Frankfurt terminal maneuvering area;surveillance capability;runway system;Frankfurt airport;Frankfurt airspace;wide area multilateration technology;WAM;Surveillance;Airports;Accuracy;Radar tracking;Transponders;Receivers}, 
doi={}, 
ISSN={2155-5745}, 
month={June},}
@INBOOK{6671244, 
author={Stuart Jacobs}, 
booktitle={Security Management of Next Generation Telecommunications Networks and Services}, 
title={Index}, 
year={2014}, 
volume={}, 
number={}, 
pages={}, 
abstract={
No abstract.

}, 
keywords={}, 
doi={10.1002/9781118741580.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6671244},}
@INPROCEEDINGS{658216, 
author={D. M. Murphy and D. M. Allen}, 
booktitle={IECEC-97 Proceedings of the Thirty-Second Intersociety Energy Conversion Engineering Conference (Cat. No.97CH6203)}, 
title={SCARLET development, fabrication, and testing for the Deep Space 1 spacecraft}, 
year={1997}, 
volume={4}, 
number={}, 
pages={2237-2245 vol.4}, 
abstract={An advanced version of "Solar Concentrator Arrays with Refractive Linear Element Technology" (SCARLET) is being assembled for use on the first NASA/JPL New Millennium spacecraft: Deep Space One (DS1). The array is scaled up from the first SCARLET array that was built for the METEOR satellite in 1995 and incorporates advanced technologies such as dual-junction solar cells and an improved structural design. Due to the failure of the Conestoga launch vehicle, this will be the first flight of a modular concentrator array. SCARLET will provide 2.6 kW to the DS1 spacecraft to be launched in July 1998 for a mission that includes fly-bys of the asteroid McAuliffe, Mars and the comet West-Kohoutek-Ikemura. This paper describes the SCARLET design, fabrication/assembly and testing program for the flight system.}, 
keywords={space vehicles;space vehicle power plants;solar cell arrays;solar energy concentrators;photovoltaic power systems;Deep Space One spacecraft;SCARLET array;Solar Concentrator Arrays with Refractive Linear Element Technology;development;fabrication;testing;NASA/JPL New Millennium spacecraft;dual-junction solar cells;structural design;modular concentrator array;assembly;testing programme;2.6 kW;Fabrication;Testing;Space technology;Space vehicles;Building integrated photovoltaics;Assembly;NASA;Satellites;Photovoltaic cells;Mars}, 
doi={10.1109/IECEC.1997.658216}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8411751, 
author={R. Prll and B. Bauer}, 
booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={A Model-Based Test Case Management Approach for Integrated Sets of Domain-Specific Models}, 
year={2018}, 
volume={}, 
number={}, 
pages={175-184}, 
abstract={Due to rapid improvements in the area of embedded processing hardware, the complexity of developed systems constantly increases. In order to ensure a high quality level of such systems, related quality assurance concepts have to evolve. The introduction of Model-Based Testing (MBT) approaches has shown promising results by automating and abstracting multiple activities of the software testing life cycle. Nevertheless, there is a strong need for approaches supporting scoped test models, i.e. subsets of test cases, reflecting specific test purposes driven by risk-oriented development strategies. Therefore, we developed an integrated and model-based approach supporting test case management, which incorporates the beneficial aspects of abstract development methodologies with predominant research for test case management in non-model-based scenarios. Based on a new model artifact, the integration model, tasks like cross-domain information mapping and the integration of domain-specific KPIs derived by analyses favor the subsequently applied constraint-based mechanism for test case management. Further, a prototypical implementation of these concepts within the Architecture And Analysis Framework (A3F) is elaborated and further evaluated based on representative application scenarios. A comparative view on related work leads to a conclusive statement regarding our future work.}, 
keywords={program testing;quality assurance;software architecture;software quality;test cases;specific test purposes;risk-oriented development strategies;integrated model-based;abstract development methodologies;nonmodel-based scenarios;model artifact;integration model;cross-domain information mapping;domain-specific KPIs;subsequently applied constraint-based mechanism;Model-Based test case management approach;integrated sets;domain-specific models;embedded processing hardware;developed systems;high quality level;related quality assurance concepts;automating activities;abstracting multiple activities;software testing life cycle;scoped test models;MBT;Analytical models;Data models;Context modeling;Software;Software testing;Model-Based Testing;Test Case Management;Test Selection;Test Prioritization;Test Suite Reduction;Test Model Scoping}, 
doi={10.1109/ICSTW.2018.00048}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{5381934, 
author={C. Xin}, 
booktitle={2009 International Conference on Wireless Networks and Information Systems}, 
title={Wireless Communications Trends}, 
year={2009}, 
volume={}, 
number={}, 
pages={278-281}, 
abstract={This paper reviews the wireless communications roadmap, and discusses the trends of wireless communications, including: higher and higher data rates, ubiquity of wireless devices, smart antennas, faster, smaller, cheaper hardware, frequency congestion, and multiple-input, multiple-output systems. Finally, this paper discusses the 4G wireless evolution.}, 
keywords={4G mobile communication;adaptive antenna arrays;MIMO communication;wireless communications roadmap;4G wireless evolution;smart antennas;frequency congestion;multiple-input multiple-output systems;Wireless communication;Internet telephony;Cable TV;Communication cables;Land mobile radio;Business;Cellular phones;Web and internet services;Hardware;Wires;wireless communication;4G;telecommunication;evolution}, 
doi={10.1109/WNIS.2009.53}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7923802, 
author={R. R. d. Oliveira and R. M. Martins and A. d. S. Simao}, 
booktitle={2017 IEEE International Conference on Cloud Engineering (IC2E)}, 
title={Impact of the Vendor Lock-in Problem on Testing as a Service (TaaS)}, 
year={2017}, 
volume={}, 
number={}, 
pages={190-196}, 
abstract={Testing as a Service (TaaS) is a new business and service model that provides efficient and effective software quality assurance and enables the use of a cloud for the meeting of quality standards, requirements and consumer's needs. However, problems that limit the effective use of TaaS involve lack of standardization in writing, execution, configuration and management of tests and lack of portability and interoperability among TaaS platforms - the so-called lock-in problem. The lock-in problem is a serious threat to software testing in the cloud and may become critical when a provider decides to suddenly increase prices, or shows serious technical availability problems. This paper proposes a novel approach for solving the lock-in problem in TaaS with the use of design patterns. The aim to assist software engineers and quality control managers in building testing solutions that are both portable and interoperable and promote a more widespread adoption of the TaaS model in cloud computing.}, 
keywords={cloud computing;object-oriented programming;open systems;program testing;quality assurance;software quality;vendor lock-in problem;testing as a service;TaaS;software quality assurance;quality standards;software testing;design patterns;interoperability;cloud computing;Cloud computing;Testing;Computational modeling;Interoperability;Browsers;Context;Cloud Computing;Testing as a Service (TaaS);Design Patterns;Vendor Lock-in;Testing Service}, 
doi={10.1109/IC2E.2017.30}, 
ISSN={}, 
month={April},}
@ARTICLE{531663, 
author={}, 
journal={IEEE Std 743-1995}, 
title={IEEE Standard Equipment Requirements and Measurement Techniques for Analog Transmission Parameters for Telecommunications}, 
year={1996}, 
volume={}, 
number={}, 
pages={i-}, 
abstract={Performance requirements for test equipment that measures the analog transmission parameters of subscriber loops, message trunks, PBX trunks, and ties lines are specified. Requirements for these measurements with DS1 bit stream access are also provided. The measurement of loss, noise, and impulse noise on non-loaded cable pairs used for digital subscriber lines is also addressed.}, 
keywords={telecommunication standards;nonloaded cable pairs;measurement techniques;IEEE standard;IEEE Std 743-1995;test equipment;analog transmission parameters;subscriber loops;message trunks;PBX trunks;ties lines;DS1 bit stream access;loss;noise;impulse noise;digital subscriber lines;Communication standards}, 
doi={10.1109/IEEESTD.1996.81076}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{7081882, 
author={M. Laverdire and B. J. Berger and E. Merloz}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Taint analysis of manual service compositions using Cross-Application Call Graphs}, 
year={2015}, 
volume={}, 
number={}, 
pages={585-589}, 
abstract={We propose an extension over the traditional call graph to incorporate edges representing control flow between web services, named the Cross-Application Call Graph (CACG). We introduce a construction algorithm for applications built on the Jax-WS standard and validate its effectiveness on sample applications from Apache CXF and JBossWS. Then, we demonstrate its applicability for taint analysis over a sample application of our making. Our CACG construction algorithm accurately identifies service call targets 81.07% of the time on average. Our taint analysis obtains a F-Measure of 95.60% over a benchmark. The use of a CACG, compared to a naive approach, improves the F-Measure of a taint analysis from 66.67% to 100.00% for our sample application.}, 
keywords={data flow analysis;flow graphs;Web services;manual service compositions;control flow;Web services;cross-application call graph;Jax-WS standard;Apache CXF;JBossWS;taint analysis;CACG construction algorithm;service call targets;F-measure;Web services;Benchmark testing;Java;Security;Manuals;Algorithm design and analysis;Androids}, 
doi={10.1109/SANER.2015.7081882}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{1316702, 
author={J. Sprinkle}, 
booktitle={Proceedings. 11th IEEE International Conference and Workshop on the Engineering of Computer-Based Systems, 2004.}, 
title={Improving CBS tool development with technological spaces}, 
year={2004}, 
volume={}, 
number={}, 
pages={218-224}, 
abstract={The complexity of computer based systems (CBSs) requires that multiple levels of abstraction be available to a designer in order to facilitate their formal specification. Generating final executable code from the model of the system is preferred to hand-coding the implementation, but this is seldom done in one step-usually there are several cascading transformations that eventually result in the executable system. We explain how the concept of the technological space (TS) can be used to define and describe the layers between cascading transformations, and the transformations themselves. TSs are also shown as a categorization that better distinguishes between a domain and the technology used to store information in a domain.}, 
keywords={formal specification;software metrics;Unified Modeling Language;software architecture;computer aided software engineering;software tools;CBS tool development;computer based system;formal specification;hand-coding implementation;cascading transformation;technological space;Space technology;Software engineering;Software systems;Systems engineering and theory;Computer architecture;Software performance;Programming;Design engineering;Object oriented modeling;Large-scale systems}, 
doi={10.1109/ECBS.2004.1316702}, 
ISSN={}, 
month={May},}
@ARTICLE{4447435, 
author={}, 
journal={IEEE Unapproved Draft Std P2600_D33b, Feb 2008}, 
title={IEEE Draft Standard for Information Technology: Hardcopy Device and System Security}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={This standard defines security requirements (all aspects of security including but not limited to authentication, authorization, privacy, integrity, device management, physical security and information security) for manufacturers, users and others on the selection, installation, configuration and usage of hardcopy devices and systems including printers, copiers, and multifunction devices and the computer systems that support these devices. This standard identifies security exposures for these hardcopy devices and systems and instructs manufacturers and software developers on appropriate security capabilities to include in their devices and systems and instructs users on appropriate ways to use these security capabilities.}, 
keywords={copier;hardcopy device;information security;printer;multifunction device;MFD;MFP;All-in-One;scanner;HCD;multifunction product;fax;facsimile;hardcopy security}, 
doi={}, 
ISSN={}, 
month={},}
@INBOOK{5273597, 
author={Frank Hargrave}, 
booktitle={Hargrave's Communications Dictionary}, 
title={Index}, 
year={2001}, 
volume={}, 
number={}, 
pages={}, 
abstract={
}, 
keywords={Dictionaries;Indexes;Communication systems}, 
doi={10.1109/9780470544822.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273597},}
@INPROCEEDINGS{8502650, 
author={K. Meixner and S. Biffl and D. Winkler}, 
booktitle={2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Towards Flexible and Automated Testing in Production Systems Engineering Projects}, 
year={2018}, 
volume={1}, 
number={}, 
pages={169-176}, 
abstract={Automated and systematic testing of automation systems (AS) and production systems (PS) require an integrated testing tool chain for test case development, execution and reporting. In practice, the test automation tool chain cannot be fully automated because of missing links between different tools used in the test automation process. Closing these gaps typically require (high) human effort. Furthermore, domain and software testing expertise is often bundled by one (expensive) engineer who is responsible for the application domain (reflected in use cases and test cases) and software tests (software test code). This paper presents a flexible Testing Automation Framework (TAF) that enables the configuration of test processes involving different tools and various layers for test automation and enables separated roles for the application domain and software tests. We build on best-practice test automation from Software Engineering and design a test automation process for the automation systems domain. We demonstrate the feasibility with a use case, derived from production systems automation, with selected tools covering all test automation layers. First results showed the feasibility of the framework in the evaluation use case making test processes more flexible and automated. Although the successful implementation of the TAF can support the efficient configuration and execution of test processes, there is additional effort for preparing the flexible and automated tool chain.}, 
keywords={Automation;Tools;Testing;Software;Production systems;Unified modeling language;Software engineering;software and system testing;test automation framework;automation systems;production systems;test configuration;feasibility study}, 
doi={10.1109/ETFA.2018.8502650}, 
ISSN={1946-0759}, 
month={Sept},}
@ARTICLE{8360943, 
author={K. Gallaba and S. McIntosh}, 
journal={IEEE Transactions on Software Engineering}, 
title={Use and Misuse of Continuous Integration Features: An Empirical Study of Projects that (mis)use Travis CI}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Continuous Integration (CI) is a popular practice where software systems are automatically compiled and tested as changes appear in the version control system of a project. Like other software artifacts, CI specifications require maintenance effort. Although there are several service providers like Travis CI offering various CI features, it is unclear which features are being (mis)used. In this paper, we present a study of feature use and misuse in 9,312 open source systems that use Travis CI. Analysis of the features that are adopted by projects reveals that explicit deployment code is rare&#x2014;48.16% of the studied Travis CI specification code is instead associated with configuring job processing nodes. To analyze feature misuse, we propose Hansel&#x2014;an anti-pattern detection tool for Travis CI specifications. We define four anti-patterns and Hansel detects anti-patterns in the Travis CI specifications of 894 projects in the corpus (9.60%), and achieves a recall of 82.76% in a sample of 100 projects. Furthermore, we propose Gretel&#x2014;an anti-pattern removal tool for Travis CI specifications, which can remove 69.60% of the most frequently occurring anti-pattern automatically. Using Gretel, we have produced 36 accepted pull requests that remove Travis CI anti-patterns automatically.}, 
keywords={Tools;Organizations;Software;Computer languages;Control systems;Electronic mail;Feature extraction}, 
doi={10.1109/TSE.2018.2838131}, 
ISSN={0098-5589}, 
month={},}
@ARTICLE{898825, 
author={D. MacMillen and R. Camposano and D. Hill and T. W. Williams}, 
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
title={An industrial view of electronic design automation}, 
year={2000}, 
volume={19}, 
number={12}, 
pages={1428-1448}, 
abstract={The automation of the design of electronic systems and circuits [electronic design automation (EDA)] has a history of strong innovation. The EDA business has profoundly influenced the integrated circuit (IC) business and vice-versa. This paper reviews the technologies, algorithms, and methodologies that have been used in EDA tools and the business impact of these technologies. In particular, we focus on four areas that have been key in defining the design methodologies over time: physical design, simulation/verification, synthesis, and test. We then look briefly into the future. Design will evolve toward more software programmability or some other kind of field configurability like field programmable gate arrays (FPGAs). We discuss the kinds of tool sets needed to support design in this environment.}, 
keywords={formal verification;high level synthesis;integrated circuit design;circuit CAD;network routing;circuit simulation;electronic design automation;history;EDA tools;design methodologies;physical design;simulation;verification;synthesis;software programmability;field configurability;field programmable gate arrays;Electronics industry;Electronic design automation and methodology;Integrated circuit technology;Field programmable gate arrays;Design automation;History;Technological innovation;Design methodology;Circuit simulation;Circuit synthesis}, 
doi={10.1109/43.898825}, 
ISSN={0278-0070}, 
month={Dec},}
@ARTICLE{6770564, 
author={P. F. DeDuck and S. R. Johnson}, 
journal={AT T Technical Journal}, 
title={The FT-2000 OC-48 lightwave system}, 
year={1992}, 
volume={71}, 
number={1}, 
pages={14-22}, 
abstract={Next generation terrestrial lightwave terminals must do more than transport digital information from one location to another. FT-2000, AT&T's newest high-capacity lightwave transmission system, is designed to meet the needs of customers into the next century. It combines a flexible hardware platform and a powerful software-based architecture. As an intelligent lightwave system, FT-2000 can operate in sophisticated self-healing networks, and is managed by an advanced control system that simplifies installing, provisioning, monitoring, and maintaining it. It is fully compliant with the American National Standards Institute (ANSI) optical interface standard, the Synchronous Optical Network (SONET). We explore the broad range of applications and customer needs that drove the specification of FT-2000, and present the architectural solution that achieves the flexibility to meet those specifications.}, 
keywords={}, 
doi={10.1002/j.1538-7305.1992.tb00143.x}, 
ISSN={8756-2324}, 
month={Jan},}
@INPROCEEDINGS{7374941, 
author={P. Suthar and M. Stolic}, 
booktitle={2015 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob)}, 
title={Carrier grade Telco-Cloud}, 
year={2015}, 
volume={}, 
number={}, 
pages={101-107}, 
abstract={The Telco service providers business is undergoing a fundamental shift, and operators are transforming their network to meet new business challenges. Biggest focus area for Telco Cloud is rapid time to market (TTM) for new services and reduction in total cost of ownership (TCO). Telco service providers are facing challenges from web and content providers because of agility and convergence of voice and data. Telco Cloud is transformation of traditional wireline, wireless, voice, text, data, and web etc. services to common compute cloud infrastructures. Cloud infrastructures can be set-up on-premise, off-premise or hybrid based upon service level agreements (SLA), security and maturity of services. Key component of Telco Cloud is IP Multimedia System (IMS) which provides convergence of voice, data, video, multimedia messaging etc. Designing and developing Telco Cloud, which meets criteria of carrier grade, is very important to gain confidence and comfort level of different stakeholders. This paper discusses design and deployment criteria for building high quality Telco Cloud.}, 
keywords={cloud computing;electronic messaging;IP networks;multimedia communication;carrier grade Telco-cloud infrastructure;time to market;total cost of ownership reduction;Telco service provider;service level agreement;IP multimedia system;service maturity;service security;voice convergence;data convergence;video convergence;multimedia messaging convergence;Cloud computing;Business;Logic gates;Mobile communication;Hardware;Wireless communication}, 
doi={10.1109/APWiMob.2015.7374941}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4559609, 
author={}, 
journal={IEEE Unapproved Draft Std P11073-00101/D5, Jun 2008}, 
title={IEEE Draft Health Informatics - Point-Of-Care Medical Device Communication - Technical Report - Guidelines for the Use of RF Wireless Technology}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={The following Guidance document addresses the use of radio frequency 1 (RF) wireless technology for the transport of medical data both to and from point-of-care (PoC) medical devices. The context of such wireless medical data transport can range from home- or mobile-based healthcare to in hospital ambulatory and stationary situations. The intent of the guidance document is to be global with respect to wireless spectrum and equipment, although working group participation and expertise have favored detail of scenarios from the US. At the time of this Guidance document several applicable RF wireless technologies exist with a range of capabilities and characteristics, and in different stages of maturity, standardization, and adoption in healthcare. It is recognized that RF technologies are rapidly evolving, and new options may become available (or sufficiently established) after the publication of this Guidance document. The recommendations, therefore, avoid being overly prescriptive and instead attempt to assist medical device manufacturers, wireless equipment manufacturers, healthcare providers, government agencies and any other end-user of this document to make reasonable judgments regarding performance and practical implementation of wireless solutions. The Guidance document defines specific use cases to estimate, compare, and contrast performance of known technologies operating on wireless personal area (WPAN), wireless local area (WLAN), wireless metropolitan area (WMAN), and wireless wide area (WWAN) networks. Major considerations are 1) the quality-of-service (QoS) requirements (reliability, latency, priority, bandwidth) associated with the data being transported, 2) the expected performance (power, link range, throughput, link establishment and maintenance) of the wireless technology, and 3) the specific needs and resources of the end user. Related issues include network architecture, EMI/EMC, coexistence with other data streams, security, cost, power consumption, and technology configurability. Performance summaries for specific wireless technologies that support defined use cases are not intended as an endorsement of optimal solution because different needs, resources, sizes, and environments cannot be comprehensively addressed. This overview document is meant to be a foundation and reference for several follow-on IEEE 11073.3.5.x standards that will profile specific classes of off-the-shelf RF wireless technologies for medical data transport. Importantly, this guidance document is not envisioned to be periodically updated, but instead will act as a source of information for the follow-on IEEE 11073-0305.x standards that will supplant it. Periodic updates will be performed on the IEEE 11073-0305.x standards only.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{4138229, 
author={R. Sombrutzki and A. Zubow and M. Kurth and J. Redlich}, 
booktitle={2006 1st Workshop on Operator-Assisted (Wireless Mesh) Community Networks}, 
title={Self-Organization in Community Mesh Networks The Berlin RoofNet}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-11}, 
abstract={A community network must be usable for inexperienced end users; thus self-organization is essential. On the one hand, we propose an approach for self-organization in ad-hoc wireless multi-hop mesh networks, where the client is fully freed from such mundane tasks as IP configuration, etc. On the other hand, the community mesh network itself is fully self-organized thus no operator or provider is required. We present the architecture of the Berlin RoofNet (BRN) and a distributed realization of services like DHCP, ARP and Internet gateway discovery and selection. In addition, results of a detailed simulation and experimental evaluation comparing our distributed hash table based approach to traditional methods are presented. We show that our approach is more reliable, efficient and responsive}, 
keywords={ad hoc networks;Internet;protocols;telecommunication network topology;self-organization;ad-hoc wireless multihop mesh networks;dynamic host configuration protocol;DHCP;address resolution protocol;ARP;Internet gateway;Mesh networks;Computer architecture;Spread spectrum communication;IP networks;Cities and towns;Protocols;Web and internet services;Computer network reliability;Telecommunication network reliability;Wireless mesh networks;Community Networks;Self-Organization;Distributed Hash Table}, 
doi={10.1109/WOACN.2006.337188}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{5254276, 
author={L. Mei and W. K. Chan and T. H. Tse and F. Kuo}, 
booktitle={2009 33rd Annual IEEE International Computer Software and Applications Conference}, 
title={An Empirical Study of the Use of Frankl-Weyuker Data Flow Testing Criteria to Test BPEL Web Services}, 
year={2009}, 
volume={1}, 
number={}, 
pages={81-88}, 
abstract={Programs using service-oriented architecture (SOA) often feature ultra-late binding among components. These components have well-defined interfaces and are known as Web services. Messages between every pair of Web services dually conform to the output interface of a sender and the input interface of a receiver. Unit testing of Web services should not only test the logic of Web services, but also assure the correctness of the Web services during input, manipulation, and output of messages. There is, however, little software testing research in this area. In this paper, we study the unit testing problem to assure components written in orchestration languages, WS-BPEL in particular. We report an empirical study of the effectiveness of the Frankl-Weyuker data flow testing criteria (particularly the all-uses criterion) on WS-BPEL subject programs. Our study shows that conventional data flow testing criteria can be much less effective in revealing faults in interface artifacts (WSDL documents) and message manipulations (XPath queries) than revealing faults in BPEL artifacts.}, 
keywords={business data processing;data flow analysis;program testing;software architecture;software fault tolerance;specification languages;Web services;Frankl-Weyuker data flow testing criteria;BPEL Web service;service-oriented architecture;unit testing;software testing;WS-BPEL;software fault;Web services;Logic testing;XML;Software testing;Application software;Service oriented architecture;Councils;Information retrieval;Computer applications;Data flow computing;WS-BPEL;XPath;data flow testing}, 
doi={10.1109/COMPSAC.2009.21}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{7342393, 
author={S. McGinty and D. Hadad and C. Nappi and B. Caquelin}, 
booktitle={2015 IEEE International Test Conference (ITC)}, 
title={Developing a modern platform for test engineering  Introducing the origen semiconductor developer's kit}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Many of the tools used today in semiconductor test engineering are single-point solutions that are concerned with the mechanics of translating test IP between domains and formats. There is no cohesive standardized framework to bind them all together; and workflow and application architecture choices are largely left up to the individual engineer. Learning from the state-of-the-art in other software engineering domains, we have developed a modern framework for semiconductor engineering that favors a convention-based approach to application architectures. By following conventions, powerful abstractions can be created to enable truly modular test development within a unified environment for the creation of test patterns, test programs, and all other test collateral. The paper reviews the background and some of the main capabilities of the framework and discusses how it is being used in production today to replace many conventional pattern flows. This is also a formal announcement that Freescale Semiconductor is open-sourcing the Origen Semiconductor Developer's Kit (SDK) to enable future development to be done in collaboration with the global semiconductor engineering community.}, 
keywords={circuit analysis computing;integrated circuit testing;software engineering;global semiconductor engineering community;SDK;Origen semiconductor developer kit;Freescale semiconductor;test programs;test patterns;truly modular test development;convention-based approach;software engineering domains;test IP;semiconductor test engineering;IP networks;Companies;Complexity theory;Testing;Industries;Computer architecture;Silicon}, 
doi={10.1109/TEST.2015.7342393}, 
ISSN={}, 
month={Oct},}
@ARTICLE{6768354, 
author={A. R. McGee and S. R. Vasireddy and K. J. Johnson and U. Chandrashekhar and S. H. Richman and M. El-Sayed}, 
journal={Bell Labs Technical Journal}, 
title={Dynamic virtual private networks}, 
year={2002}, 
volume={6}, 
number={2}, 
pages={116-135}, 
abstract={Modifications to a virtual private network's (VPN's) topology, security, service provisioning options, or quality of service (QoS) typically require an end-user request to their service provider, whose personnel currently perform the VPN management. This process incurs more provisioning delay and is more costly than user self-provisioning. This paper presents a new service approach and dynamic virtual private network (D-VPN) technology that marries VPNs with directory enabled networking and Web-based subscriber service selection. It places VPN management into the hands of the user to produce instantaneous results, lowering service-provider operations costs, and subsequently reducing the cost to the end user. The paper also describes the target architecture and framework as well as the initial types of services that could be supported by D-VPN technology.3}, 
keywords={}, 
doi={10.1002/bltj.9}, 
ISSN={1538-7305}, 
month={},}
@INPROCEEDINGS{6595796, 
author={M. Diepenbeck and M. Soeken and D. Grobe and R. Drechsler}, 
booktitle={2013 8th International Workshop on Automation of Software Test (AST)}, 
title={Towards automatic scenario generation from coverage information}, 
year={2013}, 
volume={}, 
number={}, 
pages={82-88}, 
abstract={Nowadays, the design of software systems is pushed towards agile development practices. One of its most fundamental approaches is Test Driven Development (TDD). This procedure is based on test cases which are incrementally written prior to the implementation. Recently, Behavior Driven Development (BDD) has been introduced as an extension of TDD, in which natural language scenarios are the starting point for the test cases. This description offers a ubiquitous communication mean for both the software developers and stakeholders. Following the BDD methodology thoroughly, one would expect 100 % code coverage, since code is only written to make the test cases pass. However, as we show in an empirical study this expectation is not valid in practice. It becomes even worse in the process of development, i.e. the coverage decreases over time. To close the coverage gap, we sketch an algorithm that generates BDD-style scenarios based on uncovered code.}, 
keywords={program testing;software prototyping;scenario generation;coverage information;software system design;agile development practice;test driven development;TDD approach;behavior driven development;natural language scenario;BDD methodology;code coverage;Data structures;Boolean functions;Natural languages;Software;Testing;Unified modeling language;Context}, 
doi={10.1109/IWAST.2013.6595796}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4539573, 
author={W. Hargassner and T. Hofer and C. Klammer and J. Pichler and G. Reisinger}, 
booktitle={2008 1st International Conference on Software Testing, Verification, and Validation}, 
title={A Script-Based Testbed for Mobile Software Frameworks}, 
year={2008}, 
volume={}, 
number={}, 
pages={448-457}, 
abstract={Software testing is essential and takes a large part of resources during software development. This motivates automating software testing as far as possible. Frameworks for automating unit testing are approved and applied for a plethora of programming languages to write tests for small units in the same programming language. Both constraints, unit size and programming language, inhibit automation of software testing in domain of mobile software frameworks. This circumstance has motivated the development of a new testbed for a framework in the domain of mobile systems. In this paper, we describe requirements and challenges in testing mobile software frameworks in general and present a novel testbed for the APOXI framework that addresses these requirements. The main ideas behind this testbed are the usage of a scripting language to specify test cases and to incorporate domain-specific aspects on the language level. The testbed facilitates component and system testing but can be used for unit testing as well.}, 
keywords={mobile computing;program testing;software engineering;script-based testbed;mobile software;software testing;software development;programming languages;Software testing;System testing;Protocols;Application software;Computer languages;Control systems;Automatic testing;Hardware;Mobile handsets;Electronic equipment testing;Software testing}, 
doi={10.1109/ICST.2008.51}, 
ISSN={2159-4848}, 
month={April},}
@ARTICLE{991333, 
author={E. Hieatt and R. Mee}, 
journal={IEEE Software}, 
title={Going faster: testing the Web application}, 
year={2002}, 
volume={19}, 
number={2}, 
pages={60-65}, 
abstract={This article documents the experiences of Evant's Extreme Programming team with testing XP. Testing is fundamental to XP but is a practice that often falls by the wayside in today's fast-paced Web application development culture. From the beginning, Evant adhered to each of XP's principles, and testing was no exception. This article explains how the team found that testing, positioned as the drive behind development, was critical to the success of building Evant's application at speed while maintaining high quality.}, 
keywords={Internet;information resources;program testing;hypermedia markup languages;Web application testing;Internet;HTML;Extreme Programming;XP;application development;Evant;software quality;Application software;Software testing;Writing;Buildings;Software engineering;Internet;Software quality;Software maintenance;Software tools;Java}, 
doi={10.1109/52.991333}, 
ISSN={0740-7459}, 
month={March},}
@INPROCEEDINGS{6201497, 
author={Z. Hui and P. Lei and W. Yifei}, 
booktitle={2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)}, 
title={Design amp;amp; implementation of laboratory information management system based on agile method}, 
year={2012}, 
volume={}, 
number={}, 
pages={2490-2493}, 
abstract={Agile software development is a new methodology of developing high quality software timely when facing significant change; it is convenient for managers to accomplish the collection, disposal, output and other work to the data. There are chart and table functions to the output data and information of laboratories, functions of the output of original and final reports of laboratories, making it affiance to precede the quality control of the data and to help managers arrange analytical plans, staff and other daily work.}, 
keywords={design engineering;information management;laboratories;quality control;software prototyping;software quality;laboratory information management system;agile method;agile software development;high quality software development;chart;table functions;data quality control;Laboratories;Programming;Information management;Software;Quality assurance;Servers;agile Method;management system;LMIS;agile software development}, 
doi={10.1109/CECNet.2012.6201497}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{140801, 
author={F. Godon and D. Al-Khalili and R. Inkol}, 
booktitle={Proceedings of the 33rd Midwest Symposium on Circuits and Systems}, 
title={A memory controller for mapping an array of circular buffers into a RAM}, 
year={1990}, 
volume={}, 
number={}, 
pages={645-648 vol.2}, 
abstract={A 1.5- mu m CMOS ASIC with a total complexity of over 22000 gates has been developed to generate and keep track of the offsets within 32 circular buffers. It offers a fair arbitration of interleaved read/write operations at a maximum data transfer rate of 20 MHz. Although the device is intended for a specialized electronic warfare system application, the design features incorporated make it generic and suitable for other applications such as communications interfaces in multiprocessor systems.<<ETX>>}, 
keywords={application specific integrated circuits;buffer storage;cellular arrays;CMOS integrated circuits;electronic warfare;random-access storage;storage management chips;VLSI;circular buffer memory controller;buffers mapping into RAM;gate arrays;CMOS ASIC;arbitration of interleaved read/write operations;data transfer rate;electronic warfare system;communications interfaces;multiprocessor systems;20 Mbit/s;1.5 micron;Random access memory;Read-write memory;Buffer storage;Radar;Very large scale integration;Military computing;Process control;Computer architecture;Counting circuits;Physics computing}, 
doi={10.1109/MWSCAS.1990.140801}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{1383107, 
author={M. Lil and Y. Wei and D. Desovski and H. Nejad and S. Ghose and B. Cukic and C. Smidts}, 
booktitle={15th International Symposium on Software Reliability Engineering}, 
title={Validation of a methodology for assessing software reliability}, 
year={2004}, 
volume={}, 
number={}, 
pages={66-76}, 
abstract={Software-based digital systems are progressively replacing analog systems in safety-critical applications. However the ability to predict their reliability is not well understood and needs further study. A first step towards systematic resolution of this issue was presented in a recent software engineering measure study. In that study a set of software engineering measures were ranked with respect to their ability in predicting software reliability through an expert opinion elicitation process. This study also proposed a concept of reliability prediction system (RePS) to bridge the gap between software engineering measures and software reliability. The research presented in this paper validates the rankings obtained and the concept of RePS proposed in the previous study.}, 
keywords={software reliability;program verification;software metrics;software validation;software reliability;software-based digital system;safety-critical application;software engineering measure study;opinion elicitation process;reliability prediction system;Software reliability;Software measurement;Software engineering;Reliability engineering;Phase measurement;Application software;Object oriented modeling;Usability;Software testing;Educational institutions}, 
doi={10.1109/ISSRE.2004.47}, 
ISSN={1071-9458}, 
month={Nov},}
@INPROCEEDINGS{7019292, 
author={C. M. Prathibhan and A. Malini and N. Venkatesh and K. Sundarakantham}, 
booktitle={2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies}, 
title={An automated testing framework for testing Android mobile applications in the cloud}, 
year={2014}, 
volume={}, 
number={}, 
pages={1216-1219}, 
abstract={The testing of mobile application faces many issues due to the complexity of testing these applications and the limited resources available in mobile devices. Testing in various mobile devices under varying conditions takes a lot of time when done manually. Also by using emulators it is not possible to generate the same real time network connections and real device characteristics. There is a need for a testing framework that allows automated testing of mobile applications in many mobile devices in limited time. In this paper we propose a mobile testing framework in the cloud environment that aims to provide automated testing of mobile applications in various mobile devices. This testing framework has an automated testing tool, the Mobile Application Testing (MAT) Tool integrated to it that performs functional, performance and compatibility testing of mobile applications.}, 
keywords={cloud computing;mobile computing;program testing;smart phones;automated testing framework;Android mobile application testing tool;mobile computing;time network connections;cloud environment;MAT Tool;Testing;Mobile communication;Performance evaluation;Androids;Humanoid robots;Mobile Testing;Automated Testing;Testing as a Service;Cloud Testing}, 
doi={10.1109/ICACCCT.2014.7019292}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6903575, 
author={A. Kane and T. Fuhrman and P. Koopman}, 
booktitle={2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks}, 
title={Monitor Based Oracles for Cyber-Physical System Testing: Practical Experience Report}, 
year={2014}, 
volume={}, 
number={}, 
pages={148-155}, 
abstract={Testing Cyber-Physical Systems is becoming increasingly challenging as they incorporate advanced autonomy features. We investigate using an external runtime monitor as a partial test oracle to detect violations of critical system behavioral requirements on an automotive development platform. Despite limited source code access and using only existing network messages, we were able to monitor a hardware-in-the-loop vehicle simulator and analyze prototype vehicle log data to detect violations of high-level critical properties. Interface robustness testing was useful to further exercise the monitors. Beyond demonstrating feasibility, the experience emphasized a number of remaining research challenges, including: approximating system intent based on limited system state observability, how to best balance the simplicity and expressiveness of the specification language used to define monitored properties, how to warm up monitoring of system variable state after mode change discontinuities, and managing the differences between simulation and real vehicles when conducting such tests.}, 
keywords={automotive engineering;observability;program testing;safety-critical software;source code (software);specification languages;monitor based oracles;cyber-physical system testing;practical experience report;advanced autonomy feature;external runtime monitor;partial test oracle;critical system behavioral requirements;automotive development platform;limited source code access;hardware-in-the-loop vehicle simulator;prototype vehicle log data;high-level critical property;interface robustness testing;limited system state observability;specification language;monitored property;system variable state;mode change discontinuity;Monitoring;Vehicles;Testing;Runtime;Robustness;Safety;Prototypes;runtime monitoring;testing;cyber-physical systems}, 
doi={10.1109/DSN.2014.28}, 
ISSN={1530-0889}, 
month={June},}
@ARTICLE{5233611, 
author={J. C. Duenas and J. L. Ruiz and F. Cuadrado and B. Garcia and H. A. Parada G.}, 
journal={IEEE Internet Computing}, 
title={System Virtualization Tools for Software Development}, 
year={2009}, 
volume={13}, 
number={5}, 
pages={52-59}, 
abstract={The configuration complexity of preproduction sites coupled with access-control mechanisms often impede the software development life cycle. Virtualization is a cost-effective way to remove such barriers and provide a test environment similar to the production site, reducing the burden in IT administrators. An eclipse-based virtualization tool framework can offer developers a personal runtime environment for launching and testing their applications. The authors have followed a model-driven architecture (MDA) approach that integrates best-of-breed virtualization technologies, such as Xen and VDE.}, 
keywords={authorisation;software engineering;software tools;system virtualization tool;configuration complexity;access-control mechanism;software development life cycle;IT administrator;eclipse-based virtualization tool;model-driven architecture approach;Programming;Testing;Impedance;Production;Application virtualization;Runtime environment;Application software;virtualization;software development;distributed systems;Eclipse;model-driven architecture;MDA}, 
doi={10.1109/MIC.2009.115}, 
ISSN={1089-7801}, 
month={Sept},}
@INPROCEEDINGS{6120072, 
author={Jiao Yu and B. M. Wilamowski}, 
booktitle={IECON 2011 - 37th Annual Conference of the IEEE Industrial Electronics Society}, 
title={Recent advances in in-vehicle embedded systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={4623-4625}, 
abstract={The number of computer based functions embedded in vehicles has increased significantly in the past two decades. An in-vehicle embedded electronic architecture is a complex distributed system; the development of which is a cooperative work involving different manufacturers and suppliers. There are several key demands in the development process, such as safety requirements, real-time assessment, schedulability, composability, etc. Intensive research is being conducted to address these issues. This paper reviews recent technology advances in relevant aspects and covers a range of topics highlighted above.}, 
keywords={automotive electronics;computer architecture;embedded systems;in-vehicle embedded electronic architecture;distributed system;computer based functions;Real time systems;Embedded systems;Automotive engineering;Field programmable gate arrays;Multicore processing;In-vehicle embedded electronic architecture;FPGA;real-time assessment;composability}, 
doi={10.1109/IECON.2011.6120072}, 
ISSN={1553-572X}, 
month={Nov},}
@INPROCEEDINGS{1342755, 
author={J. H. Andrews}, 
booktitle={Proceedings. 19th International Conference on Automated Software Engineering, 2004.}, 
title={A case study of coverage-checked random data structure testing}, 
year={2004}, 
volume={}, 
number={}, 
pages={316-319}, 
abstract={We study coverage-checked random unit testing (CRUT), the practice of repeatedly testing units on sequences of random function calls until given code coverage goals are achieved. Previous research has shown that this practice can be a useful complement to traditional testing methods. However, questions remained as to the breadth of its applicability. In this paper, we report on a case study in which we applied CRUT to the testing of two mature public-domain data structures packages. We show that CRUT helped in identifying faults, in debugging, in extracting and specifying actual behaviour, and in achieving greater assurance of the correctness of the debugged software}, 
keywords={data structures;fault diagnosis;program debugging;program testing;public domain software;software engineering;coverage-checked random data structure testing;coverage-checked random unit testing;random function calls;code coverage goals;public-domain data structures packages;fault identification;behaviour extraction;behaviour specification;debugging correctness;software debugging;Computer aided software engineering;Data structures;Software testing;Packaging;Fault diagnosis;Software engineering;Automatic testing;Documentation;Computer science;Debugging}, 
doi={10.1109/ASE.2004.1342755}, 
ISSN={1938-4300}, 
month={Sept},}
@INPROCEEDINGS{7880429, 
author={N. Jamous and S. Bosse and C. Grling and J. Hintsch and A. Khan and F. Kramer and H. Mller and K. Turowski}, 
booktitle={2016 4th International Conference on Enterprise Systems (ES)}, 
title={Towards an IT Service Lifecycle Management (ITSLM) Concept}, 
year={2016}, 
volume={}, 
number={}, 
pages={29-38}, 
abstract={Information Technology (IT) usage in enterprises has evolved over the last years. This led to today's complex, heterogeneous, and dynamic IT system landscapes that support business processes in enterprises. To manage these landscapes, the IT Service Management (ITSM) concept is gaining more importance in today's business and research. Studies demonstrate that introducing ITSM standards lead to positive effects, such as improved customer-orientation as well as efficiency and transparency of IT support, which justify the costs of implementation. However, companies still face difficulties in deciding which processes to be implement (first), and to which extent. Questions like: "How can the currently applied ITSM be adapted or extended when new business-related or technological challenges appear?" arise. Goods producing companies started early relying on Product Lifecycle Management (PLM). PLM delivers a solid means to define, discuss, analyze, and better standardize value creation processes. With PLM in mind, we propose a concept to adopt and further develop it towards IT Service Lifecycle Management (ITSLM) suitable for the IT services provider environment. After introducing ITSLM, analyzing its processes, and its correlation to PLM, we design ITSLM as a model-driven process support. The selection of appropriate models with different complexity can be used to implement and adapt standard supporting tasks with minimum effort. Two use cases are detailed: fault-tolerance design optimization as well as automation of IT service provisioning. In these areas, suitable model complexity levels, computer-aided task support as well as the knowledge transfer among these models are discussed.}, 
keywords={business data processing;product life cycle management;IT service lifecycle management;ITSLM;information technology;IT usage;enterprises;dynamic IT system;business processes;ITSM standards;companies;product lifecycle management;PLM;value creation processes;model-driven process support;fault-tolerance design optimization;IT service provisioning;model complexity levels;computer-aided task support;knowledge transfer;Business;Biological system modeling;Computational modeling;Adaptation models;Standards;Complexity theory;Information technology;IT Service Management (ITSM);Moddeling;Information Technology Infrastructure Library (ITIL);A Model-Driven IT Service Engineering}, 
doi={10.1109/ES.2016.10}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6080786, 
author={A. R. Yazdanshenas and L. Moonen}, 
booktitle={2011 27th IEEE International Conference on Software Maintenance (ICSM)}, 
title={Crossing the boundaries while analyzing heterogeneous component-based software systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={193-202}, 
abstract={One way to manage the complexity of software systems is to compose them from reusable components, instead of starting from scratch. Components may be implemented in different programming languages and are tied together using configuration files, or glue code, defining instantiation, initialization and interconnections. Although correctly engineering the composition and configuration of components is crucial for the overall behavior, there is surprisingly little support for incorporating this information in the static verification and validation of these systems. Analyzing the properties of programs within closed code boundaries has been studied for some decades and is well-established. This paper contributes a method to support analysis across the components of a component-based system. We build upon the Knowledge Discovery Metamodel to reverse engineer homogeneous models for systems composed of heterogeneous artifacts. Our method is implemented in a prototype tool that has been successfully used to track information flow across the components of a component-based system using program slicing.}, 
keywords={Complexity theory;Software systems;Component architectures;Computer languages;Prototypes;Knowledge engineering}, 
doi={10.1109/ICSM.2011.6080786}, 
ISSN={1063-6773}, 
month={Sept},}
@INPROCEEDINGS{6405446, 
author={G. Carrozza and M. Faella and F. Fucci and R. Pietrantuono and S. Russo}, 
booktitle={2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops}, 
title={Integrating MDT in an Industrial Process in the Air Traffic Control Domain}, 
year={2012}, 
volume={}, 
number={}, 
pages={225-230}, 
abstract={Air Traffic Control (ATC) systems are typical software-intensive mission-critical systems with stringent dependability requirements. The major providers of ATC systems are system integrators that address such requirements at the cost of a very expensive testing effort. They envisage Model Driven Testing (MDT) as a promising approach to reduce this effort while achieving better product quality. Within the context of a public-private partnership for software innovation in the ATC domain, we address the problem of integrating MDT into a software development process based on Model Driven Architecture. Specifically, we propose a solution to the integration of MDT into a V-model, focusing on a parallel MDA-MDT flow in a real industrial software process.}, 
keywords={air traffic control;formal verification;program testing;safety-critical software;software architecture;parallel MDA-MDT flow;industrial process;air traffic control;ATC system;software-intensive mission-critical system;model driven testing;public-private partnership;software innovation;software development process;model driven architecture;V-model;Unified modeling language;Software;Testing;Computer architecture;Adaptation models;Atmospheric modeling;Europe;MDA;MDT;Testing automation}, 
doi={10.1109/ISSREW.2012.87}, 
ISSN={}, 
month={Nov},}
@ARTICLE{6834762, 
author={W. Xia and Y. Wen and C. H. Foh and D. Niyato and H. Xie}, 
journal={IEEE Communications Surveys Tutorials}, 
title={A Survey on Software-Defined Networking}, 
year={2015}, 
volume={17}, 
number={1}, 
pages={27-51}, 
abstract={Emerging mega-trends (e.g., mobile, social, cloud, and big data) in information and communication technologies (ICT) are commanding new challenges to future Internet, for which ubiquitous accessibility, high bandwidth, and dynamic management are crucial. However, traditional approaches based on manual configuration of proprietary devices are cumbersome and error-prone, and they cannot fully utilize the capability of physical network infrastructure. Recently, software-defined networking (SDN) has been touted as one of the most promising solutions for future Internet. SDN is characterized by its two distinguished features, including decoupling the control plane from the data plane and providing programmability for network application development. As a result, SDN is positioned to provide more efficient configuration, better performance, and higher flexibility to accommodate innovative network designs. This paper surveys latest developments in this active research area of SDN. We first present a generally accepted definition for SDN with the aforementioned two characteristic features and potential benefits of SDN. We then dwell on its three-layer architecture, including an infrastructure layer, a control layer, and an application layer, and substantiate each layer with existing research efforts and its related research areas. We follow that with an overview of the de facto SDN implementation (i.e., OpenFlow). Finally, we conclude this survey paper with some suggested open research challenges.}, 
keywords={computer network management;Internet;software defined networking;software-defined networking;mega-trends;information and communication technologies;ICT;future Internet;ubiquitous accessibility;dynamic management;physical network infrastructure;SDN;innovative network designs;three-layer architecture;infrastructure layer;control layer;application layer;Optical switches;Routing;Software;Computer architecture;Complexity theory;Software-defined networking;SDN;network virtualization;OpenFlow}, 
doi={10.1109/COMST.2014.2330903}, 
ISSN={1553-877X}, 
month={Firstquarter},}
@INPROCEEDINGS{6716404, 
author={J. Kolek and Z. Jovanovic and N. ljivic and D. Narancic}, 
booktitle={2013 21st Telecommunications Forum Telfor (TELFOR)}, 
title={Adding microMIPS backend to the LLVM compiler infrastructure}, 
year={2013}, 
volume={}, 
number={}, 
pages={1015-1018}, 
abstract={This work describes extending of the LLVM Compiler Infrastructure with the new backend support for microMIPS, which is an architecture from MIPS family of architectures. New backend consists of 16- and 32-bit instructions, out of which 180 of 32-bit instructions are recoded MIPS32 instructions, and 14 of 32-bit instructions are new microMIPS instructions. There are the 39 highly optimized 16-bit instructions.}, 
keywords={instruction sets;program compilers;microMIPS backend;LLVM compiler infrastructure;backend support;MIPS family of architectures;16-bit instructions;32-bit instructions;Encoding;Registers;Computer architecture;Libraries;Generators;Switches;Computers;Compilers;LLVM;microMIPS}, 
doi={10.1109/TELFOR.2013.6716404}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{5615100, 
author={C. Torens and L. Ebrecht}, 
booktitle={2010 Fifth International Conference on Software Engineering Advances}, 
title={RemoteTest: A Framework for Testing Distributed Systems}, 
year={2010}, 
volume={}, 
number={}, 
pages={441-446}, 
abstract={This work deals with general difficulties and aims when testing complex distributed systems, especially when heterogeneous interfaces are used. As a solution RemoteTest is proposed, a framework for the test of distributed systems and their interfaces. This is done by integrating individual system components into a virtual environment that emulates the adjacent modules of the system. The interface details are thereby abstracted by the framework and there is no special interface knowledge necessary by the tester. In addition to the decoupling of components and interface abstraction, RemoteTest facilitates the testing of distributed systems with flexible mechanisms to write test scripts and an architecture that can be easily adapted to different systems.}, 
keywords={data structures;distributed processing;program testing;virtual reality;distributed system;RemoteTest;virtual environment;components decoupling;interface abstraction;software testing;Testing;Software;Computer architecture;Virtual environment;Hardware;Programming;Complexity theory;software test;distributed systems;test framework;test tools;test methods}, 
doi={10.1109/ICSEA.2010.75}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{271291, 
author={H. Dai and M. Choo and J. A. Starzyk}, 
booktitle={[1992] Proceedings of the 35th Midwest Symposium on Circuits and Systems}, 
title={Noninvasive voltage measurement through an on-chip test structure (IC testing)}, 
year={1992}, 
volume={}, 
number={}, 
pages={340-343 vol.1}, 
abstract={A method to evaluate internal voltages through a built-in test structure is presented. Multiplexers are used to increase accessibility. The test structure does not affect normal operation of the circuit. Individual subcircuits can be tested selectively based on evaluated internal voltages.<<ETX>>}, 
keywords={application specific integrated circuits;built-in self test;integrated circuit testing;voltage measurement;noninvasive voltage measurement;digital subcircuits;multiplexer test structure;VLSI;IC testing;analogue circuits;mixed signal testing;on-chip test structure;internal voltages;built-in test structure;Integrated circuit testing;Voltage measurement;Circuit testing;Multiplexing;Large-scale systems;MOSFET circuits;MOS capacitors;Built-in self-test;Performance evaluation;Analog circuits}, 
doi={10.1109/MWSCAS.1992.271291}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6984094, 
author={P. Costa and A. C. R. Paiva and M. Nabuco}, 
booktitle={2014 9th International Conference on the Quality of Information and Communications Technology}, 
title={Pattern Based GUI Testing for Mobile Applications}, 
year={2014}, 
volume={}, 
number={}, 
pages={66-74}, 
abstract={This paper presents a study aiming to assess the feasibility of using the Pattern Based GUI Testing approach, PBGT, to test mobile applications. PBGT is a new model based testing approach that aims to increase systematization, reusability and diminish the effort in modelling and testing. It is based on the concept of User Interface Test Patterns (UITP) that contain generic test strategies for testing common recurrent behaviour, the so-called UI Patterns, on GUIs through its possible different implementations after a configuration step. Although PBGT was developed having web applications in mind, it is possible to develop drivers for other platforms in order to test a wide set of applications. However, web and mobile applications are different and only the development of a new driver to execute test cases over mobile applications may not be enough. This paper describes a study aiming to identify the adaptations and updates the PBGT should undergo in order to test mobile applications.}, 
keywords={graphical user interfaces;Internet;mobile computing;pattern recognition;program testing;Web applications;recurrent behaviour testing;UITP;user interface test patterns;model based testing;mobile application testing;PBGT;pattern based GUI testing;Mobile communication;Testing;Graphical user interfaces;Connectors;Androids;Humanoid robots;Optical character recognition software}, 
doi={10.1109/QUATIC.2014.16}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{776011, 
author={Y. Hodge and P. Bajpay and C. -. Chao and G. Grammer and H. Kan and D. Nadle}, 
booktitle={IEEE GLOBECOM 1998 (Cat. NO. 98CH36250)}, 
title={AT amp;amp;T service maintenance platform for next century}, 
year={1998}, 
volume={6}, 
number={}, 
pages={3757-3762 vol.6}, 
abstract={With rapid deployment of new services and increasing competitive pressure have come new challenges in the telecommunications management arena. This paper presents an evolved service maintenance platform intended to streamline, simplify and automate network management operations. A unified Business Maintenance Platform (BMP) for AT&T voice and data services is a key enabler for supporting AT&T continuous commitment to quality of service (QoS). The BMP is critical to the seamless and cost effective integration of voice, data and frame relay services and provides a flexible platform to encompass local, ATM, wireless services and new services in the future.}, 
keywords={maintenance engineering;telecommunication network management;quality of service;asynchronous transfer mode;business communication;integrated voice/data communication;frame relay;telecommunication computing;AT&T service maintenance platform;telecommunications management;telecommunication services;automated network management;Business Maintenance Platform;data services;voice services;quality of service;QoS;frame relay services;local services;wireless services;ATM services;SONET;GUI;Quality of service;Costs;Frame relay;Asynchronous transfer mode;Customer service;SONET;Availability;Chaos;Laboratories;Pressing}, 
doi={10.1109/GLOCOM.1998.776011}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7102608, 
author={S. H. Jensen and S. Thummalapenta and S. Sinha and S. Chandra}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Test Generation from Business Rules}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Enterprise applications are difficult to test because their intended functionality is either not described precisely enough or described in cumbersome business rules. It takes a lot of effort on the part of a test architect to understand all the business rules and design tests that "cover" them, i.e., exercise all their constituent scenarios. Part of the problem is that it takes a complicated set up sequence to drive an application to a state in which a business rule can even fire. In this paper, we present a business rule modeling language that can be used to capture functional specification of an enterprise system. The language makes it possible to build tool support for rule authoring, so that obvious deficiencies in rules can be detected mechanically. Most importantly, we show how to mechanically generate test sequences--i.e., test steps and test data--needed to exercise these business rules. To this end, we translate the rules into logical formulae and use constraint solving to generate test sequences. One of our contributions is to overcome scalability issues in this process, and we do this by using a novel algorithm for organizing search through the space of candidate sequences to discover covering sequences. Our results on three case studies show the promise of our approach.}, 
keywords={business data processing;formal specification;program testing;specification languages;test generation;test architect;design test;business rule modeling language;functional specification;enterprise system;rule authoring;logical formulae;constraint solving;test sequence;candidate sequence;covering sequence;Business;Databases;Testing;Syntactics;Algorithm design and analysis;Systematics;Context}, 
doi={10.1109/ICST.2015.7102608}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{6188784, 
author={}, 
booktitle={2012 International Conference on Devices, Circuits and Systems (ICDCS)}, 
title={2012 International Conference on Devices, Circuits and Systems (ICDCS)}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-748}, 
abstract={Provides the entire conference content.}, 
keywords={CMOS integrated circuits;CMOS technology;MESFETs;MOSFETs;Optical imaging;Random access memory;Field programmable gate arrays}, 
doi={10.1109/ICDCSyst.2012.6188784}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7338253, 
author={R. Rodriguez-Echeverria and F. Macias}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={A statistical analysis approach to assist model transformation evolution}, 
year={2015}, 
volume={}, 
number={}, 
pages={226-235}, 
abstract={Model Driven Engineering (MDE) is essentially based in metamodel definition, model edition and the specification of model transformations (MT) among these. In many cases the development, evolution and adaptation of these transformations is still carried out without the support of proper methods and tools to reduce the effort and related costs to these activities. In this work, a novel model testing approach specifically designed to assist the engineer in model transformation evolution is presented. A statistical analysis of the actual behavior of the transformations is performed by means of the computation of well-known information extraction metrics. In order to assist the MT adaptation, a detailed interpretation of the possible results of those metrics is also presented. And finally, the results of applying this approach on a Model-Driven Reverse Engineering (MDRE) scenario defined in the context of the MIGRARIA project are discussed.}, 
keywords={formal specification;program testing;reverse engineering;software metrics;statistical analysis;statistical analysis approach;metamodel definition;model edition;model transformation specification;cost reduction;model testing approach;information extraction metrics;MT adaptation;model-driven reverse engineering scenario;MDRE scenario;MIGRARIA project;software metrics;Adaptation models;Concrete;Contracts;Testing;Context modeling;Measurement;Unified modeling language;Model Transformation;Model Transformation Evolution;Model Transformation Testing;Testing Oracle}, 
doi={10.1109/MODELS.2015.7338253}, 
ISSN={}, 
month={Sept},}
@ARTICLE{8456508, 
author={Y. Yu and X. Li and X. Leng and L. Song and K. Bu and Y. Chen and J. Yang and L. Zhang and K. Cheng and X. Xiao}, 
journal={IEEE Communications Surveys Tutorials}, 
title={Fault Management in Software-Defined Networking: A Survey}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Software-defined networking (SDN) has emerged as a new network paradigm that promises control/data plane separation and centralized network control. While these features simplify network management and enable innovative networking, they give rise to persistent concerns about reliability. The new paradigm suffers from the disadvantage that various network faults may consistently undermine the reliability of such a network, and such faults are often new and difficult to resolve with existing solutions. To ensure SDN reliability, fault management, which is concerned with detecting, localizing, correcting and preventing faults, has become a key component in SDN networks. Although many SDN fault management solutions have been proposed, we find that they often resolve SDN faults from an incomplete perspective which may result in side effects. More critically, as the SDN paradigm evolves, additional fault types are being exposed. Therefore, comprehensive reviews and constant improvements are required to remain on the leading edge of SDN fault management. In this paper, we present the first comprehensive and systematic survey of SDN faults and related management solutions identified through advancements in both the research community and industry. We apply a systematic classification of SDN faults, compare and analyze existing SDN fault management solutions in the literature, and conduct a gap analysis between solutions developed in an academic research context and practical deployments. The current challenges and emerging trends are also noted as potential future research directions. This paper aims to provide academic researchers and industrial engineers with a comprehensive survey with the hope of advancing SDN and inspiring new solutions.}, 
keywords={Software;Hardware;Industries;Fault tolerance;Fault tolerant systems;Computer architecture;Software-defined networking (SDN);SDN reliability;SDN faults;fault classification;system monitoring;fault diagnosis;fault recovery and repair;fault tolerance.}, 
doi={10.1109/COMST.2018.2868922}, 
ISSN={1553-877X}, 
month={},}
@INPROCEEDINGS{4063813, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={1}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284569}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4064168, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={3}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284942}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4063976, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={2}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284735}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4064349, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={4}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.285102}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7927913, 
author={}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Table of contents}, 
year={2017}, 
volume={}, 
number={}, 
pages={v-x}, 
abstract={The following topics are dealt with: Presents the table of contents/splash page of the proceedings record.}, 
keywords={mobile computing;parallel processing;program testing;security of data;software engineering;software testing;software verification;software validation;fault localization;fault injection;program debugging;complexity analysis;composite faults;security testing;regression testing;Web applications;mobile applications;parallel systems;concurrency;model-based testing;automated testing;run-time testing;search-based testing;model checking;white box testing;DSL-based testing;code analysis;dynamic analysis}, 
doi={10.1109/ICST.2017.4}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5501146, 
author={}, 
booktitle={2009 5th Central and Eastern European Software Engineering Conference in Russia (CEE-SECR)}, 
title={[Title page]}, 
year={2009}, 
volume={}, 
number={}, 
pages={i-ii}, 
abstract={The following topics are dealt with: crisis-time distributed systems development; regression test selection technique; agile project management; software project feasibility study; graphical processing units; industrial C/C++ software; video registration and security systems; reliable software development; industrial Java applications; parallel programs; e-government and outsourcing; program reliability; operation-friendly software development; software product management; SaaS concept; SOA testing stack; complex hardware-software systems; UML-model; Microsoft DSL technology; Microsoft.NET micro framework; WBEM/CIM &amp; WS-MAN technology application; and agile Web development.}, 
keywords={C++ listings;government data processing;Java;outsourcing;regression analysis;software development management;software prototyping;software reliability;Unified Modeling Language;distributed systems development;regression test selection technique;agile project management;software project feasibility;graphical processing units;industrial C++ software;video registration;security systems;reliable software development;industrial Java applications;parallel programs;e-government;outsourcing;program reliability;operation-friendly software development;software product management;SaaS concept;SOA testing stack;hardware-software systems;UML-model;Microsoft.NET;Microsoft DSL technology;WS-MAN technology;agile Web development}, 
doi={10.1109/CEE-SECR.2009.5501146}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8449407, 
author={}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={Table of contents}, 
year={2018}, 
volume={}, 
number={}, 
pages={5-26}, 
abstract={Presents the table of contents/splash page of the proceedings record.}, 
keywords={}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@ARTICLE{8472900, 
author={P. Duan and Y. Zhou and X. Gong and B. Li}, 
journal={IEEE Access}, 
title={A Systematic Mapping Study on the Verification of Cyber-Physical Systems}, 
year={2018}, 
volume={6}, 
number={}, 
pages={59043-59064}, 
abstract={Cyber-physical system (CPS) is a kind of complex real-time hybrid system which involves deep interactions between computation processors, communication network, and physical environments are deemed as the key enablers of next generation computer applications. However, how to verify CPS effectively is always a great challenge. Based on current scientific works about CPS verification, this paper aims at identifying the gap of current studies and suggesting promising areas for the future works. For this purpose, we conduct a systematic mapping study over the topic on verification of cyber-physical system. We carry out a widely search of publications from 2006 to 2018 in 11 electronic databases. After the step of study selection, 80 papers are selected as primary studies for answering proposed research questions, focused questions, and statistical questions. According to these questions and their answers, this paper not only presents a quantitative and comprehensive analysis of verification challenges, abstraction methods, verification techniques, assistance tools, and verification scenarios that represent each step of verification works, but also summarizes CPS systematic natures, main routine of verification and future research directions. We believe that this survey can identify gaps in current research works and reveal new insights for the future works.}, 
keywords={Systematics;Cyber-physical systems;Tools;Databases;Guidelines;Sociology;Statistics;Systematic mapping study;verification of cyber-physical system;verification challenges;abstraction methods;verification techniques;assistance tools;verification scenarios}, 
doi={10.1109/ACCESS.2018.2872015}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8449448, 
author={C. Krher and S. El-Sharkawy and K. Schmid}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={KernelHaven  An Experimentation Workbench for Analyzing Software Product Lines}, 
year={2018}, 
volume={}, 
number={}, 
pages={73-76}, 
abstract={Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem. KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.}, 
keywords={Data mining;Pipelines;Data models;Feature extraction;Tools;Analytical models;Software engineering;Software product line analysis;variability extraction;static analysis;empirical software engineering}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}

% Encoding: UTF-8
@article{OTADUY2017212,
title = "User acceptance testing for Agile-developed web-based applications: Empowering customers through wikis and mind maps",
journal = "Journal of Systems and Software",
volume = "133",
pages = "212 - 229",
year = "2017",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.01.002",
url = "http://www.sciencedirect.com/science/article/pii/S016412121730002X",
author = "I. Otaduy and O. Diaz",
keywords = "Agile development, User acceptance testing, Test automation",
abstract = "User Acceptance Testing (UAT) involves validating software in a real setting by the intended audience. The aim is not so much to check the defined requirements but to ensure that the software satisfies the customers needs. Agile methodologies put stringent demands on UAT, if only for the frequency at which it needs to be conducted due to the iterative development of small product releases. In this setting, traditional in-person meetings might not scale up well. Complementary ways are needed to reduce the costs of developer-customer collaboration during UAT. This work introduces a wiki-based approach where customers and developers asynchronously collaborate: developers set the UAT scaffolding that will later shepherd customers when testing. To facilitate understanding, mind maps are used to represent UAT sessions. To facilitate engagement, a popular mind map editor, FreeMind, is turned into an editor for FitNesse, the wiki engine in which these ideas are borne out. The approach is evaluated through a case study involving three real customers. First evaluations are promising. Though at different levels of completeness, the three customers were able to complete a UAT. Customers valued asynchronicity, mind map structuredness, and the transparent generation of documentation out of the UAT session."
}
@article{TICHY2017159,
title = "Rapid Continuous Software Engineering",
journal = "Journal of Systems and Software",
volume = "133",
pages = "159",
year = "2017",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.08.046",
url = "http://www.sciencedirect.com/science/article/pii/S0164121217301887",
author = "Matthias Tichy and Michael Goedicke and Jan Bosch and Brian Fitzgerald"
}
@article{BRIENZA2014124,
title = "3T diffusion tensor imaging and electroneurography of peripheral nerve: A morphofunctional analysis in carpal tunnel syndrome",
journal = "Journal of Neuroradiology",
volume = "41",
number = "2",
pages = "124 - 130",
year = "2014",
issn = "0150-9861",
doi = "https://doi.org/10.1016/j.neurad.2013.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S0150986113000540",
author = "Marianna Brienza and Francesco Pujia and M. Chiara Colaiacomo and M. Grazia Anastasio and Francesco Pierelli and Claudio Di Biasi and Chiara Andreoli and Gianfranco Gualdi and Gabriele O.R. Valente",
keywords = "Diffusion tensor imaging, Median nerve, Fractional anisotropy, Apparent diffusion coefficient, Electroneurography",
abstract = "Summary
Objective
The aim of the study was to assess the diagnostic potential of diffusion tensor imaging (DTI) for pathologies of the peripheral nervous system (PNS) through clinical, electrophysiological and morphological evaluation of the median nerve.
Methods
The present work was a multilevel prospective study involving 30 subjects, 15 of whom had carpal tunnel syndrome (CTS) and 15 healthy controls. All subjects underwent clinical evaluation through administration of the Boston Carpal Tunnel Questionnaire (BCTQ), electroneurography (ENG), 3-Tesla magnetic resonance imaging with DTI, and calculation of fractional anisotropy (FA) and the apparent diffusion coefficient (ADC) at the flexor retinaculum. Tractography was also performed for three-dimensional reconstruction of the route of the median nerve through the carpal tunnel. The degree of functional impairment was compared with the anatomical damage to the median nerve according to ENG and DTI.
Results
FA and ADC were significantly correlated with ENG parameters of CTS and BCTQ data. Mean FA and ADC values in the CTS patients were 0.3590.06 and 1.8660.050103mm2/s, respectively, vs 0.590.014 and 1.3950.035103mm2/s, respectively, in the controls. FA was decreased and ADC increased in patients with CTS compared with healthy controls (P<0.05).
Conclusion
DTI parameters were clearly confirmed by both clinical and ENG data and, therefore, may be used for the diagnosis of CTS."
}
@incollection{BIALY201739,
title = "3 - Software Engineering for Model-Based Development by Domain Experts",
editor = "Edward Griffor",
booktitle = "Handbook of System Safety and Security",
publisher = "Syngress",
address = "Boston",
pages = "39 - 64",
year = "2017",
isbn = "978-0-12-803773-7",
doi = "https://doi.org/10.1016/B978-0-12-803773-7.00003-6",
url = "http://www.sciencedirect.com/science/article/pii/B9780128037737000036",
author = "M. Bialy and V. Pantelic and J. Jaskolka and A. Schaap and L. Patcas and M. Lawford and A. Wassyng",
keywords = "Software engineering, domain experts, functional safety, embedded software, model-based development, Simulink, automotive",
abstract = "Model-Based Development (MBD) has been gaining traction in the development of embedded software in many industries, especially in safety-critical domains. The models are typically described using domain-specific languages and tools that are readily accessible to the domain experts. Consequently, domain experts, despite not having formal software engineering training, find themselves creating models (designs) from which code is generated, thus effectively contributing to the design and coding activities of software development. This new role for domain experts as software developers can have a direct impact on the system safety if the domain experts do not follow software engineering best practices. In this chapter, we describe our experiences as software engineers in multiyear collaborations with domain experts from the automotive industry, who are developing embedded software with the MBD approach. We provide guidelines that strengthen the collaboration between domain experts and software engineers and improve the quality, and hence safety, of embedded software systems developed using MBD. We clarify the role of some of the most commonly used software engineering principles and artefacts, while also addressing issues and misconceptions encountered in adopting software engineering practices in MBD. Although this chapter focuses on the MBD of automotive embedded software in Matlab Simulink, the guidelines we provide are applicable to the MBD of software in general."
}
@article{ZHU2015447,
title = "Serum DHEAS levels are associated with the development of depression",
journal = "Psychiatry Research",
volume = "229",
number = "1",
pages = "447 - 453",
year = "2015",
issn = "0165-1781",
doi = "https://doi.org/10.1016/j.psychres.2015.05.093",
url = "http://www.sciencedirect.com/science/article/pii/S0165178115004084",
author = "Guang Zhu and You Yin and Chun-Lan Xiao and Rong-Jie Mao and Bo-Hai Shi and Yong Jie and Zuo-Wei Wang",
keywords = "Dehydroepiandrosterone sulfate, Depression, Serum levels, Casecontrol study, Meta-analysis",
abstract = "The aim of study was to evaluate the association between serum DHEAS levels and depression with a casecontrol study together with a meta-analysis. Radioimmunoassay (RIA) was performed to measure the serum DHEAS levels of all participants before and after treatment. Depression Patients were divided into mild depression and severe depression based on Hamilton depression scale (HAMD24) and received 5-hydroxytryptamine (5-HT) and citalopram (20mg/d) for 8 weeks. Casecontrol studies related to our study theme were enrolled for meta-analysis and Comprehensive Meta-analysis 2.0 (CMA 2.0) was used for statistical analysis. After treatment, DHEAS levels in depression patients were significantly increased, while before and after treatment, DHEAS levels were all lower in depression patients than in controls (all P<0.001); further analysis on age revealed that DHEAS levels were decreased with the rising of age. Meta-analysis results suggested that serum DHEAS levels (ng/mL) were significantly higher in healthy controls compared to depression patients (SMD=0.777, 95%CI=0.1561.399, P=0.014). In conclusion, our study suggests that serum DHEAS levels are associated with the development of depression and it decreased with the rising of age."
}
@incollection{UTTING201653,
title = "Chapter Two - Recent Advances in Model-Based Testing",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "101",
pages = "53 - 120",
year = "2016",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2015.11.004",
url = "http://www.sciencedirect.com/science/article/pii/S0065245815000650",
author = "Mark Utting and Bruno Legeard and Fabrice Bouquet and Elizabeta Fourneret and Fabien Peureux and Alexandre Vernotte",
keywords = "Model-based testing, Modeling approaches, Test generation Technology, Security testing",
abstract = "This chapter gives an overview of the field of model-based testing (MBT), particularly the recent advances in the last decade. It gives a summary of the MBT process, the modeling languages that are currently used by the various communities who practice MBT, the technologies used to generate tests from models, and discusses best practices, such as traceability between models and tests. It also briefly describes several findings from a recent survey of MBT users in industry, outlines the increasingly popular use of MBT for security testing, and discusses future challenges for MBT."
}
@article{GRAUPP2011175,
title = "Association of genetic variants in the two isoforms of 5-reductase, SRD5A1 and SRD5A2, in lean patients with polycystic ovary syndrome",
journal = "European Journal of Obstetrics & Gynecology and Reproductive Biology",
volume = "157",
number = "2",
pages = "175 - 179",
year = "2011",
issn = "0301-2115",
doi = "https://doi.org/10.1016/j.ejogrb.2011.03.026",
url = "http://www.sciencedirect.com/science/article/pii/S0301211511001849",
author = "M. Graupp and E. Wehr and N. Schweighofer and T.R. Pieber and B. Obermayer-Pietsch",
keywords = "Polycystic ovary syndrome, 5-Reductase, Polymorphisms, Hyperandrogenemia",
abstract = "Objective
Given its role in converting testosterone to dihydrotestosterone and cortisol to dihydrocortisol, 5-reductase may be important in the pathophysiology of the polycystic ovary syndrome (PCOS). Increased activity of this enzyme has already been demonstrated in ovaries of affected women, and might be caused by genetic alterations. The aim of this study was to analyze representative genetic variants of both isoforms of 5-reductase with regard to PCOS parameters in lean and obese women.
Study design
We analyzed one single nucleotide polymorphism (SNP) (rs523349) of the isoform 2 (SRD5A2) and one haplotype of the isoform 1 (SRD5A1), consisting of the two SNPs rs39848 and rs3797179, in 249 women with PCOS and 226 healthy women using a 5-exonuclease-assay. The genotypes were associated with anthropometric, metabolic and hormonal as well as functional tests in these women.
Results
In the investigated haplotype of SRD5A1, the TA variant was associated with an increased frequency of PCOS (P=0.022) and an increased FerrimanGallwey Score (hirsutism) (P=0.016) in women with normal weight. The G allele at the examined position of the SRD5A2 showed a decreased frequency of PCOS (P=0.03) in women with normal weight.
Conclusion
One of the keys in the development of the PCOS is hyperandrogenism, which might be caused by an increased 5-reductase activity, as it is often seen in obesity. This mechanism might therefore be of importance in lean PCOS patients and contribute to the clinical findings."
}
@incollection{FELDERER20161,
title = "Chapter One - Security Testing: A Survey",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "101",
pages = "1 - 51",
year = "2016",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2015.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S0065245815000649",
author = "Michael Felderer and Matthias Bchler and Martin Johns and Achim D. Brucker and Ruth Breu and Alexander Pretschner",
keywords = "Security testing, Security testing techniques, Model-based security testing, White-box security testing, Black-box security testing, Penetration testing, Security regression testing, Security engineering, Software testing, Survey",
abstract = "Identifying vulnerabilities and ensuring security functionality by security testing is a widely applied measure to evaluate and improve the security of software. Due to the openness of modern software-based systems, applying appropriate security testing techniques is of growing importance and essential to perform effective and efficient security testing. Therefore, an overview of actual security testing techniques is of high value both for researchers to evaluate and refine the techniques and for practitioners to apply and disseminate them. This chapter fulfills this need and provides an overview of recent security testing techniques. For this purpose, it first summarize the required background of testing and security engineering. Then, basics and recent developments of security testing techniques applied during the secure software development life cycle, ie, model-based security testing, code-based testing and static analysis, penetration testing and dynamic analysis, as well as security regression testing are discussed. Finally, the security testing techniques are illustrated by adopting them for an example three-tiered web-based business application."
}
@article{MOHIYIDDEEN2012677,
title = "Follicle-stimulating hormone receptor gene polymorphisms are not associated with ovarian reserve markers",
journal = "Fertility and Sterility",
volume = "97",
number = "3",
pages = "677 - 681",
year = "2012",
issn = "0015-0282",
doi = "https://doi.org/10.1016/j.fertnstert.2011.12.040",
url = "http://www.sciencedirect.com/science/article/pii/S0015028211029244",
author = "Lamiya Mohiyiddeen and William G. Newman and Helen McBurney and Betselot Mulugeta and Stephen A. Roberts and Luciano G. Nardo",
keywords = "Single nucleotide polymorphism, Ser680Asn, Thr307Ala FSH receptor, antimllerian hormone, antral follicle count, follicle-stimulating hormone",
abstract = "Objective
To evaluate the association between variants in the FSHR receptor (FSHR) gene and current markers of ovarian reserve (antimllerian hormone, antral follicle count, FSH).
Design
Prospective observational study.
Setting
Tertiary referral center for reproductive medicine.
Patient(s)
Women (n = 421) undergoing their first cycle of controlled ovarian stimulation for IVF.
Intervention(s)
Baseline pelvic ultrasound and blood tests were taken on day 23 of the cycle for assessment of baseline hormones and for DNA extraction. Genotypes for FSHR p.Asn680Ser and p.Thr307Ala variants were determined using TaqMan allelic discrimination assays.
Main Outcome Measure(s)
Association of FSHR single nucleotide polymorphisms with markers of ovarian reserve.
Result(s)
There was no evidence of any difference in basal FSH, antimllerian hormone, or antral follicle count between the patients with different genotypes, with or without an adjustment for age or body mass index.
Conclusion(s)
No associations of FSHR genotypes with markers of ovarian reserve were detected in our cohort."
}
@article{WANG2011181,
title = "A Modeling Language Based on UML for Modeling Simulation Testing System of Avionic Software",
journal = "Chinese Journal of Aeronautics",
volume = "24",
number = "2",
pages = "181 - 194",
year = "2011",
issn = "1000-9361",
doi = "https://doi.org/10.1016/S1000-9361(11)60022-8",
url = "http://www.sciencedirect.com/science/article/pii/S1000936111600228",
author = "Lize WANG and Bin LIU and Minyan LU",
keywords = "avionics, hardware-in-the-loop, test facilities, meta-model, UML profile, domain-specific modeling language, abstract state machine",
abstract = "With direct expression of individual application domain patterns and ideas, domain-specific modeling language (DSML) is more and more frequently used to build models instead of using a combination of one or more general constructs. Based on the profile mechanism of unified modeling language (UML) 2.2, a kind of DSML is presented to model simulation testing systems of avionic software (STSAS). To define the syntax, semantics and notions of the DSML, the domain model of the STSAS from which we generalize the domain concepts and relationships among these concepts is given, and then, the domain model is mapped into a UML meta-model, named UML-STSAS profile. Assuming a flight control system (FCS) as system under test (SUT), we design the relevant STSAS. The results indicate that extending UML to the simulation testing domain can effectively and precisely model STSAS."
}
@article{SYRIANI201843,
title = "Systematic mapping study of template-based code generation",
journal = "Computer Languages, Systems & Structures",
volume = "52",
pages = "43 - 62",
year = "2018",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2017.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S1477842417301239",
author = "Eugene Syriani and Lechanceux Luhunu and Houari Sahraoui",
keywords = "Code generation, Systematic mapping study, Model-driven engineering",
abstract = "Context: Template-based code generation (TBCG) is a synthesis technique that produces code from high-level specifications, called templates. TBCG is a popular technique in model-driven engineering (MDE) given that they both emphasize abstraction and automation. Given the diversity of tools and approaches, it is necessary to classify existing TBCG techniques to better guide developers in their choices. Objective: The goal of this article is to better understand the characteristics of TBCG techniques and associated tools, identify research trends, and assess the importance of the role of MDE in this code synthesis approach. Method: We survey the literature to paint an interesting picture about the trends and uses of TBCG in research. To this end, we follow a systematic mapping study process. Results: Our study shows, among other observations, that the research community has been diversely using TBCG over the past 16 years. An important observation is that TBCG has greatly benefited from MDE. It has favored a template style that is output-based and high-level modeling languages as input. TBCG is mainly used to generate source code and has been applied to many domains. Conclusion: TBCG is now a mature technique and much research work is still conducted in this area. However, some issues remain to be addressed, such as support for template definition and assessment of the correctness and quality of the generated code."
}
@article{STURM20141390,
title = "Evaluating the productivity of a reference-based programming approach: A controlled experiment",
journal = "Information and Software Technology",
volume = "56",
number = "10",
pages = "1390 - 1402",
year = "2014",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.05.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914001062",
author = "Arnon Sturm and Oded Kramer",
keywords = "Productivity, Programming, Software reusability, Software quality, Domain engineering",
abstract = "Context
Domain engineering aims at facilitating software development in an efficient and economical way. One way to measure that is through productivity indicators, which refer to the ability of creating a quality software product in a limited period and with limited resources. Many approaches have been devised to increase productivity; however, these approaches seem to suffer from a tension between expressiveness on the one hand, and applicability (or the lack of it) in providing guidance for developers.
Objective
This paper evaluates the applicability and efficiency of adopting a domain engineering approach, called Application-based DOmain Modeling (ADOM), in the context of the programming task with Java, and thus termed ADOM-Java, for improving productivity in terms of code quality and development time.
Method
To achieve that objective we have qualitatively evaluate the approach using questionnaires and following a text analysis procedure. We also set a controlled experiment in which 50 undergraduate students performed a Java-based programming task using either ADOM-Java or Java alone.
Results
The qualitative evaluation reveal that the approach is easy to uses and provides valuable guidance. Nevertheless, it requires training. The outcomes of the experiment indicate that the approach is applicable and that the students that used ADOM-Java achieved better code quality, as well as better functionality and within less time than the students who used only Java.
Conclusion
The results of the experiments imply that by providing a code base equipped with reuse guidelines for programmers can increase programming productivity in terms of quality and development time. These guidelines may also enforce coding standards and architectural design."
}
@article{BASSO2016612,
title = "Automated design of multi-layered web information systems",
journal = "Journal of Systems and Software",
volume = "117",
pages = "612 - 637",
year = "2016",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2016.04.060",
url = "http://www.sciencedirect.com/science/article/pii/S0164121216300358",
author = "Fbio Paulo Basso and Raquel Mainardi Pillat and Toacy Cavalcante Oliveira and Fabricia Roos-Frantz and Rafael Z. Frantz",
keywords = "Model-driven web engineering, Rapid application prototype, Domain-specific language, Prototyping, Automated design, Mockup, Experience report",
abstract = "In the development of web information systems, design tasks are commonly used in approaches for Model-Driven Web Engineering (MDWE) to represent models. To generate fully implemented prototypes, these models require a rich representation of the semantics for actions (e.g., database persistence operations). In the development of some use case scenarios for the multi-layered development of web information systems, these design tasks may consume weeks of work even for experienced designers. The literature pointed out that the impossibility for executing a software project with short iterations hampers the adoption of some approaches for design in some contexts, such as start-up companies. A possible solution to introduce design tasks in short iterations is the use of automated design techniques, which assist the production of models by means of transformation tasks and refinements. This paper details our methodology for MDWE, which is supported by automated design techniques strictly associated with use case patterns of type CRUD. The novelty relies on iterations that are possible for execution with short time-scales. This is a benefit from automated design techniques not observed in MDWE approaches based on manual design tasks. We also report on previous experiences and address open questions relevant for the theory and practice of MDWE."
}
@article{GAROUSI20102251,
title = "A replicated survey of software testing practices in the Canadian province of Alberta: What has changed from 2004 to 2009?",
journal = "Journal of Systems and Software",
volume = "83",
number = "11",
pages = "2251 - 2262",
year = "2010",
note = "Interplay between Usability Evaluation and Software Development",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2010.07.012",
url = "http://www.sciencedirect.com/science/article/pii/S0164121210001834",
author = "Vahid Garousi and Tan Varma",
keywords = "Survey, Software testing practices, Canada",
abstract = "Software organizations have typically de-emphasized the importance of software testing. In an earlier study in 2004, our colleagues reported the results of an Alberta-wide regional survey of software testing techniques in practice. Five years after that first study, the authors felt it is time to replicate the survey and analyze what has changed and what not from 2004 to 2009. This study was conducted during the summer of 2009 by surveying software organizations in the Canadian province of Alberta. The survey results reveal important and interesting findings about software testing practices in Alberta, and point out what has changed from 2004 to 2009 and what not. Note that although our study is conducted in the province of Alberta, we have compared the results to few international similar studies, such as the ones conducted in the US, Turkey, Hong Kong and Australia, The study should thus be of interest to all testing professionals world-wide. Among the findings are the followings: (1) almost all companies perform unit and system testing with a slight increase since 2004, (2) automation of unit, integration and systems tests has increased sharply since 2004, (3) more organization are using observations and expert opinion to conduct usability testing, (4) the choices of test-case generation mechanisms have not changed much from 2004, (5) JUnit and IBM Rational tools are the most widely used test tools, (6) Alberta companies still face approximately the same defect-related economic issues as do companies in other jurisdictions, (7) Alberta software firms have improved their test automation capability since 2004, but there is still some room for improvement, and (8) compared to 2004, more companies are spending more effort on pre-release testing."
}
@article{CORONA2010574,
title = "Liquid chromatography tandem mass spectrometry assay for fast and sensitive quantification of estrone-sulfate",
journal = "Clinica Chimica Acta",
volume = "411",
number = "7",
pages = "574 - 580",
year = "2010",
issn = "0009-8981",
doi = "https://doi.org/10.1016/j.cca.2010.01.019",
url = "http://www.sciencedirect.com/science/article/pii/S000989811000032X",
author = "Giuseppe Corona and Caterina Elia and Bruno Casetta and Alessandro Da Ponte and Lino Del Pup and Enzo Ottavian and Giuseppe Toffoli",
keywords = "Estrone-sulfate, LCMS/MS, Estrogen, Menopausal, Estrogen suppression, Aromatase inhibitors",
abstract = "Background
The circulating pool of estrone-sulfate is considered as a reservoir of slowly-metabolized estrogen that can be exploited for assessing overall individual estrogenicity. The aim of this study was to develop a rapid and sensitive liquid chromatographytandem mass spectrometry assay for the determination of estrone-sulfate, suitable for routine clinical investigations.
Methods
The proposed assay is based on a simple protein precipitation procedure and on a fast measurement with a triplequadrupole mass spectrometer operating in negative ion mode and in multiple reaction monitoring. The method was assessed for intra- and inter-day precision, accuracy, recovery, and clinical suitability. A comparison with available radioimmunoassay was also performed.
Results
The LCMS/MS method is able to detect estrone-sulfate concentrations 1pg/mL and has a low limit of quantification of 7.8pg/mL. Intra- and inter-day precision and accuracy were less than 10.5% and 5.0% respectively. The recovery was in the range of 93%110%. When compared with radioimmunoassay the method resulted more accurate and therefore more suitable for quantifying the estrone-sulfate in different clinical settings, including patients treated with aromatase inhibitors.
Conclusions
The proposed LCMS/MS method represents a convincing alternative to the immunoassay for a fast, cost-effective and reliable measurement of estrone-sulfate in routine clinical investigations and in large epidemiological studies. It may contribute in shedding a new light on the diagnostic value of estrone-sulfate in normal and pathological conditions."
}
@article{DAMOTASILVEIRANETO2011407,
title = "A systematic mapping study of software product lines testing",
journal = "Information and Software Technology",
volume = "53",
number = "5",
pages = "407 - 423",
year = "2011",
note = "Special Section on Best Papers from XP2010",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2010.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584910002193",
author = "Paulo Anselmo da Mota Silveira Neto and Ivan do Carmo Machado and John D. McGregor and Eduardo Santana de Almeida and Silvio Romero de Lemos Meira",
keywords = "Software product lines, Software testing, Mapping study",
abstract = "Context
In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development.
Objective
This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature.
Method
A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated.
Results
Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed.
Conclusion
The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet."
}
@article{ELALLAOUI2016221,
title = "Automated Model Driven Testing Using AndroMDA and UML2 Testing Profile in Scrum Process",
journal = "Procedia Computer Science",
volume = "83",
pages = "221 - 228",
year = "2016",
note = "The 7th International Conference on Ambient Systems, Networks and Technologies (ANT 2016) / The 6th International Conference on Sustainable Energy Information Technology (SEIT-2016) / Affiliated Workshops",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2016.04.119",
url = "http://www.sciencedirect.com/science/article/pii/S1877050916301429",
author = "Meryem Elallaoui and Khalid Nafil and Raja Touahni and Rochdi Messoussi",
keywords = "Testing, Model-Driven Testing, UML2, UML2 Testing Profile (U2TP), MDA",
abstract = "Software testing is an important step in the life cycle of agile development; it represents an efficient way to ensure the good functioning of the product. But as the complexity of a system increases, the effort and expertise to test it also increases. To significantly reduce these efforts, and reduce the cost and time; several studies have been carried out and various tools and test automation techniques have been proposed. In this paper, we present an approach to automatic generation of test cases from UML 2 Models at the Scrum agile process. This approach automates two important steps: the transformation of design models into test models and generating test cases, based on an open source MDA framework."
}
@incollection{WILLIAMS2014201,
title = "Chapter 10 - Change Management",
editor = "Timothy J. Shimeall and Jonathan M. Spring",
booktitle = "Introduction to Information Security",
publisher = "Syngress",
address = "Boston",
pages = "201 - 231",
year = "2014",
isbn = "978-1-59749-969-9",
doi = "https://doi.org/10.1016/B978-1-59749-969-9.00010-9",
url = "http://www.sciencedirect.com/science/article/pii/B9781597499699000109",
author = "James G. Williams",
keywords = "change management, configuration management, configuration management database (CMDB), change automation, configuration automation, information assurance, Alan Turing, SNMP, Chef, CFEngine, automation systems, change process, patch management, system maintenance, certifications, change types, critical change, major change, minor change, insignificant change",
abstract = "This chapter covers the related but distinct fields of change and configuration management. The chapter introduces the two topics, why they are useful and important, and how they fit in the project management life cycle. The two fields are distinguished, and then each is described in detail. Change management topics discussed are phases of the process, differential processes for different severities of changes (e.g., critical, minor, or insignificant), and how change management directly bears on security. Automation and documentation of change management processes are also discussed. Configuration management begins with an item that straddles change and configuration management: software patch management. Configuration management is then discussed in the context of management systems, configuring software, configuring network devices, configuration information-assurance critical items, configuration and system maintenance, and finally configuration management databases for tracking the various configurations. The chapter concludes with the relevant change and configuration management certification bodies."
}
@incollection{EELES20141,
title = "Chapter 1 - Relating System Quality and Software Architecture: Foundations and Approaches",
editor = "Ivan Mistrik and Rami Bahsoon and Peter Eeles and Roshanak Roshandel and Michael Stal",
booktitle = "Relating System Quality and Software Architecture",
publisher = "Morgan Kaufmann",
address = "Boston",
pages = "1 - 20",
year = "2014",
isbn = "978-0-12-417009-4",
doi = "https://doi.org/10.1016/B978-0-12-417009-4.00001-6",
url = "http://www.sciencedirect.com/science/article/pii/B9780124170094000016",
author = "Peter Eeles and Rami Bahsoon and Ivan Mistrik and Roshanak Roshandel and Michael Stal",
keywords = "Quality Attributes, System quality, Architecture, Assessment, Documentation, Design, Development Process, Lifecycle Approach, Scenario",
abstract = "The field of software architecture has gone through significant evolution over the past two decades. Early research in software architecture focused on technological contributions such as the modeling of structural and behavioral properties of software systems. Automated analysis of these models resulted in the development of tools and approaches aimed at ensuring a systems functional and nonfunctional properties such as performance, interoperability, and schedulability. More recently, however, software architecture research has shifted in fundamental ways. The emphasis on capturing design decisions and their relationship to both a software systems requirements and its implementation is predominant. The synergy between the design decisions captured in the software architecture and system quality is the primary motivation behind this book."
}
@article{HAVEMAN2013293,
title = "Requirements for High Level Models Supporting Design Space Exploration in Model-based Systems Engineering",
journal = "Procedia Computer Science",
volume = "16",
pages = "293 - 302",
year = "2013",
note = "2013 Conference on Systems Engineering Research",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.01.031",
url = "http://www.sciencedirect.com/science/article/pii/S187705091300032X",
author = "Steven P. Haveman and G. Maarten Bonnema",
keywords = "Model-based systems engineering, systems engineering challenges, high level models, communication, design space exploration, design trade-offs",
abstract = "Most formal models are used in detailed design and focus on a single domain. Few effective approaches exist that can effectively tie these lower level models to a high level system model during design space exploration. This complicates the validation of high level system requirements during detailed design. In this paper, we define requirements for a high level model that is firstly driven by key systems engineering challenges present in industry and secondly connects to several formal and domain specific models used in model-based design. We analysed part of the systems engineering process at a company developing complex systems, by observing the design process and by analysing design documentation and development databases. By generalizing these observations, we identified several high level systems engineering challenges. They are compared to literature, focusing on reported systems engineering challenges and on existing approaches that incorporate high level models in model-based systems engineering. Finally, we argue that high level system models supporting design space exploration should be able to communicate information regarding design trade-offs (e.g. safety versus ease of use) effectively in a multidisciplinary setting. In our outlook, we propose how to continue our research, by recommending further research and defining a research question."
}
@incollection{SAMPATH2016155,
title = "Chapter Four - Advances in Web Application Testing, 20102014",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "101",
pages = "155 - 191",
year = "2016",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2015.11.006",
url = "http://www.sciencedirect.com/science/article/pii/S0065245815000674",
author = "Sreedevi Sampath and Sara Sprenkle",
keywords = "web applications, Software testing, Web testing, Test case generation, Oracles, Test effectiveness, Regression testing",
abstract = "As web applications increase in popularity, complexity, and size, approaches and tools to automate testing the correctness of web applications must continually evolve. In this chapter, we provide a broad background on web applications and the challenges in testing these distributed, dynamic applications made up of heterogeneous components. We then focus on the recent advances in web application testing that were published between 2010 and 2014, including work on test-case generation, oracles, testing evaluation, and regression testing. Through this targeted survey, we identify trends in web application testing and open problems that still need to be addressed."
}
@article{VANDENBRAND201575,
title = "Software engineering: Redundancy is key",
journal = "Science of Computer Programming",
volume = "97",
pages = "75 - 81",
year = "2015",
note = "Special Issue on New Ideas and Emerging Results in Understanding Software",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2013.11.020",
url = "http://www.sciencedirect.com/science/article/pii/S0167642313003043",
author = "Mark van den Brand and Jan Friso Groote",
keywords = "Software engineering, Software quality, Redundancy",
abstract = "Software engineers are humans and so they make lots of mistakes. Typically 1 out of 10 to 100 tasks go wrong. The only way to avoid these mistakes is to introduce redundancy in the software engineering process. This article is a plea to consciously introduce several levels of redundancy for each programming task. Depending on the required level of correctness, expressed in a residual error probability (typically 103 to 1010), each programming task must be carried out redundantly 4 to 8 times. This number is hardly influenced by the size of a programming endeavour. Training software engineers do have some effect as non-trained software engineers require a double amount of redundant tasks to deliver software of a desired quality. More compact programming, for instance by using domain specific languages, only reduces the number of redundant tasks by a small constant."
}
@article{TOKMAKOFF2016537,
title = "AusPlots Rangelands field data collection and publication: Infrastructure for ecological monitoring",
journal = "Future Generation Computer Systems",
volume = "56",
pages = "537 - 549",
year = "2016",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2015.08.016",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X15002782",
author = "Andrew Tokmakoff and Ben Sparrow and David Turner and Andrew Lowe",
keywords = "Ecological data, Mobile, Data collection, Data publishing",
abstract = "The TERN AusPlots Rangelands field data collection system has been developed to facilitate simple and efficient data collection by ecologists operating in the Australian outback. The infrastructure provides tooling for clean data collection on mobile (tablet) devices, associated data storage in a cloud-based server infrastructure, facilities for data curation and management and interfaces with the Australian Ecological Knowledge and Observation System (KOS) data repository for long-term data management and semantic enrichment. In this paper, we introduce the AusPlots Rangelands field data collection solution, providing a systems-level view and motivating its development through the discussion of key functional requirements. We provide an outline of the KOS data repository and demonstrate that the combined system provides a unique end-to-end data collection, curation, archiving and publishing mechanism for ecological data."
}
@incollection{2007185,
title = "Index",
editor = "Michael Guttman and John Parodi",
booktitle = "Real-Life MDA",
publisher = "Morgan Kaufmann",
address = "San Francisco",
pages = "185 - 200",
year = "2007",
series = "The MK/OMG Press",
isbn = "978-0-12-370592-1",
doi = "https://doi.org/10.1016/B978-012370592-1/50014-3",
url = "http://www.sciencedirect.com/science/article/pii/B9780123705921500143"
}
@article{KORAKAKIS2018235,
title = "Blood Flow Restriction induces hypoalgesia in recreationally active adult male anterior knee pain patients allowing therapeutic exercise loading",
journal = "Physical Therapy in Sport",
volume = "32",
pages = "235 - 243",
year = "2018",
issn = "1466-853X",
doi = "https://doi.org/10.1016/j.ptsp.2018.05.021",
url = "http://www.sciencedirect.com/science/article/pii/S1466853X17305035",
author = "Vasileios Korakakis and Rodney Whiteley and Konstantinos Epameinontidis",
keywords = "Blood flow restriction, Occlusion, Resistance training, Ischaemia, Rehabilitation",
abstract = "Objective
To evaluate if a single blood flow restriction (BFR)-exercise bout would induce hypoalgaesia in patients with anterior knee pain (AKP) and allow painless application of therapeutic exercise.
Design
Cross-sectional repeated measures design.
Setting
Institutional out-patients physiotherapy clinic.
Patients
Convenience sample of 30 AKP patients.
Intervention
BFR was applied at 80% of complete vascular occlusion. Four sets of low-load open kinetic chain knee extensions were implemented using a pain monitoring model.
Main outcome measurements
Pain (010) was assessed immediately after BFR application and after a physiotherapy session (45min) during shallow and deep single-leg squat (SSLS, DSLS), and step-down test (SDT). To estimate the patient rating of clinical effectiveness, previously described thresholds for pain change (40%) were used, with appropriate adjustments for baseline pain levels.
Results
Significant effects were found with greater pain relief immediate after BFR in SSLS (d=0.61, p<0.001), DSLS (d=0.61, p<0.001), and SDT (d=0.60, p<0.001). Time analysis revealed that pain reduction was sustained after the physiotherapy session for all tests (d(SSLS)=0.60, d(DSLS)=0.60, d(SDT)=0.58, all p<0.001). The reduction in pain effect size was found to be clinically significant in both post-BFR assessments.
Conclusion
A single BFR-exercise bout immediately reduced AKP with the effect sustained for at least 45min."
}
@article{RUTLE2015545,
title = "Model-driven Software Engineering in Practice: A Content Analysis Software for Health Reform Agreements",
journal = "Procedia Computer Science",
volume = "63",
pages = "545 - 552",
year = "2015",
note = "The 6th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2015)/ The 5th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2015)/ Affiliated Workshops",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.08.383",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915025181",
author = "Adrian Rutle and Kent Inge Fagerland Simonsen and Hans Georg Schaathun and Ralf Kirchhoff",
keywords = "Model-driven Software Engineering, Coordination Reforms in healthcare, Metamodelling, MVCore",
abstract = "The Coordination Reform of 2012 requires Norwegian municipalities and regional health authorities to enter into legally binding service agreements. Although several research projects have been undertaken to analyse the implications of this reform, there is no central database where researches can be given access and analyse the service agreements. In this paper we present how we use model-driven software engineering and user-centric design in an initial development of an information system designed to allow researches to access and analyse service agreements. For this project, it was crucial to discuss the requirements of the system with domain-experts at a high level of abstraction in order to elicit feedback so that the development could proceed at a fast pace and in the right direction. Furthermore, given time and resource constraints, we elected to use a model driven approach using automatic code generation coupled with high-productivity frameworks. In this way we were able to create prototypes so that the developers could get fast feedback from the domain-experts and improvements could be implemented with minimal effort."
}
@article{HAIDER201765,
title = "Notch signalling in placental development and gestational diseases",
journal = "Placenta",
volume = "56",
pages = "65 - 72",
year = "2017",
note = "Exploring common mechanisms between placental and tumour growth",
issn = "0143-4004",
doi = "https://doi.org/10.1016/j.placenta.2017.01.117",
url = "http://www.sciencedirect.com/science/article/pii/S0143400417301194",
author = "S. Haider and J. Pollheimer and M. Knfler",
keywords = "Placenta, Trophoblast, Notch signalling, Gestational diseases",
abstract = "Activation of Notch signalling upon cell-cell contact of neighbouring cells controls a plethora of cellular processes such as stem cell maintenance, cell lineage determination, cell proliferation, and survival. Accumulating evidence suggests that the pathway also critically regulates these events during placental development and differentiation. Herein, we summarize our present knowledge about Notch signalling in murine and human placentation and discuss its potential role in the pathophysiology of gestational disorders. Studies in mice suggest that Notch controls trophectoderm formation, decidualization, placental branching morphogenesis and endovascular trophoblast invasion. In humans, the particular signalling cascade promotes formation of the extravillous trophoblast lineage and regulates trophoblast proliferation, survival and differentiation. Expression patterns as well as functional analyses indicate distinct roles of Notch receptors in different trophoblast subtypes. Altered effects of Notch signalling have been detected in choriocarcinoma cells, consistent with its role in cancer development and progression. Moreover, deregulation of Notch signalling components were observed in pregnancy disorders such as preeclampsia and fetal growth restriction. In summary, Notch plays fundamental roles in different developmental processes of the placenta. Abnormal signalling through this pathway could contribute to the pathogenesis of gestational diseases with aberrant placentation and trophoblast function."
}
@article{RYSSEL201283,
title = "Automatic library migration for the generation of hardware-in-the-loop models",
journal = "Science of Computer Programming",
volume = "77",
number = "2",
pages = "83 - 95",
year = "2012",
note = "Special Issue on Automatic Program Generation for Embedded Systems",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2010.06.005",
url = "http://www.sciencedirect.com/science/article/pii/S0167642310001115",
author = "Uwe Ryssel and Joern Ploennigs and Klaus Kabitzsch",
keywords = "Generative programming, Function-block-based design, Library migration, Structural comparison",
abstract = "Embedded systems are widely used in several applications nowadays. As they integrate hard- and software elements, their functionality and reliability are often tested by hardware-in-the-loop methods, in which the system under test runs in a simulated environment. Due to the rising complexity of the embedded functions, performance limitations and practicability reasons, the simulations are often specialized to test specific aspects of the embedded system and develop a high diversity by themselves. This diversity is difficult to manage for a user and results in erroneously selected test components and compatibility problems in the test configuration. This paper presents a generative programming approach that handles the diversity of test libraries. Compatibility issues are explicitly evaluated by a new interface concept. Furthermore, a novel model analyzer facilitates the efficient application in practice by migrating existing libraries. The approach is evaluated for an example from the automotive domain using MATLAB/Simulink."
}
@incollection{2010525,
editor = "Eric Conrad and Seth Misenar and Joshua Feldman",
booktitle = "CISSP Study Guide",
publisher = "Syngress",
address = "Boston",
pages = "525 - 567",
year = "2010",
isbn = "978-1-59749-563-9",
doi = "https://doi.org/10.1016/B978-1-59749-563-9.00022-6",
url = "http://www.sciencedirect.com/science/article/pii/B9781597495639000226"
}
@incollection{2017207,
title = "Index",
editor = "Eric Conrad and Seth Misenar and Joshua Feldman",
booktitle = "Eleventh Hour CISSP (Third Edition)",
publisher = "Syngress",
edition = "Third Edition",
pages = "207 - 221",
year = "2017",
isbn = "978-0-12-811248-9",
doi = "https://doi.org/10.1016/B978-0-12-811248-9.09992-7",
url = "http://www.sciencedirect.com/science/article/pii/B9780128112489099927"
}
@article{JIMENEZ20153,
title = "MeTAGeM-Trace: Improving trace generation in model transformation by leveraging the role of transformation models",
journal = "Science of Computer Programming",
volume = "98",
pages = "3 - 27",
year = "2015",
note = "Fifth issue of Experimental Software and Toolkits (EST): A special issue on Academics Modelling with Eclipse (ACME2012)",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2014.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167642314003700",
author = "lvaro Jimnez and Juan M. Vara and Vernica A. Bollati and Esperanza Marcos",
keywords = "Model-driven engineering, Traceability, Model transformation",
abstract = "In the context of Model-Driven Engineering (MDE), generation of traces can be automated using the implicit traceability relationships contained in any model transformation. Besides, if transformations are developed adopting a Model-Driven Engineering (MDE) approach, i.e. promoting the role of models and the level of automation, model transformation will benefit from the promised advantages of MDE in terms of less costly software development while reducing the inherent complexity of coding model transformations. To put these ideas into practice, this work introduces MeTAGeM-Trace, the first prototype of an EMF-based toolkit for the MDD of model-to-model transformations which supports trace generation, i.e. it allows developing model transformations that produce not only the corresponding target models, but also a trace model between the elements of the source and target models involved in the transformation."
}
@article{WOMELDORFF2017555,
title = "Taking Lessons Learned from a Proxy Application to a Full Application for SNAP and PARTISN",
journal = "Procedia Computer Science",
volume = "108",
pages = "555 - 565",
year = "2017",
note = "International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2017.05.243",
url = "http://www.sciencedirect.com/science/article/pii/S1877050917308700",
author = "Geoff Womeldorff and Joshua Payne and Ben Bergen",
abstract = "SNAP is a proxy application which simulates the computational motion of a neutral particle transport code, PARTISN. In this work, we have adapted parts of SNAP separately; we have re-implemented the iterative shell of SNAP in the task-model runtime Legion, showing an improvement to the original schedule, and we have created multiple Kokkos implementations of the computational kernel of SNAP, displaying similar performance to the native Fortran. We then translate our Kokkos experiments in SNAP to PARTISN, necessitating engineering development, regression testing, and further thought."
}
@article{HASER201652,
title = "Is business domain language support beneficial for creating test case specifications: A controlled experiment",
journal = "Information and Software Technology",
volume = "79",
pages = "52 - 62",
year = "2016",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2016.07.001",
url = "http://www.sciencedirect.com/science/article/pii/S095058491630115X",
author = "Florian Hser and Michael Felderer and Ruth Breu",
keywords = "Domain Specific Languages (DSL), Behavior driven development, Controlled experiment, Software testing, Student experiment",
abstract = "Context: Behavior Driven Development (BDD), widely used in modern software development, enables easy creation of acceptance test case specifications and serves as a communication basis between business- and technical-oriented stakeholders. BDD is largely facilitated through simple domain specific languages (DSL) and usually restricted to technical test domain concepts. Integrating business domain concepts to implement a ubiquitous language for all members of the development team is an appealing test language improvement issue. But the integration of business domain concepts into BDD toolkits has so far not been investigated. Objective: The objective of the study presented in this paper is to examine whether supporting the ubiquitous language features inside a DSL, by extending a DSL with business domain concepts, is beneficial over using a DSL without those concepts. In the context of the study, benefit is measured in terms of perceived quality, creation time and length of the created test case specifications. In addition, we analyze if participants feel supported when using predefined business domain concepts. Method: We investigate the creation of test case specifications, similar to BDD, in a controlled student experiment performed with graduate students based on a novel platform for DSL experimentation. The experiment was carried out by two groups, each solving a similar comparable test case, one with the simple DSL, the other one with the DSL that includes business domain concepts. A crossover design was chosen for evaluating the perceived quality of the resulting specifications. Results: Our experiment indicates that a business domain aware language allows significant faster creation of documents without lowering the perceived quality. Subjects felt better supported by the DSL with business concepts. Conclusion: Based on our findings we propose that existing BDD toolkits could be further improved by integrating business domain concepts."
}
@article{ZHU2007265,
title = "MDABench: Customized benchmark generation using MDA",
journal = "Journal of Systems and Software",
volume = "80",
number = "2",
pages = "265 - 282",
year = "2007",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2006.10.052",
url = "http://www.sciencedirect.com/science/article/pii/S0164121206003384",
author = "Liming Zhu and Ngoc Bao Bui and Yan Liu and Ian Gorton",
keywords = "MDA, Model-driven development, Performance, Testing, Code generation",
abstract = "This paper describes an approach for generating customized benchmark suites from a software architecture description following a Model Driven Architecture (MDA) approach. The benchmark generation and performance data capture tool implementation (MDABench) is based on widely used open source MDA frameworks. The benchmark application is modeled in UML and generated by taking advantage of the existing community-maintained code generation cartridges so that current component technology can be exploited. We have also tailored the UML 2.0 Testing Profile so architects can model the performance testing and data collection architecture in a standards compatible way. We then extended the MDA framework to generate a load testing suite and automatic performance measurement infrastructure. This greatly reduces the effort and expertise needed for benchmarking with complex component and Web service technologies while being fully MDA standard compatible. The approach complements current model-based performance prediction and analysis methods by generating the benchmark application from the same application architecture that the performance models are derived from. We illustrate the approach using two case studies based on Enterprise JavaBean component technology and Web services."
}
@article{ZITKO20112259,
title = "SNEG  Mathematica package for symbolic calculations with second-quantization-operator expressions",
journal = "Computer Physics Communications",
volume = "182",
number = "10",
pages = "2259 - 2264",
year = "2011",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2011.05.013",
url = "http://www.sciencedirect.com/science/article/pii/S0010465511001792",
author = "Rok itko",
keywords = "Symbolic manipulation, Second-quantization operators, Wicks theorem, Occupation-number representation, Braket notation",
abstract = "In many-particle problems involving interacting fermions or bosons, the most natural language for expressing the Hamiltonian, the observables, and the basis states is the language of the second-quantization operators. It thus appears advantageous to write numerical computer codes which allow the user to define the problem and the quantities of interest directly in terms of operator strings, rather than in some low-level programming language. Here I describe a Mathematica package which provides a flexible framework for performing the required translations between several different representations of operator expressions: condensed notation using pure ASCII character strings, traditional notation (pretty printing), internal Mathematica representation using nested lists (used for automatic symbolic manipulations), and various higher-level (macro) expressions. The package consists of a collection of transformation rules that define the algebra of operators and a comprehensive library of utility functions. While the emphasis is given on the problems from solid-state and atomic physics, the package can be easily adapted to any given problem involving non-commuting operators. It can be used for educational and demonstration purposes, but also for direct calculations of problems of moderate size.
Program summary
Program title: SNEG Catalogue identifier: AEJL_vl_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEJL_vl_0.html Program obtainable from: CPC Program Library, Queens University, Belfast, N. Ireland Licensing provisions: GNU General Public License No. of lines in distributed program, including test data, etc.: 319808 No. of bytes in distributed program, including test data, etc.: 1081247 Distribution format: tar.gz Programming language: Mathematica Computer: Any computer which runs Mathematica Operating system: Any OS which runs Mathematica RAM: Problem dependent Classification: 2.9, 5, 6.2 Nature of problem: Manipulation of expressions involving second-quantization operators and other non-commuting objects. Calculation of commutators, anticommutators, expectation values. Generation of matrix representations of the Hamiltonians expressed in the second-quantization language. Solution method: Automatic reordering of operator strings in some well specified canonical order; (anti)commutation rules are used where needed. States may be represented in occupation-number representation. Dirac braket notation may be intermixed with non-commuting operator expressions. Restrictions: For very long operator strings, the brute-force automatic reordering becomes slow, but it can be turned off. In such cases, the expectation values may still be evaluated using Wicks theorem. Unusual features: SNEG provides the natural notation of second-quantization operators (dagger for creation operators, etc.) when used interactively using the Mathematica notebook interface. Running time: Problem dependent"
}
@article{SANCHEZGORDON2017162,
title = "A standard-based framework to integrate software work in small settings",
journal = "Computer Standards & Interfaces",
volume = "54",
pages = "162 - 175",
year = "2017",
note = "Standards in Software Process Improvement and Capability Determination",
issn = "0920-5489",
doi = "https://doi.org/10.1016/j.csi.2016.11.009",
url = "http://www.sciencedirect.com/science/article/pii/S0920548916301891",
author = "Mary-Luz Sanchez-Gordon and Antonio de Amescua and Rory V. OConnor and Xabier Larrucea",
keywords = "Small companies, VSE, Small settings, Software process improvement, Human factors, Socio-technical system",
abstract = "Small software companies have to work hard in order to survive. They usually find it challenging to spend time and effort on improving their operations and processes. Therefore, it is important to address such needs by the introduction of a proposed framework that specifies ways of getting things done while consciously encourage them to enhance their ability to improve. Although there are many software process improvement approaches, none of them address the human factors of small companies in a comprehensive and holistic way. Samay is a proposed framework to integrate human factors in the daily work as a way to deal with that challenge. This study suggests managing human factors but pointing out the software process life cycle. The purpose is to converge toward a continuous improvement by means of alternative mechanisms that impact on people. This framework was developed based upon reviews of relevant standards (such as ISO/IEC 29110, ISO 10018, OMG Essence and ISO/IEC 33014) and previously published studies in this field. Moreover, an expert review and validation findings supported the view that Samay could support practitioners when small software companies want to start improving their ways of work."
}
@article{LUCENA2013890,
title = "Contributions to the emergence and consolidation of Agent-oriented Software Engineering",
journal = "Journal of Systems and Software",
volume = "86",
number = "4",
pages = "890 - 904",
year = "2013",
note = "SI : Software Engineering in Brazil: Retrospective and Prospective Views",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2012.09.016",
url = "http://www.sciencedirect.com/science/article/pii/S0164121212002567",
author = "Carlos Lucena and Ingrid Nunes",
keywords = "Multiagent systems, Agent-oriented Software Engineering, LES, PUC-Rio, SBES 25 years",
abstract = "Many of the issues addressed with multi-agent approaches, such as distributed coordination and self-organization, are now becoming part of industrial and business systems. However, Multiagent Systems (MASs) are still not widely adopted in industry owing to the lack of a connection between MAS and software engineering. Since 2000, there is an effort to bridge this gap and to produce software engineering techniques for agent-based systems that guide the processes of design, development and maintenance. In Brazil, Agent-oriented Software Engineering (AOSE) was first investigated by the research group in the Software Engineering Laboratory (LES) at PUC-Rio, which after one decade of study in this area has built an AOSE community. This paper presents the history of AOSE at LES by discussing the sub-areas of MAS Software Engineering research and development that have been focus of the LES research group. We give examples of relevant results and present a subset of the extensive literature the group has produced during the last decade. We also report how we faced the challenges that emerged from our research by organizing and developing a research community at the intersection of software engineering, programming and MASs with a concern for scalability of solutions."
}
@incollection{2007371,
title = "Chapter 11 - Putting it into practice",
editor = "Mark Utting and Bruno Legeard",
booktitle = "Practical Model-Based Testing",
publisher = "Morgan Kaufmann",
address = "San Francisco",
pages = "371 - 389",
year = "2007",
isbn = "978-0-12-372501-1",
doi = "https://doi.org/10.1016/B978-012372501-1/50012-0",
url = "http://www.sciencedirect.com/science/article/pii/B9780123725011500120",
abstract = "Publisher Summary
This chapter discusses several practical issues and techniques for adopting model-based testing. It starts with the prerequisites of model-based testing and then goes through taxonomy of the possible approaches with the goal of helping one to choose a good approach for one's needs. It deals with people and training issues and discusses how model based testing can fit into agile development processes and the Unified Modeling Language (UML) unified process. One benefit of model-based testing is that it identifies faults in the analysis model and in the requirements earlier in the design process than is usually the case. This can prevent those faults from flowing into the design model and the SUT implementation. This happens because the modeling and model-validation stages of model-based testing raise questions about the requirements and can expose faults well before any tests are executed."
}
@article{TYUGASHEV20181457,
title = "Verification and online updating of decision making control logic for onboard real-time control systems",
journal = "Procedia Computer Science",
volume = "126",
pages = "1457 - 1466",
year = "2018",
note = "Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2018.08.118",
url = "http://www.sciencedirect.com/science/article/pii/S1877050918313966",
author = "Andrey Tyugashev and Dmitrii Zheleznov",
keywords = "Control logic, real-time control algorithm, complex technical system, flight control software, intelligent control",
abstract = "The errors during the decision making process in the control system of a modern complex technical system such as a ship, a plane, a spaceship or a power station can lead to unacceptable consequences. Meanwhile, decision making is based on so-named control logic described in dedicated specification documents and then implemented by the hardware and software in the real time mode. There are some problems in this process, caused by contradictions and incompletenesses in the specification documents written in natural language, and misunderstanding between specialists in onboard systems, operational engineers, and programmers. In this paper, two practical examples of verification and online updating of spacecraft control logic are described. The approaches we used allow avoiding the mentioned problems. The theoretical basis for verification is Real-Time Control Algorithms Logic RTCAL. Using this, we have developed and successfully applied software tools and domain-specific languages used at the design and operational stages of spacecraft control. The ongoing work includes introducing SMT solvers into our approach, and automatic generation of valid control logic."
}
@article{LOCHAU201463,
title = "Delta-oriented model-based integration testing of large-scale systems",
journal = "Journal of Systems and Software",
volume = "91",
pages = "63 - 84",
year = "2014",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2013.11.1096",
url = "http://www.sciencedirect.com/science/article/pii/S0164121213002781",
author = "Malte Lochau and Sascha Lity and Remo Lachmann and Ina Schaefer and Ursula Goltz",
keywords = "Large-scale systems, Model-based testing, Regression testing, Variable software architectures",
abstract = "Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain."
}
@article{HUMMER20131884,
title = "Enforcement of entailment constraints in distributed service-based business processes",
journal = "Information and Software Technology",
volume = "55",
number = "11",
pages = "1884 - 1903",
year = "2013",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2013.05.001",
url = "http://www.sciencedirect.com/science/article/pii/S0950584913001006",
author = "Waldemar Hummer and Patrick Gaubatz and Mark Strembeck and Uwe Zdun and Schahram Dustdar",
keywords = "Identity and access management, Business process management, Entailment constraints, Service-Oriented Architecture (SOA), WS-BPEL",
abstract = "Context
A distributed business process is executed in a distributed computing environment. The service-oriented architecture (SOA) paradigm is a popular option for the integration of software services and execution of distributed business processes. Entailment constraints, such as mutual exclusion and binding constraints, are important means to control process execution. Mutually exclusive tasks result from the division of powerful rights and responsibilities to prevent fraud and abuse. In contrast, binding constraints define that a subject who performed one task must also perform the corresponding bound task(s).
Objective
We aim to provide a model-driven approach for the specification and enforcement of task-based entailment constraints in distributed service-based business processes.
Method
Based on a generic metamodel, we define a domain-specific language (DSL) that maps the different modeling-level artifacts to the implementation-level. The DSL integrates elements from role-based access control (RBAC) with the tasks that are performed in a business process. Process definitions are annotated using the DSL, and our software platform uses automated model transformations to produce executable WS-BPEL specifications which enforce the entailment constraints. We evaluate the impact of constraint enforcement on runtime performance for five selected service-based processes from existing literature.
Results
Our evaluation demonstrates that the approach correctly enforces task-based entailment constraints at runtime. The performance experiments illustrate that the runtime enforcement operates with an overhead that scales well up to the order of several ten thousand logged invocations. Using our DSL annotations, the user-defined process definition remains declarative and clean of security enforcement code.
Conclusion
Our approach decouples the concerns of (non-technical) domain experts from technical details of entailment constraint enforcement. The developed framework integrates seamlessly with WS-BPEL and the Web services technology stack. Our prototype implementation shows the feasibility of the approach, and the evaluation points to future work and further performance optimizations."
}
@article{SLAWIK2018846,
title = "Establishing User-centric Cloud Service Registries",
journal = "Future Generation Computer Systems",
volume = "87",
pages = "846 - 867",
year = "2018",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2018.03.010",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X18304813",
author = "Mathias Slawik and Begm lke Zilci and Axel Kpper",
keywords = "Cloud service registry, Cloud computing, Service matchmaking, Cloud brokering",
abstract = "Many potential cloud consumers are overburdened by the challenges persisting when discovering, assessing, and selecting contemporary Cloud Service offerings: the cloud market is vast and fast-moving, the selection criteria are ambiguous, service knowledge is scattered through the Internet, and features as well as prices are complex and incomparable. Much research has been carried out to create cloud service registries to help users select cloud services for eventual consumption, especially within the field of semantic web services. Through analyzing real-world requirements of six use cases we identified a gap in research for user-centric technologies. We fill this gap by creating a business vocabulary reflecting common service selection criteria, defining a textual domain specific language to let any user describe services easily, and implementing a novel brokering and matchmaking component to support users in their selection process. As a combination of those technologies, we create the Open Service Compendium (OSC), a crowd-sourced cloud service registry. Our evaluation activities highlight how these developments solve real-world challenges in diverse near-production settings. All of this implies that a substantial benefit for service registry users can be created by following a simple architecture that is focused on their concrete needs instead of aiming for highest sophistication and broadest applicability as observed in many of the related works."
}
@article{KUMAR2015859,
title = "Model Based Distributed Testing of Object Oriented Programs",
journal = "Procedia Computer Science",
volume = "46",
pages = "859 - 866",
year = "2015",
note = "Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace & Island Resort, Kochi, India",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.02.155",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915002197",
author = "K.S. Vipin Kumar and Sheena Mathew",
keywords = "Distributed Testing, SDG, FCFS, Object Oriented Program Testing ;",
abstract = "In recent times the software systems have evolved in size and complexity. This has resulted in usage of object oriented programming in the development of such systems. Though object oriented programs are helpful in programming large systems, testing of such systems requires much more effort and time. For this the program is analyzed to create a model based on System Dependence Graph(SDG) which is then used to find locations within the program where the state of the program can be freezed and reused while executing other test cases."
}
@incollection{OSIS201753,
title = "Chapter 2 - Software Designing With Unified Modeling Language Driven Approaches",
editor = "Janis Osis and Uldis Donins",
booktitle = "Topological UML Modeling",
publisher = "Elsevier",
address = "Boston",
pages = "53 - 82",
year = "2017",
series = "Computer Science Reviews and Trends",
isbn = "978-0-12-805476-5",
doi = "https://doi.org/10.1016/B978-0-12-805476-5.00002-2",
url = "http://www.sciencedirect.com/science/article/pii/B9780128054765000022",
author = "Janis Osis and Uldis Donins",
keywords = "Software modeling methods and approaches, UML application, benefits and limitations of UML modeling driven approaches",
abstract = "The Unified Modeling Language (UML) is a notation and as such its specification does not contain any guidelines for software development process. Despite that UML is independent of particular methods and approaches, most of the UML modeling driven methods uses use case driven approach thus raising incomplete analysis of the problem domain functioning. Since UML modeling driven approaches are elaborated by different authors, their prescriptions differ. There is also difference in the use of use case narratives across various methods due to the lack of guidance on narrative format in the UML specification. The UML specification only states that use cases are typically specified in various idiosyncratic formats such as natural language, tables, trees, etc. Therefore, it is not easy to capture its structure accurately or generally by a formal model. This chapter discusses the current state of the art of UML-based software development approaches. Most attention is paid on the artifacts created by using the UML."
}
@article{DIAZ2012737,
title = "Wiki Scaffolding: Aligning wikis with the corporate strategy",
journal = "Information Systems",
volume = "37",
number = "8",
pages = "737 - 752",
year = "2012",
note = "Special Issue: Advanced Information Systems Engineering (CAiSE'11)",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2012.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0306437912000695",
author = "Oscar Daz and Gorka Puente",
keywords = "Wiki, Wiki management, DSL, Mind map, FreeMind",
abstract = "Wikis are main exponents of collaborative development by user communities. This community may be created around the wiki itself (e.g., community of contributors in Wikipedia) or already exist (e.g., company employees in corporate wikis). In the latter case, the wiki is not created in a vacuum but as part of the information ecosystem of the hosting organization. As any other Information System resource, wiki success highly depends on the interplay of technology, work practice and the organization. Thus, wiki contributions should be framed along the concerns already in use in the hosting organization in terms of glossaries, schedules, policies, organigrams and the like. The question is then, how can corporate strategies permeate wiki construction while preserving wiki openness and accessibility? We advocate for the use of Wiki Scaffoldings, i.e., a wiki installation that is provided at the onset to mimic these corporate concerns: categories, users, templates, articles initialized with boilerplate text, are all introduced in the wiki before any contribution is made. To retain wikis' friendliness and engage layman participation, we propose scaffoldings to be described as mind maps. Mind maps are next exported as wiki installations. We show the feasibility of the approach introducing a Wiki Scaffolding Language (WSL). WSL is realized as a plugin for FreeMind, a popular tool for mind mapping. Finally, we validate the expressiveness of WSL in four case studies. WSL is available for download."
}
@incollection{AHMAD20181,
title = "Chapter One - Model-Based Testing for Internet of Things Systems",
editor = "Atif M. Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "108",
pages = "1 - 58",
year = "2018",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2017.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S0065245817300517",
author = "Abbas Ahmad and Fabrice Bouquet and Elizabeta Fourneret and Bruno Legeard",
keywords = "Model-based testing, Modeling approaches, Test generation technology, Security testing",
abstract = "The Internet of Things (IoT) is nowadays globally a mean of innovation and transformation for many companies. Applications extend to a large number of domains, such as smart cities, smart homes, and health care. The Gartner Group estimates an increase up to 21 billion connected things by 2020. The large span of things introduces problematic aspects, such as interoperability due to the heterogeneity of communication protocols and the lack of a globally accepted standard. The large span of usages introduces problems regarding secure deployments and scalability of the network over large-scale infrastructures. This chapter describes the challenges for the IoT testing, includes state-of-the-art testing of IoT systems using models, and presents a model-based testing as a service approach to respond to its challenges through demonstrations with real use cases involving two of the most accepted standards worldwide: FIWARE and oneM2M."
}
@incollection{KAWAMOTO2014819,
title = "Chapter 29 - Integration of Knowledge Resources into Applications to Enable CDS: Architectural Considerations",
editor = "Robert A. Greenes",
booktitle = "Clinical Decision Support (Second Edition)",
publisher = "Academic Press",
edition = "Second Edition",
address = "Oxford",
pages = "819 - 849",
year = "2014",
isbn = "978-0-12-398476-0",
doi = "https://doi.org/10.1016/B978-0-12-398476-0.00029-4",
url = "http://www.sciencedirect.com/science/article/pii/B9780123984760000294",
author = "Kensaku Kawamoto and Emory Fry and Robert Greenes",
keywords = "Clinical decision support, scalable CDS, Health eDecisions, OpenCDS, Service-Oriented Architecture (SOA)",
abstract = "This chapter examines in detail the bridge between the creation and delivery of CDS content  in other words, how knowledge resources can be integrated with clinical information systems (CISs) to enable CDS. While such integration can be relatively straightforward for a single instance  that is, the integration of a specific knowledge resource in a specific clinical information system  the challenge lies in the fact that both knowledge resources and clinical information systems are quite diverse. Consequently, there is no single knowledge integration architecture that can address all circumstances. However, there are several architectural patterns for knowledge integration that can, taken together, enable the effective integration of knowledge resources into applications. The primary purpose of this chapter, then, is to outline the main CDS knowledge integration architectures that are available and to detail the pros and cons of each approach. The appropriateness of a given architecture for a particular organization depends on a variety of factors, including the existing clinical information system infrastructure and the type of CDS capability involved (e.g. real-time vs. non-real-time applications. These various approaches are outlined here, with special attention being placed on knowledge integration architectures aligned with broad trends in the IT landscape, such as service-oriented architectures, cloud-based computing, and app-based software ecosystems. The chapter also discusses how CDS architectures must align with larger changes in the health care industry as a whole, as shifting health care reimbursement models are requiring continuity of care across multiple organizations and health IT systems centered around patients and populations rather than care episodes at individual care settings."
}
@incollection{2017267,
title = "Index",
editor = "Edward Griffor",
booktitle = "Handbook of System Safety and Security",
publisher = "Syngress",
address = "Boston",
pages = "267 - 273",
year = "2017",
isbn = "978-0-12-803773-7",
doi = "https://doi.org/10.1016/B978-0-12-803773-7.00022-X",
url = "http://www.sciencedirect.com/science/article/pii/B978012803773700022X"
}
@article{BJARNASON201661,
title = "A multi-case study of agile requirements engineering and the use of test cases as requirements",
journal = "Information and Software Technology",
volume = "77",
pages = "61 - 79",
year = "2016",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2016.03.008",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916300544",
author = "Elizabeth Bjarnason and Michael Unterkalmsteiner and Markus Borg and Emelie Engstrm",
keywords = "Agile development, Requirements, Testing, Test-first development, Test-driven development, Behaviour-driven development, Acceptance test, Case study, Empirical software engineering",
abstract = "Context
It is an enigma that agile projects can succeed without requirements when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases.
Objective
We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies.
Method
We performed an iterative case study at three companies and collected data through 14 interviews and two focus groups.
Results
The use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements.
Conclusions
The findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change."
}
@article{CHAVARRIAGA2017133,
title = "An approach to build XML-based domain specific languages solutions for client-side web applications",
journal = "Computer Languages, Systems & Structures",
volume = "49",
pages = "133 - 151",
year = "2017",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2017.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S1477842416301634",
author = "Enrique Chavarriaga and Francisco Jurado and Fernando Dez",
keywords = "Domain-Specific Languages, XML interpreter, JavaScript, Web Application, XML programing language",
abstract = "Summary
Domain-Specific Languages (DSLs) allow for the building of applications that ease the labour of both software engineers and domain experts thanks to the level of abstraction they provide. In cases where the domain is restricted to Client-Side Web Applications (CSWA), XML-based languages, frameworks and widgets are commonly combined in order to provide fast, robust and flexible solutions. This article presents an approach designed to create XML-based DSL solutions for CSWA that includes an evaluation engine, a programming model and a lightweight development environment. The approach is able to evaluate multiple XML-based DSL programs simultaneously to provide solutions to those Domain Specific Problems for CSWAs. To better demonstrate the capabilities and potential of this novel approach, we will employ a couple of case studies, namely Anisha and FeedPsi."
}
@incollection{BUCHGEHER2014161,
title = "Chapter 7 - Continuous Software Architecture Analysis",
editor = "Muhammad Ali Babar and Alan W. Brown and Ivan Mistrik",
booktitle = "Agile Software Architecture",
publisher = "Morgan Kaufmann",
address = "Boston",
pages = "161 - 188",
year = "2014",
isbn = "978-0-12-407772-0",
doi = "https://doi.org/10.1016/B978-0-12-407772-0.00006-X",
url = "http://www.sciencedirect.com/science/article/pii/B978012407772000006X",
author = "Georg Buchgeher and Rainer Weinreich",
keywords = "Agile development, Continuous quality control, Continuous software architecture analysis, Software architecture, Software architecture analysis",
abstract = "This chapter discusses software architecture analysis in the context of agile processes. Agile processes are characterized by incremental and interleaved activities and by a focus on continuous improvement and delivery. Most approaches to software architecture analysis, however, have been developed to be performed at dedicated points in the development process or as external evaluation activities and not as continuous activities throughout the development process. This chapter discusses continuous software architecture analysis (CSAA). It reviews important requirements for CSAA and outlines how CSAA is supported by current software architecture analysis approaches. It further presents experiences with an approach for continuous structural and conformance analysis and identifies future research challenges."
}
@article{LAZAR201091,
title = "Behaviour-Driven Development of Foundational UML Components",
journal = "Electronic Notes in Theoretical Computer Science",
volume = "264",
number = "1",
pages = "91 - 105",
year = "2010",
note = "Proceedings of the 7th International Workshop on Formal Engineering approaches to Software Components and Architectures (FESCA 2010)",
issn = "1571-0661",
doi = "https://doi.org/10.1016/j.entcs.2010.07.007",
url = "http://www.sciencedirect.com/science/article/pii/S1571066110000666",
author = "Ioan Lazr and Simona Motogna and Bazil Prv",
keywords = "behaviour-driven development, executable UML, user story, executable specification, acceptance criteria",
abstract = "Behaviour-Driven Development (BDD) focuses all development activities on the delivery of behaviours  what a system should do, described such that developers and domain experts speak the same language. BDD frameworks allow users to represent the required system behaviour as executable user stories and the acceptance criteria as executable scenarios attached to user stories. In this paper we define a UML profile that allows users to create executable Foundational UML (fUML) stories and scenarios. In order to easily construct scenarios we introduce a BDD model library which contains fUML activities for testing equalities and inclusions. We also present an Eclipse-based development tool that supports a BDD approach for developing fUML components. The tool provides developers a concrete syntax for defining executable scenarios, and automatically updates the project status based on verified delivered behaviorus."
}
@article{KOS201674,
title = "Test automation of a measurement system using a domain-specific modelling language",
journal = "Journal of Systems and Software",
volume = "111",
pages = "74 - 88",
year = "2016",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2015.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S0164121215002058",
author = "Toma Kos and Marjan Mernik and Toma Kosar",
keywords = "Test automation, Domain-specific modelling languages, Usage experience",
abstract = "The construction of domain-specific modelling languages (DSMLs) is only the first step within the needed toolchain. Models need to be maintained, modified or functional errors searched for. Therefore, tool support is vital for the DSML end-users efficiency. This paper presents SeTT, a simple but very useful tool for DSML end-users, a testing framework integrated within a DSML Sequencer. This Sequencer, part of the DEWESoft data acquisition system, supports the development of model-based tests using a high-level abstraction. The tests are used during the whole data acquisition process and able to test different systems parts. This paper shows how high-level specifications can be extended to describe a testing infrastructure for a specific DSML. In this manner, the Sequencer and SeTT were combined at the metamodel level. The contribution of the paper is to show that one can leverage on the DSML to build a testing framework with relatively little effort, by implementing assertions to it."
}
@article{CALDERON2018238,
title = "MEdit4CEP-Gam: A model-driven approach for user-friendly gamification design, monitoring and code generation in CEP-based systems",
journal = "Information and Software Technology",
volume = "95",
pages = "238 - 264",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.11.009",
url = "http://www.sciencedirect.com/science/article/pii/S0950584917303464",
author = "Alejandro Caldern and Juan Boubeta-Puig and Mercedes Ruiz",
keywords = "Gamification, Model-Driven Engineering, Complex event processing, Strategy design, Monitoring, Graphical modelling editor",
abstract = "Context
Gamification has been proven to increase engagement and motivation in multiple and different non-game contexts such as healthcare, education, workplace, and marketing, among others. However, many of these applications fail to achieve the desired benefits of gamification, mainly because of a poor design.
Objective
This paper explores the conceptualization, implementation and monitoring phases of meaningful gamification strategies and proposes a solution for strategy experts that hides the implementation details and helps them focus only on what is crucial for the success of the strategy. The solution makes use of Model-Driven Engineering (MDE) and Complex Event Processing (CEP) technology.
Method
An easy-to-use graphical editor is used to provide the high-level models that represent the design of the gamification strategy and its deployment and monitoring. These models contain the event pattern definitions to be automatically transformed into code. This code is then deployed both in a CEP engine to detect the conditions expressed in such patterns and in an enterprise service bus to execute the corresponding pattern actions.
Results
The paper reports on the use of both a graphical modeling editor for gamification domain definition and a graphical modeling editor for gamification strategy design, monitoring and code generation in event-based systems. It also shows how the proposal can be used to design and automate the implementation and monitoring of a gamification strategy in an educational domain supported by a well-known Learning Management System (LMS) such as Moodle.
Conclusion
It can be concluded that this unprecedented model-driven approach leveraging gamification and CEP technology provides strategy experts with the ability to graphically define gamification strategies, which can be directly transformed into code executable by event-based systems. Therefore, this is a novel solution for bringing CEP closer to any strategy expert, positively influencing the gamification strategy design, implementation and real-time monitoring processes."
}
@article{TYUGASHEV2016120,
title = "Language and Toolset for Visual Construction of Programs for Intelligent Autonomous Spacecraft Control",
journal = "IFAC-PapersOnLine",
volume = "49",
number = "5",
pages = "120 - 125",
year = "2016",
note = "4th IFAC Conference on Intelligent Control and Automation SciencesICONS 2016",
issn = "2405-8963",
doi = "https://doi.org/10.1016/j.ifacol.2016.07.100",
url = "http://www.sciencedirect.com/science/article/pii/S2405896316302968",
author = "Andrey A. Tyugashev",
keywords = "Autonomous Control, Satellite Control, Diagnostic Programs, Software Tools, Software Engineering, Domain Specific Language, Visual Software Construction",
abstract = "The paper describes approach to Autonomous Fault Tolerant Intelligent Control of Spacecraft based on usage of Onboard Real-Time Interpreter of Integrated Control Programs, including special Diagnostic Routine. Rules of the Autonomous Control Program could be added or refined from Earth in operative manner by radio channel. Specially designed Visual Domain Specific Language allowing Control Logic Designers check, analyze and construct Rules in user friendly graphical environment excluding necessity to involve Software Developers. Proposed approach allows reducing of costs and labor consuming of Space Mission because of reducing of efforts needed for common-style Flight Control Software coding, multi-stage testing and support. Special Software Engineering Toolset that including Visualizer and Graphical Constructor of these autonomous control programs presented as well as the principles of its design and development. The Prototype of the Toolset has been successfully introduced at JSC Information Satellite Systems, Krasnoyarsk Region, Russia."
}
@article{ALTURJMAN2017,
title = "5G-enabled devices and smart-spaces in social-IoT: An overview",
journal = "Future Generation Computer Systems",
year = "2017",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2017.11.035",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X17311962",
author = "Fadi Al-Turjman",
keywords = "Internet of Things (IoT), Smart environments, Sensors, 5G, Smartphones",
abstract = "The abundance of smartphones, with their growing capabilities potentiates applications in numerous domains. A typical smartphone nowadays is equipped with an array of embedded sensors (e.g., GPS, accelerometers, gyroscopes, RFID readers, cameras, and microphones) along with different communication interfaces (e.g. Cellular, WiFi, Bluetooth, etc.). Thus, a smartphone is a significant provider for sensory data that awaits the utilization in many critical applications. Primers of this vision have demonstrated success, both in the literature and applications market. In this literature review, we present the main motivations in carrying these smart devices, and the correlation between the user surrounding context and the application usage. We focus on context-awareness in smart systems and space discovery paradigms; online versus offline, the femtocell usage and energy aspects to be considered, and about the ongoing social IoT applications. Moreover, we highlight the most up-to-date open research issues in this area."
}
@article{BRYCE2006960,
title = "Prioritized interaction testing for pair-wise coverage with seeding and constraints",
journal = "Information and Software Technology",
volume = "48",
number = "10",
pages = "960 - 970",
year = "2006",
note = "Advances in Model-based Testing",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2006.03.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584906000401",
author = "Rene C. Bryce and Charles J. Colbourn",
keywords = "Biased covering arrays, Covering arrays, Greedy algorithm, Mixed-level covering arrays, Pair-wise interaction coverage, Software interaction testing, Test prioritization",
abstract = "Interaction testing is widely used in screening for faults. In software testing, it provides a natural mechanism for testing systems to be deployed on a variety of hardware and software configurations. In many applications where interaction testing is needed, the entire test suite is not run as a result of time or budget constraints. In these situations, it is essential to prioritize the tests. Here, we adapt a one-test-at-a-time greedy method to take importance of pairs into account. The method can be used to generate a set of tests in order, so that when run to completion all pair-wise interactions are tested, but when terminated after any intermediate number of tests, those deemed most important are tested. In addition, practical concerns of seeding and avoids are addressed. Computational results are reported."
}
@article{DOGAN2014174,
title = "Web application testing: A systematic literature review",
journal = "Journal of Systems and Software",
volume = "91",
pages = "174 - 201",
year = "2014",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2014.01.010",
url = "http://www.sciencedirect.com/science/article/pii/S0164121214000223",
author = "Serdar Doan and Aysu Betin-Can and Vahid Garousi",
keywords = "Systematic literature review, Web application, Testing",
abstract = "Context
The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013.
Objective
As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field.
Methods
We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area.
Results
Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance.
Conclusion
We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community."
}
@article{SMITH2016404,
title = "A Document-Driven Method for Certifying Scientific Computing Software for Use in Nuclear Safety Analysis",
journal = "Nuclear Engineering and Technology",
volume = "48",
number = "2",
pages = "404 - 418",
year = "2016",
issn = "1738-5733",
doi = "https://doi.org/10.1016/j.net.2015.11.008",
url = "http://www.sciencedirect.com/science/article/pii/S1738573315002582",
author = "W. Spencer Smith and Nirmitha Koothoor",
keywords = "Literate Programming, Nuclear Safety Analysis, Numerical Simulation, Requirements Specification, Software Engineering, Software Quality",
abstract = "This paper presents a documentation and development method to facilitate the certification of scientific computing software used in the safety analysis of nuclear facilities. To study the problems faced during quality assurance and certification activities, a case study was performed on legacy software used for thermal analysis of a fuelpin in a nuclear reactor. Although no errors were uncovered in the code, 27 issues of incompleteness and inconsistency were found with the documentation. This work proposes that software documentation follow a rational process, which includes a software requirements specification following a template that is reusable, maintainable, and understandable. To develop the design and implementation, this paper suggests literate programming as an alternative to traditional structured programming. Literate programming allows for documenting of numerical algorithms and code together in what is termed the literate programmer's manual. This manual is developed with explicit traceability to the software requirements specification. The traceability between the theory, numerical algorithms, and implementation facilitates achieving completeness and consistency, as well as simplifies the process of verification and the associated certification."
}
@article{REBAHI201839,
title = "Towards a next generation 112 testbed: The EMYNOS ESInet",
journal = "International Journal of Critical Infrastructure Protection",
volume = "22",
pages = "39 - 50",
year = "2018",
issn = "1874-5482",
doi = "https://doi.org/10.1016/j.ijcip.2018.05.001",
url = "http://www.sciencedirect.com/science/article/pii/S1874548217302081",
author = "Yacine Rebahi and Kin Tsun Chiu and Nikolay Tcholtchev and Simon Hohberg and Evangelos Pallis and Evangelos Markakis",
keywords = "Emergency services, 112, 911, EMYNOS, SIP, IMS, ESInet, Location, i3 architecture, RTT",
abstract = "This paper belongs to a series of research documents describing the progress in the specification and development of the EMYNOS framework offering an IP based platform for emergency services. EMYNOS is an international research project funded by the European Commission. Although migrating to Next Generation 112 and 911 is not new as a topic, no real testbed that can be used for evaluating the relevant standards exists so far. In this paper we discuss the EMYNOS approach and in particular some mechanisms that have been developed in this context. Special attention is paid to the EMYNOS testbed that was assessed during the ETSI NG112 plugtests in 2016 and 2017. Some experiments and test results are provided as well."
}
@article{IZADI2018191,
title = "Tolerance induction by surface immobilization of Jagged-1 for immunoprotection of pancreatic islets",
journal = "Biomaterials",
volume = "182",
pages = "191 - 201",
year = "2018",
issn = "0142-9612",
doi = "https://doi.org/10.1016/j.biomaterials.2018.08.017",
url = "http://www.sciencedirect.com/science/article/pii/S0142961218305714",
author = "Zhila Izadi and Ensiyeh Hajizadeh-Saffar and Jamshid Hadjati and Mahdi Habibi-Anbouhi and Mohammad Hossein Ghanian and Hamid Sadeghi-Abandansari and Mohammad Kazemi Ashtiani and Zakieh Samsonchi and Mohammad Raoufi and Maedeh Moazenchi and Mahmoud Izadi and Anava sadat Sadr Hashemi Nejad and Haideh Namdari and Yaser Tahamtani and Seyed Nasser Ostad and Hamid Akbari-Javar and Hossein Baharvand",
keywords = "Type 1 diabetes, Immunomodulation, Surface immobilization, Jagged-1, Islet PEGylation",
abstract = "Although transplantation of pancreatic islets is a promising approach for treatment of type 1 diabetes mellitus, the engraftment efficiency of these islets is limited by host immune responses. Extensive efforts have been made to immunoisolate these islets by introducing barriers on the islet surface. To date, these barriers have not successfully protected islets from attack by the immune system. In addition, the inevitable permeability of an islet capsule cannot prevent filtration by proinflammatory cytokines and islet self-antigens. Thus, we have developed a surface engineering approach for localized immonumodulation of the islet microenvironment. Jagged-1 (JAG-1), as a potent immunomodulatory factor, was immobilized on the islet surface by mediation of a double-layer of heterobifunctional poly (ethylene glycol) (PEG). Immobilization and functionality of JAG-1 on PEGylated islet surfaces were established. When co-cultured with splenocytes, the JAG-1 conjugated islets induced a significant increase in regulatory T cells and regulated the cytokine levels produced by immune cells. The results demonstrated that JAG-1 immobilization could improve immunoprotection of pancreatic islets by localized modulation of the immune milieu from an inflammatory to an anti-inflammatory state. We also evaluated the effects of surface modification of these islets by JAG-1 in a xenotransplantation model. The transplanted JAG-1/PEG/islets group showed a significantly reduced blood glucose levels compared with the control group of diabetic mice during the acute phase of the immune response to the transplanted islets. Our results demonstrated that surface modification has the potential to shift the immune system from an inflammatory to anti-inflammatory milieu and may offer a new prospective for immunoprotection of pancreatic islets."
}
@article{PONCELET2016143,
title = "Model-based testing for building reliable realtime interactive music systems",
journal = "Science of Computer Programming",
volume = "132",
pages = "143 - 172",
year = "2016",
note = "Special Issue on Software Verification and Testing (SAC-SVT'15)",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2016.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S0167642316301022",
author = "Clement Poncelet and Florent Jacquemard",
keywords = "Model based testing, Interactive music systems, Timed automata",
abstract = "The role of an Interactive Music System (IMS) is to accompany musicians during live performances, acting like a real musician. It must react in realtime to audio signals from musicians, according to a timed high-level requirement called mixed score, written in a domain specific language. Such goals imply strong requirements of temporal reliability and robustness to unforeseen errors in input, yet not much addressed by the computer music community. We present the application of Model-Based Testing techniques and tools to a state-of-the-art IMS, including in particular: offline and on-the-fly approaches for the generation of relevant input data for testing (including timing values), with coverage criteria, the computation of the corresponding expected output, according to the semantics of a given mixed score, the black-box execution of the test data on the System Under Test and the production of a verdict. Our method is based on formal models in a dedicated intermediate representation, compiled directly from mixed scores (high-level requirements), and either passed, to the model-checker Uppaal (after conversion to Timed Automata) in the offline approach, or executed by a virtual machine in the online approach. Our fully automatic framework has been applied to real mixed scores used in concerts and the results obtained have permitted to identify bugs in the target IMS."
}
@incollection{KHAN2012141,
title = "Chapter 4 - Pragmatic Directions in Engineering Secure Dependable Systems",
editor = "Ali Hurson and Sahra Sedigh",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "84",
pages = "141 - 167",
year = "2012",
booktitle = "Dependable and Secure Systems Engineering",
issn = "0065-2458",
doi = "https://doi.org/10.1016/B978-0-12-396525-7.00005-8",
url = "http://www.sciencedirect.com/science/article/pii/B9780123965257000058",
author = "M. Farrukh Khan and Raymond A. Paul",
keywords = "Engineering dependable systems, Engineering complex systems, Dependable software/hardware systems, Dependable cloud infrastructure, Secure and reliable systems",
abstract = "All large and complex computer and communications systems have an intrinsic requirement to be dependable since their failure can cause significant losses in terms of life or treasure. Such the systems are expected to have the attributes of reliability, availability, safety, confidentiality, survivability, integrity, and maintainability. Current software and hardware systems continue to evolve in complexity at rapid rates. Although the increase in the complexity of single artifact (such as number of logical decision points in a software package) can often be tracked with Moore's Law like approximations, systems constructed out of larger number of smaller subsystems defy such classifications. The reason for this added complexity is that interactions between the subsystems explode exponentially in the size of the parent system. Yet all component interactions must be addressed exhaustively to predict accurate behavior of the whole system. The challenge that we face is that it is seldom possible to model or test all such interactions in a given system. As a result, building dependable complex systems with realistic assessment of risks of failure is an extremely difficult endeavor. Attempts have been made to ameliorate the difficulty in the engineering of dependable complex systems using lessons from engineering methodologies in other domains. We discuss key attributes of dependable complex systems, with a special emphasis on security where information is involved. We review classical approaches to designing, building, and maintaining dependable complex systems. We present promising features and novel ideas applicable to the lifecycle of dependable complex systems. Most of our discussion is focused within the domain of hardware and software systems. Over time, practitioners in dependable engineering have learned lessons from previous experience and continue to present prescriptive approaches discovered through research and analysis. These lessons and approaches are often applicable to other engineering domains such as construction, transportation, and industrial control. We look at specific engineering challenges and proposed solutions pertaining to the following general domains, with occasional examples from any branch of engineering:dependable hardware/software systems;secure dependable systems;dependable cloud infrastructure and applications. Finally, we conclude with the observation that several approaches are applicable across all these domains and identify accessible techniques that have good potential to increase the dependability of systems. These approaches can be considered as axiomatic in building any future complex systems with a high degree of dependability."
}
@article{JAMRO20181,
title = "Agile and hierarchical round-trip engineering of IEC 61131-3 control software",
journal = "Computers in Industry",
volume = "96",
pages = "1 - 9",
year = "2018",
issn = "0166-3615",
doi = "https://doi.org/10.1016/j.compind.2018.01.004",
url = "http://www.sciencedirect.com/science/article/pii/S0166361517304165",
author = "Marcin Jamro and Dariusz Rzonca",
keywords = "Control software, IEC 61131-3, Implementation, Modeling, Round-trip engineering, Synchronization, Testing",
abstract = "The control software often performs complex, important, and responsible operations in industrial manufacturing systems. The size and complexity of such software are still increasing, thus it is important to provide engineers with methods and tools that simplify the development process. The situation can be improved by modeling using the Model-Driven Development (MDD) approach, standardized implementation process, as well as various testing methods. In the paper, the authors propose the further step, which consists of the comprehensive agile hierarchical round-trip engineering approach dedicated to the IEC 61131-3 control software. It divides the project into four connected parts  model, configuration, implementation, and tests. Such a solution allows developers to work independently and iteratively on various project parts, because changes are discovered automatically and are propagated to suitable views inside the project. The synchronization mechanism has been introduced into the CPDev engineering environment for programming industrial controllers."
}
@article{HE2018109,
title = "Testing bidirectional model transformation using metamorphic testing",
journal = "Information and Software Technology",
volume = "104",
pages = "109 - 129",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.07.010",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301538",
author = "Xiao He and Xing Chen and Sibo Cai and Ying Zhang and Gang Huang",
keywords = "Metamorphic testing, Bidirectional model transformation, Model driven engineering",
abstract = "Context
In model-based software development, bidirectional model transformation (BX) provides a fundamental solution to model synchronization that can retain the consistency among models. Similar to conventional programs, a BX program may also contain bugs. Accordingly, a BX program must be tested prior to being used in practice. A challenging problem of testing BX programs is to construct test oracles (e.g., assertions and expected output models), which are usually difficult and/or expensive to manually specify.
Objective
In we paper, we investigate how to alleviate the oracle problem in BX testing via reducing the costs of developing test oracles.
Method
We propose a metamorphic testing approach for BX. First, we identify three generic metamorphic relations for BX. Afterwards, we define a metamodel MT4MT to establish metamorphic test groups and test scripts. We also propose a testing framework to support metamorphic testing based on MT4MT.
Results
We conducted an experimental study of mutation analysis and a case study on three ATL-based ad-hoc BXs. The results of the experimental study and the case study showed that our approach killed 79.38% mutants and enabled us to test real-world ATL-based ad-hoc BXs. We also demonstrated that MT4MT can be used to test the semantics properties of BXs.
Conclusion
Our approach is an effective and practical approach with lower costs of developing test oracles."
}
@article{AYNUTDINOV2009227,
title = "The prototype string for the km3-scale Baikal neutrino telescope",
journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
volume = "602",
number = "1",
pages = "227 - 234",
year = "2009",
note = "Proceedings of the 3rd International Workshop on a Very Large Volume Neutrino Telescope for the Mediterranean Sea",
issn = "0168-9002",
doi = "https://doi.org/10.1016/j.nima.2008.12.149",
url = "http://www.sciencedirect.com/science/article/pii/S0168900208018998",
author = "V. Aynutdinov and A. Avrorin and V. Balkanov and I. Belolaptikov and D. Bogorodsky and N. Budnev and I. Danilchenko and G. Domogatsky and A. Doroshenko and A. Dyachok and Zh.-A. Dzhilkibaev and S. Fialkovsky and O. Gaponenko and K. Golubkov and O. Gress and T. Gress and O. Grishin and A. Klabukov and A. Klimov and A. Kochanov and K. Konischev and A. Koshechkin and V. Kulepov and D. Kuleshov and L. Kuzmichev and E. Middell and S. Mikheyev and M. Milenin and R. Mirgazov and E. Osipova and G. Pankov and L. Pankov and A. Panfilov and D. Petukhov and E. Pliskovsky and P. Pokhil and V. Poleschuk and E. Popova and V. Prosin and M. Rozanov and V. Rubtzov and A. Sheifler and A. Shirokov and B. Shoibonov and Ch. Spiering and O. Suvorova and B. Tarashansky and R. Wischnewski and I. Yashin and V. Zhukov",
keywords = "Neutrino telescopes, BAIKAL",
abstract = "A prototype string for the future km3-scale Baikal neutrino telescope has been deployed in April, 2008, and is fully integrated into the NT200+ telescope. All basic string elementsoptical modules (with 12/13 hemispherical photomultipliers), 200MHz FADC readout and calibration systemhave been redesigned following experience with NT200+. First results of in-situ operation of this prototype string are presented."
}
@article{TSAI20091578,
title = "Experience on knowledge-based software engineering: A logic-based requirements language and its industrial applications",
journal = "Journal of Systems and Software",
volume = "82",
number = "10",
pages = "1578 - 1587",
year = "2009",
note = "SI: YAU",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2009.03.019",
url = "http://www.sciencedirect.com/science/article/pii/S0164121209000521",
author = "Jeffrey J.P. Tsai and Alan Liu",
keywords = "Formal specification language, Nonmonotonic logic, Formal verification, Automatic code generation, Knowledge-based software engineering",
abstract = "A formal requirements specification language plays an important role in software development. Not only can such language be used for stating requirements specification, but also can be used in many phases of software development life cycle. The FRORL project started from constructing a language with a solid logical foundation and further expanded to research in verification, validation, requirements analysis, debugging, and transformation. Research in this project aided in some industrial applications in which a code generation tool produced software for embedded systems. This article reports the experiences gained from this project and states the value of research in knowledge-based software engineering."
}
@article{AFZAL201886,
title = "The MegaM@Rt2 ECSEL project: MegaModelling at Runtime  Scalable model-based framework for continuous development and runtime validation of complex systems",
journal = "Microprocessors and Microsystems",
volume = "61",
pages = "86 - 95",
year = "2018",
issn = "0141-9331",
doi = "https://doi.org/10.1016/j.micpro.2018.05.010",
url = "http://www.sciencedirect.com/science/article/pii/S014193311830022X",
author = "Wasif Afzal and Hugo Bruneliere and Davide Di Ruscio and Andrey Sadovykh and Silvia Mazzini and Eric Cariou and Dragos Truscan and Jordi Cabot and Abel Gmez and Jess Gorroogoitia and Luigi Pomante and Pavel Smrz",
keywords = "Model-driven engineering, Design time, Runtime, Megamodelling",
abstract = "A major challenge for the European electronic industry is to enhance productivity by ensuring quality of development, integration and maintenance while reducing the associated costs. Model-Driven Engineering (MDE) principles and techniques have already shown promising capabilities, but they still need to scale up to support real-world scenarios implied by the full deployment and use of complex electronic components and systems. Moreover, maintaining efficient traceability, integration, and communication between two fundamental system life cycle phases (design time and runtime) is another challenge requiring the scalability of MDE. This paper presents an overview of the ECSEL 11http://www.ecsel-ju.eu/web/index.php. project entitled MegaModelling at runtime  Scalable model-based framework for continuous development and runtime validation of complex systems (MegaM@Rt2), whose aim is to address the above mentioned challenges facing MDE. Driven by both large and small industrial enterprises, with the support of research partners and technology providers, MegaM@Rt2 aims to deliver a framework of tools and methods for: 1) system engineering/design and continuous development, 2) related runtime analysis and 3) global models and traceability management. Diverse industrial use cases (covering strategic domains such as aeronautics, railway, construction and telecommunications) will integrate and demonstrate the validity of the MegaM@Rt2 solution. This paper provides an overview of the MegaM@Rt2 project with respect to its approach, mission, objectives as well as to its implementation details. It further introduces the consortium as well as describes the work packages and few already produced deliverables."
}
@article{ALMENDROSJIMENEZ2016332,
title = "PTL: A model transformation language based on logic programming",
journal = "Journal of Logical and Algebraic Methods in Programming",
volume = "85",
number = "2",
pages = "332 - 366",
year = "2016",
issn = "2352-2208",
doi = "https://doi.org/10.1016/j.jlamp.2015.06.006",
url = "http://www.sciencedirect.com/science/article/pii/S2352220815000565",
author = "Jess M. Almendros-Jimnez and Luis Iribarne and Jess Lpez-Fernndez and ngel Mora-Segura",
keywords = "Logic programming, Model transformation, Software engineering, Model driven engineering, Domain specific languages",
abstract = "In this paper we present a model transformation language based on logic programming. The language, called PTL (Prolog based Transformation Language), can be considered as a hybrid language in which ATL (Atlas Transformation Language)-style rules are combined with logic rules for defining transformations. ATL-style rules are used to define mappings from source models to target models while logic rules are used as helpers. The implementation of PTL is based on the encoding of the ATL-style rules by Prolog rules. Thus, PTL makes use of Prolog as a transformation engine. We have provided a declarative semantics to PTL and proved the semantics equivalent to the encoded program. We have studied an encoding of OCL (Object Constraint Language) with Prolog goals in order to map ATL to PTL. Thus a subset of PTL can be considered equivalent to a subset of ATL. The proposed language can be also used for model validation, that is, for checking constraints on models and transformations. We have equipped our language with debugging and tracing capabilities which help developers to detect programming errors in PTL rules. Additionally, we have developed an Eclipse plugin for editing PTL programs, as well as for debugging, tracing and validation. Finally, we have evaluated the language with several transformation examples as well as tested the performance with large models."
}
@article{STRUBER2017196,
title = "A text-based visual notation for the unit testing of model-driven tools",
journal = "Computer Languages, Systems & Structures",
volume = "49",
pages = "196 - 215",
year = "2017",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2016.08.004",
url = "http://www.sciencedirect.com/science/article/pii/S1477842416300276",
author = "Daniel Strber and Felix Rieger and Gabriele Taentzer",
keywords = "Model-driven engineering, Tools, Model notation, Flexible modeling, Testing",
abstract = "During the unit testing of model-driven tools, a large number of models and test classes needs to be managed and maintained. Typically, some of these artifacts are specified manually, some are generated automatically. Existing approaches to test management rely on the available visual and textual modeling notations. As these notations are not tailored to unit testing, distinct maintainability trade-offs arise. In this paper, we propose a notation that aims to combine the benefits of visual and text-based approaches. The notation is at the same time visual and text-based, as it uses ASCII characters to emulate the familiar graphical notations. In our evaluation based on real models, we identify problematic model shapes challenging the scalability our notation, while finding that it is well-suited to capture typical test models."
}
@article{HUTCHESSON2013525,
title = "Trusted Product Lines",
journal = "Information and Software Technology",
volume = "55",
number = "3",
pages = "525 - 540",
year = "2013",
note = "Special Issue on Software Reuse and Product Lines",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2012.06.005",
url = "http://www.sciencedirect.com/science/article/pii/S0950584912001085",
author = "Stuart Hutchesson and John McDermid",
keywords = "Software Product Lines, High-integrity software, DO-178B/ED-12B, SPARK, Model transformation, GSN",
abstract = "Context
The paper addresses the use of a Software Product Line approach in the context of developing software for a high-integrity, regulated domain such as civil aerospace. The success of a Software Product Line approach must be judged on whether useful products can be developed more effectively (lower cost, reduced schedule) than with traditional single-system approaches. When developing products for regulated domains, the usefulness of the product is critically dependent on the ability of the development process to provide approval evidence for scrutiny by the regulating authority.
Objective
The objective of the work described is to propose a framework for arguing that a product instantiated using a Software Product Line approach can be approved and used within a regulated domain, such that the development cost of that product would be less than if it had been developed in isolation.
Method
The paper identifies and surveys the issues relating the adoption of Software Product Lines as currently understood (including related technologies such as feature modelling, component-based development and model transformation) when applied to high-integrity software development. We develop an argument framework using Goal Structuring Notation to structure the claims made and the evidence required to support the approval of an instantiated product in such domains. Any unsubstantiated claims or missing/sub-standard evidence is identified, and we propose potential approaches or pose research questions to help address this.
Results
The paper provides an argument framework supporting the use of a Software Product Line approach within a high-integrity regulated domain. It shows how lifecycle evidence can be collected, managed and used to credibly support a regulatory approval process, and provides a detailed example showing how claims regarding model transformation may be supported. Any attempt to use a Software Product Line approach in a regulated domain will need to provide evidence to support their approach in accordance with the argument outlined in the paper.
Conclusion
Product Line practices may complicate the generation of convincing evidence for approval of instantiated products, but it is possible to define a credible Trusted Product Line approach."
}
@incollection{MARIANI2015157,
title = "Chapter Four - Recent Advances in Automatic Black-Box Testing",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "99",
pages = "157 - 193",
year = "2015",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2015.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S0065245815000315",
author = "Leonardo Mariani and Mauro Pezz and Daniele Zuddas",
keywords = "Black-box testing, Model-based testing, Random testing, Testing with complex inputs, Combinatorial interaction testing, Test automation",
abstract = "Research in black-box testing has produced impressive results in the past 40 years, addressing many aspects of the problem that span from integration with the development process, to test case generation and execution. In the past few years, the research in this area has focused mostly on the automation of black-box approaches to improve applicability and scalability. This chapter surveys the recent advances in automatic black-box testing, covering contributions from 2010 to 2014, presenting the main research results and discussing the research trends."
}
@article{HABERMAIER201544,
title = "Executable Specifications of Safety-Critical Systems with S#",
journal = "IFAC-PapersOnLine",
volume = "48",
number = "7",
pages = "44 - 49",
year = "2015",
note = "5th IFAC International Workshop on Dependable Control of Discrete Systems",
issn = "2405-8963",
doi = "https://doi.org/10.1016/j.ifacol.2015.06.471",
url = "http://www.sciencedirect.com/science/article/pii/S2405896315007107",
author = "Axel Habermaier and Johannes Leupolz and Wolfgang Reif",
keywords = "safety analysis, executable specification, design tools and techniques, formal methods, model checking, deductive cause consequence analysis, software engineering",
abstract = "Abstract:
Model-based safety analysis techniques use formal methods to rigorously assess the risks associated with safety-critical systems. The adequacy of the results obtained from those formal techniques, however, is greatly influenced by the quality and comprehensibility of the underlying formal models. We introduce our S# modeling framework (pronounced "safety sharp"), an executable, systematic, high-level specification language and tool suite specifically designed for the convenient modeling and formal analysis of safety-critical systems. This paper shows how S# facilitates and improves model simulation, debugging, and testing during all stages of the development of such systems."
}
@article{LAUCIELLO2016310,
title = "A high yield optimized method for the production of acylated ACPs enabling the analysis of enzymes involved in P. falciparum fatty acid biosynthesis",
journal = "Biochemistry and Biophysics Reports",
volume = "8",
pages = "310 - 317",
year = "2016",
issn = "2405-5808",
doi = "https://doi.org/10.1016/j.bbrep.2016.09.017",
url = "http://www.sciencedirect.com/science/article/pii/S2405580816302114",
author = "Leonardo Lauciello and Gabriela Lack and Leonardo Scapozza and Remo Perozzo",
keywords = "Acyl carrier protein, Acylation, Fatty acid biosynthesis, , Enzyme kinetics, Natural substrates",
abstract = "The natural substrates of the enzymes involved in type-II fatty acid biosynthesis (FAS-II) are acylated acyl carrier proteins (acyl-ACPs). The state of the art method to produce acyl-ACPs involves the transfer of a phosphopantetheine moiety from CoA to apo-ACP by E. coli holo-ACP synthase (EcACPS), yielding holo-ACP which subsequently becomes thioesterified with free fatty acids by the E. coli acyl-ACP synthase (EcAAS). Alternatively, acyl-ACPs can be synthesized by direct transfer of acylated phosphopantetheine moieties from acyl-CoA to apo-ACP by means of EcACPS. The need for native substrates to characterize the FAS-II enzymes of P. falciparum prompted us to investigate the potential and limit of the two methods to efficiently acylate P. falciparum ACP (PfACP) with respect to chain length and -modification and in preparative amounts. The EcAAS activity is found to be independent from the oxidation state at the -position and accepts fatty acids as substrates with chain lengths starting from C8 to C20, whereas EcACPS accepts very efficiently acyl-CoAs with chain lengths up to C16, and with decreasing activity also longer chains (C18 to C20). Methods were developed to synthesize and purify preparative amounts of high quality natural substrates that are fully functional for the enzymes of the P. falciparum FAS-II system."
}
@article{PRASETYA2018223,
title = "Temporal algebraic query of test sequences",
journal = "Journal of Systems and Software",
volume = "136",
pages = "223 - 236",
year = "2018",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.07.014",
url = "http://www.sciencedirect.com/science/article/pii/S016412121730153X",
author = "I.S.W.B. Prasetya",
keywords = "Query based testing, Property based testing, LTL Test oracles, Algebraic test oracles, Dynamic analysis",
abstract = "Nowadays tools can generate test suites consisting of large number of test sequences. The used algorithms are typically random-based. Although more advanced variations may incorporate an advanced search algorithm to cover difficult scenarios, many decisions still have to be made randomly simply because no information is available to calculate the best decision. Because of this, many of the generated sequences may be redundant, while some others may be rare and hard to get. This paper presents a rich formalism that is based on a mix of algebraic relations and Linear Temporal Logic (LTL) to query test suites, and an efficient algorithm to execute such queries. Queries can be used as correctness specifications (oracles) to validate a test suite. They are however more general as they can be used to filter out test sequences with interesting properties, e.g. to archive them for future use. The proposed formalism is quite expressive: it can express algebraic equations with logical variables, Hoare triples, class invariants, as well as their temporal modalities. An evaluation of the query algorithms performance is included in this paper. The whole query framework has been implemented in a testing tool for Java called T3i."
}
@incollection{JILANI2018,
title = "Advances in Applications of Object Constraint Language for Software Engineering",
series = "Advances in Computers",
publisher = "Elsevier",
year = "2018",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2017.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0065245817300554",
author = "Atif A. Jilani and Muhammad Z. Iqbal and Muhammad U. Khan and Muhammad Usman",
keywords = "Object Constraint Language, Model-driven engineering, Secondary study, Software engineering",
abstract = "Object Constraint Language (OCL) is a standard language defined by Object Management Group for specifying constraints on models. Since its introduction as part of Unified Modeling Language, OCL has received significant attention by researchers with works in the literature ranging from temporal extensions of OCL to automated test generation by solving OCL constraints. In this chapter, we provide a survey of the various works discussed in literature related to OCL with the aim of highlighting the advances made in the field. We classify the literature into five broad categories and provide summaries for various works in the literature. The chapter also provides insights and highlights the potentials areas of further research in the field."
}
@incollection{2011183,
editor = "Eric Conrad",
booktitle = "Eleventh Hour CISSP",
publisher = "Syngress",
address = "Boston",
pages = "183 - 196",
year = "2011",
isbn = "978-1-59749-566-0",
doi = "https://doi.org/10.1016/B978-1-59749-566-0.00016-3",
url = "http://www.sciencedirect.com/science/article/pii/B9781597495660000163"
}
@article{RAMLER2018248,
title = "Adapting automated test generation to GUI testing of industry applications",
journal = "Information and Software Technology",
volume = "93",
pages = "248 - 263",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916303676",
author = "Rudolf Ramler and Georg Buchgeher and Claus Klammer",
abstract = "Context
Automated test generation promises to improve the effectiveness of software testing and to reduce the involved manual effort. While automated test generation has been successfully applied for code-level API testing, it has not found widespread adoption in practice for testing of graphical user interfaces. Tools for test generation do not support GUI testing out-of-the-box but require dedicated extensions.
Objective
This paper explores the applicability of automated test generation for testing GUIs of industry applications. We propose a test adapter approach to bridge the gap between automated test generation tools and industry applications.
Method
A multiple case study was conducted in which automated test generation with test adapters has been applied at the unit, integration, and system test level in three industry projects from two different companies.
Results
Automated test generation via test adapters could be applied at all test levels. It has led to an increase of coverage as well as the detection of new defects that were not found by preceding testing activities in the projects. While test adapters can easily be implemented at the unit test level, their complexity and the corresponding effort for providing adapter implementations rises at higher test levels.
Conclusion
Test adapters can be used for applying automated test generation for testing GUIs of industry applications. They bridge the gap between automated test generation tools and industry applications. The development of test adapters requires no tool-specific knowledge and can be performed by members of the development team."
}
@incollection{ALEXANDER2007109,
title = "Chapter 5 - Protocol testing",
editor = "Tom Alexander",
booktitle = "Optimizing and Testing WLANs",
publisher = "Newnes",
address = "Burlington",
pages = "109 - 135",
year = "2007",
isbn = "978-0-7506-7986-2",
doi = "https://doi.org/10.1016/B978-075067986-2/50006-9",
url = "http://www.sciencedirect.com/science/article/pii/B9780750679862500069",
author = "Tom Alexander",
abstract = "Publisher Summary
This chapter describes the metrics and measurements pertinent to the wireless LAN (WLAN) Medium Access Control (MAC), as well as the Transmission Control Protocol (TCP)/Internet Protocol (IP) stack. A functional test is concerned with verifying that a device or system functions properly. A performance test is much more broad-based and generic, and is aimed at characterizing a device or system according to some well-defined metric. Indeed, a single performance metric may be applicable to a wide variety of devices. WLAN protocol tests, almost without exception, inject packet traffic into the device under test (DUT) as test stimuli, and measure the DUT response in terms of the number or rate of specific types of packets that it generates in turn. As tests are being performed on the MAC layer, the RF characteristics of the packet traffic are rarely of much interest, beyond ensuring that these characteristics do not skew the test results by causing unexpected issues at the physical (PHY) layer. Conformance tests are almost exclusively performed in a highly controlled and isolated environment, as external interference at the wrong moment can invalidate an entire test. It is found that IEEE 802.11 protocol conformance tests are performed on the different aspects of the WLAN-MAC and PHY protocol specifications."
}
@incollection{FRIEDRICH2014139,
title = "Chapter 11 - Knowledge Engineering for Configuration Systems",
editor = "Alexander Felfernig and Lothar Hotz and Claire Bagley and Juha Tiihonen",
booktitle = "Knowledge-Based Configuration",
publisher = "Morgan Kaufmann",
address = "Boston",
pages = "139 - 155",
year = "2014",
isbn = "978-0-12-415817-7",
doi = "https://doi.org/10.1016/B978-0-12-415817-7.00011-6",
url = "http://www.sciencedirect.com/science/article/pii/B9780124158177000116",
author = "Gerhard Friedrich and Dietmar Jannach and Markus Stumptner and Markus Zanker",
keywords = "Knowledge-based Configuration, Development Life Cycle of Configurators, Debugging of Configuration Knowledge Bases, Organizational Challenges",
abstract = "Developing a product configuration system is a nontrivial and challenging task for various reasons. First, the domain knowledge that has to be encoded into the system is often spread over several departments or functions within a company. Besides that, in many cases data from existing information systems have to be integrated into the configurator. Finally, the business rules or technical constraints that define the space of possible configurations can be relatively complex and also subject to frequent changes. This makes acquiring and encoding domain knowledge as well as testing and debugging particularly demanding tasks. In this chapter, we give an overview of the challenges when developing a knowledge-based configuration system. We will particularly focus on questions related to the knowledge acquisition process and will additionally show how model-based debugging techniques can be applied to support the knowledge engineer in the testing and debugging process."
}
@article{KAMEYAMA2015120,
title = "Combinators for impure yet hygienic code generation",
journal = "Science of Computer Programming",
volume = "112",
pages = "120 - 144",
year = "2015",
note = "Selected and extended papers from Partial Evaluation and Program Manipulation 2014",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2015.08.007",
url = "http://www.sciencedirect.com/science/article/pii/S016764231500249X",
author = "Yukiyoshi Kameyama and Oleg Kiselyov and Chung-chieh Shan",
keywords = "Multi-stage programming, Mutable state and control effects, CPS, Higher-order abstract syntax, Lexical scope",
abstract = "Code generation is the leading approach to making high-performance software reusable. Effects are indispensable in code generators, whether to report failures or to insert let-statements and if-guards. Extensive painful experience shows that unrestricted effects interact with generated binders in undesirable ways to produce unexpectedly unbound variables, or worse, unexpectedly bound ones. These subtleties hinder domain experts in using and extending the generator. A pressing problem is thus to express the desired effects while regulating them so that the generated code is correct, or at least correctly scoped, by construction. We present a code-combinator framework that lets us express arbitrary monadic effects, including mutable references and delimited control, that move open code across generated binders. The static types of our generator expressions not only ensure that a well-typed generator produces well-typed and well-scoped code. They also express the lexical scopes of generated binders and prevent mixing up variables with different scopes. For the first time ever we demonstrate statically safe and well-scoped loop interchange and constant factoring from arbitrarily nested loops. Our framework is implemented as a Haskell library that embeds an extensible typed higher-order domain-specific language. It may be regarded as staged Haskell. To become practical, the library relies on higher-order abstract syntax and polymorphism over generated type environments, and is written in a mature language."
}
@article{MARTINEZ201846,
title = "Feature location benchmark for extractive software product line adoption research using realistic and synthetic Eclipse variants",
journal = "Information and Software Technology",
volume = "104",
pages = "46 - 59",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301472",
author = "Jabier Martinez and Tewfik Ziadi and Mike Papadakis and Tegawend F. Bissyand and Jacques Klein and Yves le Traon",
keywords = "Feature location, Software families, Eclipse, Benchmark, Software product lines, Static analysis, Information retrieval",
abstract = "Context: It is common belief that high impact research in software reuse requires assessment in non-trivial, comparable, and reproducible settings. However, software artefacts and common representations are usually unavailable. Also, establishing a representative ground truth is a challenging and debatable subject. Feature location in the context of software families, which is key for software product line adoption, is a research field that is becoming more mature with a high proliferation of techniques. Objective: We present EFLBench, a benchmark and a framework to provide a common ground for the evaluation of feature location techniques in families of systems. Method: EFLBench leverages the efforts made by the Eclipse Community which provides feature-based family artefacts and their plugin-based implementations. Eclipse is an active and non-trivial project and thus, it establishes an unbiased ground truth which is realistic and challenging. Results: EFLBench is publicly available and supports all tasks for feature location techniques integration, benchmark construction and benchmark usage. We demonstrate its usage, simplicity and reproducibility by comparing four techniques in Eclipse releases. As an extension of our previously published work, we consider a decade of Eclipse releases and we also contribute an approach to automatically generate synthetic Eclipse variants to benchmark feature location techniques in tailored settings. We present and discuss three strategies for this automatic generation and we present the results using different settings. Conclusion: EFLBench is a contribution to foster the research in feature location in families of systems providing a common framework and a set of baseline techniques and results."
}
@incollection{MEHTA2015479,
title = "Chapter 16 - Asset management systems",
editor = "B.R. Mehta and Y.J. Reddy",
booktitle = "Industrial Process Automation Systems",
publisher = "Butterworth-Heinemann",
address = "Oxford",
pages = "479 - 506",
year = "2015",
isbn = "978-0-12-800939-0",
doi = "https://doi.org/10.1016/B978-0-12-800939-0.00016-4",
url = "http://www.sciencedirect.com/science/article/pii/B9780128009390000164",
author = "B.R. Mehta and Y.J. Reddy",
keywords = "Asset, smart instrument, fault models, EDDL, diagnostics, predictive, preventive, FDI",
abstract = "Asset management systems are a class of software and hardware applications used in the process plants for the efficient and optimum utilization of the equipment. An asset management system helps the plant to drive the operational and maintenance excellence. There are various systems developed for various applications; they together as a suite help to monitor, analyze, predict, and report the asset performance. This chapter deals with plant asset management systems, instrument asset management systems (IAMS), and some of the key rendering technologies and standards. The key drivers for the deployment of such systems and role-based access to the systems are discussed at length. Some of the communication technologies mentioned in this chapter are dealt separately as individual chapters. At the end of the topic, the key features from the asset management systems from an instrumentation perspective are dealt in much more length with little overview on enterprise asset management systems. Some of the rendering technologies such as DD/EDDL and FDT are discussed in more detail for the instrumentation engineers to get more familiar with these technologies and hence help them to choose the same selectively, considering the needs of the plants. The plant asset management systems and the modeling and underlying benefits of such systems are also discussed to create awareness on the possibilities of asset management in a plant context extending the limits from instrumentation and control to assets such as pumps, electrical machinery, and rotating equipment."
}
@article{RODRIGUESDASILVA2015139,
title = "Model-driven engineering: A survey supported by the unified conceptual model",
journal = "Computer Languages, Systems & Structures",
volume = "43",
pages = "139 - 155",
year = "2015",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2015.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S1477842415000408",
author = "Alberto Rodrigues da Silva",
keywords = "Model, Metamodel, Modeling language, Software system, Model-driven engineering, Model-driven approaches",
abstract = "During the last decade a new trend of approaches has emerged, which considers models not just documentation artefacts, but also central artefacts in the software engineering field, allowing the creation or automatic execution of software systems starting from those models. These proposals have been classified generically as Model-Driven Engineering (MDE) and share common concepts and terms that need to be abstracted, discussed and understood. This paper presents a survey on MDE based on a unified conceptual model that clearly identifies and relates these essential concepts, namely the concepts of system, model, metamodel, modeling language, transformations, software platform, and software product. In addition, this paper discusses the terminologies relating MDE, MDD, MDA and others. This survey is based on earlier work, however, contrary to those, it intends to give a simple, broader and integrated view of the essential concepts and respective terminology commonly involved in the MDE, answering to key questions such as: What is a model? What is the relation between a model and a metamodel? What are the key facets of a modeling language? How can I use models in the context of a software development process? What are the relations between models and source code artefacts and software platforms? and What are the relations between MDE, MDD, MDA and other MD approaches?"
}
@article{GEORG2015109,
title = "Synergy between Activity Theory and goal/scenario modeling for requirements elicitation, analysis, and evolution",
journal = "Information and Software Technology",
volume = "59",
pages = "109 - 135",
year = "2015",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914002419",
author = "Geri Georg and Gunter Mussbacher and Daniel Amyot and Dorina Petriu and Lucy Troup and Saul Lozano-Fuentes and Robert France",
keywords = "Requirements engineering, Activity Theory, User Requirements Notation, Goal modeling, Scenario modeling",
abstract = "Context
It is challenging to develop comprehensive, consistent, analyzable requirements models for evolving requirements. This is particularly critical for certain highly interactive types of socio-technical systems that involve a wide range of stakeholders with disparate backgrounds; system success is often dependent on how well local social constraints are addressed in system design.
Objective
This paper describes feasibility research, combining a holistic social system perspective provided by Activity Theory (AT), a psychological paradigm, with existing system development methodologies and tools, specifically goal and scenario modeling.
Method
AT is used to understand the relationships between a system, its stakeholders, and the systems evolving context. The User Requirements Notation (URN) is used to produce rigorous, analyzable specifications combining goal and scenario models. First, an AT language was developed constraining the framework for automation, second consistency heuristics were developed for constructing and analyzing combined AT/URN models, third a combined AT/URN methodology was developed, and consequently applied to a proof-of-concept system.
Results
An AT language with limited tool support was developed, as was a combined AT/URN methodology. This methodology was applied to an evolving disease management system to demonstrate the feasibility of adapting AT for use in system development with existing methodologies and tools. Bi-directional transformations between the languages allow proposed changes in system design to be propagated to AT models for use in stakeholder discussions regarding system evolution.
Conclusions
The AT framework can be constrained for use in requirements elicitation and combined with URN tools to provide system designs that include social system perspectives. The developed AT/URN methodology can help engineers to track the impact on system design due to requirement changes triggered by changes in the systems social context. The methodology also allows engineers to assess the impact of proposed system design changes on the social elements of the system context."
}
@article{KESSENTINI2018,
title = "Automated metamodel/model co-evolution: A search-based approach",
journal = "Information and Software Technology",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301915",
author = "Wael Kessentini and Houari Sahraoui and Manuel Wimmer",
keywords = "Metamodel/model co-evolution, Model migration, Coupled evolution, Search based software engineering",
abstract = "Context: Metamodels evolve over time to accommodate new features, improve existing designs, and fix errors identified in previous releases. One of the obstacles that may limit the adaptation of new metamodels by developers is the extensive manual changes that have been applied to migrate existing models. Recent studies addressed the problem of automating the metamodel/model co-evolution based on manually defined migration rules. The definition of these rules requires the list of changes at the metamodel level which are difficult to fully identify. Furthermore, different possible alternatives may be available to translate a metamodel change to a model change. Thus, it is hard to generalize these co-evolution rules. Objective: We propose an alternative automated approach for the metamodel/model co-evolution. The proposed approach refines an initial model instantiated from the previous metamodel version to make it as conformant as possible to the new metamodel version by finding the best compromise between three objectives, namely minimizing (i) the non-conformities with new metamodel version, (ii) the changes to existing models, and (iii) the textual and structural dissimilarities between the initial and revised models. Method: We formulated the metamodel/model co-evolution as a multi-objective optimization problem to handle the different conflicting objectives using the Non-dominated Sorting Genetic Algorithm II (NSGA-II) and the Multi-Objective Particle Swarm Optimization (MOPSO). Results: We evaluated our approach on several evolution scenarios extracted from different widely used metamodels. The results confirm the effectiveness of our approach with average manual correctness, precision and recall respectively higher than 91%, 88% and 89% on the different co-evolution scenarios. Conclusion: A comparison with our previous work confirms the out-performance of our multi-objective formulation."
}
@article{OHARA201473,
title = "Glycan receptors of the Polyomaviridae: structure, function, and pathogenesis",
journal = "Current Opinion in Virology",
volume = "7",
pages = "73 - 78",
year = "2014",
note = "Virus-glycan interactions and pathogenesis / Viruses and RNA interference",
issn = "1879-6257",
doi = "https://doi.org/10.1016/j.coviro.2014.05.004",
url = "http://www.sciencedirect.com/science/article/pii/S1879625714001242",
author = "Samantha D OHara and Thilo Stehle and Robert Garcea",
abstract = "Multiple glycans have been identified as potential cell surface binding motifs for polyomaviruses (PyVs) using both crystallographic structural determinations and in vitro binding assays. However, binding alone does not necessarily imply that a glycan is a functional receptor, and confirmation that specific glycans are important for infection has proved challenging. In vivo analysis of murine polyomavirus (MPyV) infection has shown that subtle alterations in PyVglycan interactions alone can result in dramatic changes in pathogenicity, implying that similar effects will be found for other PyVs. Our discussion will review the assays used for determining virusglycan binding, and how these relate to known PyV tropism and pathogenesis."
}
@article{GONZALEZALONSO2012889,
title = "Towards a new open communication standard between homes and service robots, the DHCompliant case",
journal = "Robotics and Autonomous Systems",
volume = "60",
number = "6",
pages = "889 - 900",
year = "2012",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2012.01.006",
url = "http://www.sciencedirect.com/science/article/pii/S0921889012000188",
author = "Ignacio Gonzlez Alonso and Omar lvarez Fres and Alberto Alonso Fernndez and Pablo Gmez del Torno and J.M. Maestre and M.d.P. Almudena Garca Fuente",
keywords = "Home&Building automation, Interoperability, Service robot, UPnP, SysML, DHCompliant",
abstract = "The interoperability of service robots and digital home was a user demand from the past years. In response to that necessity, the researchers from the Infobotica Research Group, in cooperation with a group of companies and universities, have proposed a new open standard and architecture. It is composed of different virtual services, protocols as well as an open adapters architecture, on top of the UPnP protocol stack. The proposed application protocols and the general architecture provide a communication environment for positioning devices, rules compliance checks, the collaboration between devices and managing energy efficiently. The different tools, adapters, and protocols, developed within the DHCompliant architecture, have defined a new level of application protocol that has allowed increased integration of those modules into home automation, improving their interoperability, and allowing the addition of new services to the same standard and commercial hardware."
}
@article{DABHOLKAR2009756,
title = "The role of perceived control and gender in consumer reactions to download delays",
journal = "Journal of Business Research",
volume = "62",
number = "7",
pages = "756 - 760",
year = "2009",
issn = "0148-2963",
doi = "https://doi.org/10.1016/j.jbusres.2008.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S0148296308001380",
author = "Pratibha A. Dabholkar and Xiaojing Sheng",
keywords = "Perceived control, Gender differences, Download delays, Attitudes, Intentions, Online marketing",
abstract = "An empirical study finds that perceived control strongly mediates the effects of perceived speed of a Web site download on consumers' attitudes and intentions to use the Web site. Moreover, results show that men are more likely to react positively to the perceived speed of a Web site download, whereas women are more likely to base their reactions on perceptions of control in the context of download delays. In contrast to past online research, the gender differences are intrinsic in two waysthey are context independent, and they are not caused by length of Internet experience, extent of Internet usage, or type of Internet connections."
}
@incollection{AERTS2017287,
title = "Chapter 19 - Model-Based Testing of Cyber-Physical Systems",
editor = "Houbing Song and Danda B. Rawat and Sabina Jeschke and Christian Brecher",
booktitle = "Cyber-Physical Systems",
publisher = "Academic Press",
address = "Boston",
pages = "287 - 304",
year = "2017",
series = "Intelligent Data-Centric Systems",
isbn = "978-0-12-803801-7",
doi = "https://doi.org/10.1016/B978-0-12-803801-7.00019-5",
url = "http://www.sciencedirect.com/science/article/pii/B9780128038017000195",
author = "A. Aerts and M. Reniers and M.R. Mousavi",
keywords = "Cyber-physical systems, V-model, Model-based testing, Conformance, Test-case generation, Test coverage",
abstract = "Cyber-physical systems (CPSs) are the result of the integration of connected computer systems with the physical world. They feature complex interactions that go beyond traditional communication schemes and protocols in computer systems. One distinguished feature of such complex interactions is the tight coupling between discrete and continuous interactions, captured by hybrid system models. Due to the complexity of CPSs, providing rigorous and model-based analysis methods and tools for verifying correctness of such systems is of the utmost importance. Model-based testing (MBT) is one such verification technique that can be used for checking the conformance of an implementation of a system to its specification (model). In this chapter, we first review the main concepts and techniques in MBT. Subsequently, we review the most common modeling formalisms for CPSs, with focus on hybrid system models. Subsequently, we provide a brief overview of conformance relations and conformance testing techniques for CPSs."
}
@incollection{SLATTEN2013119,
title = "Chapter 4 - Model-Driven Engineering of Reliable Fault-Tolerant SystemsA State-of-the-Art Survey",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "91",
pages = "119 - 205",
year = "2013",
booktitle = "Advances in Computers",
issn = "0065-2458",
doi = "https://doi.org/10.1016/B978-0-12-408089-8.00004-5",
url = "http://www.sciencedirect.com/science/article/pii/B9780124080898000045",
author = "Vidar Sltten and Peter Herrmann and Frank Alexander Kraemer",
keywords = "Model-driven engineering, Fault tolerance, Reliability, Verification, Survey",
abstract = "To improve the reliability of a system, we can add fault-tolerance mechanisms. This, however, leads to a rise of complexity that increases the probability of software faults being introduced. Hence, unless the process is handled carefully, adding fault tolerance may even lead to a less reliable system. As a way to deal with the inherently high level of complexity of fault-tolerant systems, some research groups have turned to the paradigm of model-driven engineering. This results in a research field that crosscuts the established fields of software engineering, system verification, fault-tolerant systems and distributed systems. Many works are presented in the context of one of these traditional fields, making it difficult to get a good overview of what is presently offered. We survey 10 approaches for model-driven engineering of reliable fault-tolerant systems and present 13 characteristics classifying the approaches in a manner useful for both users and developers of such approaches. We further discuss the state of the field and what the future may bring."
}
@article{ZHANG2006209,
title = "Using source transformation to test and model check implicit-invocation systems",
journal = "Science of Computer Programming",
volume = "62",
number = "3",
pages = "209 - 227",
year = "2006",
note = "Special issue on Source code analysis and manipulation (SCAM 2005)",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2006.04.008",
url = "http://www.sciencedirect.com/science/article/pii/S0167642306000955",
author = "Hongyu Zhang and Jeremy S. Bradbury and James R. Cordy and Juergen Dingel",
keywords = "Source transformation, Domain-specific language, Verification, Testing, Model checking, Implicit invocation",
abstract = "In this paper we present a source transformation-based framework to support uniform testing and model checking of implicit-invocation software systems. The framework includes a new domain-specific programming language, the Implicit-Invocation Language (IIL), explicitly designed for directly expressing implicit-invocation software systems, and a set of formal rule-based source transformation tools that allow automatic generation of both executable and formal verification artifacts. We provide details of these transformation tools, evaluate the framework in practice, and discuss the benefits of formal automatic transformation in this context. Our approach is designed not only to advance the state-of-the-art in validating implicit-invocation systems, but also to further explore the use of automated source transformation as a uniform vehicle to assist in the implementation, validation and verification of programming languages and software systems in general."
}
@article{BELETE201749,
title = "An overview of the model integration process: From pre-integration assessment to testing",
journal = "Environmental Modelling & Software",
volume = "87",
pages = "49 - 63",
year = "2017",
issn = "1364-8152",
doi = "https://doi.org/10.1016/j.envsoft.2016.10.013",
url = "http://www.sciencedirect.com/science/article/pii/S1364815216308805",
author = "Getachew F. Belete and Alexey Voinov and Gerard F. Laniak",
keywords = "Integrated modeling, Interoperability, Interfaces, Wrapping, Components, Web services",
abstract = "Integration of models requires linking models which can be developed using different tools, methodologies, and assumptions. We performed a literature review with the aim of improving our understanding of model integration process, and also presenting better strategies for building integrated modeling systems. We identified five different phases to characterize integration process: pre-integration assessment, preparation of models for integration, orchestration of models during simulation, data interoperability, and testing. Commonly, there is little reuse of existing frameworks beyond the development teams and not much sharing of science components across frameworks. We believe this must change to enable researchers and assessors to form complex workflows that leverage the current environmental science available. In this paper, we characterize the model integration process and compare integration practices of different groups. We highlight key strategies, features, standards, and practices that can be employed by developers to increase reuse and interoperability of science software components and systems."
}
@article{DASILVA2015527,
title = "Using a multi-method approach to understand Agile software product lines",
journal = "Information and Software Technology",
volume = "57",
pages = "527 - 542",
year = "2015",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914001438",
author = "Ivonei Freitas da Silva and Paulo Anselmo da Mota Silveira Neto and Pdraig OLeary and Eduardo Santana de Almeida and Silvio Romero de Lemos Meira",
keywords = "Agile, Software product lines, Multi-method approach, Case study, Mapping study, Expert opinion",
abstract = "Context
Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method.
Objective
This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach.
Method
Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence.
Results
This combination results in 23 findings that provide evidence on how Agile and SPL could be combined.
Conclusion
Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies."
}
@incollection{2012549,
editor = "Eric Conrad and Seth Misenar and Joshua Feldman",
booktitle = "CISSP Study Guide (Second Edition)",
publisher = "Syngress",
edition = "Second Edition",
address = "Boston",
pages = "549 - 577",
year = "2012",
isbn = "978-1-59749-961-3",
doi = "https://doi.org/10.1016/B978-1-59749-961-3.09984-2",
url = "http://www.sciencedirect.com/science/article/pii/B9781597499613099842"
}
@article{PENG201633,
title = "Reusing simulation experiment specifications to support developing models by successive extension",
journal = "Simulation Modelling Practice and Theory",
volume = "68",
pages = "33 - 53",
year = "2016",
issn = "1569-190X",
doi = "https://doi.org/10.1016/j.simpat.2016.07.006",
url = "http://www.sciencedirect.com/science/article/pii/S1569190X16302118",
author = "Danhua Peng and Tom Warnke and Fiete Haack and Adelinde M. Uhrmacher",
keywords = "Model extension, Stochastic modeling, Simulation experiments, Experiment specification, Experiment generation and execution",
abstract = "Model development is a successive process of validating, revising, and extending models, and requires iterative execution of simulation experiments. While developing a model by extension, executing similar simulation experiments to those performed with the original model reveals important behavioral insights into the extended model. An automatic generation and execution of these simulation experiments can provide valuable support in the process of developing models. A prerequisite is an explicit specification of simulation experiments. Therefore, we annotate models with simulation experiments that are specified in a declarative domain specific language SESSL (Simulation Experiment Specification via a Scala Layer). Based on experiment specifications of the original model, we introduce a mechanism to automatically generate and execute simulation experiments for the extended model with necessary adaptations. Furthermore, as we experiment with stochastic models, we exploit statistical model checking and specify the expected model behavioral properties, against which the simulation results are checked. Thereby, when a model is extended, the original experiment specifications are reused, adapted, and applied to the extended model. Accordingly, the generated simulation trajectories are probed to check whether the expected properties hold with a certain probability or not. Thus, more fast and frequent feedback during model development can be provided to the modeler. Based on a model of membrane related dynamics, we show how the developed approach can be used in successively extending models."
}
@incollection{2015481,
title = "Index",
editor = "Carl S. Young",
booktitle = "The Science and Technology of Counterterrorism",
publisher = "Butterworth-Heinemann",
address = "Boston",
pages = "481 - 492",
year = "2015",
isbn = "978-0-12-420056-2",
doi = "https://doi.org/10.1016/B978-0-12-420056-2.09981-4",
url = "http://www.sciencedirect.com/science/article/pii/B9780124200562099814"
}
@article{LOPEZFERNANDEZ2016104,
title = "Combining unit and specification-based testing for meta-model validation and verification",
journal = "Information Systems",
volume = "62",
pages = "104 - 135",
year = "2016",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2016.06.008",
url = "http://www.sciencedirect.com/science/article/pii/S0306437916301934",
author = "Jess J. Lpez-Fernndez and Esther Guerra and Juan de Lara",
keywords = "Model-driven engineering, Meta-modelling, Domain-specific modelling languages, Validation & verification, Meta-model quality",
abstract = "Meta-models play a cornerstone role in Model-Driven Engineering as they are used to define the abstract syntax of modelling languages, and so models and all sorts of model transformations depend on them. However, there are scarce tools and methods supporting their Validation and Verification (V&V), which are essential activities for the proper engineering of meta-models. In order to fill this gap, we propose two complementary meta-model V&V languages. The first one has similar philosophy to the xUnit framework, as it enables the definition of meta-model unit test suites comprising model fragments and assertions on their (in-)correctness. The second one is directed to express and verify expected properties of a meta-model, including domain and design properties, quality criteria and platform-specific requirements. As a proof of concept, we have developed tooling for both languages in the Eclipse platform, and illustrate its use within an example-driven approach for meta-model construction. The expressiveness of our languages is demonstrated by their application to build a library of meta-model quality issues, which has been evaluated over the ATL zoo of meta-models and some OMG specifications. The results show that integrated support for meta-model V&V (as the one we propose here) is urgently needed in meta-modelling environments."
}
@article{JIANG2006584,
title = "Modeling real-time communication systems: Practices and experiences in Motorola",
journal = "Journal of Visual Languages & Computing",
volume = "17",
number = "6",
pages = "584 - 605",
year = "2006",
note = "Visual Modeling for Software Intensive Systems",
issn = "1045-926X",
doi = "https://doi.org/10.1016/j.jvlc.2006.10.003",
url = "http://www.sciencedirect.com/science/article/pii/S1045926X06000607",
author = "Michael Jiang and Michael Groble and Andrij Neczwid and Allan Willey",
keywords = "UML modeling, SDL modeling, MDE code generation, Model validation, Real-time communication systems, TTCN, Structured methods",
abstract = "Visual modeling languages and techniques have been increasingly adopted for software specification, design, development, and testing. With the major improvements of UML 2.0 and tools support, visual modeling technologies have significant potential for simplifying design, facilitating collaborations, and reducing development cost. In this paper, we describe our practices and experiences of applying visual modeling techniques to the design and development of real-time wireless communication systems within Motorola. A model-driven engineering approach of integrating visual modeling with development and validation is described. Results, issues, and our viewpoints are also discussed."
}
@article{VIANA20133123,
title = "Domain-Specific Modeling Languages to improve framework instantiation",
journal = "Journal of Systems and Software",
volume = "86",
number = "12",
pages = "3123 - 3139",
year = "2013",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2013.07.030",
url = "http://www.sciencedirect.com/science/article/pii/S0164121213001775",
author = "Matheus C. Viana and Rosngela A.D. Penteado and Antnio F. do Prado",
keywords = "Framework, Domain-Specific Modeling Language, Reuse",
abstract = "Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The DSML of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the DSML into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach."
}
@InCollection{Sturm2017,
  author        = {Rick Sturm and Carol Pollard and Julie Craig},
  title         = {Chapter 16 - The Case for Standards},
  booktitle     = {Application Performance Management (APM) in the Digital Enterprise},
  publisher     = {Morgan Kaufmann},
  year          = {2017},
  editor        = {Rick Sturm and Carol Pollard and Julie Craig},
  pages         = {211 - 235},
  address       = {Boston},
  isbn          = {978-0-12-804018-8},
  __markedentry = {[Juliana:]},
  abstract      = {In response to customer demands for increased efficiency and effectiveness in application manageability, portability, and interoperability, several organizations have stepped up to develop standards to guide deployment and management of software and hardware components. These organizations recognize the importance of creating standards to enable the development of standardized technologies to instrument applications. They include the Internet Engineering Task Force, Desktop Management Task Force, Institute of Electronic Engineers, Tivoli, the ASL-BiSL Foundation, and the International Organization of Standards. In this chapter, these various standards organizations are introduced and the standards developed by these influential organizations are described vis--vis the different aspects of the application management lifecycle. Finally, the pros and cons of using standards to facilitate the management of applications are presented. The primary utility of this chapter is that it shows how application management standards have evolved over the past 25-plus years and provides an overview of the various standards in one concise resource.},
  doi           = {https://doi.org/10.1016/B978-0-12-804018-8.00016-4},
  keywords      = {Application description files (ADFs), Application response measurement (ARM), Cloud application management for platforms (CAMP), Cloud auditing data federation (CADF), Common information model (CIM), Component description files (CDFs), Desktop and mobile architecture for system hardware (DASH), Global description file (GDF), IEEE 1220, ISO/IEC 16350, ISO/IEC 17023:2011, ISO/IEC 17963:2013, Object identifier (OID), Organization for advancing open standards for the information society (OASIS), POSIX 1387.2, System application MIB (sysApplMIB), System management architecture for server management (SMASH), Tivoli application management specification (AMS), Web services management (WS-MAN)},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128040188000164},
}

@Article{Catala2013,
  author        = {Alejandro Catala and Patricia Pons and Javier Jaen and Jose A. Mocholi and Elena Navarro},
  title         = {A meta-model for dataflow-based rules in smart environments: Evaluating user comprehension and performance},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {10},
  pages         = {1930 - 1950},
  issn          = {0167-6423},
  note          = {Special section on Language Descriptions Tools and Applications (LDTA08 \& 09) \& Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
  __markedentry = {[Juliana:]},
  abstract      = {A considerable part of the behavior in smart environments relies on event-driven and rule specification. Rules are the mechanism most often used to enable user customization of the environment. However, the expressiveness of the rules available to users in editing and other tools is usually either limited or the available rule editing interfaces are not designed for end-users with low skills in programming. This means we have to look for interaction techniques and new ways to define user customization rules. This paper describes a generic and flexible meta-model to support expressive rules enhanced with data flow expressions that will graphically support the definition of rules without writing code. An empirical study was conducted on the ease of understanding of the visual data flow expressions, which are the key elements in our rule proposal. The visual dataflow language was compared to its corresponding textual version in terms of comprehension and ease of learning by teenagers in exercises involving calculations, modifications, writing and detecting equivalences in expressions in both languages. Although the subjects had some previous experience in editing mathematical expressions on spreadsheets, the study found their performance with visual dataflows to be significantly better in calculation and modification exercises. This makes our dataflow approach a promising mechanism for expressing user-customized reactive behavior in Ambient Intelligence (AmI) environments. The performance of the rule matching processor was validated by means of two stress tests to ensure that the meta-model approach adopted would be able to scale up with the number of types and instances in the space.},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.010},
  keywords      = {Ambient intelligence, Customization, Dataflow, Visual language, Rule, Event based, Non-expert programmer, Smart home},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001232},
}

@InCollection{Zelkowitz1999,
  title         = {Subject Index},
  publisher     = {Elsevier},
  year          = {1999},
  editor        = {Marvin V. Zelkowitz},
  volume        = {49},
  series        = {Advances in Computers},
  pages         = {365 - 373},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S0065-2458(08)60290-9},
  issn          = {0065-2458},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245808602909},
}

@Article{Jozwiak1992,
  author        = {Lech Jwiak and Hein Mijland},
  title         = {On the use of OR-BDDs for test generation},
  journal       = {Microprocessing and Microprogramming},
  year          = {1992},
  volume        = {35},
  number        = {1},
  pages         = {159 - 166},
  issn          = {0165-6074},
  note          = {Software and Hardware: Specification and Design},
  __markedentry = {[Juliana:]},
  abstract      = {The binary decision diagrams (BDDs) have recently been recognized as efficient means for modelling the Boolean functions for the purpose of testability analysis and test pattern generation; however, they have too low modelling power in order to model the logical implementation structure accurately and, therefore, there is no direct correspondence between the BDD's fault models and the more realistic structural fault models. In this paper, we propose an extension to BDDs, referred to as OR-BDDs, that enables the accurate modelling of the circuit structure in a compact manner. We show how to construct the minimal OR-BDDs. We introduce the fault model for OR-BDDs and show it's one-to-one correspondence with the structural stuck-at-value model. Finally, we present an algorithm for test pattern generation that uses OR-BDDs and their fault model. This algorithm discovers efficiently circuit redundancy and enables 100% fault coverage for all detectable faults by a compact set of test vectors.},
  doi           = {https://doi.org/10.1016/0165-6074(92)90310-4},
  url           = {http://www.sciencedirect.com/science/article/pii/0165607492903104},
}

@Article{Rodriguez2017,
  author        = {Pilar Rodrguez and Alireza Haghighatkhah and Lucy Ellen Lwakatare and Susanna Teppola and Tanja Suomalainen and Juho Eskeli and Teemu Karvonen and Pasi Kuvaja and June M. Verner and Markku Oivo},
  title         = {Continuous deployment of software intensive products and services: A systematic mapping study},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {123},
  pages         = {263 - 291},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {The software intensive industry is moving towards the adoption of a value-driven and adaptive real-time business paradigm. The traditional view of software as an item that evolves through releases every few months is being replaced by the continuous evolution of software functionality. This study aims to classify and analyse the literature related to continuous deployment in the software domain in order to scope the phenomenon, provide an overview of the state-of-the-art, investigate the scientific evidence in the reported results and identify areas suitable for further research. We conducted a systematic mapping study and classified the continuous deployment literature. The benefits and challenges related to continuous deployment were also analysed. RESULTS: The systematic mapping study includes 50 primary studies published between 2001 and 2014. An in-depth analysis of the primary studies revealed ten recurrent themes that characterize continuous deployment and provide researchers with directions for future work. In addition, a set of benefits and challenges of which practitioners may take advantage were identified. CONCLUSION: Overall, although the topic area is very promising, it is still in its infancy, thus offering a plethora of new opportunities for both researchers and software intensive companies.},
  doi           = {https://doi.org/10.1016/j.jss.2015.12.015},
  keywords      = {Continuous deployment, Software development, Systematic mapping study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002812},
}

@InCollection{Betz2007,
  author        = {Charles T. Betz},
  title         = {Chapter 4 - A Supporting Systems Architecture},
  booktitle     = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance},
  publisher     = {Morgan Kaufmann},
  year          = {2007},
  editor        = {Charles T. Betz},
  pages         = {227 - 305},
  address       = {Burlington},
  isbn          = {978-0-12-370593-8},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
As discussed at the outset of this chapter, companies often turn to the research organizations, the vendors, or both. However, the vendors are clearly self-interested, and the research organizations receive much of their funding from the vendors and are not articulating a comprehensive vision for how IT enablement tooling must interoperate. Hence, this chapter. Enablement tools and architectures are important and must be given their due. This chapter has several goals. First, it discusses the generic categories of internal IT enablement systems and potential interactions and overlaps. One issue in particular in this area is the proliferation of single-point systems. Providing a framework to support such efforts is a primary goal of this chapter. Finally, the use of a common logical process and data model can assist in the integration of the vendor packages available, and in particular the master data subjects be clearly established with defined systems of record so that the diverse systems can be aligned.},
  doi           = {https://doi.org/10.1016/B978-012370593-8/50031-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123705938500319},
}

@Article{1977,
  title         = {Useful list of abbreviations commonly used in reliability},
  journal       = {Microelectronics Reliability},
  year          = {1977},
  volume        = {16},
  number        = {3},
  pages         = {197 - 206},
  issn          = {0026-2714},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/0026-2714(77)90757-0},
  url           = {http://www.sciencedirect.com/science/article/pii/0026271477907570},
}

@InCollection{Noergaard2010,
  title         = {Appendix B - Embedded Systems Glossary},
  booktitle     = {Demystifying Embedded Systems Middleware},
  publisher     = {Newnes},
  year          = {2010},
  editor        = {Tammy Noergaard},
  pages         = {367 - 387},
  address       = {Burlington},
  isbn          = {978-0-7506-8455-2},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-7506-8455-2.00011-X},
  url           = {http://www.sciencedirect.com/science/article/pii/B978075068455200011X},
}

@Article{Gratacos2010,
  author        = {M. Gratacos and N. Raguer and J. Gamez and J.L. Seoane and M. Benito},
  title         = {P12-16 Neurophysiological characterization of Miller Fisher syndrome patients: Report of 10 patients},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S175 - S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60720-1},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607201},
}

@InCollection{Sittig2017,
  author        = {Dean F. Sittig},
  title         = {1 - Category Definitions},
  booktitle     = {Clinical Informatics Literacy},
  publisher     = {Academic Press},
  year          = {2017},
  editor        = {Dean F. Sittig},
  pages         = {1 - 170},
  isbn          = {978-0-12-803206-0},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-803206-0.00001-8},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128032060000018},
}

@InCollection{Cho1991,
  author        = {Chang H. Cho and James R. Armstrong},
  title         = {VHDL Semantics for Behavioral Test Generation},
  booktitle     = {Computer Hardware Description Languages and their Applications},
  publisher     = {North-Holland},
  year          = {1991},
  editor        = {DOMINIQUE BORRIONE and RONALD WAXMAN},
  pages         = {427 - 444},
  address       = {Amsterdam},
  isbn          = {978-0-444-89208-9},
  __markedentry = {[Juliana:]},
  abstract      = {In this paper, we discuss how the VHDL semantics which represent the concepts of event-driven simulation and bus resolution function affect the test generation algorithm, and present methods of generating realistic tests without being affected by the VHDL semantics. A formal representation of the VHDL process statement is described and the concept of event-driven simulation and its impact on test generation are discussed using the formal representation. The new test generation method generates realistic tests by ignoring the sensitivity list of a process statement and identifying the type of the behavior described by the statements inside the process statement (two types of behavior - synchronous and asynchronous, are defined.). A systematic way of converting a VHDL model to one suitable for checking the validity of the generated tests is presented. A method of further compacting the generated tests is also presented. Finally, an approach to generating tests in the presence of different types of bus resolution functions is discussed.},
  doi           = {https://doi.org/10.1016/B978-0-444-89208-9.50030-2},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780444892089500302},
}

@Article{Mosterman2009,
  author        = {Pieter J. Mosterman and Justyna Zander and Gregoire Hamon and Ben Denckla},
  title         = {Towards Computational Hybrid System Semantics for Time-Based Block Diagrams},
  journal       = {IFAC Proceedings Volumes},
  year          = {2009},
  volume        = {42},
  number        = {17},
  pages         = {376 - 385},
  issn          = {1474-6670},
  note          = {3rd IFAC Conference on Analysis and Design of Hybrid Systems},
  __markedentry = {[Juliana:]},
  abstract      = {At the core of Model-Based Design, computational models have caused an autocatalytic trend to use computation in design by unlocking the potential of model transformations. Precisely specifying a computational transformation requires well-defined semantics of the source and target representations. In this regard, continuous-time behavior is an essential aspect of time-based block diagrams that is typically approximated by numerical integration. The corresponding theory, however, is mostly concerned with local error and the mathematical semantics of long time behavior fails to be sufficiently precise from a computational perspective. In this work, first a computational semantics is developed based on a multi-stage variablestep solver. Next, the computational semantics of the discrete and continuous parts of hybrid systems and their interaction are formalized in a unifying framework. The framework exploits a successful functional approach to defining discrete-time and discrete-event behavior established in other work. Unification is then achieved by developing a computational representation of the continuous-time behavior as pure functions on streams.},
  doi           = {https://doi.org/10.3182/20090916-3-ES-3003.00065},
  keywords      = {Computational methods, Computer simulation, Computer-aided control system design, Embedded systems, Numerical simulation, Synchronous data flow, Systems design, Variable-structure systems, Verification, Zero crossings},
  url           = {http://www.sciencedirect.com/science/article/pii/S1474667015307916},
}

@Article{Lopez-Sanchez2011,
  author        = {Almudena Lpez-Snchez and Alejandra Senz and Cristina Casals},
  title         = {Surfactant protein A (SP-A)-tacrolimus complexes have a greater anti-inflammatory effect than either SP-A or tacrolimus alone on human macrophage-like U937 cells},
  journal       = {European Journal of Pharmaceutics and Biopharmaceutics},
  year          = {2011},
  volume        = {77},
  number        = {3},
  pages         = {384 - 391},
  issn          = {0939-6411},
  note          = {Biological Barriers},
  __markedentry = {[Juliana:]},
  abstract      = {Intratracheal administration of immunosuppressive agents to the lung is a novel treatment after lung transplantation. Nanoparticles of tacrolimus (FK506) might interact with human SP-A, which is the most abundant lipoprotein in the alveolar fluid. This study was undertaken to determine whether the formation of FK506/SP-A complexes interferes with FK506 immunosuppressive actions on stimulated human macrophage-like U937 cells. We found that SP-A was avidly bound to FK506 (Kd=354nM), as determined by solid phasebinding assays and dynamic light scattering. Free FK506, at concentrations 1M, had no effect on the inflammatory response of LPS-stimulated U937 macrophages. However, coincubation of FK506 and SP-A, at concentrations where each component alone did not affect LPS-stimulated macrophage response, significantly inhibited LPS-induced NF-B activation and TNF-alpha secretion. Free FK506, but not FK506/SP-A, functioned as substrate for the efflux transporter P-glycoprotein. FK506 bound to SP-A was delivered to macrophages by endocytosis, since several endocytosis inhibitors blocked FK506/SP-A anti-inflammatory effects. This process depended partly on SP-A binding to its receptor, SP-R210. These results indicate that FK506/SP-A complexes have a greater anti-inflammatory effect than either FK506 or SP-A alone and suggest that SP-A strengthened FK506 anti-inflammatory activity by facilitating FK506 entrance into the cell, overcoming P-glycoprotein.},
  doi           = {https://doi.org/10.1016/j.ejpb.2010.12.013},
  keywords      = {Lung, Tacrolimus, Surfactant protein A, Inflammation, Macrophages, P-glycoprotein},
  url           = {http://www.sciencedirect.com/science/article/pii/S0939641110003310},
}

@Article{Verhoef2005,
  author        = {C. Verhoef},
  title         = {Quantitative aspects of outsourcing deals},
  journal       = {Science of Computer Programming},
  year          = {2005},
  volume        = {56},
  number        = {3},
  pages         = {275 - 313},
  issn          = {0167-6423},
  __markedentry = {[Juliana:]},
  abstract      = {There are many goals for outsourcing information technology: for instance, cost reduction, speed to market, quality improvement, or new business opportunities. Based on our real-world experience in advising organizations with goal-driven outsourcing deals, we identified the most prominent quantitative input needed to close such deals. These comprise what we named the five executive issues enabling rational decision making. They concern cost, duration, risk, return, and financing aspects of outsourcing. They add an important quantitative financial/economic dimension to the decision making process. Based on inferred outcomes for the five executive issues, we address the easily overlooked aspects of selecting partners, contracting, monitoring progress, and acceptance and delivery conditions for contracts.},
  doi           = {https://doi.org/10.1016/j.scico.2004.08.003},
  keywords      = {Outsourcing, Goalsourcing, Smartsourcing, Fastsourcing, Costsourcing, Offshore outsourcing, Eastsourcing, Tasksourcing, Backsourcing, Insourcing, Scalesourcing, Profitsourcing, Activity-based cost estimation, Total cost of ownership (TCO), Requirements creep risk, Time compression risk, Litigation risk, Failure risk, Overtime risk, Deglubitor risk, Payback period risk},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642304001406},
}

@InCollection{Ali2017,
  author        = {Shaukat Ali and Hong Lu and Shuai Wang and Tao Yue and Man Zhang},
  title         = {Chapter Two - Uncertainty-Wise Testing of Cyber-Physical Systems},
  publisher     = {Elsevier},
  year          = {2017},
  editor        = {Atif M. Memon},
  volume        = {107},
  series        = {Advances in Computers},
  pages         = {23 - 94},
  __markedentry = {[Juliana:]},
  abstract      = {As compared with classical software/system testing, uncertainty-wise testing explicitly addresses known uncertainty about the behavior of a System Under Test (SUT), its operating environment, and interactions between the SUT and its operational environment, across all testing phases, including test design, test generation, test optimization, and test execution, with the aim to mainly achieve the following two goals. First, uncertainty-wise testing aims to ensure that the SUT deals with known uncertainty adequately. Second, uncertainty-wise testing should be also capable of learning new (previously unknown) uncertainties such that the SUT's implementation can be improved to guard against newly learned uncertainties during its operation. The necessity to integrate uncertainty in testing is becoming imperative because of the emergence of new types of intelligent and communicating software-based systems such as Cyber-Physical Systems (CPSs). Intrinsically, such systems are exposed to uncertainty because of their interactions with highly indeterminate physical environments. In this chapter, we provide our understanding and experience of uncertainty-wise testing from the aspects of uncertainty-wise model-based testing, uncertainty-wise modeling and evolution of test ready models, and uncertainty-wise multiobjective test optimization, in the context of testing CPSs under uncertainty. Furthermore, we present our vision about this new testing paradigm and its plausible future research directions.},
  doi           = {https://doi.org/10.1016/bs.adcom.2017.06.001},
  issn          = {0065-2458},
  keywords      = {Uncertainty-wise testing, Cyber-Physical System, Belief Test Ready Model, Model evolution, Model-based testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245817300189},
}

@Article{Sangiovanni-Vincentelli2012,
  author        = {Alberto Sangiovanni-Vincentelli and Werner Damm and Roberto Passerone},
  title         = {Taming Dr. Frankenstein: Contract-Based Design for Cyber-Physical Systems*},
  journal       = {European Journal of Control},
  year          = {2012},
  volume        = {18},
  number        = {3},
  pages         = {217 - 238},
  issn          = {0947-3580},
  __markedentry = {[Juliana:]},
  abstract      = {Cyber-physical systems combine a cyber side (computing and networking) with a physical side (mechanical, electrical, and chemical processes). In many cases, the cyber component controls the physical side using sensors and actuators that observe the physical system and actuate the controls. Such systems present the biggest challenges as well as the biggest opportunities in several large industries, including electronics, energy, automotive, defense and aerospace, telecommunications, instrumentation, industrial automation. Engineers today do successfully design cyber-physical systems in a variety of industries. Unfortunately, the development of systems is costly, and development schedules are difficult to stick to. The complexity of cyber-physical systems, and particularly the increased performance that is offered from interconnecting what in the past have been separate systems, increases the design and verification challenges. As the complexity of these systems increases, our inability to rigorously model the interactions between the physical and the cyber sides creates serious vulnerabilities. Systems become unsafe, with disastrous inexplicable failures that could not have been predicted. Distributed control of multi-scale complex systems is largely an unsolved problem. A common view that is emerging in research programs in Europe and the US is enabling contract-based design (CBD), which formulates a broad and aggressive scope to address urgent needs in the systems industry. We present a design methodology and a few examples in controller design whereby contract-based design can be merged with platform-based design to formulate the design process as a meet-in-the-middle approach, where design requirements are implemented in a subsequent refinement process using as much as possible elements from a library of available components. Contracts are formalizations of the conditions for correctness of element integration (horizontal contracts), for lower level of abstraction to be consistent with the higher ones, and for abstractions of available components to be faithful representations of the actual parts (vertical contracts).},
  doi           = {https://doi.org/10.3166/ejc.18.217-238},
  keywords      = {Contract, cyber-physical, design methodologies, platform-based, correctness},
  url           = {http://www.sciencedirect.com/science/article/pii/S0947358012709433},
}

@InCollection{Hurtarte2007,
  author        = {Jeorge S. Hurtarte and Evert A. Wolsheimer and Lisa M. Tafoya},
  title         = {Chapter 7 - Intellectual Property},
  booktitle     = {Understanding Fabless IC Technology},
  publisher     = {Newnes},
  year          = {2007},
  editor        = {Jeorge S. Hurtarte and Evert A. Wolsheimer and Lisa M. Tafoya},
  pages         = {65 - 121},
  address       = {Burlington},
  isbn          = {978-0-7506-7944-2},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
This chapter explains the semiconductor intellectual property (SIP) overview, business environment, sourcing products, evaluating business models, product enablers, licensing products, and the provider and buyer perspectives. SIP has existed since the advent of the semiconductor industry. SIP business practices include elements similar to those found in the traditional semiconductor or application-specific integrated circuit (ASIC), electronic design automation (EDA), and design services markets. Outsourcing is unlikely if the SIP product is seen by the potential buyer as a core competency or a key differentiator in its product, or if its use requires third-party access to the buyer's patents or trade secrets. SIP License Agreements may require a significant effort depending upon the business objectives of the parties, the intended use of the SIP Product, the nature of the SIP product, and the risks associated with using the SIP product in the end application. The scope of license, warranty, indemnity, and limitation of liability provisions usually consume the majority of the effort in negotiating an SIP License Agreement. However, IC developers are finding more low-cost solutions at their fingertips, and the semiconductor supply chain has become increasingly stratified.},
  doi           = {https://doi.org/10.1016/B978-075067944-2/50008-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780750679442500084},
}

@Article{Tibermacine2010,
  author        = {Chouki Tibermacine and Rgis Fleurquin and Salah Sadou},
  title         = {A family of languages for architecture constraint specification},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {5},
  pages         = {815 - 831},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {During software development, architecture decisions should be documented so that quality attributes guaranteed by these decisions and required in the software specification could be persisted. An important part of these architectural decisions is often formalized using constraint languages which differ from one stage to another in the development process. In this paper, we present a family of architectural constraint languages, called ACL. Each member of this family, called a profile, can be used to formalize architectural decisions at a given stage of the development process. An ACL profile is composed of a core constraint language, which is shared with the other profiles, and a MOF architecture metamodel. In addition to this family of languages, this paper introduces a transformation-based interpretation method of profiles and its associated tool.},
  doi           = {https://doi.org/10.1016/j.jss.2009.11.736},
  keywords      = {Architecture constraint, Constraint language, ADL, Software component, MOF, OCL, Constraint transformation},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120900315X},
}

@Article{Singh2017,
  author        = {Satwinder Singh and Sharanpreet Kaur},
  title         = {A systematic literature review: Refactoring for disclosing code smells in object oriented software},
  journal       = {Ain Shams Engineering Journal},
  year          = {2017},
  issn          = {2090-4479},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Reusing a design pattern is not always in the favor of developers. Thus, the code starts smelling. The presence of Code Smells leads to more difficulties for the developers. This racket of code smells is sometimes called Anti-Patterns.
Objective
The paper aimed at a systematic literature review of refactoring with respect to code smells. However the review of refactoring is done in general and the identification of code smells and anti-patterns is performed in depth.
Method
A systematic literature survey has been performed on 238 research items that includes articles from leading Conferences, Workshops and premier journals, theses of researchers and book chapters.
Results
Several data sets and tools for performing refactoring have been revealed under the specified research questions.
Conclusion
The work done in the paper is an addition to prior systematic literature surveys. With the study of paper the attentiveness of readers about code smells and anti-patterns will be enhanced.},
  doi           = {https://doi.org/10.1016/j.asej.2017.03.002},
  keywords      = {Code smells, Anti-patterns, Refactoring},
  url           = {http://www.sciencedirect.com/science/article/pii/S2090447917300412},
}

@Article{Le2010,
  author        = {T.T.Q. Le},
  title         = {P12-17 Clinical features and laboratory findings of 7 cases with Miller-Fisher syndrome in ChoRay hospital},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60721-3},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607213},
}

@InCollection{Breakfield2002,
  author        = {Charles V. Breakfield and Roxanne E. Burkey},
  title         = {4 - Development Cycle},
  booktitle     = {Managing Systems Migrations and Upgrades},
  publisher     = {Digital Press},
  year          = {2002},
  editor        = {Charles V. Breakfield and Roxanne E. Burkey},
  pages         = {93 - 144},
  address       = {Woburn},
  isbn          = {978-1-55558-256-2},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-155558256-2/50005-3},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781555582562500053},
}

@Article{Karam2008,
  author        = {Marcel Karam and Sergiu Dascalu and Haidar Safa and Rami Santina and Zeina Koteich},
  title         = {A product-line architecture for web service-based visual composition of web applications},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {855 - 867},
  issn          = {0164-1212},
  note          = {Agile Product Line Engineering},
  __markedentry = {[Juliana:]},
  abstract      = {A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.031},
  keywords      = {Product line engineering, Product line architecture, Agile methods, Web services, Visual languages},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120700252X},
}

@Article{Huuck2015,
  author        = {Ralf Huuck},
  title         = {Technology transfer: Formal analysis, engineering, and business value},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {103},
  pages         = {3 - 12},
  issn          = {0167-6423},
  note          = {Selected papers from the First International Workshop on Formal Techniques for Safety-Critical Systems (FTSCS 2012)},
  __markedentry = {[Juliana:]},
  abstract      = {In this work we report on our experiences on developing and commercializing Goanna, a source code analyzer for detecting software bugs and security vulnerabilities in C/C++ code. Goanna is based on formal software analysis techniques such as model checking, static analysis and SMT solving. The commercial version of Goanna is currently deployed in a wide range of organizations around the world. Moreover, the underlying technology is licensed to an independent software vendor with tens of thousands of customers, making it possibly one of the largest deployments of automated formal methods technology. This paper explains some of the challenges as well as the positive results that we encountered in the technology transfer process. In particular, we provide some background on the design decisions and techniques to deal with large industrial code bases, we highlight engineering challenges and efforts that are typically outside of a more academic setting, and we address core aspects of the bigger picture for transferring formal techniques into commercial products, namely, the adoption of such technology and the value for purchasing organizations. While we provide a particular focus on Goanna and our experience with that underlying technology, we believe that many of those aspects hold true for the wider field of formal analysis and verification technology and its adoption in industry.},
  doi           = {https://doi.org/10.1016/j.scico.2014.11.003},
  keywords      = {Static analysis, Model checking, SMT solving, Industrial application, Experience report},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314005358},
}

@Article{Sunye2014,
  author        = {Gerson Suny and Eduardo Cunha de Almeida and Yves Le Traon and Benoit Baudry and Jean-Marc Jzquel},
  title         = {Model-based testing of global properties on large-scale distributed systems},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {7},
  pages         = {749 - 762},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Large-scale distributed systems are becoming commonplace with the large popularity of peer-to-peer and cloud computing. The increasing importance of these systems contrasts with the lack of integrated solutions to build trustworthy software. A key concern of any large-scale distributed system is the validation of global properties, which cannot be evaluated on a single node. Thus, it is necessary to gather data from distributed nodes and to aggregate these data into a global view. This turns out to be very challenging because of the systems dynamism that imposes very frequent changes in local values that affect global properties. This implies that the global view has to be frequently updated to ensure an accurate validation of global properties.
Objective
In this paper, we present a model-based approach to define a dynamic oracle for checking global properties. Our objective is to abstract relevant aspects of such systems into models. These models are updated at runtime, by monitoring the corresponding distributed system.
Method
We conduce real-scale experimental validation to evaluate the ability of our approach to check global properties. In this validation, we apply our approach to test two open-source implementations of distributed hash tables. The experiments are deployed on two clusters of 32 nodes.
Results
The experiments reveal an important defect on one implementation and show clear performance differences between the two implementations. The defect would not be detected without a global view of the system.
Conclusion
Testing global properties on distributed software consists of gathering data from different nodes and building a global view of the system, where properties are validated. This process requires a distributed test architecture and tools for representing and validating global properties. Model-based techniques are an expressive mean for building oracles that validate global properties on distributed systems.},
  doi           = {https://doi.org/10.1016/j.infsof.2014.02.002},
  keywords      = {Software testing, Distributed software, Model-based testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914000366},
}

@Article{Easley1998,
  author        = {David Easley and Maureen O'Hara and Joseph Paperman},
  title         = {Financial analysts and information-based trade},
  journal       = {Journal of Financial Markets},
  year          = {1998},
  volume        = {1},
  number        = {2},
  pages         = {175 - 201},
  issn          = {1386-4181},
  __markedentry = {[Juliana:]},
  abstract      = {In this research, we investigate the informational role of financial analysts. Using a trade-based empirical technique, we estimate the probability of information-based trading for a sample of NYSE stocks that differ in analyst coverage. We determine how this probability differs across stocks followed by many analysts, and we investigate whether analysts increase or create the flow of information. We also determine the `normal' level of noise trading in each sample stock, thereby giving us the ability to assess the depth of the market for stocks with differing analysts followings. Our most important empirical result is that the number of financial analysts is not a good proxy for information-based trading.},
  doi           = {https://doi.org/10.1016/S1386-4181(98)00002-0},
  keywords      = {Microstructure, Financial analysts, Trade},
  url           = {http://www.sciencedirect.com/science/article/pii/S1386418198000020},
}

@Article{Mesbah2008,
  author        = {Ali Mesbah and Arie van Deursen},
  title         = {A component- and push-based architectural style for ajax applications},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {12},
  pages         = {2194 - 2209},
  issn          = {0164-1212},
  note          = {Best papers from the 2007 Australian Software Engineering Conference (ASWEC 2007), Melbourne, Australia, April 10-13, 2007},
  __markedentry = {[Juliana:]},
  abstract      = {A new breed of web application, dubbed ajax, is emerging in response to a limited degree of interactivity in large-grain stateless Web interactions. At the heart of this new approach lies a single page interaction model that facilitates rich interactivity. Also push-based solutions from the distributed systems are being adopted on the web for ajax applications. The field is, however, characterized by the lack of a coherent and precisely described set of architectural concepts. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. We have studied and experimented with several ajax frameworks trying to understand their architectural properties. In this paper, we summarize four of these frameworks and examine their properties and introduce the spiar architectural style which captures the essence of ajax applications. We describe the guiding software engineering principles and the constraints chosen to induce the desired properties. The style emphasizes user interface component development, intermediary delta-communication between client/server components, and push-based event notification of state changes through the components, to improve a number of properties such as user interactivity, user-perceived latency, data coherence, and ease of development. In addition, we use the concepts and principles to discuss various open issues in ajax frameworks and application development.},
  doi           = {https://doi.org/10.1016/j.jss.2008.04.005},
  keywords      = {, Web architectural style, Web engineering, Single page interface, Rich internet application},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121208000769},
}

@InCollection{Conrad2016,
  title         = {Index},
  booktitle     = {CISSP Study Guide (Third Edition)},
  publisher     = {Syngress},
  year          = {2016},
  editor        = {Eric Conrad and Seth Misenar and Joshua Feldman},
  pages         = {559 - 599},
  address       = {Boston},
  edition       = {Third Edition},
  isbn          = {978-0-12-802437-9},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-802437-9.00018-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128024379000187},
}

@Article{Straver2010,
  author        = {D.C.G. Straver and L.H. Van den Berg and H. Franssen},
  title         = {P12-18 Exercise-induced weakness in demyelinating neuropathies},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60722-5},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607225},
}

@Article{Boonstra2008,
  author        = {Rudy Boonstra and Jeffrey E. Lane and Stan Boutin and Adrian Bradley and Lanna Desantis and Amy E.M. Newman and Kiran K. Soma},
  title         = {Plasma DHEA levels in wild, territorial red squirrels: Seasonal variation and effect of ACTH},
  journal       = {General and Comparative Endocrinology},
  year          = {2008},
  volume        = {158},
  number        = {1},
  pages         = {61 - 67},
  issn          = {0016-6480},
  __markedentry = {[Juliana:]},
  abstract      = {In many species, territorial behavior is limited to the breeding season and is tightly coupled to circulating gonadal steroid levels. In contrast, both male and female red squirrels (Tamiasciurus hudsonicus) are highly aggressive in both the breeding and non-breeding seasons in defense of food stores on their individual territories throughout the boreal and northern forests of North America. Dehydroepiandrosterone (DHEA), an androgen precursor, is secreted from the adrenal cortex in some mammals, and DHEA has been linked to aggression in non-breeding songbirds. Here, we examined plasma DHEA levels in a natural population of red squirrels in the Yukon, Canada. Plasma DHEA levels in both males and females reached high concentrations (up to 16.952ng/ml in males and 14.602ng/ml in females), markedly exceeding plasma DHEA concentrations in laboratory rats and mice and similar to plasma DHEA concentrations in some primates. Circulating DHEA levels showed both seasonal and yearly variation. Seasonal variation in male plasma DHEA levels was negatively correlated with testes mass. Yearly variation in male DHEA levels was positively correlated with population density. In both males and females, circulating DHEA rapidly increased after ACTH treatment, implying an adrenal origin. This is the first examination of plasma DHEA concentrations in a wild rodent and the first field experiment on the regulation of plasma DHEA in any wild mammal. These data lay the foundation for future studies on the role of DHEA in non-breeding territoriality in this species and other mammals.},
  doi           = {https://doi.org/10.1016/j.ygcen.2008.05.004},
  keywords      = {ACTH, Adrenal cortex, Aggression, Boreal forest, Cortisol, DHEA, Food supply, Population density, Season, Sex difference, Stress, Territorial behavior, Testosterone, Winter},
  url           = {http://www.sciencedirect.com/science/article/pii/S0016648008001950},
}

@Article{Trainor2008,
  author        = {Brian C. Trainor and M. Sima Finy and Randy J. Nelson},
  title         = {Rapid effects of estradiol on male aggression depend on photoperiod in reproductively non-responsive mice},
  journal       = {Hormones and Behavior},
  year          = {2008},
  volume        = {53},
  number        = {1},
  pages         = {192 - 199},
  issn          = {0018-506X},
  __markedentry = {[Juliana:]},
  abstract      = {In three genuses and four species of rodents, housing in winter-like short days (8L:16D) increases male aggressive behavior. In all of these species, males undergo short-day induced regression of the reproductive system. Some studies, however, suggest that the effect of photoperiod on aggression may be independent of reproductive responses. We examined the effects of photoperiod on aggressive behavior in California mice (Peromyscus californicus), which do not display reproductive responsiveness to short days. As expected, short days had no effect on plasma testosterone. Estrogen receptor alpha and estrogen receptor beta immunostaining did not differ in the lateral septum, medial preoptic area, bed nucleus of the stria terminalis, or medial amygdala. However, males housed in short days were significantly more aggressive than males housed in long days. Similar to previous work in beach mice (Peromyscus polionotus), estradiol rapidly increased aggression when male California mice were housed in short days but not when housed in long days. These data suggest that the effects of photoperiod on aggression and estrogen signaling are independent of reproductive responses. The rapid action of estradiol on aggression in short-day mice also suggests that nongenomic mechanisms mediate the effects of estrogens in short days.},
  doi           = {https://doi.org/10.1016/j.yhbeh.2007.09.016},
  keywords      = {Aggressive behavior, , California mouse, c-fos, Nongenomic effects, Estrogen receptor alpha, Estrogen receptor beta},
  url           = {http://www.sciencedirect.com/science/article/pii/S0018506X07002334},
}

@InCollection{Reed1991,
  author        = {David P. Reed and Marvin A. Sirbu},
  title         = {AN ENGINEERING COST AND POLICY ANALYSIS OF INTRODUCING FIBER INTO THE RESIDENTIAL SUBSCRIBER LOOP},
  booktitle     = {Integrated Broadband Networks},
  publisher     = {North-Holland},
  year          = {1991},
  editor        = {MARTIN C.J. ELTON},
  pages         = {89 - 134},
  address       = {Amsterdam},
  isbn          = {978-0-444-89068-9},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
This chapter presents an analysis of the principal engineering and economic issues that have emerged as telephone companies consider rewiring the nation's residences with fiber optics. Assuming significant future reductions in component costs, running fiber optic to the home is likely to remain more expensive than copper, where current loop plant costs are roughly $920 per subscriber. To realize the introduction of a fiber Integrated Broadband Networks (IBN), it must be justified on the basis of additional revenue producing services, such as the delivery of entertainment video. Fiber optic network capable of providing both voice and video services to the home can be constructed for $1800 to $2500 per home passed. Future developments in microelectronics, lasers, photodetectors, and powering architectures will greatly influence the economics of network evolution.},
  doi           = {https://doi.org/10.1016/B978-0-444-89068-9.50013-5},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780444890689500135},
}

@Article{Deb2016,
  author        = {Novarun Deb and Nabendu Chaki and Aditya Ghose},
  title         = {Extracting finite state models from i* models},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {121},
  pages         = {265 - 280},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm.},
  doi           = {https://doi.org/10.1016/j.jss.2016.03.038},
  keywords      = {i model, Model transformation, Model checking},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300048},
}

@Article{Clariso2016,
  author        = {Robert Claris and Jordi Cabot and Esther Guerra and Juan de Lara},
  title         = {Backwards reasoning for model transformations: Method and applications},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {116},
  pages         = {113 - 132},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Model transformations are key elements of model driven engineering. Current challenges for transformation languages include improving usability (i.e., succinct means to express the transformation intent) and devising powerful analysis methods. In this paper, we show how backwards reasoning helps in both respects. The reasoning is based on a method that, given an OCL expression and a transformation rule, calculates a constraint that is satisfiable before the rule application if and only if the original OCL expression is satisfiable afterwards. With this method we can improve the usability of the rule execution process by automatically deriving suitable application conditions for a rule (or rule sequence) to guarantee that applying that rule does not break any integrity constraint (e.g. meta-model constraints). When combined with model finders, this method facilitates the validation, verification, testing and diagnosis of transformations, and we show several applications for both in-place and exogenous transformations.},
  doi           = {https://doi.org/10.1016/j.jss.2015.08.017},
  keywords      = {Model transformation, OCL, Weakest pre-condition},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215001788},
}

@Article{Landys2013,
  author        = {Mta M. Landys and Wolfgang Goymann and Kiran K. Soma and Tore Slagsvold},
  title         = {Year-round territorial aggression is independent of plasma DHEA in the European nuthatch Sitta europaea},
  journal       = {Hormones and Behavior},
  year          = {2013},
  volume        = {63},
  number        = {1},
  pages         = {166 - 172},
  issn          = {0018-506X},
  __markedentry = {[Juliana:]},
  abstract      = {Plasma testosterone can play an important role in promoting aggressive behaviors relating to territory defense in breeding male birds. Some birds defend territories also during the non-breeding phase, when testosterone circulates at basal levels. In such species, plasma levels of the pro-hormone dehydroepiandrosterone (DHEA) may support non-breeding territoriality by acting as a local substrate for sex steroids. To test this possible role of plasma DHEA, we examined the seasonal DHEA profile of male (and female) European nuthatches Sitta europaea: a male and female nuthatch pair will defend an all-purpose territory throughout the year. We hypothesized that plasma DHEA would be detectable in wintering nuthatches with a territory. However, only ca. half of the territorial wintering males (and females) displayed detectable DHEA levels, suggesting that plasma DHEA is not a major sex steroid precursor during non-breeding. Further, among hatching-year birds, plasma DHEA was significantly lower in territorial birds than in floaters, i.e., subordinate birds without a territory. To experimentally examine the role of DHEA in non-breeding territoriality, we treated adult wintering males with DHEA and measured effects on aggressive responses to conspecific challenge. DHEA treatment elevated plasma levels of DHEA (and testosterone), but did not enhance territorial behaviors or their persistence. Taken together, our data suggest that DHEA (and, indeed, sex steroids per se) do not regulate non-breeding territoriality in the nuthatch. Given that territorial aggression in nuthatches is expressed year-round, a hormone for its activation may be redundant.},
  doi           = {https://doi.org/10.1016/j.yhbeh.2012.10.002},
  keywords      = {Nuthatch, Seasonal DHEA profile, Implant, Territorial aggression, Testosterone, Non-breeding, Wintering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0018506X12002449},
}

@Article{Scarabelli2004,
  author        = {Tiziano M Scarabelli and Evasio Pasini and Anastasis Stephanou and Carol Chen-Scarabelli and Louis Saravolatz and Richard A Knight and David S Latchman and Julius M Gardin},
  title         = {Nutritional supplementation with mixed essential amino acids enhances myocyte survival, preserving mitochondrial functional capacity during ischemia-reperfusion injury},
  journal       = {The American Journal of Cardiology},
  year          = {2004},
  volume        = {93},
  number        = {8, Supplement 1},
  pages         = {35 - 40},
  issn          = {0002-9149},
  __markedentry = {[Juliana:]},
  abstract      = {In patients undergoing coronary surgery, the uptake of amino acids, which has been shown to correlate with oxygen consumption, is a mechanism of cardiac adaptation to the iatrogenic ischemia-reperfusion injury associated with cardioplegic arrest. Based on these premises, we sought to determine whether oral supplementation with mixed amino acids may protect the rat heart exposed to ischemia-reperfusion and to address whether this hypothesized cardioprotection is achieved, at least in part, through preservation of the energy-producing properties of mitochondria. SpragueDawley rats were fed (by enteral route) a liquid diet, with or without mixed essential amino acids (daily dose of 1 g/kg) for 30 days. Hearts from anesthetized rats were perfused by the Langendorff method and randomized to 3 groups. The control group was perfused with buffer for 60 minutes; the ischemia-reperfusion control and the amino acidtreated groups were exposed to 35 minutes of ischemia, followed by 60 or 120 minutes of reperfusion. Amino acid supplements minimized infarct size (22  1.8% vs 33  2.5%; p <0.05) and occurrence of cardiomyocyte apoptosis, as assessed by co-localization of terminal deoxynucleotidyl transferasemediated dUTP nick end labeling (TUNEL) and caspase-3positive staining (p <0.01). Long-term treatment with amino acids also reduced the proportion of cardiomyocytes exhibiting immunostaining for cleaved caspase-9 (p <0.01) but was ineffective on processing of caspase-8. Similar results were obtained in the whole heart by caspase activity assays (p <0.01). The lessened activation of caspase-9 detected in amino acid-treated hearts paralleled a strong reduction in mitochondrial release of cytochrome c. Adenosine triphosphate (ATP) content and rate of ATP production in isolated mitochondria were reduced by >75% in control hearts after 2 hours of reperfusion (p <0.05 vs control hearts); these values returned toward those of the control group in hearts supplemented with amino acids (p <0.01). Finally, the oxygen consumption rate in myocardial skinned bundles was markedly reduced in ischemia-reperfusion control hearts and almost normalized in amino acid-treated hearts (approximately 20% and 93% of the value for normoxic hearts; p <0.01). These results suggest that oral amino acid supplementation attenuates the extent of ischemia-reperfusion injury in the rat heart, through preservation of the mitochondria-generated production of high-energy phosphates.},
  doi           = {https://doi.org/10.1016/j.amjcard.2003.11.008},
  url           = {http://www.sciencedirect.com/science/article/pii/S0002914903015145},
}

@Article{Katkalov2014,
  author        = {Kuzman Katkalov and Nina Moebius and Kurt Stenzel and Marian Borek and Wolfgang Reif},
  title         = {Modeling test cases for security protocols with SecureMDD},
  journal       = {Computer Networks},
  year          = {2014},
  volume        = {58},
  pages         = {99 - 111},
  issn          = {1389-1286},
  __markedentry = {[Juliana:]},
  abstract      = {Designing and executing test cases for security-critical protocols is a technically complicated and tedious process. SecureMDD is a model-driven approach that enables development of security-critical applications based on cryptographic protocols. In this paper we introduce a method which combines the model-driven approach used in SecureMDD with the design of functional and security tests. We construct and evaluate new modeling guidelines that allow the modeler to easily define such test cases during the modeling stage. We also implement model transformation routines to generate runnable tests for actual implementation of applications developed with SecureMDD.},
  doi           = {https://doi.org/10.1016/j.comnet.2013.08.024},
  keywords      = {Model-driven testing, Security protocols, Security tests, Model-driven software development, Unit tests},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128613002983},
}

@Article{Gran2014,
  author        = {Ernst Gunnar Gran and Thomas Dreibholz and Amund Kvalbein},
  title         = {NorNet Core  A multi-homed research testbed},
  journal       = {Computer Networks},
  year          = {2014},
  volume        = {61},
  pages         = {75 - 87},
  issn          = {1389-1286},
  note          = {Special issue on Future Internet Testbeds  Part I},
  __markedentry = {[Juliana:]},
  abstract      = {Over the last decade, the Internet has grown at a tremendous speed in both size and complexity. Nowadays, a large number of important services  for instance e-commerce, healthcare and many others  depend on the availability of the underlying network. Clearly, service interruptions due to network problems may have a severe impact. On the long way towards the Future Internet, the complexity will grow even further. Therefore, new ideas and concepts must be evaluated thoroughly, and particularly in realistic, real-world Internet scenarios, before they can be deployed for production networks. For this purpose, various testbeds  for instance PlanetLab, GpENI or G-Lab  have been established and are intensively used for research. However, all of these testbeds lack the support for so-called multi-homing. Multi-homing denotes the connection of a site to multiple Internet service providers, in order to achieve redundancy. Clearly, with the need for network availability, there is a steadily growing demand for multi-homing. The idea of the NorNet Core project is to establish a Future Internet research testbed with multi-homed sites, in order to allow researchers to perform experiments with multi-homed systems. Particular use cases for this testbed include realistic experiments in the areas of multi-path routing, load balancing, multi-path transport protocols, overlay networks and network resilience. In this paper, we introduce the NorNet Core testbed as well as its architecture.},
  doi           = {https://doi.org/10.1016/j.bjp.2013.12.035},
  keywords      = {NN C, Testbed, Multi-homing, Routing, Transport, Applications},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128613004489},
}

@InCollection{Young2015,
  author        = {Carl S. Young},
  title         = {Chapter 8 - Electronic Terrorism Threats, Risk, andRiskMitigation},
  booktitle     = {The Science and Technology of Counterterrorism},
  publisher     = {Butterworth-Heinemann},
  year          = {2015},
  editor        = {Carl S. Young},
  pages         = {221 - 281},
  address       = {Boston},
  isbn          = {978-0-12-420056-2},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-420056-2.00008-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780124200562000087},
}

@Article{Panunzio2014,
  author        = {Marco Panunzio and Tullio Vardanega},
  title         = {An architectural approach with separation of concerns to address extra-functional requirements in the development of embedded real-time software systems},
  journal       = {Journal of Systems Architecture},
  year          = {2014},
  volume        = {60},
  number        = {9},
  pages         = {770 - 781},
  issn          = {1383-7621},
  __markedentry = {[Juliana:]},
  abstract      = {A large proportion of the requirements on embedded real-time systems stems from the extra-functional dimensions of time and space determinism, dependability, safety and security, and it is addressed at the software level. The adoption of a sound software architecture provides crucial aid in conveniently apportioning the relevant development concerns. This paper takes a software-centered interpretation of the ISO 42010 notion of architecture, enhancing it with a component model that attributes separate concerns to distinct design views. The component boundary becomes the border between functional and extra-functional concerns. The latter are treated as decorations placed on the outside of components, satisfied by implementation artifacts separate from and composable with the implementation of the component internals. The approach was evaluated by industrial users from several domains, with remarkably positive results.},
  doi           = {https://doi.org/10.1016/j.sysarc.2014.06.001},
  keywords      = {Embedded real-time systems, Extra-functional properties, Software architecture, Component-based software engineering, Separation of concerns},
  url           = {http://www.sciencedirect.com/science/article/pii/S1383762114000824},
}

@InCollection{Rodriguez2018,
  author        = {Pilar Rodrguez and Mika Mntyl and Markku Oivo and Lucy Ellen Lwakatare and Pertti Seppnen and Pasi Kuvaja},
  title         = {Advances in Using Agile and Lean Processes for Software Development},
  publisher     = {Elsevier},
  year          = {2018},
  series        = {Advances in Computers},
  __markedentry = {[Juliana:]},
  abstract      = {Software development processes have evolved according to market needs. Fast changing conditions that characterize current software markets have favored methods advocating speed and flexibility. Agile and Lean software development are in the forefront of these methods. This chapter presents a unified view of Agile software development, Lean software development, and most recent advances toward rapid releases. First, we introduce the area and explain the reasons why the software development industry begun to move into this direction in the late 1990s. Section 2 characterizes the research trends on Agile software development. This section helps understand the relevance of Agile software development in the research literature. Section 3 provides a walk through the roots of Agile and Lean thinking, as they originally emerged in manufacturing. Section 4 develops into Agile and Lean for software development. Main characteristics and most popular methods and practices of Agile and Lean software development are developed in this section. Section 5 centers on rapid releases, continuous delivery, and continuous deployment, the latest advances in the area to get speed. The concepts of DevOps, as a means to take full (end-to-end) advantage of Agile and Lean, and Lean start-up, as an approach to foster innovation, are the focus of the two following 6 DevOps, 7 The Lean Startup Movement. Finally, Section 8 focuses on two important aspects of Agile and Lean software development: (1) metrics to guide decision making and (2) technical debt as a mechanism to gain business advantage. To wrap up the chapter, we peer into future directions in the area.},
  doi           = {https://doi.org/10.1016/bs.adcom.2018.03.014},
  issn          = {0065-2458},
  keywords      = {Software processes, Agile software development, Lean software development, Lean thinking, Leagility, Rapid releases, Continuous delivery, Continuous deployment, DevOps, Lean startup, Metrics, Technical debt},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245818300299},
}

@Article{Huang1984,
  author        = {Roger D. Huang},
  title         = {Some alternative tests of forward exchange rates as predictors of future spot rates},
  journal       = {Journal of International Money and Finance},
  year          = {1984},
  volume        = {3},
  number        = {2},
  pages         = {153 - 167},
  issn          = {0261-5606},
  __markedentry = {[Juliana:]},
  abstract      = {The paper provides empirical analysis on the issue of forward premiums as predictors of future exchange depreciations. The need to specify an alternative to the null hypothesis, other than its complement is emphasized. Two such alternatives are considered: the random walk model and the possibility of excessive or insufficient exchange rate volatility to accord with the efficiency of exchange markets.},
  doi           = {https://doi.org/10.1016/0261-5606(84)90003-2},
  url           = {http://www.sciencedirect.com/science/article/pii/0261560684900032},
}

@InCollection{Barbier2010,
  author        = {Gabriel Barbier and Hugo Bruneliere and Frdric Jouault and Yves Lennon and Frdric Madiot},
  title         = {Chapter 14 - MoDisco, a Model-Driven Platform to Support Real Legacy Modernization Use Cases},
  booktitle     = {Information Systems Transformation},
  publisher     = {Morgan Kaufmann},
  year          = {2010},
  editor        = {William M. Ulrich and Philip H. Newcomb},
  series        = {The MK/OMG Press},
  pages         = {365 - 400},
  address       = {Boston},
  isbn          = {978-0-12-374913-0},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
The case study presented in this chapter concentrates on a two-phase discussion of model-driven modernization. To deal with the myriad of technological combinations found in modernization roadmaps, model-driven approaches, and tools offer the requisite abstraction level to build up mature and flexible modernization solutions. This chapter presents the initial collaboration between AtlanMod and Sodifrance, which led to a model-driven legacy modernization approach. The use of model-driven technologies on major migration projects has proven the benefit of this approach. Based on metamodeling standards, the tools used to extract the knowledge from existing applications, to transform the knowledge into new paradigms and architecture, and to regenerate the application according to specific technical platforms and patterns are more flexible and reusable. The new MoDisco platform brings all these benefits into an open extensible framework containing model-driven, reverse-engineering tools and components. The current process and tools used by Sodifrance on its modernization projects are elaborated in the discussion. An illustration drawn from a real migration project carried out for Amadeus Hospitality is also included. A new Eclipse initiative capitalizing on this experience to deliver a model-driven platform for the development of legacy modernization tools is presented.},
  doi           = {https://doi.org/10.1016/B978-0-12-374913-0.00014-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123749130000147},
}

@InCollection{Wolf2007,
  title         = {Glossary},
  booktitle     = {High-Performance Embedded Computing},
  publisher     = {Morgan Kaufmann},
  year          = {2007},
  editor        = {Wayne Wolf},
  pages         = {433 - 466},
  address       = {San Francisco},
  isbn          = {978-0-12-369485-0},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-012369485-0/50009-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123694850500099},
}

@InCollection{Khalil2018,
  author        = {Amal Khalil and Juergen Dingel},
  title         = {Chapter Four - Optimizing the Symbolic Execution of Evolving Rhapsody Statecharts},
  publisher     = {Elsevier},
  year          = {2018},
  editor        = {Atif M. Memon},
  volume        = {108},
  series        = {Advances in Computers},
  pages         = {145 - 281},
  __markedentry = {[Juliana:]},
  abstract      = {Model-driven engineering (MDE) is an iterative and incremental software development process. Supporting the analysis and the verification of software systems developed following the MDE paradigm requires to adopt incrementality when carrying out these crucial tasks in a more optimized way. Communicating state machines are one of the various formalisms used in MDE tools to model and describe the behavior of distributed, concurrent, and real-time reactive systems (e.g., automotive and avionics systems). Modeling the overall behavior of such systems is carried out in a modular way and on different levels of abstraction (i.e., it starts with modeling the behavior of the individual objects in the system first then modeling the interaction between these objects). Similarly, analyzing and verifying the correctness of the developed models to ensure their quality and their integrity is performed on two main levels. The intralevel is used to analyze the correctness of the individual models in isolation of the others, while the interlevel is used to analyze the overall interoperability of those that are communicating with each other. One way to facilitate the analysis of the overall behavior of a system of communicating state machines is to build the global state space (also known as the global reachability tree) of the system. This process is very expensive and in some cases it may suffer from the state explosion problem. Symbolic execution is a technique that can be used to construct an abstract and a bounded version of the system global state space that is known as a symbolic execution tree (SET), yet the size of the generated trees can be very large especially with big and complex systems that are composed of multiple objects. As the system evolves, one way to avoid regenerating the entire SET and repeating any SET-based analyses that have been already conducted is to utilize the previous SET and its analysis results in optimizing the process of generating the SET of the system after the change. In this chapter, we propose two optimization techniques to direct the successive runs of the symbolic execution technique toward the impacted parts of an evolving state machine model using memoization (MSE) and dependency analysis (DSE), respectively. The evaluation results of both techniques showed significant reduction in some cases compared with the standard symbolic execution technique.},
  doi           = {https://doi.org/10.1016/bs.adcom.2017.09.003},
  issn          = {0065-2458},
  keywords      = {Model-driven engineering, Symbolic execution, Incremental verification, State-based behavioral models, State machines, Memoization, Dependency analysis, Model-based analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245817300487},
}

@Article{Bennett1991,
  author        = {KH Bennett},
  title         = {Automated support of software maintenance},
  journal       = {Information and Software Technology},
  year          = {1991},
  volume        = {33},
  number        = {1},
  pages         = {74 - 85},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Software maintenance is the general name given to the set of activities undertaken on a software system following its release for operational use. Surveys have shown that for many projects, software maintenance consumes the majority of the overall software life-cycle costs, and there are indications that the proportion is increasing. Inability to cope with software maintenance can also result in a backlog of application modifications. Despite the importance of software maintenance, it has acquired the reputation of being a second-class area in which to work. The paper defines in more detail the term software maintenance, and then addresses the issues of maintaining existing code, and producing maintainable systems, stressing the role of reengineering. Three projects that focus on software maintenance are then summarized. All three aim to provide automated assistance to the software maintainer, but in contrasting ways. The ReForm project is based on a formal method to extract specifications from code using transformations. MACS and REDO are both transnational European projects funded by the Esprit collaborative programme of research; the former uses expert system technology to assist the maintainer, while REDO aims to provide a set of integratable tools within a single environment, to support the reverse engineering process.},
  doi           = {https://doi.org/10.1016/0950-5849(91)90026-8},
  keywords      = {software maintenance, reengineering, reverse engineering, software process, tools},
  url           = {http://www.sciencedirect.com/science/article/pii/0950584991900268},
}

@Article{Straver2010a,
  author        = {D.C. Straver and Jan-Thies H. Van Asseldonk and N.C. Notermans and J.H. Wokke and L.H. Van den Berg and H. Franssen},
  title         = {P12-19 Cold paresis in multifocal motor neuropathy},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60723-7},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607237},
}

@Article{Navarro2011,
  author        = {Luis Daniel Benavides Navarro and Andrs Barrera and Kiyoshige Garcs and Hugo Arboleda},
  title         = {Detecting and Coordinating Complex Patterns of Distributed Events with KETAL},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2011},
  volume        = {281},
  pages         = {127 - 141},
  issn          = {1571-0661},
  note          = {Proceedings of the 2011 Latin American Conference in Informatics (CLEI)},
  __markedentry = {[Juliana:]},
  abstract      = {This paper presents an event-based kernel library designed to explicitly construct and coordinate complex interactions and communication patterns in distributed applications. The library integrates facilities for explicitly defining complex event patterns, detecting events in distributed systems, and validating sequences of events having into account causal ordering. Concretely we present the following contributions: i) An analysis of non trivial scenarios found in distributed applications in order to formulate a set of requirements and restrictions for a kernel event-based library, ii) the design and implementation of the library supporting the detection and coordination of complex event patterns and the support of causal manipulation of distributed events, iii) a qualitative evaluation of our approach showing how this library can be used to build a sophisticated distributed aspect oriented language.},
  doi           = {https://doi.org/10.1016/j.entcs.2011.11.030},
  keywords      = {Distributed event model, event patterns, causality, automata},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066111001794},
}

@Article{Garousi2015,
  author        = {Vahid Garousi and Ahmet Cokunay and Aysu Betin-Can and Onur Demirrs},
  title         = {A survey of software engineering practices in Turkey},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {108},
  pages         = {148 - 177},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Understanding the types of software engineering (SE) practices and techniques used in industry is important. There is a wide spectrum in terms of the types and maturity of SE practices conducted in industry. Turkey has a vibrant software industry and it is important to characterize and understand the state of its SE practices. Our objective is to characterize and grasp a high-level view on type of SE practices in the Turkish software industry. To achieve this objective, we systematically designed an online survey with 46 questions based on our past experience in the Canadian and Turkish contexts and using the Software Engineering Body of Knowledge (SWEBOK). Two hundred and two practicing software engineers from the Turkish software industry participated in the survey. The survey results reveal important and interesting findings about SE practices in Turkey and beyond. They also help track the profession of SE, and suggest areas for improved training, education and research. Among the findings are the followings: (1) The military and defense software sectors are quite prominent in Turkey, especially in the capital Ankara region, and many SE practitioners work for those companies. (2) 54% of the participants reported not using any software size measurement methods, while 33% mentioned that they have measured lines of code (LOC). (3) In terms of effort, after the development phase (on average, 31% of overall project effort), software testing, requirements, design and maintenance phases come next and have similar average values (14%, 12%, 12% and 11% respectively). (4) Respondents experience the most challenge in the requirements phase. (5) Waterfall, as a rather old but still widely used lifecycle model, is the model that more than half of the respondents (53%) use. The next most preferred lifecycle models are incremental and Agile/lean development models with usage rates of 38% and 34%, respectively. (6) The Waterfall and Agile methodologies have slight negative correlations, denoting that if one is used in a company, the other will less likely to be used. The results of our survey will be of interest to SE professionals both in Turkey and world-wide. It will also benefit researchers in observing the latest trends in SE industry identifying the areas of strength and weakness, which would then hopefully encourage further industryacademia collaborations in those areas.},
  doi           = {https://doi.org/10.1016/j.jss.2015.06.036},
  keywords      = {Software engineering, Industry practices, Turkey},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215001314},
}

@InCollection{Kalman2003,
  author        = {Deborah Bayles Kalman},
  title         = {Extranets},
  booktitle     = {Encyclopedia of Information Systems},
  publisher     = {Elsevier},
  year          = {2003},
  editor        = {Hossein Bidgoli},
  pages         = {301 - 312},
  address       = {New York},
  isbn          = {978-0-12-227240-0},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B0-12-227240-4/00069-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B0122272404000691},
}

@Article{Vidali2008,
  author        = {Matteo Vidali and Marie-Francoise Tripodi and Alesandra Ivaldi and Rosa Zampino and Giuseppa Occhino and Luciano Restivo and Salvatore Sutti and Aldo Marrone and Giuseppe Ruggiero and Emanuele Albano and Luigi E. Adinolfi},
  title         = {Interplay between oxidative stress and hepatic steatosis in the progression of chronic hepatitis C},
  journal       = {Journal of Hepatology},
  year          = {2008},
  volume        = {48},
  number        = {3},
  pages         = {399 - 406},
  issn          = {0168-8278},
  __markedentry = {[Juliana:]},
  abstract      = {Background/Aims
The contribution of oxidative stress to the pathogenesis of chronic hepatitis C (CHC) is still poorly elucidated. This study investigated the relationship between oxidative stress, insulin resistance, steatosis and fibrosis in CHC.
Methods
IgG against malondialdehyde-albumin adducts and HOMA-IR were measured as markers of oxidative stress and insulin resistance, respectively, in 107 consecutive CHC patients.
Results
Oxidative stress was present in 61% of the patients, irrespective of age, gender, viral load, BMI, aminotransferase level, histology activity index (HAI) and HCV genotype. Insulin resistance and steatosis were evident in 80% and 70% of the patients, respectively. In the patients infected by HCV genotype non-3, but not in those with genotype 3 infection HOMA-IR (p<0.03), steatosis (p=0.02) and fibrosis (p<0.05) were higher in the subjects with oxidative stress than in those without. Multiple regression analysis revealed that, HOMA-IR (p<0.01), fibrosis (p<0.01) and oxidative stress (p<0.05) were independently associated with steatosis, whereas steatosis was independently associated with oxidative stress (p<0.03) and HOMA-IR (p<0.02). Steatosis (p<0.02) and HAI (p=0.007) were also independent predictors of fibrosis.
Conclusions
In patients infected by HCV genotype non-3, oxidative stress and insulin resistance contribute to steatosis, which in turn exacerbates both insulin resistance and oxidative stress and accelerates the progression of fibrosis.},
  doi           = {https://doi.org/10.1016/j.jhep.2007.10.011},
  keywords      = {Oxidative stress, Steatosis, Lipid peroxidation, HCV infection, HOMA-IR, Liver fibrosis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0168827807006277},
}

@Article{Liaskos2012,
  author        = {Sotirios Liaskos and Shakil M. Khan and Marin Litoiu and Marina Daoud Jungblut and Vyacheslav Rogozhkin and John Mylopoulos},
  title         = {Behavioral adaptation of information systems through goal models},
  journal       = {Information Systems},
  year          = {2012},
  volume        = {37},
  number        = {8},
  pages         = {767 - 783},
  issn          = {0306-4379},
  note          = {Special Issue: Advanced Information Systems Engineering (CAiSE'11)},
  __markedentry = {[Juliana:]},
  abstract      = {Customizing software to perfectly fit individual needs is becoming increasingly important in information systems engineering. Users want to be able to customize software behavior through reference to terms familiar to their diverse needs and experience. We present a requirements-driven approach to behavioral customization of software systems. Goal models are constructed to represent alternative behaviors that users can exhibit to achieve their goals. Customization information is then added to restrict the space of possibilities to those that fit specific users, contexts, or situations. Meanwhile, elements of the goal models are mapped to units of source code. This way, customization preferences posed at the requirements level are directly translated into system customizations. Our approach, which we apply to an on-line shopping cart system and an automated teller machine simulator, does not assume adoption of a particular development methodology, platform, or variability implementation technique and keeps the reasoning computation overhead from interfering with the execution of the configured application.},
  doi           = {https://doi.org/10.1016/j.is.2012.05.006},
  keywords      = {Information systems engineering, Goal modeling, Software customization, Adaptive systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437912000737},
}

@InCollection{Walls2012,
  booktitle     = {Embedded Software (Second Edition)},
  publisher     = {Newnes},
  year          = {2012},
  editor        = {Colin Walls},
  pages         = {385 - 395},
  address       = {Oxford},
  edition       = {Second Edition},
  isbn          = {978-0-12-415822-1},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-415822-1.00021-0},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780124158221000210},
}

@Article{Barenghi2015,
  author        = {Alessandro Barenghi and Stefano Crespi Reghizzi and Dino Mandrioli and Federica Panella and Matteo Pradella},
  title         = {Parallel parsing made practical},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {112},
  pages         = {195 - 226},
  issn          = {0167-6423},
  __markedentry = {[Juliana:]},
  abstract      = {The property of local parsability allows to parse inputs through inspecting only a bounded-length string around the current token. This in turn enables the construction of a scalable, data-parallel parsing algorithm, which is presented in this work. Such an algorithm is easily amenable to be automatically generated via a parser generator tool, which was realized, and is also presented in the following. Furthermore, to complete the framework of a parallel input analysis, a parallel scanner can also combined with the parser. To prove the practicality of a parallel lexing and parsing approach, we report the results of the adaptation of JSON and Lua to a form fit for parallel parsing (i.e. an operator-precedence grammar) through simple grammar changes and scanning transformations. The approach is validated with performance figures from both high performance and embedded multicore platforms, obtained analyzing real-world inputs as a test-bench. The results show that our approach matches or dominates the performances of production-grade LR parsers in sequential execution, and achieves significant speedups and good scaling on multi-core machines. The work is concluded by a broad and critical survey of the past work on parallel parsing and future directions on the integration with semantic analysis and incremental parsing.},
  doi           = {https://doi.org/10.1016/j.scico.2015.09.002},
  keywords      = {Parallel parsing algorithms, Syntax analysis, Parallel parser, Operator precedence grammar},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642315002610},
}

@InCollection{Meillour2003,
  author        = {Patricia Nagnan-Le Meillour and Emmanuelle Jacquin-Joly},
  title         = {17 - Biochemistry and diversity of insect odorant-binding proteins},
  booktitle     = {Insect Pheromone Biochemistry and Molecular Biology},
  publisher     = {Academic Press},
  year          = {2003},
  editor        = {Gary Blomquist and Richard Vogt},
  pages         = {509 - 537},
  address       = {San Diego},
  isbn          = {978-0-12-107151-6},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
Odorant-binding proteins (OBPs) are abundant in the sensillar lymph of insect antennae and participate in olfactory perireceptor events, such as transport of the hydrophobic odorant through the aqueous medium, presentation of the odor to olfactory receptors, and deactivation of the signal. This chapter describes the biochemistry and molecular biology of pheromone detection in the noctuid moth, Mamestra brassicae. The OBPs are diverse ranging from numerous species of several insect orders including Lepidoptera, Diptera, Coleoptera, and Hymenoptera, Hemiptera, and Phasmatodea. Classification defines OBP-Type 1 and OBP-Type 2 based on phylogeny, tissue localization, and structural features. The advantage of the functional approach to characterize the diversity of OBPs inside species is characterized. The construction of EST antennal libraries with a high EST number to be representative of the sequence diversity has identified not only new OBPs, but also SAPs, pheromone-degrading enzymes, sensory-neuron-membrane-protein-like sequences, and other elements involved in the pheromone transduction process.},
  doi           = {https://doi.org/10.1016/B978-012107151-6/50019-0},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780121071516500190},
}

@Article{Peterson2018,
  author        = {John W. Peterson and Alexander D. Lindsay and Fande Kong},
  title         = {Overview of the incompressible NavierStokes simulation capabilities in the MOOSE framework},
  journal       = {Advances in Engineering Software},
  year          = {2018},
  volume        = {119},
  pages         = {68 - 92},
  issn          = {0965-9978},
  __markedentry = {[Juliana:]},
  abstract      = {The Multiphysics Object Oriented Simulation Environment (MOOSE) framework is a high-performance, open source, C++ finite element toolkit developed at Idaho National Laboratory. MOOSE was created with the aim of assisting domain scientists and engineers in creating customizable, high-quality tools for multiphysics simulations. While the core MOOSE framework itself does not contain code for simulating any particular physical application, it is distributed with a number of physics modules which are tailored to solving e.g. heat conduction, phase field, and solid/fluid mechanics problems. In this report, we describe the basic equations, finite element formulations, software implementation, and regression/verification tests currently available in MOOSEs navier_stokes module for solving the Incompressible Navier-Stokes (INS) equations.},
  doi           = {https://doi.org/10.1016/j.advengsoft.2018.02.004},
  url           = {http://www.sciencedirect.com/science/article/pii/S0965997817310591},
}

@InCollection{Aarno2015,
  author        = {Daniel Aarno and Jakob Engblom},
  title         = {Chapter 6 - Building virtual platforms},
  booktitle     = {Full-System Simulation with Simics},
  publisher     = {Morgan Kaufmann},
  year          = {2015},
  editor        = {Daniel Aarno and Jakob Engblom},
  pages         = {161 - 210},
  address       = {Boston},
  isbn          = {978-0-12-800725-9},
  __markedentry = {[Juliana:]},
  abstract      = {Chapter 6 introduces the reader to how to best perform transaction-level modeling of individual devices and how such models are built in Simics. It covers the Device Modeling Language (DML), as well as device modeling in C, C++, Python, and SystemC. Chapter 6 provides detailed step-by-step instructions for how to create a simple device model in Simics.},
  doi           = {https://doi.org/10.1016/B978-0-12-800725-9.00006-8},
  keywords      = {DML, device model, TLM, SystemC, modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128007259000068},
}

@Article{Penton2012,
  author        = {Andrea L. Penton and Laura D. Leonard and Nancy B. Spinner},
  title         = {Notch signaling in human development and disease},
  journal       = {Seminars in Cell \& Developmental Biology},
  year          = {2012},
  volume        = {23},
  number        = {4},
  pages         = {450 - 457},
  issn          = {1084-9521},
  note          = {Cancer Cell Metabolism \& Notch Signaling},
  __markedentry = {[Juliana:]},
  abstract      = {Mutations in Notch signaling pathway members cause developmental phenotypes that affect the liver, skeleton, heart, eye, face, kidney, and vasculature. Notch associated disorders include the autosomal dominant, multi-system, Alagille syndrome caused by mutations in both a ligand (Jagged1 (JAG1)) and receptor (NOTCH2) and autosomal recessive spondylocostal dysostosis, caused by mutations in a ligand (Delta-like-3 (DLL3)), as well as several other members of the Notch signaling pathway. Mutations in NOTCH2 have also recently been connected to Hajdu-Cheney syndrome, a dominant disorder causing focal bone destruction, osteoporosis, craniofacial morphology and renal cysts. Mutations in the NOTCH1 receptor are associated with several types of cardiac disease and mutations in NOTCH3 cause the dominant adult onset disorder CADASIL (cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy), a vascular disorder with onset in the 4th or 5th decades. Studies of these human disorders and their inheritance patterns and types of mutations reveal insights into the mechanisms of Notch signaling.},
  doi           = {https://doi.org/10.1016/j.semcdb.2012.01.010},
  keywords      = {Alagille syndrome, Spondylocostal dysostosis, Hajdu Cheney, Cardiac disease, Notch signaling},
  url           = {http://www.sciencedirect.com/science/article/pii/S1084952112000146},
}

@Article{Ochoa2018,
  author        = {Lina Ochoa and Oscar Gonzlez-Rojas and Alves Pereira Juliana and Harold Castro and Gunter Saake},
  title         = {A systematic literature review on the semi-automatic configuration of extended product lines},
  journal       = {Journal of Systems and Software},
  year          = {2018},
  volume        = {144},
  pages         = {511 - 532},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Product line engineering has become essential in mass customisation given its ability to reduce production costs and time to market, and to improve product quality and customer satisfaction. In product line literature, mass customisation is known as product configuration. Currently, there are multiple heterogeneous contributions in the product line configuration domain. However, a secondary study that shows an overview of the progress, trends, and gaps faced by researchers in this domain is still missing. In this context, we provide a comprehensive systematic literature review to discover which approaches exist to support the configuration process of extended product lines and how these approaches perform in practice. Extend product lines consider non-functional properties in the product line modelling. We compare and classify a total of 66 primary studies from 2000 to 2016. Mainly, we give an in-depth view of techniques used by each work, how these techniques are evaluated and their main shortcomings. As main results, our review identified (i) the need to improve the quality of the evaluation of existing approaches, (ii) a lack of hybrid solutions to support multiple configuration constraints, and (iii) a need to improve scalability and performance conditions.},
  doi           = {https://doi.org/10.1016/j.jss.2018.07.054},
  keywords      = {Extended product line, Product configuration, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121218301511},
}

@Article{Kraetzig2007,
  author        = {Markus Krtzig},
  title         = {A software framework for data analysis},
  journal       = {Computational Statistics \& Data Analysis},
  year          = {2007},
  volume        = {52},
  number        = {2},
  pages         = {618 - 634},
  issn          = {0167-9473},
  __markedentry = {[Juliana:]},
  abstract      = {The open-source Java software framework JStatCom is presented which supports the development of rich desktop clients for data analysis in a rather general way. The concept is to solve all recurring tasks with the help of reusable components and to enable rapid application development by adopting a standards based approach which is readily supported by existing programming tools. Furthermore, JStatCom allows to call external procedures from within Java that are written in other languages, for example Gauss, Ox or Matlab. This way it is possible to reuse an already existing code base for numerical routines written in domain-specific programming languages and to link them with the Java world. A reference application for JStatCom is the econometric software package JMulTi, which will shortly be introduced.},
  doi           = {https://doi.org/10.1016/j.csda.2006.08.007},
  keywords      = {Java, Object-oriented programming, Econometrics, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167947306002672},
}

@Article{Goncalves2009,
  author        = {Raquel M. Gonalves and M. Cristina L. Martins and Graa Almeida-Porada and Mrio A. Barbosa},
  title         = {Induction of notch signaling by immobilization of jagged-1 on self-assembled monolayers},
  journal       = {Biomaterials},
  year          = {2009},
  volume        = {30},
  number        = {36},
  pages         = {6879 - 6887},
  issn          = {0142-9612},
  __markedentry = {[Juliana:]},
  abstract      = {Notch signaling is a key mechanism during mammal development and stem cell regulation. This study aims to target and control Notch signaling by ligands immobilization using self-assembled monolayers (SAMs) as model surfaces. Non-fouling substrates were prepared by immersion of gold substrates in (1-Mercapto-11-undecyl)tetra(ethylene glycol) thiol solutions. These surfaces were activated with N,N-carbonyldiimidazole (CDI) at different concentrations (0, 0.03, 0.3, 3 and 30mg/ml) and an anti-human IgG, Fc specific fragment antibody (Ab) was covalently bound to EG4-SAMs to guarantee the correct exposure of the Notch ligand Jagged-1/Fc chimera (Jag-1). The presence of Ab and Jag-1 was confirmed by radiolabeling, X-ray photoelectron spectroscopy (XPS), ellipsometry and ELISA. The biological activity of Jag-1-Ab-SAMs was assessed by real-time PCR for Hes-1 family gene expression, a Notch pathway target gene, in HL-60 cell line. Results have shown an increase of the amount of immobilized Ab with increasing surface activator concentrations. Jag-1 concentration also increases with Ab concentration. Interestingly, a higher Jagged-1 exposure and fold increase in Hes-1 expression were obtained for surfaces activated with the lowest concentration of CDI (0.03mg/ml). These results illustrate the great importance of ligands orientation and exposure, when compared with density. This investigation brings new insights into Notch signaling mechanisms. In particular, Jag-1-Ab-SAMs have shown to be adequate model surfaces to study Notch pathway activation and may provide a basis to develop new interfaces in biomaterials to control Notch mechanism in different cell systems.},
  doi           = {https://doi.org/10.1016/j.biomaterials.2009.09.010},
  keywords      = {Nanostructured surfaces, Self-assembled monolayers, Protein immobilization, Protein adsorption, Notch signaling},
  url           = {http://www.sciencedirect.com/science/article/pii/S0142961209009260},
}

@Article{Meyrand2013,
  author        = {M. Meyrand and D.C. Dallas and H. Caillat and F. Bouvier and P. Martin and D. Barile},
  title         = {Comparison of milk oligosaccharides between goats with and without the genetic ability to synthesize s1-casein},
  journal       = {Small Ruminant Research},
  year          = {2013},
  volume        = {113},
  number        = {2},
  pages         = {411 - 420},
  issn          = {0921-4488},
  __markedentry = {[Juliana:]},
  abstract      = {Milk oligosaccharides (OS)free complex carbohydratesconfer unique health benefits to the nursing neonate. Though human digestive enzymes cannot degrade these sugars, they provide nourishment to specific commensal microbes and act as decoys to prevent the adhesion of pathogenic micro-organisms to gastrointestinal cells. At present, the limited quantities of human milk oligosaccharides (HMO) impede research on these molecules and their potential applications in functional food formulations. Considerable progress has been made in the study of OS structures; however, the synthetic pathways leading to their synthesis in the mammary gland are poorly understood. Recent studies show that complex OS with fucose and N-acetyl neuraminic acid (key structural elements of HMO bioactivity) exist in goat milk. Polymorphisms in the CSN1S1 locus, which is responsible for synthesis of s1-casein, affect lipid and casein micelle structure in goat milk. The present study sought to determine whether CSN1S1 polymorphisms also influence goat milk oligosaccharide (GMO) production and secretion. The GMO compositions of thirty-two goat milk samples, half of which were from genotype A/A (s1-casein producers) and half from genotype O/O (s1-casein non-producers), were determined with nanoflow liquid chromatography high-accuracy mass spectrometry. This study represents the most exhaustive characterization of GMO to date. A systematic and comprehensive GMO library was created, consolidating information available in the literature with the new findings. Nearly 30 GMO, 11 of which were novel, were confirmed via tandem mass spectrometric analyses. Six fucosylated OS were identified; 4 of these matched HMO compositions and three were identified for the first time in goat milk. Importantly, multivariate statistical analysis demonstrated that the OS profiles of the A/A and O/O genotype milks could be discriminated by the fucosylated OS. Quantitative analysis revealed that the goat milk samples contained 1.17g/L of OS; however, their concentration in milks from A/A and O/O genotypes was not different. This study provides evidence of a genetic influence on specific OS biosynthesis but not total OS production. The presence of fucosylated GMO suggests that goat milk represents a potential source of bioactive milk OS suitable as a functional food ingredient.},
  doi           = {https://doi.org/10.1016/j.smallrumres.2013.03.014},
  keywords      = {, Fucose, Genetic polymorphisms, Goat milk, Mass spectrometry, Oligosaccharides},
  url           = {http://www.sciencedirect.com/science/article/pii/S0921448813000990},
}

@Article{Schermann2018,
  author        = {Gerald Schermann and Jrgen Cito and Philipp Leitner and Uwe Zdun and Harald C. Gall},
  title         = {Were doing it live: A multi-method empirical study on continuous experimentation},
  journal       = {Information and Software Technology},
  year          = {2018},
  volume        = {99},
  pages         = {41 - 57},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Continuous experimentation guides development activities based on data collected on a subset of online users on a new experimental version of the software. It includes practices such as canary releases, gradual rollouts, dark launches, or A/B testing.
Objective
Unfortunately, our knowledge of continuous experimentation is currently primarily based on well-known and outspoken industrial leaders. To assess the actual state of practice in continuous experimentation, we conducted a mixed-method empirical study.
Method
In our empirical study consisting of four steps, we interviewed 31 developers or release engineers, and performed a survey that attracted 187 complete responses. We analyzed the resulting data using statistical analysis and open coding.
Results
Our results lead to several conclusions: (1) from a software architecture perspective, continuous experimentation is especially enabled by architectures that foster independently deployable services, such as microservices-based architectures; (2) from a developer perspective, experiments require extensive monitoring and analytics to discover runtime problems, consequently leading to developer on call policies and influencing the role and skill sets required by developers; and (3) from a process perspective, many organizations conduct experiments based on intuition rather than clear guidelines and robust statistics.
Conclusion
Our findings show that more principled and structured approaches for release decision making are needed, striving for highly automated, systematic, and data- and hypothesis-driven deployment and experimentation.},
  doi           = {https://doi.org/10.1016/j.infsof.2018.02.010},
  keywords      = {Release engineering, Continuous deployment, Continuous experimentation, Empirical study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917302136},
}

@Article{Dijk2010,
  author        = {J.P. van Dijk and C. Verhamme and I.N. van Schaik and H.J. Schelhaas and E. Mans and L.J. Bour and D.F. Stegeman and M.J. Zwarts},
  title         = {P12-20 Age-related changes in motor unit number estimates in adult patients with Charcot-Marie-Tooth type 1A},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60724-9},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607249},
}

@Article{Melvin1989,
  author        = {Michael Melvin and Su Zhou},
  title         = {Do centrally planned exchange rates behave differently from capitalist rates?},
  journal       = {Journal of Comparative Economics},
  year          = {1989},
  volume        = {13},
  number        = {2},
  pages         = {325 - 334},
  issn          = {0147-5967},
  __markedentry = {[Juliana:]},
  abstract      = {We conduct a statistical analysis of the time series of the dollar value of the pound, mark, yen, yuan, dinar, and forint exchange rates. The evidence indicates that the centrally planned exchange rates are well represented by random walks, as are the capitalist rates. This might be expected if the planned rates are pegged to the capitalist rates. However, a lack of cointegration between the planned rates and the capitalist rates suggests that this is not the reason for the nonstationarity of planned exchange rates.},
  doi           = {https://doi.org/10.1016/0147-5967(89)90008-5},
  url           = {http://www.sciencedirect.com/science/article/pii/0147596789900085},
}

@InCollection{Dyro2004,
  booktitle     = {Clinical Engineering Handbook},
  publisher     = {Academic Press},
  year          = {2004},
  editor        = {Joseph F Dyro},
  series        = {Biomedical Engineering},
  pages         = {665 - 674},
  address       = {Burlington},
  isbn          = {978-0-12-226570-9},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-012226570-9/50159-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780122265709501599},
}

@InCollection{Zuellighoven2005,
  title         = {INDEX},
  booktitle     = {Object-Oriented Construction Handbook},
  publisher     = {Morgan Kaufmann},
  year          = {2005},
  editor        = {Heinz Zllighoven},
  pages         = {501 - 520},
  address       = {San Francisco},
  isbn          = {978-1-55860-687-6},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-155860687-6/50014-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781558606876500141},
}

@InCollection{Kenyon2002,
  title         = {Index},
  booktitle     = {Data Networks},
  publisher     = {Digital Press},
  year          = {2002},
  editor        = {Tony Kenyon},
  pages         = {775 - 807},
  address       = {Burlington},
  isbn          = {978-1-55558-271-5},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-155558271-5/50038-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781555582715500389},
}

@Article{Adams2009,
  author        = {Bram Adams and Kris De Schutter and Andy Zaidman and Serge Demeyer and Herman Tromp and Wolfgang De Meuter},
  title         = {Using aspect orientation in legacy environments for reverse engineering using dynamic analysisAn industrial experience report},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {4},
  pages         = {668 - 684},
  issn          = {0164-1212},
  note          = {Special Issue: Selected papers from the 2008 IEEE Conference on Software Engineering Education and Training (CSEET08)},
  __markedentry = {[Juliana:]},
  abstract      = {This paper reports on the challenges of using aspect-oriented programming (AOP) to aid in re-engineering a legacy C application. More specifically, we describe how AOP helps in the important reverse engineering step which typically precedes a re-engineering effort. We first present a comparison of the available AOP tools for legacy C code bases, and then argue on our choice of Aspicere, our own AOP implementation for C. Then, we report on Aspiceres application in reverse engineering a legacy industrial software system and we show how we apply a dynamic analysis to regain insight into the system. AOP is used for instrumenting the system and for gathering the data. This approach works and is conceptually very clean, but comes with a major quid pro quo: integration of AOP tools with the build system proves an important issue. This leads to the question of how to reconcile the notion of modular reasoning within traditional build systems with a programming paradigm which breaks this notion.},
  doi           = {https://doi.org/10.1016/j.jss.2008.09.031},
  keywords      = {Dynamic analysis, Aspect-oriented programming, Industrial case study, Program comprehension C},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121208002173},
}

@Article{Fecko2005,
  author        = {Mariusz A. Fecko and Christopher M. Lott},
  title         = {XML-based requirements engineering for an electronic clearinghouse},
  journal       = {Information and Software Technology},
  year          = {2005},
  volume        = {47},
  number        = {13},
  pages         = {841 - 858},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {We present methods and tools to support XML-based requirements engineering for an electronic clearinghouse that connects trading partners in the telecommunications area. The original semi-structured requirements, locally known as business rules, were written as message specifications in a non-standardized and error-prone format using MS Word. To remedy the resulting software failures and faults, we first formalized the requirements by designing an W3C XML Schema for the precise definition of the requirements structure. The schema allows a highly structured representation of the essential information in eXtensible Markup Language (XML). Second, to offer the requirements engineers the ability to edit the XML documents in a friendly way while preserving their information structure, we developed a custom editor called XLEdit. Third, by developing a converter from MS Word to the target XML format, we helped the requirements engineers to migrate the existing business rules. Fourth, we developed translators from the structured requirements to schema languages, which enabled automated generation of message-validation code. The increase in customer satisfaction and clearinghouse-service efficiency are primary gains from the investment in the technology for structured requirements editing and validation.},
  doi           = {https://doi.org/10.1016/j.infsof.2005.01.005},
  keywords      = {GUI, Message-processing, XML, Business rules, Requirements engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584905000145},
}

@InCollection{Sittig2017a,
  title         = {Index},
  booktitle     = {Clinical Informatics Literacy},
  publisher     = {Academic Press},
  year          = {2017},
  editor        = {Dean F. Sittig},
  pages         = {171 - 231},
  isbn          = {978-0-12-803206-0},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-803206-0.18001-0},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128032060180010},
}

@Article{Nishida2014,
  author        = {Tetsushi Nishida and James B. Pick and Avijit Sarkar},
  title         = {Japans prefectural digital divide: A multivariate and spatial analysis},
  journal       = {Telecommunications Policy},
  year          = {2014},
  volume        = {38},
  number        = {11},
  pages         = {992 - 1010},
  issn          = {0308-5961},
  __markedentry = {[Juliana:]},
  abstract      = {This study of the digital divide within Japan utilizes data from the countrys 47 prefectures for multivariate and spatial analysis of distributions of information and communication technology (ICT) variables. The paper constructs an exploratory conceptual model of technology utilization and expenditures in Japan, induced from prior literature. Ten dependent ICT utilization and expenditure factors are posited to be related to 12 independent demographic, economic, infrastructure, education, innovation and openness factors. The relationship of the independent to dependent factors is moderated by analysis of spatial patterns of technology utilization to examine proximities and reduce spatial bias. Based on the model, a multivariate analysis identifies correlates of the nations digital divide, including patents registered by Japanese citizens, newspaper circulation, students and pupils per capita, household expenditures on education, rural/urban status, and Japans aged population structure which has wide generational gaps. Spatial clusters and outliers of ICTs in prefectures are analyzed, with attention to their policy impacts. Findings suggest modifications to the conceptual model. Implications of findings for the countrys official national technology planning policies are considered and recommendations made to expand them.},
  doi           = {https://doi.org/10.1016/j.telpol.2014.05.004},
  keywords      = {Japan, Digital divide, Information and communication technologies, ICT use factors, Theoretical model, Regression, Spatial autocorrelation, Cluster analysis, ICT policy implications},
  url           = {http://www.sciencedirect.com/science/article/pii/S0308596114000937},
}

@Article{Vogel-Heuser2017,
  author        = {Birgit Vogel-Heuser and Juliane Fischer and Stefan Feldmann and Sebastian Ulewicz and Susanne Rsch},
  title         = {Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {35 - 62},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry.},
  doi           = {https://doi.org/10.1016/j.jss.2017.05.051},
  keywords      = {Factory automation, Automated production systems, Maturity, Modularity, Control software, Programmable logic controller},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300985},
}

@InCollection{Valli1993,
  author        = {V.E.O. Valli and B.W. Parry},
  title         = {CHAPTER 2 - The Hematopoietic System},
  booktitle     = {Pathology of Domestic Animals (Fourth Edition)},
  publisher     = {Academic Press},
  year          = {1993},
  editor        = {K.V.F. JUBB and PETER C. KENNEDY and NIGEL PALMER},
  pages         = {101 - 265},
  address       = {San Diego},
  edition       = {Fourth Edition},
  isbn          = {978-0-12-391607-5},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-391607-5.50010-8},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123916075500108},
}

@InCollection{Noergaard2005,
  title         = {Appendix D - Glossary},
  booktitle     = {Embedded Systems Architecture},
  publisher     = {Newnes},
  year          = {2005},
  editor        = {Tammy Noergaard},
  series        = {Embedded Technology},
  pages         = {610 - 626},
  address       = {Burlington},
  isbn          = {978-0-7506-7792-9},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-075067792-9/50022-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780750677929500224},
}

@Article{Myers2005,
  author        = {Toshia R. Myers and Iva Greenwald},
  title         = {lin-35 Rb Acts in the Major Hypodermis to Oppose Ras-Mediated Vulval Induction in C. elegans},
  journal       = {Developmental Cell},
  year          = {2005},
  volume        = {8},
  number        = {1},
  pages         = {117 - 123},
  issn          = {1534-5807},
  __markedentry = {[Juliana:]},
  abstract      = {Specification of vulval precursor cell (VPC) fates in C. elegans has served as an important signal transduction paradigm. Genetic studies have indicated that a large group of synthetic multivulva (SynMuv) genes, including the Rb ortholog lin-35, antagonizes the activity of the EGF receptor-Ras-MAP kinase pathway during VPC specification. A prevalent view has been that Rb-mediated transcriptional regulation and chromatin remodeling activities act in the VPCs to antagonize Ras activation through effects on promoters of target genes of the EGF receptor-Ras-MAP kinase pathway that promote vulval fates. Here, we have investigated the cellular focus of lin-35 using conventional genetic mosaic analysis and tissue-specific expression. Our results indicate that lin-35 activity is required in the major hypodermal syncytium and not in the VPCs to inhibit vulval fates. LIN-35 Rb may inhibit vulval fates by regulating a signal from hyp7 to the VPCs or the physiological state of hyp7.},
  doi           = {https://doi.org/10.1016/j.devcel.2004.11.015},
  url           = {http://www.sciencedirect.com/science/article/pii/S1534580704004228},
}

@Article{Tsai2016,
  author        = {Wei-Tek Tsai and Guanqiu Qi},
  title         = {Integrated fault detection and test algebra for combinatorial testing in TaaS (Testing-as-a-Service)},
  journal       = {Simulation Modelling Practice and Theory},
  year          = {2016},
  volume        = {68},
  pages         = {108 - 124},
  issn          = {1569-190X},
  __markedentry = {[Juliana:]},
  abstract      = {Testing-as-a-Service (TaaS) is a software testing service in a cloud that can leverage the computation power provided by the cloud. Specifically, a TaaS can be scaled to large and dynamic workloads, executed in a distributed environment with hundreds of thousands of processors, and these processors may support concurrent and distributed test execution and analysis. This paper proposes a TaaS system based on Adaptive Reasoning (AR) and Test Algebra (TA) for Combinatorial Testing (CT). AR performs testing and identifies faulty interactions, and TA eliminates related configurations from testing and there can be carried out concurrently. By combining these two, it is possible to perform large CT that were not possible before. Specifically, we performed experiments with 250 components with 2.83*1087 6-way interactions with about 21.11015 configurations, and this may be the largest CT experimentation as 2014. 98.6% of configurations have been eliminated out of total number of configurations.},
  doi           = {https://doi.org/10.1016/j.simpat.2016.08.003},
  keywords      = {Combinatorial testing, TaaS, Concurrent testing, Test algebra, Adaptive reasoning},
  url           = {http://www.sciencedirect.com/science/article/pii/S1569190X16302210},
}

@Article{Congote2005,
  author        = {Luis F. Congote},
  title         = {Monitoring insulin-like growth factors in HIV infection and AIDS},
  journal       = {Clinica Chimica Acta},
  year          = {2005},
  volume        = {361},
  number        = {1},
  pages         = {30 - 53},
  issn          = {0009-8981},
  __markedentry = {[Juliana:]},
  abstract      = {There is a close association between the growth hormone (GH)insulin-like growth factor I (IGF-I) axis, infection and immunity. Infection with the human immunodeficiency virus (HIV) is often associated with a decrease of the concentrations of IGF-I, IGF-II, IGF-binding protein 3 (IGFBP-3) and an increase of IGFBP-1 and -2. Many investigators have studied the relationship between the GH-IGF-I system and some of the most common characteristics of disease progression, such as decreased CD4 cell counts, weight loss and fat redistribution. Although conditions for restoration of thymic function and lymphopoiesis with GH or IGF-I are still not well defined, many studies led to the development of clinical trials on the therapeutic use of GH, IGF-I and GHRH for the treatment of weight loss or fat redistribution, two problems which persist despite the introduction of highly active antiretroviral therapy. Monitoring IGF-I concentrations during treatment with GH and GHRH is likely to become an essential component of their therapeutic use. IGF-I levels are the first indicator of treatment efficacy and can be used to monitor compliance. High levels of IGF-I are a warning sign for the increased risk of potential adverse effects, such as acromegalic-like symptoms or malignancy. This could lead to a reduction of the therapeutic dose or the temporary interruption of treatment until IGF levels reach a safe range. IGF-I levels are also likely to increase with other hormones used in HIV patients, such as erythropoietin for the treatment of anemia or anabolic androgens in HIV-infected women.},
  doi           = {https://doi.org/10.1016/j.cccn.2005.05.001},
  keywords      = {IGF, Growth hormone, HAART, HIV, Wasting, Lipodystrophy},
  url           = {http://www.sciencedirect.com/science/article/pii/S0009898105002901},
}

@Article{Hueber2016,
  author        = {Thomas Hueber and Grard Bailly},
  title         = {Statistical conversion of silent articulation into audible speech using full-covariance HMM},
  journal       = {Computer Speech \& Language},
  year          = {2016},
  volume        = {36},
  pages         = {274 - 293},
  issn          = {0885-2308},
  __markedentry = {[Juliana:]},
  abstract      = {This article investigates the use of statistical mapping techniques for the conversion of articulatory movements into audible speech with no restriction on the vocabulary, in the context of a silent speech interface driven by ultrasound and video imaging. As a baseline, we first evaluated the GMM-based mapping considering dynamic features, proposed by Toda et al. (2007) for voice conversion. Then, we proposed a phonetically-informed version of this technique, based on full-covariance HMM. This approach aims (1) at modeling explicitly the articulatory timing for each phonetic class, and (2) at exploiting linguistic knowledge to regularize the problem of silent speech conversion. Both techniques were compared on continuous speech, for two French speakers (one male, one female). For modal speech, the HMM-based technique showed a lower spectral distortion (objective evaluation). However, perceptual tests (transcription and XAB discrimination tests) showed a better intelligibility of the GMM-based technique, probably related to its less fluctuant quality. For silent speech, a perceptual identification test revealed a better segmental intelligibility for the HMM-based technique on consonants.},
  doi           = {https://doi.org/10.1016/j.csl.2015.03.005},
  keywords      = {Silent speech interface, GMM, HMM, Ultrasound, Articulatoryacoustic mapping},
  url           = {http://www.sciencedirect.com/science/article/pii/S0885230815000340},
}

@Article{Crawford1985,
  author        = {S.G. Crawford and A.A. McIntosh and D. Pregibon},
  title         = {An analysis of static metrics and faults in C software},
  journal       = {Journal of Systems and Software},
  year          = {1985},
  volume        = {5},
  number        = {1},
  pages         = {37 - 48},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {In this empirical study, we evaluate the extent to which a set of software measures are correlated with the number of faults and the total estimated repair effort for a large software system. The measures we use are basic counts reflecting program size and structure and metrics proposed by McCabe and Halstead. The effect of program size has a major influence on these metrics, and we present a suitable method of adjusting the metrics for size. In modeling faults or repair effort as a function of one variable, a number of measures individually explain approximately one-quarter of the variation observed in the fault data. No one measure does significantly better than size in explaining the variation in faults found across software units, and thus multiple variable models are necessary to find metrics of importance in addition to program size. The best multivariate model explains approximately one-half the variation in the fault data. The metrics included in this model (in addition to size) are: the ratio of block comments to total lines of code, the number of decisions per function, and the relative vocabulary of program variables and operators. These metrics have potential for future use in the quality control of software.},
  doi           = {https://doi.org/10.1016/0164-1212(85)90005-6},
  url           = {http://www.sciencedirect.com/science/article/pii/0164121285900056},
}

@InCollection{Lackner2017,
  author        = {Hartmut Lackner and Bernd-Holger Schlingloff},
  title         = {Chapter Four - Advances in Testing Software Product Lines},
  publisher     = {Elsevier},
  year          = {2017},
  editor        = {Atif M. Memon},
  volume        = {107},
  series        = {Advances in Computers},
  pages         = {157 - 217},
  __markedentry = {[Juliana:]},
  abstract      = {In this chapter, we describe some recent techniques and results in model-based testing of software product lines. Presently, more and more software-based products and services are available in many different variants to choose from. However, this brings about challenges for the software quality assurance processes. Since only few of all possible variants can be tested at the developer's site, several questions arise. How shall the variability be described in order to make sure that all features are being tested? Is it better to test selected variants on a concrete level, or shall the whole software product line be tested abstractly? What is the quality of a test suite for a product line, anyway? If it is impossible to test all possible variants, which products should be selected for testing? Given a certain product, which test cases are appropriate for it, and given a test case, which products can be tested with it? We address these questions from an empirical software engineering point of view. We sketch modeling formalisms for software product lines. Then, we compare domain-centered and application-centered approaches to software product line testing. We define mutation operators for assessing software product line test suites. Subsequently, we analyze methods for selecting product variants on the basis of a given test suite. Finally, we show how model checking can be used to determine whether a certain test case is applicable for a certain product variant. For all our methods we describe supporting tools and algorithms. Currently, we are integrating these in an integrated tool suite supporting several aspects of model-based testing for software product lines.},
  doi           = {https://doi.org/10.1016/bs.adcom.2017.07.001},
  issn          = {0065-2458},
  keywords      = {Software product lines, Cyber physical systems, Model-based testing, Test generation, Variant management, Feature modeling, Domain analysis, Fault injection, Product sampling, Test case assignment},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245817300311},
}

@Article{Jia2016,
  author        = {Changjiang Jia and Yan Cai and Yuen Tak Yu and T.H. Tse},
  title         = {5W+1H pattern: A perspective of systematic mapping studies and a case study on cloud software testing},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {116},
  pages         = {206 - 219},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {A common type of study used by researchers to map out the landscape of a research topic is known as mapping study. Such a study typically begins with an exploratory search on the possible ideas of the research topic, which is often done in an unsystematic manner. Hence, the activity of formulating research questions in mapping studies is ill-defined, rendering it difficult for researchers who are new to the topic. There is a need to guide them kicking off a mapping study of an unfamiliar domain. This paper proposes a 5W+1H pattern to help investigators systematically examine a generic set of dimensions in a mapping study toward the formulation of research questions before identifying, reading, and analyzing sufficient articles of the topic. We have validated the feasibility of our proposal by conducting a case study of a mapping study on cloud software testing, that is, software testing for and on cloud computing platforms. The case study reveals that the 5W+1H pattern can lead investigators to define a set of systematic, generic, and complementary research questions, enabling them to kick off and expedite the mapping study process in a well-defined manner. We also share our experiences and lessons learned from our case study on the use of the 5W+1H pattern in mapping studies.},
  doi           = {https://doi.org/10.1016/j.jss.2015.01.058},
  keywords      = {5W+1H pattern, Cloud software testing, Systematic mapping study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215000370},
}

@Article{Santiago2012,
  author        = {Ivn Santiago and lvaro Jimnez and Juan Manuel Vara and Valeria De Castro and Vernica A. Bollati and Esperanza Marcos},
  title         = {Model-Driven Engineering as a new landscape for traceability management: A systematic literature review},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {12},
  pages         = {1340 - 1356},
  issn          = {0950-5849},
  note          = {Special Section on Software Reliability and Security},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Model-Driven Engineering provides a new landscape for dealing with traceability in software development.
Objective
Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering.
Method
We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments.
Results
Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals.
Conclusion
The evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information.},
  doi           = {https://doi.org/10.1016/j.infsof.2012.07.008},
  keywords      = {Traceability, Model-Driven Engineering, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001346},
}

@Article{1981,
  title         = {Preface},
  journal       = {IFAC Proceedings Volumes},
  year          = {1981},
  volume        = {14},
  number        = {3},
  pages         = {vi},
  issn          = {1474-6670},
  note          = {IFAC/IFIP Workshop on Real Time Programming, Kyoto, Japan, 31 August-2 September 1981},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1474-6670(17)63427-X},
  url           = {http://www.sciencedirect.com/science/article/pii/S147466701763427X},
}

@InCollection{Alunni2013,
  author        = {A. Alunni and M. Coolen and I. Foucher and L. Bally-Cuif},
  title         = {Chapter 32 - Neurogenesis in Zebrafish},
  booktitle     = {Patterning and Cell Type Specification in the Developing CNS and PNS},
  publisher     = {Academic Press},
  year          = {2013},
  editor        = {John L.R. Rubenstein and Pasko Rakic},
  pages         = {645 - 677},
  address       = {Oxford},
  isbn          = {978-0-12-397265-1},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-397265-1.00069-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123972651000691},
}

@Article{Karapantazis2009,
  author        = {Stylianos Karapantazis and Fotini-Niovi Pavlidou},
  title         = {VoIP: A comprehensive survey on a promising technology},
  journal       = {Computer Networks},
  year          = {2009},
  volume        = {53},
  number        = {12},
  pages         = {2050 - 2090},
  issn          = {1389-1286},
  __markedentry = {[Juliana:]},
  abstract      = {The Internet has burgeoned into a worldwide information superhighway during the past few years, giving rise to a host of new applications and services. Among them, Voice over IP (VoIP) is the most prominent one. Beginning more as a frolic among computer enthusiasts, VoIP has set off a feeding frenzy in both the industrial and scientific communities and has the potential to radically change telephone communications. In this article, we survey all these aspects that have the greatest impact on the quality of voice communications over IP networks. The survey begins with the merits and demerits of VoIP, followed by the Quality of Service (QoS) requirements that voice imposes and a description of test methods for the assessment of speech quality. We then proceed with a delineation of the issues related to the conversion of analog voice to packets, namely we spell out the details of the most well-known voice codecs, while light is also thrown on voice activity detection and voice packetization. Header compression schemes receive intense scrutiny as well. We also provide an overview of the signaling protocols that are tailored to the needs of VoIP, and we continue with the comparison of the call admission schemes that are geared towards the QoS constraints of VoIP. The pivotal issue of security is then discussed, pointing out potential threats as well as approaches for tackling them. Finally, the survey concludes with a discussion on the feasibility of providing VoIP over challenging satellite links.},
  doi           = {https://doi.org/10.1016/j.comnet.2009.03.010},
  keywords      = {VoIP, IP Telephony, Voice quality, Voice codecs, Signaling protocols, Call admission control, Security},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128609001200},
}

@InCollection{Kuhn2015,
  author        = {D. Richard Kuhn and Renee Bryce and Feng Duan and Laleh Sh. Ghandehari and Yu Lei and Raghu N. Kacker},
  title         = {Chapter One - Combinatorial Testing: Theory and Practice},
  publisher     = {Elsevier},
  year          = {2015},
  editor        = {Atif Memon},
  volume        = {99},
  series        = {Advances in Computers},
  pages         = {1 - 66},
  __markedentry = {[Juliana:]},
  abstract      = {Combinatorial testing has rapidly gained favor among software testers in the past decade as improved algorithms have become available and practical success has been demonstrated. This chapter reviews the theory and application of this method, focusing particularly on research since 2010, with a brief background providing the rationale and development of combinatorial methods for software testing. Significant advances have occurred in algorithm performance, and the critical area of constraint representation and processing. In addition to these foundational topics, we take a look at advances in specialized areas including test suite prioritization, sequence testing, fault localization, the relationship between combinatorial testing and structural coverage, and approaches to very large testing problems.},
  doi           = {https://doi.org/10.1016/bs.adcom.2015.05.003},
  issn          = {0065-2458},
  keywords      = {Algorithms, Combinatorial testing, Constraints, Covering array, Fault localization, Interaction testing, Sequence testing, Software faults, Software testing, Test suite prioritization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245815000352},
}

@Article{Mahmood2005,
  author        = {Sajjad Mahmood and Richard Lai and Yong Soo Kim and Ji Hong Kim and Seok Cheon Park and Hae Suk Oh},
  title         = {A survey of component based system quality assurance and assessment},
  journal       = {Information and Software Technology},
  year          = {2005},
  volume        = {47},
  number        = {10},
  pages         = {693 - 707},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Component Based Software Development (CBSD) is focused on assembling existing components to build a software system, with a potential benefit of delivering quality systems by using quality components. It departs from the conventional software development process in that it is integration centric as opposed to development centric. The quality of a component based system using high quality components does not therefore necessarily guarantee a system of high quality, but depends on the quality of its components, and a framework and integration process used. Hence, techniques and methods for quality assurance and assessment of a component based system would be different from those of the traditional software engineering methodology. It is essential to quantify factors that contribute to the overall quality, for instances, the trade off between cost and quality of a component, analytical techniques and formal methods, and quality attribute definitions and measurements. This paper presents a literature survey of component based system quality assurance and assessment; the areas surveyed include formalism, cost estimation, and assessment and measurement techniques for the following quality attributes: performance, reliability, maintainability and testability. The aim of this survey is to help provide a better understanding of CBSD in these aspects in order to facilitate the realisation of its potential benefits of delivering quality systems.},
  doi           = {https://doi.org/10.1016/j.infsof.2005.03.007},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584905000601},
}

@InCollection{Watson2013,
  author        = {David Watson and Andrew Jones},
  title         = {Chapter 7 - IT Infrastructure},
  booktitle     = {Digital Forensics Processing and Procedures},
  publisher     = {Syngress},
  year          = {2013},
  editor        = {David Watson and Andrew Jones},
  pages         = {233 - 312},
  address       = {Boston},
  isbn          = {978-1-59749-742-8},
  __markedentry = {[Juliana:]},
  abstract      = {This chapter looks at the policies and issues related to the IT infrastructure within the laboratory. It looks at the hardware, the software, and the infrastructure in some detail. It then looks at process management, addressing issues including incident and problem management; change control; Release Management; and configuration, capacity, and service management.},
  doi           = {https://doi.org/10.1016/B978-1-59749-742-8.00007-8},
  keywords      = {process, incident, change, release, hardware, software, capacity, service, management},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781597497428000078},
}

@Article{Chen2011,
  author        = {Lianping Chen and Muhammad Ali Babar},
  title         = {A systematic review of evaluation of variability management approaches in software product lines},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {4},
  pages         = {344 - 362},
  issn          = {0950-5849},
  note          = {Special section: Software Engineering track of the 24th Annual Symposium on Applied Computing},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated.
Objective
The objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches.
Method
We carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007.
Results
We selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects.
Conclusion
The findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
  doi           = {https://doi.org/10.1016/j.infsof.2010.12.006},
  keywords      = {Software product line, Variability management, Systematic literature reviews, Empirical studies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584910002223},
}

@Article{Carette2011,
  author        = {Jacques Carette and Oleg Kiselyov},
  title         = {Multi-stage programming with functors and monads: Eliminating abstraction overhead from generic code},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {5},
  pages         = {349 - 375},
  issn          = {0167-6423},
  note          = {Special Issue on Generative Programming and Component Engineering (Selected Papers from GPCE 2004/2005)},
  __markedentry = {[Juliana:]},
  abstract      = {We use multi-stage programming, monads and Ocamls advanced module system to demonstrate how to eliminate all abstraction overhead from generic programs, while avoiding any inspection of the resulting code. We demonstrate this clearly with Gaussian Elimination as a representative family of symbolic and numeric algorithms. We parameterize our code to a great extentover domain, input and permutation matrix representations, determinant and rank tracking, pivoting policies, result types, etc.at no run-time cost. Because the resulting code is generated just right and not changed afterward, MetaOCaml guarantees that the generated code is well-typed. We further demonstrate that various abstraction parameters (aspects) can be made orthogonal and compositional, even in the presence of name-generation for temporaries, and interleaving of aspects. We also show how to encode some domain-specific knowledge so that clearly wrong compositions can be rejected at or before generation time, rather than during the compilation or running of the generated code.},
  doi           = {https://doi.org/10.1016/j.scico.2008.09.008},
  keywords      = {MetaOCaml, Linear algebra, Genericity, Generative, Staging, Functor, Symbolic},
  url           = {http://www.sciencedirect.com/science/article/pii/S016764230800110X},
}

@Article{Martinez2017,
  author        = {Salvador Martnez and Valerio Cosentino and Jordi Cabot},
  title         = {Model-based analysis of Java EE web security misconfigurations},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2017},
  volume        = {49},
  pages         = {36 - 61},
  issn          = {1477-8424},
  __markedentry = {[Juliana:]},
  abstract      = {The Java EE framework, a popular technology of choice for the development of web applications, provides developers with the means to define access-control policies to protect application resources from unauthorized disclosures and manipulations. Unfortunately, the definition and manipulation of such security policies remains a complex and error prone task, requiring expert-level knowledge on the syntax and semantics of the Java EE access-control mechanisms. Thus, misconfigurations that may lead to unintentional security and/or availability problems can be easily introduced. In response to this problem, we present a (model-based) reverse engineering approach that automatically evaluates a set of security properties on reverse engineered Java EE security configurations, helping to detect the presence of anomalies. We evaluate the efficacy and pertinence of our approach by applying our prototype tool on a sample of real Java EE applications extracted from GitHub.},
  doi           = {https://doi.org/10.1016/j.cl.2017.02.001},
  keywords      = {Model-driven engineering, Security, Reverse-engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842416301348},
}

@Article{Lopez-Herrejon2015,
  author        = {Roberto E. Lopez-Herrejon and Lukas Linsbauer and Alexander Egyed},
  title         = {A systematic mapping study of search-based software engineering for software product lines},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {61},
  pages         = {33 - 51},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Search-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces.
Objective
The main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published.
Method
A systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014.
Results
The most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation.
Conclusions
Our study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
  doi           = {https://doi.org/10.1016/j.infsof.2015.01.008},
  keywords      = {Software product line, Systematic mapping study, Search based software engineering, Evolutionary algorithm, Metaheuristics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915000166},
}

@Article{Hoey2009,
  author        = {Timothy Hoey and Wan-Ching Yen and Fumiko Axelrod and Jesspreet Basi and Lucas Donigian and Scott Dylla and Maureen Fitch-Bruhns and Sasha Lazetic and In-Kyung Park and Aaron Sato and Sanjeev Satyal and Xinhao Wang and Michael F. Clarke and John Lewicki and Austin Gurney},
  title         = {DLL4 Blockade Inhibits Tumor Growth and Reduces Tumor-Initiating Cell Frequency},
  journal       = {Cell Stem Cell},
  year          = {2009},
  volume        = {5},
  number        = {2},
  pages         = {168 - 177},
  issn          = {1934-5909},
  __markedentry = {[Juliana:]},
  abstract      = {Summary
Previous studies have shown that blocking DLL4 signaling reduced tumor growth by disrupting productive angiogenesis. We developed selective anti-human and anti-mouse DLL4 antibodies to dissect the mechanisms involved by analyzing the contributions of selectively targeting DLL4 in the tumor or in the host vasculature and stroma in xenograft models derived from primary human tumors. We found that each antibody inhibited tumor growth and that the combination of the two antibodies was more effective than either alone. Treatment with anti-human DLL4 inhibited the expression of Notch target genes and reduced proliferation of tumor cells. Furthermore, we found that specifically inhibiting human DLL4 in the tumor, either alone or in combination with the chemotherapeutic agent irinotecan, reduced cancer stem cell frequency, as shown by flow cytometric and invivo tumorigenicity studies.},
  doi           = {https://doi.org/10.1016/j.stem.2009.05.019},
  keywords      = {STEMCELL, CELLCYCLE},
  url           = {http://www.sciencedirect.com/science/article/pii/S1934590909002288},
}

@Article{Badros2000,
  author        = {Greg J Badros},
  title         = {JavaML: a markup language for Java source code},
  journal       = {Computer Networks},
  year          = {2000},
  volume        = {33},
  number        = {1},
  pages         = {159 - 177},
  issn          = {1389-1286},
  __markedentry = {[Juliana:]},
  abstract      = {The classical plain-text representation of source code is convenient for programmers but requires parsing to uncover the deep structure of the program. While sophisticated software tools parse source code to gain access to the program's structure, many lightweight programming aids such as grep rely instead on only the lexical structure of source code. I describe a new XML application that provides an alternative representation of Java source code. This XML-based representation, called JavaML, is more natural for tools and permits easy specification of numerous software-engineering analyses by leveraging the abundance of XML tools and techniques. A robust converter built with the Jikes Java compiler framework translates from the classical Java source code representation to JavaML, and an XSLT stylesheet converts from JavaML back into the classical textual form.},
  doi           = {https://doi.org/10.1016/S1389-1286(00)00037-2},
  keywords      = {Java, XML, Abstract syntax tree representation, Software-engineering analysis, Jikes compiler},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128600000372},
}

@Article{Thuem2014,
  author        = {Thomas Thm and Christian Kstner and Fabian Benduhn and Jens Meinicke and Gunter Saake and Thomas Leich},
  title         = {FeatureIDE: An extensible framework for feature-oriented software development},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {79},
  pages         = {70 - 85},
  issn          = {0167-6423},
  note          = {Experimental Software and Toolkits (EST 4): A special issue of the Workshop on Academic Software Development Tools and Techniques (WASDeTT-3 2010)},
  __markedentry = {[Juliana:]},
  abstract      = {FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.002},
  keywords      = {Feature-oriented software development, Software product lines, Feature modeling, Feature-oriented programming, Aspect-oriented programming, Delta-oriented programming, Preprocessors, Tool support},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001128},
}

@Article{Ebeid2018,
  author        = {Emad Ebeid and Martin Skriver and Kristian Husum Terkildsen and Kjeld Jensen and Ulrik Pagh Schultz},
  title         = {A survey of Open-Source UAV flight controllers and flight simulators},
  journal       = {Microprocessors and Microsystems},
  year          = {2018},
  volume        = {61},
  pages         = {11 - 20},
  issn          = {0141-9331},
  __markedentry = {[Juliana:]},
  abstract      = {The current disruptive innovation in civilian drone (UAV) applications has led to an increased need for research and development in UAV technology. The key challenges currently being addressed are related to UAV platform properties such as functionality, reliability, fault tolerance, and endurance, which are all tightly linked to the UAV flight controller hardware and software. The lack of standardization of flight controller architectures and the use of proprietary closed-source flight controllers on many UAV platforms, however, complicates this work: solutions developed for one flight controller may be difficult to port to another without substantial extra development and testing. Using open-source flight controllers mitigates some of these challenges and enables other researchers to validate and build upon existing research. This paper presents a survey of the publicly available open-source drone platform elements that can be used for research and development. The survey covers open-source hardware, software, and simulation drone platforms and compares their main features.},
  doi           = {https://doi.org/10.1016/j.micpro.2018.05.002},
  keywords      = {Unmanned Aerial Vehicle (UAV), Drones, Flight controllers, Drone simulators, Open platforms, Survey},
  url           = {http://www.sciencedirect.com/science/article/pii/S0141933118300930},
}

@InCollection{Morrow2003,
  author        = {Monique Morrow and Kateel Vijayananda},
  title         = {Chapter 8 - Case Studies},
  booktitle     = {Developing IP-Based Services},
  publisher     = {Morgan Kaufmann},
  year          = {2003},
  editor        = {Monique Morrow and Kateel Vijayananda},
  series        = {The Morgan Kaufmann Series in Networking},
  pages         = {221 - 277},
  address       = {San Francisco},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
This chapter outlines two conceptual case studies as well as a real world case study, which together demonstrate how Internet Protocol (IP)-based services can be developed and deployed by service providers to generate revenue. The case studies emphasize the role of operations support system (OSS), architecture in implementing IP-based services. The two conceptual case studies are composites of real experiences of various service providers. The combination of these two conceptual studies with a real-world example demonstrates the full impact of IP in both business and technical aspects. The conceptual case studies present scenarios for both Greenfield and incumbent service providers. The conceptual case studies and the Meta Telecom experience have drawn a picture of the business and technical aspects associated with IP-based service creation. There are common success factors in all three examples that include upper-management support and vision to use IP technology to transform a company, the redefinition of businessengineering processes for company-wide IP-based service development and deployment, an emphasis on OSS architecture as the base for service creation, and, finally, the positive interaction between customers, service providers, and vendors.},
  doi           = {https://doi.org/10.1016/B978-155860779-8/50010-5},
  issn          = {18759351},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781558607798500105},
}

@Article{Barrea2017,
  author        = {Luigi Barrea and Carolina Di Somma and Paolo Emidio Macchia and Andrea Falco and Maria Cristina Savanelli and Francesco Orio and Annamaria Colao and Silvia Savastano},
  title         = {Influence of nutrition on somatotropic axis: Milk consumption inadult individuals with moderate-severe obesity},
  journal       = {Clinical Nutrition},
  year          = {2017},
  volume        = {36},
  number        = {1},
  pages         = {293 - 301},
  issn          = {0261-5614},
  __markedentry = {[Juliana:]},
  abstract      = {Summary
Background & aims
Nutrition is the major environmental factor that influences the risk of developing pathologies, such as obesity. Although a number of recent reviews pinpoint a protective effects of milk on body weight and obesity related co-morbidities, an inaccurate estimate of milk might contribute to hamper its beneficial effects on health outcomes. Seven-day food records provide prospective food intake data, reducing recall bias and providing extra details about specific food items. Milk intake stimulates the somatotropic axis at multiple levels by increasing both growth hormone (GH) and insulin-like growth factor-1 (IGF-1) secretion. On the other hand, obesity is associated with reduced spontaneous and stimulated GH secretion and basal IGF-1 levels. Aim of this study was to evaluate the milk consumption by using the 7-days food record in obese individuals and to investigate the association between milk intake and GH secretory status in these subjects.
Methods
Cross-sectional observational study carried out on 281 adult individuals (200 women and 81 men, aged 1874 years) with moderate-severe obesity (BMI 35.269.4kg/m2). Baseline milk intake data were collected using a 7 day food record. Anthropometric measurements and biochemical profile were determined. The GH/IGF-1 axis was evaluated by peak GH response after GHRH+ARGININE and IGF-1 standard deviation score (SDS).
Results
The majority of individuals (72.2%) reported consuming milk; 250mL low-fat milk was the most frequently serving of milk consumed, while no subjects reported to consume whole milk. Milk consumers vs no milk consumers presented the better anthropometric measurements and metabolic profile. At the bivariate proportional odds ratio model, after adjusting for BMI, age and gender, milk consumption was associated the better GH status (OR=0.60; p<0.001). Among milk consumers, subjects consuming 250mL reduced-fat milk vs 250mL low-fat milk presented the better anthropometric measurements and metabolic profile. At the bivariate proportional odds ratio model, after adjusting for BMI, age and gender, the consume of 250mL reduced-fat milk was associated better GH status (OR=0.54; p=0.003).
Conclusions
A novel positive association between milk consumption, GH status, and metabolic profile in obese individuals was evidenced. Regardless of the pathogenetic mechanisms, this novel association might be relevant in a context where commonly obese individuals skip breakfast, and suggests the need of a growing cooperation between Nutritionists and Endocrinologists in the management of the obese patients.},
  doi           = {https://doi.org/10.1016/j.clnu.2015.12.007},
  keywords      = {Environmental factors, Milk consumption, Nutrition, Somatotropic axis, Obesity},
  url           = {http://www.sciencedirect.com/science/article/pii/S0261561415003453},
}

@Article{Hu2016,
  author        = {Fei Hu and Yu Lu and Athanasios V. Vasilakos and Qi Hao and Rui Ma and Yogendra Patil and Ting Zhang and Jiang Lu and Xin Li and Neal N. Xiong},
  title         = {Robust CyberPhysical Systems: Concept, models, and implementation},
  journal       = {Future Generation Computer Systems},
  year          = {2016},
  volume        = {56},
  pages         = {449 - 475},
  issn          = {0167-739X},
  __markedentry = {[Juliana:]},
  abstract      = {In this paper we comprehensively survey the concept and strategies for building a resilient and integrated cyberphysical system (CPS). Here resilience refers to a 3S-oriented design, that is, stability, security, and systematicness: Stability means the CPS can achieve a stable sensing-actuation close-loop control even though the inputs (sensing data) have noise or attacks; Security means that the system can overcome the cyberphysical interaction attacks; and Systematicness means that the system has a seamless integration of sensors and actuators. We will also explain the CPS modeling issues since they serve as the basics of 3S design. We will use two detailed examples from our achieved projects to explain how to achieve arobust, systematic CPS design: Case study 1 is on the design of a rehabilitation system with cyber (sensors) and physical (robots) integration. Case Study 2 is on the implantable medical device design. It illustrates the nature of CPS security principle. The dominant feature of this survey is that it has both principle discussions and practical cyberphysical coupling design.},
  doi           = {https://doi.org/10.1016/j.future.2015.06.006},
  keywords      = {CyberPhysical Systems, Stability, Security, Sensors and actuators, Survey},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X15002071},
}

@InCollection{Ahmad2002,
  title         = {Index},
  booktitle     = {Hack Proofing Your Network (Second Edition)},
  publisher     = {Syngress},
  year          = {2002},
  editor        = {David R. Mirza Ahmad and Ido Dubrawsky and Hal Flynn and Joseph Kingpin Grand and Robert Graham and Norris L. Johnson and Dan Effugas Kaminsky and F. William Lynch and Steve W. Manzuik and Ryan Permeh and Ken Pfeil and Rain Forest Puppy},
  pages         = {767 - 789},
  address       = {Burlington},
  edition       = {Second Edition},
  isbn          = {978-1-928994-70-1},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-192899470-1/50022-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781928994701500227},
}

@InCollection{Betz2011,
  title         = {Appendix A - Extended Definitions for the IT Architectural Catalogs},
  booktitle     = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
  publisher     = {Morgan Kaufmann},
  year          = {2011},
  editor        = {Charles T. Betz},
  pages         = {315 - 408},
  address       = {Boston},
  edition       = {Second Edition},
  isbn          = {978-0-12-385017-1},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-12-385017-1.00005-5},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123850171000055},
}

@InCollection{Jackler2005,
  title         = {Index},
  booktitle     = {Neurotology (Second Edition)},
  publisher     = {Mosby},
  year          = {2005},
  editor        = {Robert K. Jackler and Derald E. Brackmann},
  pages         = {1363 - 1411},
  address       = {Philadelphia},
  edition       = {Second Edition},
  isbn          = {978-0-323-01830-2},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-323-01830-2.50092-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780323018302500924},
}

@InCollection{Bidgoli2004,
  title         = {Subject Index},
  booktitle     = {Encyclopedia of Information Systems},
  publisher     = {Elsevier},
  year          = {2004},
  editor        = {Hossein Bidgoli},
  pages         = {703 - 807},
  address       = {New York},
  isbn          = {978-0-12-227240-0},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B0-12-227240-4/00202-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B0122272404002021},
}

@InCollection{Noergaard2013,
  booktitle     = {Embedded Systems Architecture (Second Edition)},
  publisher     = {Newnes},
  year          = {2013},
  editor        = {Tammy Noergaard},
  pages         = {623 - 638},
  edition       = {Second Edition},
  isbn          = {978-0-12-382196-6},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-12-382196-6.00026-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123821966000261},
}

@InCollection{Kitchin2009,
  title         = {Subject Index},
  booktitle     = {International Encyclopedia of Human Geography},
  publisher     = {Elsevier},
  year          = {2009},
  editor        = {Rob Kitchin and Nigel Thrift},
  pages         = {289 - 586},
  address       = {Oxford},
  isbn          = {978-0-08-044910-4},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-008044910-4.09007-6},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780080449104090076},
}

@InCollection{Betz2011a,
  author        = {Charles T. Betz},
  title         = {Chapter 2 - Architecture Approach},
  booktitle     = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
  publisher     = {Morgan Kaufmann},
  year          = {2011},
  editor        = {Charles T. Betz},
  pages         = {33 - 149},
  address       = {Boston},
  edition       = {Second Edition},
  isbn          = {978-0-12-385017-1},
  __markedentry = {[Juliana:6]},
  abstract      = {Publisher Summary
This chapter establishes a series of related principles that both support the architectural analysis and serve as high-level scoping and requirements for the value system. The chapter attempts to build a new kind of process framework for large scale IT management in the enterprise. The concept of an overall IT value chain and the four major, long-lived value streams of large-scale enterprise information technology (application services, infrastructure services, IT assets, and IT technology products) are discussed in the chapter. The shorter IT processes are also elaborated. The process architecture is represented as a set of larger-grained lifecycles that are acted upon by a series of well- understood IT processes with clear beginnings and endings. Value streams and processes are the key means by which value is delivered in IT, as in any business. A relentless attention to reducing unnecessary activity along these activity sequences is essential to increasing value. Synchronization points, dependencies, and critical paths would all be of interest, and constitute the foundation of IT value stream analysis. Much in IT is related to nonprocess concepts: quality concepts such as capacity and availability, or the actual functional areas that own the various lifecycles and processes. An overall systems architecture is presented for the business of IT, discussing fundamentals of enterprise application architecture and the major classes of systems encountered in IT management. Summary matrices are introduced in this discussion as a means of showing how the various architectural elements may interact.},
  doi           = {https://doi.org/10.1016/B978-0-12-385017-1.00002-X},
  url           = {http://www.sciencedirect.com/science/article/pii/B978012385017100002X},
}

@InCollection{Sharpe1984,
  title         = {SECTION 13 - Directory of UK Companies, Societies, Institutes, and Organizations},
  booktitle     = {Quality Technology Handbook (Fourth Edition)},
  publisher     = {Butterworth-Heinemann},
  year          = {1984},
  editor        = {R S Sharpe and J West and D S Dean and D A Tyler and H A Cole},
  pages         = {233 - 379},
  edition       = {Fourth Edition},
  isbn          = {978-0-408-01331-4},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-408-01331-4.50015-5},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780408013314500155},
}

@InCollection{Fisher2005,
  author        = {Joseph A. Fisher and Paolo Faraboschi and Cliff Young},
  title         = {Chapter 10 - Application Design and Customization},
  booktitle     = {Embedded Computing},
  publisher     = {Morgan Kaufmann},
  year          = {2005},
  editor        = {Joseph A. Fisher and Paolo Faraboschi and Cliff Young},
  pages         = {443 - 492},
  address       = {San Francisco},
  isbn          = {978-1-55860-766-8},
  __markedentry = {[Juliana:6]},
  abstract      = {Publisher Summary
One of the most important differentiators between embedded and general-purpose computing is that embedded systems typically run one single application or set of applications exclusively. This affects many aspects of computing, including code development. Although in most respects, embedded application development resembles general code development, special languages can play a role when the application under development will be embedded in a product because of different lifetime, user community, and underlying hardware considerations. Similarly, the process of making code perform can well change. Techniques that might be unacceptable in the general-purpose world can be almost essential in the embedded world, in which a product may be of no use if it cannot meet a speed or power requirement. Embedded computing is unique in that the hardware too is built to run a single application, and thus methodologies for customization become relevant. This topic is part of the larger area of hardware/software codesign. Embedded designers face issues at the hardware/software boundary that are closed to designers of general-purpose systems. Customized hardware presents an opportunity to vastly speed up individual applications. This opportunity has led to a lively research area and the foundation of more than a few startup companies.},
  doi           = {https://doi.org/10.1016/B978-155860766-8/50014-X},
  url           = {http://www.sciencedirect.com/science/article/pii/B978155860766850014X},
}

@Article{2005,
  title         = {Posters displayed on Monday 9 May 2005},
  journal       = {Clinica Chimica Acta},
  year          = {2005},
  volume        = {355},
  pages         = {S81 - S201},
  issn          = {0009-8981},
  note          = {Focus on the Patient: 16th IFCC - FESCC European Congress of Clinical Biochemistry and Laboratory Medicine},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/j.cccn.2005.03.006},
  url           = {http://www.sciencedirect.com/science/article/pii/S0009898105001233},
}

@InCollection{Conrad2012,
  booktitle     = {CISSP Study Guide (Second Edition)},
  publisher     = {Syngress},
  year          = {2012},
  editor        = {Eric Conrad and Seth Misenar and Joshua Feldman},
  pages         = {513 - 547},
  address       = {Boston},
  edition       = {Second Edition},
  isbn          = {978-1-59749-961-3},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-1-59749-961-3.09985-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781597499613099854},
}

@InCollection{Meyers2003,
  title         = {Subject Index},
  booktitle     = {Encyclopedia of Physical Science and Technology (Third Edition)},
  publisher     = {Academic Press},
  year          = {2003},
  editor        = {Robert A. Meyers},
  pages         = {1 - 344},
  address       = {New York},
  edition       = {Third Edition},
  isbn          = {978-0-12-227410-7},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B0-12-227410-5/09009-8},
  url           = {http://www.sciencedirect.com/science/article/pii/B0122274105090098},
}

@Article{2002,
  title         = {Subject index},
  journal       = {Journal of the American College of Cardiology},
  year          = {2002},
  volume        = {39},
  pages         = {501 - 575},
  issn          = {0735-1097},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0735-1097(02)82076-9},
  url           = {http://www.sciencedirect.com/science/article/pii/S0735109702820769},
}

@InCollection{Conrad2010,
  booktitle     = {CISSP Study Guide},
  publisher     = {Syngress},
  year          = {2010},
  editor        = {Eric Conrad and Seth Misenar and Joshua Feldman},
  pages         = {489 - 523},
  address       = {Boston},
  isbn          = {978-1-59749-563-9},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-1-59749-563-9.00020-2},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781597495639000202},
}

@Article{1992,
  title         = {Genomes and evolution},
  journal       = {Current Opinion in Genetics \& Development},
  year          = {1992},
  volume        = {2},
  number        = {6},
  pages         = {947 - 986},
  issn          = {0959-437X},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0959-437X(05)80122-4},
  url           = {http://www.sciencedirect.com/science/article/pii/S0959437X05801224},
}

@Article{2007,
  title         = {Society for Surgery of the Alimentary Tract (SSAT) Abstracts},
  journal       = {Gastroenterology},
  year          = {2007},
  volume        = {132},
  number        = {4, Supplement 2},
  pages         = {A-832 - A-894},
  issn          = {0016-5085},
  note          = {Annual Abstract Supplement},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0016-5085(07)60011-0},
  url           = {http://www.sciencedirect.com/science/article/pii/S0016508507600110},
}

@InCollection{Zelkowitz1999a,
  title         = {Subject Index},
  booktitle     = {Index Part I Subject Index Volumes 1-49},
  publisher     = {Elsevier},
  year          = {1999},
  editor        = {Marvin V. Zelkowitz},
  volume        = {50},
  series        = {Advances in Computers},
  pages         = {1 - 420},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0065-2458(08)60717-2},
  issn          = {0065-2458},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245808607172},
}

@InCollection{Conrad2016a,
  title         = {Glossary},
  booktitle     = {CISSP Study Guide (Third Edition)},
  publisher     = {Syngress},
  year          = {2016},
  editor        = {Eric Conrad and Seth Misenar and Joshua Feldman},
  pages         = {521 - 557},
  address       = {Boston},
  edition       = {Third Edition},
  isbn          = {978-0-12-802437-9},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-12-802437-9.00011-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128024379000114},
}

@Article{2002a,
  title         = {Joint Meeting of the European Pancreatic Club (EPC) and the International Association of Pancreatology (IAP)},
  journal       = {Pancreatology},
  year          = {2002},
  volume        = {2},
  number        = {3},
  pages         = {217 - 361},
  issn          = {1424-3903},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1159/000058036},
  url           = {http://www.sciencedirect.com/science/article/pii/S1424390302800047},
}

@Article{Fabry2016,
  author        = {Johan Fabry and Coen De Roover and Carlos Noguera and Steffen Zschaler and Awais Rashid and Viviane Jonckers},
  title         = {AspectJ code analysis and verification with GASR},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {528 - 544},
  issn          = {0164-1212},
  __markedentry = {[Juliana:6]},
  abstract      = {Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked.},
  doi           = {https://doi.org/10.1016/j.jss.2016.04.014},
  keywords      = {Aspect oriented programming, Source code analysis, Logic program querying},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300279},
}

@Article{2005a,
  title         = {Posters displayed on Tuesday 10 May 2005},
  journal       = {Clinica Chimica Acta},
  year          = {2005},
  volume        = {355},
  pages         = {S203 - S318},
  issn          = {0009-8981},
  note          = {Focus on the Patient: 16th IFCC - FESCC European Congress of Clinical Biochemistry and Laboratory Medicine},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/j.cccn.2005.03.005},
  url           = {http://www.sciencedirect.com/science/article/pii/S0009898105001221},
}

@Article{2016,
  title         = {Poster Abstracts},
  journal       = {Neuromuscular Disorders},
  year          = {2016},
  volume        = {26},
  pages         = {S4 - S42},
  issn          = {0960-8966},
  note          = {Abstracts of the UK Neuromuscular Translational Research Conference 2016},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0960-8966(16)30108-0},
  url           = {http://www.sciencedirect.com/science/article/pii/S0960896616301080},
}

@Article{2005b,
  title         = {Society for Development Biology 64th Annual Meeting},
  journal       = {Developmental Biology},
  year          = {2005},
  volume        = {283},
  number        = {2},
  pages         = {575 - 707},
  issn          = {0012-1606},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/j.ydbio.2005.04.040},
  url           = {http://www.sciencedirect.com/science/article/pii/S0012160605002940},
}

@InCollection{MISRA1992,
  title         = {4 - Reliability Prediction from Stress-Strength Models},
  booktitle     = {Reliability Analysis and Prediction},
  publisher     = {Elsevier},
  year          = {1992},
  editor        = {Krishna B. MISRA},
  volume        = {15},
  series        = {Fundamental Studies in Engineering},
  pages         = {247 - 316},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-444-89606-3.50011-7},
  issn          = {1572-4433},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780444896063500117},
}

@Article{2013,
  title         = {Abstracts from the XXI World Congress of the International Society for Heart ResearchJune 30-July 4, 20132013, San Diego, California, USA},
  journal       = {Journal of Molecular and Cellular Cardiology},
  year          = {2013},
  volume        = {65},
  pages         = {S1 - S162},
  issn          = {0022-2828},
  note          = {Abstracts from the XXI World Congress of the International Society for Heart Research, June 30-July 4, 2013, 2013, San Diego, California, USA},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/j.yjmcc.2013.10.011},
  url           = {http://www.sciencedirect.com/science/article/pii/S0022282813003106},
}

@InCollection{Halstead1979,
  author        = {M.H. Halstead},
  title         = {Advances in Software Science},
  publisher     = {Elsevier},
  year          = {1979},
  editor        = {Marshall C. Yovits},
  volume        = {18},
  series        = {Advances in Computers},
  pages         = {119 - 172},
  __markedentry = {[Juliana:6]},
  abstract      = {Publisher Summary
The chapter presents the overview of the present status of software science. Software science is an intellectually exciting discipline currently undergoing rapid development. Software science can be treated as a proper basis or foundation for the field of software engineering, but not as synonymous with it. This is not unlike other branches, in that the engineering usually preceded and indeed stimulated the development of the underlying science. It is interesting to note, however, that it is only after the development of thermodynamics for power engineering, electrodynamics for electrical engineering, or statics, dynamics, and strength-of-materials for mechanical engineering that those branches could be considered quasi complete, highly competent, and dependable engineering disciplines. Such a goal for software engineering clearly motivates much of the work in software science. Despite the fact that there are no theorems, and perhaps never can be any, in the field of software science, one basic attribute shared by the equations in this field has become quite noticeable. This is the total and complete lack of arbitrary constants or unknown coefficients among the basic equations. It cannot yet be predicted what impact this discipline may be expected to have on the field of software engineering, or of computer programming in general in the future, but perhaps its greatest impact may result from one conclusion that seems inescapable. This conclusion is that natural laws govern language and the mental activity of using it far more strictly than previously recognized.},
  doi           = {https://doi.org/10.1016/S0065-2458(08)60583-5},
  issn          = {0065-2458},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245808605835},
}

@Comment{jabref-meta: databaseType:bibtex;}
Scopus
EXPORT DATE: 30 October 2018

@CONFERENCE{Felderer2018,
author={Felderer, M. and Jeschko, F.},
title={A process for evidence-based engineering of domain-specific languages},
journal={ACM International Conference Proceeding Series},
year={2018},
volume={Part F137700},
doi={10.1145/3210459.3210479},
art_number={3210479},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053713372&doi=10.1145%2f3210459.3210479&partnerID=40&md5=fd309de170e0c9f0c6d37fca46d29ed0},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2018,
title={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year={2018},
volume={2017-December},
page_count={772},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045873905&partnerID=40&md5=b1a1d60cc1dc48401deff64d90041c81},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Hser2018147,
author={Hser, F. and Felderer, M. and Breu, R.},
title={Evaluation of an integrated tool environment for experimentation in DSL engineering},
journal={Lecture Notes in Business Information Processing},
year={2018},
volume={302},
pages={147-168},
doi={10.1007/978-3-319-71440-0_9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041097449&doi=10.1007%2f978-3-319-71440-0_9&partnerID=40&md5=2b2cd5b82e6ef2bc0ba009922ab15d0d},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2017,
title={Proceedings - ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems, MODELS 2017},
journal={Proceedings - ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems, MODELS 2017},
year={2017},
page_count={360},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040587063&partnerID=40&md5=da93e88a05513c5241f09ab1013fdd30},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Honfi2017119,
author={Honfi, D. and Molnr, G. and Micskei, Z. and Majzik, I.},
title={Model-based regression testing of autonomous robots},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10567 LNCS},
pages={119-135},
doi={10.1007/978-3-319-68015-6_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030667574&doi=10.1007%2f978-3-319-68015-6_8&partnerID=40&md5=c0ede83fdf5f839c64044a13044f7747},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Laue201789,
author={Laue, R. and Storch, A. and Schndelbach, M.},
title={Regression testing for visual models},
journal={CEUR Workshop Proceedings},
year={2017},
volume={1848},
pages={89-95},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020554157&partnerID=40&md5=249749690bff7d9250f95ea548ab19f6},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Poncelet2016143,
author={Poncelet, C. and Jacquemard, F.},
title={Model-based testing for building reliable realtime interactive music systems},
journal={Science of Computer Programming},
year={2016},
volume={132},
pages={143-172},
doi={10.1016/j.scico.2016.08.002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994087111&doi=10.1016%2fj.scico.2016.08.002&partnerID=40&md5=3932c1c8a1b4411d05d32d0fa7eec5e7},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hser201652,
author={Hser, F. and Felderer, M. and Breu, R.},
title={Is business domain language support beneficial for creating test case specifications: A controlled experiment},
journal={Information and Software Technology},
year={2016},
volume={79},
pages={52-62},
doi={10.1016/j.infsof.2016.07.001},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990036753&doi=10.1016%2fj.infsof.2016.07.001&partnerID=40&md5=23042425f7a84d0ada295d7b9e0547d3},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Makki2016178,
author={Makki, M. and Van Landuyt, D. and Joosen, W.},
title={Automated regression testing of BPMN 2.0 processes a capture and replay framework for continuous delivery},
journal={GPCE 2016 - Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, co-located with SPLASH 2016},
year={2016},
pages={178-189},
doi={10.1145/2993236.2993257},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006698880&doi=10.1145%2f2993236.2993257&partnerID=40&md5=3c116cc2fd69d4e51e8545e01e06e3e4},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Al-Sibahi2016207,
author={Al-Sibahi, A.S. and Dimovski, A.S. and Wasowski, A.},
title={Symbolic execution of high-level transformations},
journal={SLE 2016 - Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering, co-located with SPLASH 2016},
year={2016},
pages={207-220},
doi={10.1145/2997364.2997382},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006790383&doi=10.1145%2f2997364.2997382&partnerID=40&md5=b701466c119c446a346df19b6b2204dc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Boussaa2016202,
author={Boussaa, M. and Barais, O. and Baudry, B. and Sune, G.},
title={Automatic non-functional testing of code generators families},
journal={GPCE 2016 - Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, co-located with SPLASH 2016},
year={2016},
pages={202-212},
doi={10.1145/2993236.2993256},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006821065&doi=10.1145%2f2993236.2993256&partnerID=40&md5=23033cd767ef631f2e6581469ad5fe2e},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hser2016,
author={Hser, F. and Felderer, M. and Breu, R.},
title={An integrated tool environment for experimentation in domain specific language engineering},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={01-03-June-2016},
doi={10.1145/2915970.2916010},
art_number={a20},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978496761&doi=10.1145%2f2915970.2916010&partnerID=40&md5=759eb12218bda8990778c15d174a8bb4},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2016,
title={ACM International Conference Proceeding Series},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={Part F128404},
page_count={182},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046053536&partnerID=40&md5=0474c10feb1b62d0561df8aa88aa843f},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{NoAuthor20161,
title={16th International Symposium on Trends in Functional Programming, TFP 2015},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9547},
pages={1-156},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978701156&partnerID=40&md5=630b3015b0a82b152ed973383152d937},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{NoAuthor20161,
title={15th International Conference on Software Reuse, ICSR 2016},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9679},
pages={1-411},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977492303&partnerID=40&md5=347f932969ae9dfb1c178ad3143fa5af},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Kumar2015,
author={Kumar, R. and Kumar, V.},
title={Process optimization for testing of domain specific languages in industrial automation},
journal={2015 World Congress on Information Technology and Computer Applications, WCITCA 2015},
year={2015},
doi={10.1109/WCITCA.2015.7367051},
art_number={7367051},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962473616&doi=10.1109%2fWCITCA.2015.7367051&partnerID=40&md5=47d0b26a48ce3ed250a04d194aa96212},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Poncelet20151759,
author={Poncelet, C. and Jacquemard, F.},
title={Model based testing of an interactive music system},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2015},
volume={13-17-April-2015},
pages={1759-1764},
doi={10.1145/2695664.2695804},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955496249&doi=10.1145%2f2695664.2695804&partnerID=40&md5=a00d9e874cc02eefa7097d0df6d69b47},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{King2014409,
author={King, T.M. and Nunez, G. and Santiago, D. and Cando, A. and Mack, C.},
title={Legend: An agile DSL toolset for web acceptance testing},
journal={2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
year={2014},
pages={409-412},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942796278&partnerID=40&md5=73edc0df6eb3a842166444a5dcb78978},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Martinez201413,
author={Martinez, J. and Thomas, T. and King, T.M.},
title={Echo: A middleware architecture for domain-specific UI test automation},
journal={2014 Workshop on Joining AcadeMiA and Industry Contributions to Test Automation and Model-Based Testing, JAMAICA 2014 - Proceedings},
year={2014},
pages={13-15},
doi={10.1145/2631890.2631893},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942409801&doi=10.1145%2f2631890.2631893&partnerID=40&md5=580120dfa5f66ea5514ea211d228169b},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bache2014320,
author={Bache, E. and Bache, G.},
title={Specification by example with gui tests - how could that work?},
journal={Lecture Notes in Business Information Processing},
year={2014},
volume={179 LNBIP},
pages={320-326},
doi={10.1007/978-3-319-06862-6},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904569844&doi=10.1007%2f978-3-319-06862-6&partnerID=40&md5=55a364d2ee51ac30ae8403299859e8da},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hser2014156,
author={Hser, F. and Felderer, M. and Breu, R.},
title={Test process improvement with documentation driven integration testing},
journal={Proceedings - 2014 9th International Conference on the Quality of Information and Communications Technology, QUATIC 2014},
year={2014},
pages={156-161},
doi={10.1109/QUATIC.2014.29},
art_number={6984109},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921059614&doi=10.1109%2fQUATIC.2014.29&partnerID=40&md5=634d14b73fda52a267350bb24816c632},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Selim2013365,
author={Selim, G.M.K. and Wang, S. and Cordy, J.R. and Dingel, J.},
title={Model transformations for migrating legacy deployment models in the automotive industry},
journal={Software and Systems Modeling},
year={2013},
volume={14},
number={1},
pages={365-381},
doi={10.1007/s10270-013-0365-1},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922338189&doi=10.1007%2fs10270-013-0365-1&partnerID=40&md5=cb7ae64fdf0c3556171f4b91381d82e1},
document_type={Article},
source={Scopus},
}

@ARTICLE{NoAuthor2012,
title={Computer Applications for Software Engineering, Disaster Recovery, and Business Continuity - International Conferences, ASEA and DRBC 2012, Held in Conjunction with GST 2012, Proceedings},
journal={Communications in Computer and Information Science},
year={2012},
volume={340 CCIS},
page_count={475},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869847795&partnerID=40&md5=50ed69d3acd9f3c6e6b80204eec1482c},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Woskowski201262,
author={Woskowski, C.},
title={Applying industrial-strength testing techniques to critical care medical equipment},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7612 LNCS},
pages={62-73},
doi={10.1007/978-3-642-33678-2_6},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867588659&doi=10.1007%2f978-3-642-33678-2_6&partnerID=40&md5=56dc43ec05802ce8928510e60a23002c},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hallenberg201296,
author={Hallenberg, N. and Carlsen, P.L.},
title={Declarative automated test},
journal={2012 7th International Workshop on Automation of Software Test, AST 2012 - Proceedings},
year={2012},
pages={96-102},
doi={10.1109/IWAST.2012.6228998},
art_number={6228998},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864262119&doi=10.1109%2fIWAST.2012.6228998&partnerID=40&md5=670bdf6333dc0c334ffece1b49a7898c},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor2012,
title={Modelling Foundations and Applications - 8th European Conference, ECMFA 2012, Proceedings},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7349 LNCS},
page_count={441},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864064431&partnerID=40&md5=d47a39e66a5c67e0c272e4c3e5157203},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{LunaRobles2011297,
author={Luna Robles, E. and Rossi, G. and Garrigs, I.},
title={WebSpec: A visual language for specifying interaction and navigation requirements in web applications},
journal={Requirements Engineering},
year={2011},
volume={16},
number={4},
pages={297-321},
doi={10.1007/s00766-011-0124-1},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80355133393&doi=10.1007%2fs00766-011-0124-1&partnerID=40&md5=f1f0f0c473c59f609af650d777b886c7},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Prasetya2011,
author={Prasetya, I.S.W.B. and Amorim, J. and Vos, T.E.J. and Baars, A.},
title={Using Haskell to script combinatoric testing of web services},
journal={Proceedings of the 6th Iberian Conference on Information Systems and Technologies, CISTI 2011},
year={2011},
art_number={5974321},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052440645&partnerID=40&md5=5cef5a3d5aadbdfa7acfc8a4db258387},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Strasser20104687,
author={Strasser, T. and Peters, T. and Jgle, H. and Zrenner, E. and Wilke, R.},
title={An integrated domain specific language for post-processing and visualizing electrophysiological signals in Java},
journal={2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC'10},
year={2010},
pages={4687-4690},
doi={10.1109/IEMBS.2010.5626417},
art_number={5626417},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650837400&doi=10.1109%2fIEMBS.2010.5626417&partnerID=40&md5=7b2189cb0d2e385f46d1ec94c4ff7f22},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vanderbauwhede2010141,
author={Vanderbauwhede, W. and Margala, M. and Chalamalasetti, S.R. and Purohit, S.},
title={A C++-embedded domain-specific language for programming the MORA soft processor array},
journal={Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors},
year={2010},
pages={141-148},
doi={10.1109/ASAP.2010.5540750},
art_number={5540750},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955948453&doi=10.1109%2fASAP.2010.5540750&partnerID=40&md5=ac257e54a23c83c1dbcc8c531fe9b659},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2009,
title={Proceedings of EuroPLoP 2009 - 14th Annual European Conference on Pattern Languages of Programming},
journal={Proceedings of EuroPLoP 2009 - 14th Annual European Conference on Pattern Languages of Programming},
year={2009},
page_count={607},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865211969&partnerID=40&md5=3e4e738b74f93ed5406f7903066944c5},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Talby2009154,
author={Talby, D.},
title={The perceived value of authoring and automating acceptance tests using a model driven development toolset},
journal={Proceedings of the 2009 ICSE Workshop on Automation of Software Test, AST 2009},
year={2009},
pages={154-157},
doi={10.1109/IWAST.2009.5069055},
art_number={5069055},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349673934&doi=10.1109%2fIWAST.2009.5069055&partnerID=40&md5=48d78d31c56c55d68b8be3f60af589cb},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sinha2006242,
author={Sinha, A. and Smidts, C.},
title={HOTTest: A model-based test design technique for enhanced testing of domain-specific applications},
journal={ACM Transactions on Software Engineering and Methodology},
year={2006},
volume={15},
number={3},
pages={242-278},
doi={10.1145/1151695.1151697},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748528112&doi=10.1145%2f1151695.1151697&partnerID=40&md5=396688bb148f87f73f3face6aa31322c},
document_type={Article},
source={Scopus},
}

% Encoding: UTF-8

@incollection{kurosu_usability_2017,
	address = {Cham},
	title = {Usability {Evaluation} of {Domain}-{Specific} {Languages}: {A} {Systematic} {Literature} {Review}},
	volume = {10271},
	isbn = {978-3-319-58070-8 978-3-319-58071-5},
	shorttitle = {Usability {Evaluation} of {Domain}-{Specific} {Languages}},
	url = {http://link.springer.com/10.1007/978-3-319-58071-5_39},
	urldate = {2018-10-31},
	booktitle = {Human-{Computer} {Interaction}. {User} {Interface} {Design}, {Development} and {Multimodality}},
	publisher = {Springer International Publishing},
	author = {Poltronieri Rodrigues, Ildevana and de Borba Campos, Mrcia and Zorzo, Avelino F.},
	editor = {Kurosu, Masaaki},
	year = {2017},
	doi = {10.1007/978-3-319-58071-5_39},
	pages = {522--534}
}

@incollection{jose_escalona_involving_2014,
	address = {Cham},
	title = {Involving {End}-{Users} in the {Design} of a {Domain}-{Specific} {Language} for the {Genetic} {Domain}},
	isbn = {978-3-319-07214-2 978-3-319-07215-9},
	url = {http://link.springer.com/10.1007/978-3-319-07215-9_8},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Information {System} {Development}},
	publisher = {Springer International Publishing},
	author = {Villanueva, Maria Jose and Valverde, Francisco and Pastor, Oscar},
	editor = {Jos Escalona, Mara and Aragn, Gustavo and Linger, Henry and Lang, Michael and Barry, Chris and Schneider, Christoph},
	year = {2014},
	doi = {10.1007/978-3-319-07215-9_8},
	pages = {99--110}
}

@incollection{pires_applying_2018,
	address = {Cham},
	title = {Applying {Integrated} {Domain}-{Specific} {Modeling} for {Multi}-concerns {Development} of {Complex} {Systems}},
	volume = {880},
	isbn = {978-3-319-94763-1 978-3-319-94764-8},
	url = {http://link.springer.com/10.1007/978-3-319-94764-8_11},
	urldate = {2018-10-31},
	booktitle = {Model-{Driven} {Engineering} and {Software} {Development}},
	publisher = {Springer International Publishing},
	author = {Prll, Reinhard and Rumpold, Adrian and Bauer, Bernhard},
	editor = {Pires, Lus Ferreira and Hammoudi, Slimane and Selic, Bran},
	year = {2018},
	doi = {10.1007/978-3-319-94764-8_11},
	pages = {247--271}
}

@incollection{desfray_textual_2015,
	address = {Cham},
	title = {A {Textual} {Domain}-{Specific} {Language} {Based} on the {UML} {Testing} {Profile}},
	volume = {580},
	isbn = {978-3-319-27868-1 978-3-319-27869-8},
	url = {http://link.springer.com/10.1007/978-3-319-27869-8_9},
	urldate = {2018-10-31},
	booktitle = {Model-{Driven} {Engineering} and {Software} {Development}},
	publisher = {Springer International Publishing},
	author = {Iber, Johannes and Kajtazovi, Nermin and Macher, Georg and Hller, Andrea and Rauter, Tobias and Kreiner, Christian},
	editor = {Desfray, Philippe and Filipe, Joaquim and Hammoudi, Slimane and Pires, Lus Ferreira},
	year = {2015},
	doi = {10.1007/978-3-319-27869-8_9},
	pages = {155--171}
}

@incollection{lammel_webdsl:_2008,
	address = {Berlin, Heidelberg},
	title = {{WebDSL}: {A} {Case} {Study} in {Domain}-{Specific} {Language} {Engineering}},
	volume = {5235},
	isbn = {978-3-540-88642-6 978-3-540-88643-3},
	shorttitle = {{WebDSL}},
	url = {http://link.springer.com/10.1007/978-3-540-88643-3_7},
	urldate = {2018-10-31},
	booktitle = {Generative and {Transformational} {Techniques} in {Software} {Engineering} {II}},
	publisher = {Springer Berlin Heidelberg},
	author = {Visser, Eelco},
	editor = {Lmmel, Ralf and Visser, Joost and Saraiva, Joo},
	year = {2008},
	doi = {10.1007/978-3-540-88643-3_7},
	pages = {291--373},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\9YG3YP8F\\Visser - 2008 - WebDSL A Case Study in Domain-Specific Language E.pdf:application/pdf}
}

@InCollection{king_dsl_2011,
  author    = {Daz, Oscar and Puente, Gorka},
  title     = {A {DSL} for {Corporate} {Wiki} {Initialization}},
  booktitle = {Active {Flow} and {Combustion} {Control} 2018},
  publisher = {Springer International Publishing},
  year      = {2011},
  editor    = {King, Rudibert},
  volume    = {141},
  pages     = {237--251},
  address   = {Cham},
  isbn      = {978-3-319-98176-5 978-3-319-98177-2},
  doi       = {10.1007/978-3-642-21640-4_19},
  url       = {http://link.springer.com/10.1007/978-3-642-21640-4_19},
  urldate   = {2018-10-31},
}

@incollection{stevens_statistical_2016,
	address = {Berlin, Heidelberg},
	title = {Statistical {Model} {Checking} of e-{Motions} {Domain}-{Specific} {Modeling} {Languages}},
	volume = {9633},
	isbn = {978-3-662-49664-0 978-3-662-49665-7},
	url = {http://link.springer.com/10.1007/978-3-662-49665-7_18},
	urldate = {2018-10-31},
	booktitle = {Fundamental {Approaches} to {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Durn, Francisco and Moreno-Delgado, Antonio and lvarez-Palomo, Jos M.},
	editor = {Stevens, Perdita and Wsowski, Andrzej},
	year = {2016},
	doi = {10.1007/978-3-662-49665-7_18},
	pages = {305--322},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\GYSB3K5K\\Durn et al. - 2016 - Statistical Model Checking of e-Motions Domain-Spe.pdf:application/pdf}
}

@incollection{ober_synthesizing_2011,
	address = {Berlin, Heidelberg},
	title = {Synthesizing {Software} {Models}: {Generating} {Train} {Station} {Models} {Automatically}},
	volume = {7083},
	isbn = {978-3-642-25263-1 978-3-642-25264-8},
	shorttitle = {Synthesizing {Software} {Models}},
	url = {http://link.springer.com/10.1007/978-3-642-25264-8_5},
	urldate = {2018-10-31},
	booktitle = {{SDL} 2011: {Integrating} {System} and {Software} {Modeling}},
	publisher = {Springer Berlin Heidelberg},
	author = {Svendsen, Andreas and Haugen, ystein and Mller-Pedersen, Birger},
	editor = {Ober, Iulian and Ober, Ileana},
	year = {2011},
	doi = {10.1007/978-3-642-25264-8_5},
	pages = {38--53}
}

@article{mohagheghi_where_2013,
	title = {Where does model-driven engineering help? {Experiences} from three industrial cases},
	volume = {12},
	issn = {1619-1366, 1619-1374},
	shorttitle = {Where does model-driven engineering help?},
	url = {http://link.springer.com/10.1007/s10270-011-0219-7},
	doi = {10.1007/s10270-011-0219-7},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Mohagheghi, Parastoo and Gilani, Wasif and Stefanescu, Alin and Fernandez, Miguel A. and Nordmoen, Bjrn and Fritzsche, Mathias},
	month = jul,
	year = {2013},
	pages = {619--639}
}

@incollection{csondes_model-based_2017,
	address = {Cham},
	title = {Model-{Based} {Regression} {Testing} of {Autonomous} {Robots}},
	volume = {10567},
	isbn = {978-3-319-68014-9 978-3-319-68015-6},
	url = {http://link.springer.com/10.1007/978-3-319-68015-6_8},
	urldate = {2018-10-31},
	booktitle = {{SDL} 2017: {Model}-{Driven} {Engineering} for {Future} {Internet}},
	publisher = {Springer International Publishing},
	author = {Honfi, Dvid and Molnr, Gbor and Micskei, Zoltn and Majzik, Istvn},
	editor = {Csndes, Tibor and Kovcs, Gbor and Rthy, Gyrgy},
	year = {2017},
	doi = {10.1007/978-3-319-68015-6_8},
	pages = {119--135}
}

@incollection{hutchison_applying_2012,
	address = {Berlin, Heidelberg},
	title = {Applying {Industrial}-{Strength} {Testing} {Techniques} to {Critical} {Care} {Medical} {Equipment}},
	volume = {7612},
	isbn = {978-3-642-33677-5 978-3-642-33678-2},
	url = {http://link.springer.com/10.1007/978-3-642-33678-2_6},
	urldate = {2018-10-31},
	booktitle = {Computer {Safety}, {Reliability}, and {Security}},
	publisher = {Springer Berlin Heidelberg},
	author = {Woskowski, Christoph},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ortmeier, Frank and Daniel, Peter},
	year = {2012},
	doi = {10.1007/978-3-642-33678-2_6},
	pages = {62--73}
}

@incollection{gruner_towards_2016,
	address = {Cham},
	title = {Towards a {Generic} {DSL} for {Automated} {Marking} {Systems}},
	volume = {642},
	isbn = {978-3-319-47679-7 978-3-319-47680-3},
	url = {http://link.springer.com/10.1007/978-3-319-47680-3_6},
	urldate = {2018-10-31},
	booktitle = {{ICT} {Education}},
	publisher = {Springer International Publishing},
	author = {Solms, Fritz and Pieterse, Vreda},
	editor = {Gruner, Stefan},
	year = {2016},
	doi = {10.1007/978-3-319-47680-3_6},
	pages = {59--66},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\JXA3P7ZZ\\Solms e Pieterse - 2016 - Towards a Generic DSL for Automated Marking System.pdf:application/pdf}
}

@incollection{hutchison_model-driven_2013,
	address = {Berlin, Heidelberg},
	title = {A {Model}-{Driven} {Approach} to {Specifying} and {Monitoring} {Controlled} {Experiments} in {Software} {Engineering}},
	volume = {7983},
	isbn = {978-3-642-39258-0 978-3-642-39259-7},
	url = {http://link.springer.com/10.1007/978-3-642-39259-7_8},
	urldate = {2018-10-31},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Freire, Marlia and Accioly, Paola and Sizlio, Gustavo and Campos Neto, Edmilson and Kulesza, Uir and Aranha, Eduardo and Borba, Paulo},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Heidrich, Jens and Oivo, Markku and Jedlitschka, Andreas and Baldassarre, Maria Teresa},
	year = {2013},
	doi = {10.1007/978-3-642-39259-7_8},
	pages = {65--79}
}

@incollection{czarnecki_future_2008,
	address = {Berlin, Heidelberg},
	title = {The {Future} of {Train} {Signaling}},
	volume = {5301},
	isbn = {978-3-540-87874-2 978-3-540-87875-9},
	url = {http://link.springer.com/10.1007/978-3-540-87875-9_9},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Svendsen, Andreas and Olsen, Gran K. and Endresen, Jan and Moen, Thomas and Carlson, Erik and Alme, Kjell-Joar and Haugen, ystein},
	editor = {Czarnecki, Krzysztof and Ober, Ileana and Bruel, Jean-Michel and Uhl, Axel and Vlter, Markus},
	year = {2008},
	doi = {10.1007/978-3-540-87875-9_9},
	pages = {128--142}
}

@incollection{guelfi_prototyping_2006,
	address = {Berlin, Heidelberg},
	title = {Prototyping {Domain} {Specific} {Languages} with {COOPN}},
	volume = {3943},
	isbn = {978-3-540-34063-8 978-3-540-34064-5},
	url = {http://link.springer.com/10.1007/11751113_13},
	urldate = {2018-10-31},
	booktitle = {Rapid {Integration} of {Software} {Engineering} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Pedro, Luis and Lucio, Levi and Buchs, Didier},
	editor = {Guelfi, Nicolas and Savidis, Anthony},
	year = {2006},
	doi = {10.1007/11751113_13},
	pages = {174--189}
}

@incollection{bayro-corrochano_extending_2012,
	address = {Cham},
	title = {Extending the {REA}-{DSL} by the {Planning} {Layer} of the {REA} {Ontology}},
	volume = {8827},
	isbn = {978-3-319-12567-1 978-3-319-12568-8},
	url = {http://link.springer.com/10.1007/978-3-642-31069-0_45},
	urldate = {2018-10-31},
	booktitle = {Progress in {Pattern} {Recognition}, {Image} {Analysis}, {Computer} {Vision}, and {Applications}},
	publisher = {Springer International Publishing},
	author = {Mayrhofer, Dieter and Huemer, Christian},
	editor = {Bayro-Corrochano, Eduardo and Hancock, Edwin},
	year = {2012},
	doi = {10.1007/978-3-642-31069-0_45},
	pages = {543--554}
}

@article{blouin_kompren:_2015,
	title = {Kompren: modeling and generating model slicers},
	volume = {14},
	issn = {1619-1366, 1619-1374},
	shorttitle = {Kompren},
	url = {http://link.springer.com/10.1007/s10270-012-0300-x},
	doi = {10.1007/s10270-012-0300-x},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Blouin, Arnaud and Combemale, Benot and Baudry, Benoit and Beaudoux, Olivier},
	month = feb,
	year = {2015},
	pages = {321--337},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\XZGXHMKK\\Blouin et al. - 2015 - Kompren modeling and generating model slicers.pdf:application/pdf}
}

@article{selim_model_2015,
	title = {Model transformations for migrating legacy deployment models in the automotive industry},
	volume = {14},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-013-0365-1},
	doi = {10.1007/s10270-013-0365-1},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Selim, Gehan M. K. and Wang, Shige and Cordy, James R. and Dingel, Juergen},
	month = feb,
	year = {2015},
	pages = {365--381}
}

@article{voelter_using_2018,
	title = {Using language workbenches and domain-specific languages for safety-critical software development},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-018-0679-0},
	doi = {10.1007/s10270-018-0679-0},
	language = {en},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Voelter, Markus and Kolb, Bernd and Birken, Klaus and Tomassetti, Federico and Alff, Patrick and Wiart, Laurent and Wortmann, Andreas and Nordmann, Arne},
	month = may,
	year = {2018}
}

@article{sinha_experimental_2006,
	title = {An experimental evaluation of a higher-ordered-typed-functional specification-based test-generation technique},
	volume = {11},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-006-6401-9},
	doi = {10.1007/s10664-006-6401-9},
	language = {en},
	number = {2},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Sinha, Avik and Smidts, Carol},
	month = jun,
	year = {2006},
	pages = {173--202}
}

@article{makedonski_test_2018,
	title = {Test descriptions with {ETSI} {TDL}},
	issn = {0963-9314, 1573-1367},
	url = {http://link.springer.com/10.1007/s11219-018-9423-9},
	doi = {10.1007/s11219-018-9423-9},
	language = {en},
	urldate = {2018-10-31},
	journal = {Software Quality Journal},
	author = {Makedonski, Philip and Adamis, Gusztv and Krik, Martti and Kristoffersen, Finn and Carignani, Michele and Ulrich, Andreas and Grabowski, Jens},
	month = oct,
	year = {2018}
}

@incollection{zsok_dsl_2015,
	address = {Cham},
	title = {{DSL} in {C}++ {Template} {Metaprogram}},
	volume = {8606},
	isbn = {978-3-319-15939-3 978-3-319-15940-9},
	url = {http://link.springer.com/10.1007/978-3-319-15940-9_3},
	urldate = {2018-10-31},
	booktitle = {Central {European} {Functional} {Programming} {School}},
	publisher = {Springer International Publishing},
	author = {Porkolb, Zoltn and Sinkovics, bel and Siroki, Istvn},
	editor = {Zsk, Viktria and Horvth, Zoltn and Csat, Lehel},
	year = {2015},
	doi = {10.1007/978-3-319-15940-9_3},
	pages = {76--114}
}

@article{holmes_tstl:_2018,
	title = {{TSTL}: the template scripting testing language},
	volume = {20},
	issn = {1433-2779, 1433-2787},
	shorttitle = {{TSTL}},
	url = {http://link.springer.com/10.1007/s10009-016-0445-y},
	doi = {10.1007/s10009-016-0445-y},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Holmes, Josie and Groce, Alex and Pinto, Jervis and Mittal, Pranjal and Azimi, Pooria and Kellar, Kevin and OBrien, James},
	month = feb,
	year = {2018},
	pages = {57--78}
}

@article{keshishzadeh_formalizing_2016,
	title = {Formalizing and testing the consistency of {DSL} transformations},
	volume = {28},
	issn = {0934-5043, 1433-299X},
	url = {http://link.springer.com/10.1007/s00165-016-0359-1},
	doi = {10.1007/s00165-016-0359-1},
	language = {en},
	number = {2},
	urldate = {2018-10-31},
	journal = {Formal Aspects of Computing},
	author = {Keshishzadeh, Sarmen and Mooij, Arjan J.},
	month = apr,
	year = {2016},
	pages = {181--206},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\39944UFQ\\Keshishzadeh e Mooij - 2016 - Formalizing and testing the consistency of DSL tra.pdf:application/pdf}
}

@incollection{helfert_using_2016,
	address = {Cham},
	title = {Using {Model}-{Driven} {Development} to {Support} {Portable} {PaaS} {Applications}},
	volume = {581},
	isbn = {978-3-319-29581-7 978-3-319-29582-4},
	url = {http://link.springer.com/10.1007/978-3-319-29582-4_7},
	urldate = {2018-10-31},
	booktitle = {Cloud {Computing} and {Services} {Science}},
	publisher = {Springer International Publishing},
	author = {Nogueira, Elias and Lucrdio, Daniel and Moreira, Ana and Fortes, Renata},
	editor = {Helfert, Markus and Mndez Muoz, Vctor and Ferguson, Donald},
	year = {2016},
	doi = {10.1007/978-3-319-29582-4_7},
	pages = {115--134}
}

@incollection{giannakopoulou_formalizing_2014,
	address = {Cham},
	title = {Formalizing {DSL} {Semantics} for {Reasoning} and {Conformance} {Testing}},
	volume = {8702},
	isbn = {978-3-319-10430-0 978-3-319-10431-7},
	url = {http://link.springer.com/10.1007/978-3-319-10431-7_7},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering} and {Formal} {Methods}},
	publisher = {Springer International Publishing},
	author = {Keshishzadeh, Sarmen and Mooij, Arjan J.},
	editor = {Giannakopoulou, Dimitra and Salan, Gwen},
	year = {2014},
	doi = {10.1007/978-3-319-10431-7_7},
	pages = {81--95}
}

@incollection{winkler_evaluation_2018,
	address = {Cham},
	title = {Evaluation of an {Integrated} {Tool} {Environment} for {Experimentation} in {DSL} {Engineering}},
	volume = {302},
	isbn = {978-3-319-71439-4 978-3-319-71440-0},
	url = {http://link.springer.com/10.1007/978-3-319-71440-0_9},
	urldate = {2018-10-31},
	booktitle = {Software {Quality}: {Methods} and {Tools} for {Better} {Software} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Hser, Florian and Felderer, Michael and Breu, Ruth},
	editor = {Winkler, Dietmar and Biffl, Stefan and Bergsmann, Johannes},
	year = {2018},
	doi = {10.1007/978-3-319-71440-0_9},
	pages = {147--168}
}

@incollection{van_der_aalst_specification_2014,
	address = {Cham},
	title = {Specification by {Example} with {GUI} {Tests} - {How} {Could} {That} {Work}?},
	volume = {179},
	isbn = {978-3-319-06861-9 978-3-319-06862-6},
	url = {http://link.springer.com/10.1007/978-3-319-06862-6_26},
	urldate = {2018-10-31},
	booktitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	publisher = {Springer International Publishing},
	author = {Bache, Emily and Bache, Geoffrey},
	editor = {van der Aalst, Wil and Mylopoulos, John and Rosemann, Michael and Shaw, Michael J. and Szyperski, Clemens and Cantone, Giovanni and Marchesi, Michele},
	year = {2014},
	doi = {10.1007/978-3-319-06862-6_26},
	pages = {320--326}
}

@incollection{hutchison_novel_2013,
	address = {Berlin, Heidelberg},
	title = {A {Novel} {Approach} to {Developing} {Applications} in the {Pervasive} {Healthcare} {Environment} through the {Use} of {Archetypes}},
	volume = {7975},
	isbn = {978-3-642-39639-7 978-3-642-39640-3},
	url = {http://link.springer.com/10.1007/978-3-642-39640-3_35},
	urldate = {2018-10-31},
	booktitle = {Computational {Science} and {Its} {Applications}  {ICCSA} 2013},
	publisher = {Springer Berlin Heidelberg},
	author = {de Moraes, Joo Lus Cardoso and de Souza, Wanderley Lopes and Ferreira Pires, Lus and Cavalini, Luciana Tricai and do Prado, Antnio Francisco},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Murgante, Beniamino and Misra, Sanjay and Carlini, Maurizio and Torre, Carmelo M. and Nguyen, Hong-Quang and Taniar, David and Apduhan, Bernady O. and Gervasi, Osvaldo},
	year = {2013},
	doi = {10.1007/978-3-642-39640-3_35},
	pages = {475--490},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\3X3XS7X2\\de Moraes et al. - 2013 - A Novel Approach to Developing Applications in the.pdf:application/pdf}
}

@article{rabiser_developing_2018,
	title = {Developing and evolving a {DSL}-based approach for runtime monitoring of systems of systems},
	volume = {25},
	issn = {0928-8910, 1573-7535},
	url = {http://link.springer.com/10.1007/s10515-018-0241-x},
	doi = {10.1007/s10515-018-0241-x},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Automated Software Engineering},
	author = {Rabiser, Rick and Thanhofer-Pilisch, Jrgen and Vierhauser, Michael and Grnbacher, Paul and Egyed, Alexander},
	month = dec,
	year = {2018},
	pages = {875--915}
}

@incollection{hutchison_environment_2010,
	address = {Berlin, Heidelberg},
	title = {Environment {Modeling} with {UML}/{MARTE} to {Support} {Black}-{Box} {System} {Testing} for {Real}-{Time} {Embedded} {Systems}: {Methodology} and {Industrial} {Case} {Studies}},
	volume = {6394},
	isbn = {978-3-642-16144-5 978-3-642-16145-2},
	shorttitle = {Environment {Modeling} with {UML}/{MARTE} to {Support} {Black}-{Box} {System} {Testing} for {Real}-{Time} {Embedded} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-642-16145-2_20},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Iqbal, Muhammad Zohaib and Arcuri, Andrea and Briand, Lionel},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Petriu, Dorina C. and Rouquette, Nicolas and Haugen, ystein},
	year = {2010},
	doi = {10.1007/978-3-642-16145-2_20},
	pages = {286--300}
}

@article{gurbuz_model-based_2018,
	title = {Model-based testing for software safety: a systematic mapping study},
	volume = {26},
	issn = {0963-9314, 1573-1367},
	shorttitle = {Model-based testing for software safety},
	url = {http://link.springer.com/10.1007/s11219-017-9386-2},
	doi = {10.1007/s11219-017-9386-2},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software Quality Journal},
	author = {Gurbuz, Havva Gulay and Tekinerdogan, Bedir},
	month = dec,
	year = {2018},
	pages = {1327--1372},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\IYGBKX6C\\Gurbuz e Tekinerdogan - 2018 - Model-based testing for software safety a systema.pdf:application/pdf}
}

@incollection{king_dsl_2011,
	address = {Cham},
	title = {A {DSL} for {Corporate} {Wiki} {Initialization}},
	volume = {141},
	isbn = {978-3-319-98176-5 978-3-319-98177-2},
	url = {http://link.springer.com/10.1007/978-3-642-21640-4_19},
	urldate = {2018-10-31},
	booktitle = {Active {Flow} and {Combustion} {Control} 2018},
	publisher = {Springer International Publishing},
	author = {Daz, Oscar and Puente, Gorka},
	editor = {King, Rudibert},
	year = {2011},
	doi = {10.1007/978-3-642-21640-4_19},
	pages = {237--251}
}

@incollection{lamprecht_learning-based_2016,
	address = {Cham},
	title = {Learning-{Based} {Cross}-{Platform} {Conformance} {Testing}},
	volume = {683},
	isbn = {978-3-319-51640-0 978-3-319-51641-7},
	url = {http://link.springer.com/10.1007/978-3-319-51641-7_4},
	urldate = {2018-10-31},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification}, and {Validation}},
	publisher = {Springer International Publishing},
	author = {Neubauer, Johannes and Steffen, Bernhard},
	editor = {Lamprecht, Anna-Lena},
	year = {2016},
	doi = {10.1007/978-3-319-51641-7_4},
	pages = {53--79}
}

@incollection{buchberger_software_2010,
	address = {Berlin, Heidelberg},
	title = {Software {Engineering}  {Processes} and {Tools}},
	isbn = {978-3-642-02126-8 978-3-642-02127-5},
	url = {http://link.springer.com/10.1007/978-3-642-02127-5_5},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Hagenberg {Research}},
	publisher = {Springer Berlin Heidelberg},
	author = {Weiss, Gerhard and Pomberger, Gustav and Beer, Wolfgang and Buchgeher, Georg and Dorninger, Bernhard and Pichler, Josef and Prhofer, Herbert and Ramler, Rudolf and Stallinger, Fritz and Weinreich, Rainer},
	editor = {Buchberger, Bruno and Affenzeller, Michael and Ferscha, Alois and Haller, Michael and Jebelean, Tudor and Klement, Erich Peter and Paule, Peter and Pomberger, Gustav and Schreiner, Wolfgang and Stubenrauch, Robert and Wagner, Roland and Wei, Gerhard and Windsteiger, Wolfgang},
	year = {2010},
	doi = {10.1007/978-3-642-02127-5_5},
	pages = {157--235}
}

@incollection{bin_choosing_2007,
	address = {Berlin, Heidelberg},
	title = {Choosing a {Test} {Modeling} {Language}: {A} {Survey}},
	volume = {4383},
	isbn = {978-3-540-70888-9 978-3-540-70889-6},
	shorttitle = {Choosing a {Test} {Modeling} {Language}},
	url = {http://link.springer.com/10.1007/978-3-540-70889-6_16},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Hardware and {Software}, {Verification} and {Testing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hartman, Alan and Katara, Mika and Olvovsky, Sergey},
	editor = {Bin, Eyal and Ziv, Avi and Ur, Shmuel},
	year = {2007},
	doi = {10.1007/978-3-540-70889-6_16},
	pages = {204--218},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\RX77GA6X\\Hartman et al. - 2007 - Choosing a Test Modeling Language A Survey.pdf:application/pdf}
}

@incollection{howar_generative_2018,
	address = {Cham},
	title = {Generative {Model} {Driven} {Design} for {Agile} {System} {Design} and {Evolution}: {A} {Tale} of {Two} {Worlds}},
	volume = {11119},
	isbn = {978-3-030-00243-5 978-3-030-00244-2},
	shorttitle = {Generative {Model} {Driven} {Design} for {Agile} {System} {Design} and {Evolution}},
	url = {http://link.springer.com/10.1007/978-3-030-00244-2_1},
	urldate = {2018-10-31},
	booktitle = {Formal {Methods} for {Industrial} {Critical} {Systems}},
	publisher = {Springer International Publishing},
	author = {Margaria, Tiziana},
	editor = {Howar, Falk and Barnat, Ji},
	year = {2018},
	doi = {10.1007/978-3-030-00244-2_1},
	pages = {3--18}
}

@incollection{amblard_xmg_2016,
	address = {Berlin, Heidelberg},
	title = {{XMG} 2: {Describing} {Description} {Languages}},
	volume = {10054},
	isbn = {978-3-662-53825-8 978-3-662-53826-5},
	shorttitle = {{XMG} 2},
	url = {http://link.springer.com/10.1007/978-3-662-53826-5_16},
	urldate = {2018-10-31},
	booktitle = {Logical {Aspects} of {Computational} {Linguistics}. {Celebrating} 20 {Years} of {LACL} (19962016)},
	publisher = {Springer Berlin Heidelberg},
	author = {Petitjean, Simon and Duchier, Denys and Parmentier, Yannick},
	editor = {Amblard, Maxime and de Groote, Philippe and Pogodalla, Sylvain and Retor, Christian},
	year = {2016},
	doi = {10.1007/978-3-662-53826-5_16},
	pages = {255--272}
}

@incollection{gervasi_approach_2016,
	address = {Cham},
	title = {An {Approach} for {Code} {Annotation} {Validation} with {Metadata} {Location} {Transparency}},
	volume = {9789},
	isbn = {978-3-319-42088-2 978-3-319-42089-9},
	url = {http://link.springer.com/10.1007/978-3-319-42089-9_30},
	urldate = {2018-10-31},
	booktitle = {Computational {Science} and {Its} {Applications} -- {ICCSA} 2016},
	publisher = {Springer International Publishing},
	author = {de Siqueira, Jos Lzaro and Silveira, Fbio Fagundes and Guerra, Eduardo Martins},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Rocha, Ana Maria A.C. and Torre, Carmelo M. and Taniar, David and Apduhan, Bernady O. and Stankova, Elena and Wang, Shangguang},
	year = {2016},
	doi = {10.1007/978-3-319-42089-9_30},
	pages = {422--438}
}

@incollection{drechsler_model_2015,
	address = {Wiesbaden},
	title = {Model {Checking} and {Model}-{Based} {Testing} in the {Railway} {Domain}},
	isbn = {978-3-658-09993-0 978-3-658-09994-7},
	url = {http://link.springer.com/10.1007/978-3-658-09994-7_4},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Formal {Modeling} and {Verification} of {Cyber}-{Physical} {Systems}},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Haxthausen, Anne E. and Peleska, Jan},
	editor = {Drechsler, Rolf and Khne, Ulrich},
	year = {2015},
	doi = {10.1007/978-3-658-09994-7_4},
	pages = {82--121}
}

@article{barcelona_practical_2017,
	title = {Practical experiences in the usage of {MIDAS} in the logistics domain},
	volume = {19},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/10.1007/s10009-016-0430-5},
	doi = {10.1007/s10009-016-0430-5},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Barcelona, M. A. and Garca-Borgon, L. and Lpez-Nicols, G.},
	month = jun,
	year = {2017},
	pages = {325--339}
}

@incollection{france_analyzing_2011,
	address = {Berlin, Heidelberg},
	title = {Analyzing {Variability}: {Capturing} {Semantic} {Ripple} {Effects}},
	volume = {6698},
	isbn = {978-3-642-21469-1 978-3-642-21470-7},
	shorttitle = {Analyzing {Variability}},
	url = {http://link.springer.com/10.1007/978-3-642-21470-7_18},
	urldate = {2018-10-31},
	booktitle = {Modelling {Foundations} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Svendsen, Andreas and Haugen, ystein and Mller-Pedersen, Birger},
	editor = {France, Robert B. and Kuester, Jochen M. and Bordbar, Behzad and Paige, Richard F.},
	year = {2011},
	doi = {10.1007/978-3-642-21470-7_18},
	pages = {253--269}
}

@incollection{wotawa_simulation_2016,
	address = {Cham},
	title = {From {Simulation} {Data} to {Test} {Cases} for {Fully} {Automated} {Driving} and {ADAS}},
	volume = {9976},
	isbn = {978-3-319-47442-7 978-3-319-47443-4},
	url = {http://link.springer.com/10.1007/978-3-319-47443-4_12},
	urldate = {2018-10-31},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Sippl, Christoph and Bock, Florian and Wittmann, David and Altinger, Harald and German, Reinhard},
	editor = {Wotawa, Franz and Nica, Mihai and Kushik, Natalia},
	year = {2016},
	doi = {10.1007/978-3-319-47443-4_12},
	pages = {191--206},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\VP4ZPA75\\Sippl et al. - 2016 - From Simulation Data to Test Cases for Fully Autom.pdf:application/pdf}
}

@incollection{garbajosa_automated_2018,
	address = {Cham},
	title = {Automated {Acceptance} {Tests} as {Software} {Requirements}: {An} {Experiment} to {Compare} the {Applicability} of {Fit} {Tables} and {Gherkin} {Language}},
	volume = {314},
	isbn = {978-3-319-91601-9 978-3-319-91602-6},
	shorttitle = {Automated {Acceptance} {Tests} as {Software} {Requirements}},
	url = {http://link.springer.com/10.1007/978-3-319-91602-6_7},
	urldate = {2018-10-31},
	booktitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	publisher = {Springer International Publishing},
	author = {dos Santos, Ernani Csar and Vilain, Patrcia},
	editor = {Garbajosa, Juan and Wang, Xiaofeng and Aguiar, Ademar},
	year = {2018},
	doi = {10.1007/978-3-319-91602-6_7},
	pages = {104--119},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\WDCPDW8X\\dos Santos e Vilain - 2018 - Automated Acceptance Tests as Software Requirement.pdf:application/pdf}
}

@incollection{hutchison_case_2013,
	address = {Berlin, Heidelberg},
	title = {A {Case} {Study} in {Evidence}-{Based} {DSL} {Evolution}},
	volume = {7949},
	isbn = {978-3-642-39012-8 978-3-642-39013-5},
	url = {http://link.springer.com/10.1007/978-3-642-39013-5_15},
	urldate = {2018-10-31},
	booktitle = {Modelling {Foundations} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {van den Bos, Jeroen and van der Storm, Tijs},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Van Gorp, Pieter and Ritter, Tom and Rose, Louis M.},
	year = {2013},
	doi = {10.1007/978-3-642-39013-5_15},
	pages = {207--219}
}

@incollection{hutchison_modbat:_2013,
	address = {Cham},
	title = {Modbat: {A} {Model}-{Based} {API} {Tester} for {Event}-{Driven} {Systems}},
	volume = {8244},
	isbn = {978-3-319-03076-0 978-3-319-03077-7},
	shorttitle = {Modbat},
	url = {http://link.springer.com/10.1007/978-3-319-03077-7_8},
	urldate = {2018-10-31},
	booktitle = {Hardware and {Software}: {Verification} and {Testing}},
	publisher = {Springer International Publishing},
	author = {Artho, Cyrille Valentin and Biere, Armin and Hagiya, Masami and Platon, Eric and Seidl, Martina and Tanabe, Yoshinori and Yamamoto, Mitsuharu},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bertacco, Valeria and Legay, Axel},
	year = {2013},
	doi = {10.1007/978-3-319-03077-7_8},
	pages = {112--128}
}

@incollection{smith_model_2007,
	address = {Berlin, Heidelberg},
	title = {Model {Based} {HMI} {Specification} in an {Automotive} {Context}},
	volume = {4557},
	isbn = {978-3-540-73344-7},
	url = {http://link.springer.com/10.1007/978-3-540-73345-4_4},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Methods}, {Techniques} and {Tools} in {Information} {Design}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fleischmann, Thomas},
	editor = {Smith, Michael J. and Salvendy, Gavriel},
	year = {2007},
	doi = {10.1007/978-3-540-73345-4_4},
	pages = {31--39}
}

@incollection{younker_test-driven_2008,
	address = {Berkeley, CA},
	title = {Test-{Driven} {Development} and {Impostors}},
	isbn = {978-1-59059-981-5 978-1-4302-0635-4},
	url = {http://link.springer.com/10.1007/978-1-4302-0635-4_7},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Foundations of {Agile} {Python} {Development}},
	publisher = {Apress},
	collaborator = {Younker, Jeff},
	year = {2008},
	doi = {10.1007/978-1-4302-0635-4_7},
	pages = {175--231}
}

@article{robles_luna_webspec:_2011,
	title = {{WebSpec}: a visual language for specifying interaction and navigation requirements in web applications},
	volume = {16},
	issn = {0947-3602, 1432-010X},
	shorttitle = {{WebSpec}},
	url = {http://link.springer.com/10.1007/s00766-011-0124-1},
	doi = {10.1007/s00766-011-0124-1},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Requirements Engineering},
	author = {Robles Luna, Esteban and Rossi, Gustavo and Garrigs, Irene},
	month = nov,
	year = {2011},
	pages = {297--321}
}

@article{pruski_tiqi:_2015,
	title = {{TiQi}: answering unstructured natural language trace queries},
	volume = {20},
	issn = {0947-3602, 1432-010X},
	shorttitle = {{TiQi}},
	url = {http://link.springer.com/10.1007/s00766-015-0224-4},
	doi = {10.1007/s00766-015-0224-4},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Requirements Engineering},
	author = {Pruski, Piotr and Lohar, Sugandha and Goss, William and Rasin, Alexander and Cleland-Huang, Jane},
	month = sep,
	year = {2015},
	pages = {215--232}
}

@incollection{fujita_towards_2015,
	address = {Cham},
	title = {Towards {Automated} {UI}-{Tests} for {Sensor}-{Based} {Mobile} {Applications}},
	volume = {532},
	isbn = {978-3-319-22688-0 978-3-319-22689-7},
	url = {http://link.springer.com/10.1007/978-3-319-22689-7_1},
	urldate = {2018-10-31},
	booktitle = {Intelligent {Software} {Methodologies}, {Tools} and {Techniques}},
	publisher = {Springer International Publishing},
	author = {Griebe, Tobias and Hesenius, Marc and Gruhn, Volker},
	editor = {Fujita, Hamido and Guizzi, Guido},
	year = {2015},
	doi = {10.1007/978-3-319-22689-7_1},
	pages = {3--17}
}

@incollection{carneiro_testing_2016,
	address = {Berkeley, CA},
	title = {Testing with {Services}},
	isbn = {978-1-4842-1936-2 978-1-4842-1937-9},
	url = {http://link.springer.com/10.1007/978-1-4842-1937-9_9},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Microservices {From} {Day} {One}},
	publisher = {Apress},
	author = {Carneiro, Cloves and Schmelmer, Tim},
	collaborator = {Carneiro, Cloves and Schmelmer, Tim},
	year = {2016},
	doi = {10.1007/978-1-4842-1937-9_9},
	pages = {127--150}
}

@incollection{hutchison_bottom-up_2012,
	address = {Berlin, Heidelberg},
	title = {Bottom-{Up} {Meta}-{Modelling}: {An} {Interactive} {Approach}},
	volume = {7590},
	isbn = {978-3-642-33665-2 978-3-642-33666-9},
	shorttitle = {Bottom-{Up} {Meta}-{Modelling}},
	url = {http://link.springer.com/10.1007/978-3-642-33666-9_2},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Snchez-Cuadrado, Jess and de Lara, Juan and Guerra, Esther},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and France, Robert B. and Kazmeier, Jrgen and Breu, Ruth and Atkinson, Colin},
	year = {2012},
	doi = {10.1007/978-3-642-33666-9_2},
	pages = {3--19},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\U867JT92\\Snchez-Cuadrado et al. - 2012 - Bottom-Up Meta-Modelling An Interactive Approach.pdf:application/pdf}
}

@incollection{mahmood_testing_2013,
	address = {London},
	title = {Testing in the {Cloud}: {Strategies}, {Risks} and {Benefits}},
	isbn = {978-1-4471-5030-5 978-1-4471-5031-2},
	shorttitle = {Testing in the {Cloud}},
	url = {http://link.springer.com/10.1007/978-1-4471-5031-2_8},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering} {Frameworks} for the {Cloud} {Computing} {Paradigm}},
	publisher = {Springer London},
	author = {Akerele, Olumide and Ramachandran, Muthu and Dixon, Mark},
	editor = {Mahmood, Zaigham and Saeed, Saqib},
	year = {2013},
	doi = {10.1007/978-1-4471-5031-2_8},
	pages = {165--185}
}

@article{zech_knowledge-based_2017,
	title = {Knowledge-based security testing of web applications by logic programming},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/10.1007/s10009-017-0472-3},
	doi = {10.1007/s10009-017-0472-3},
	language = {en},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Zech, Philipp and Felderer, Michael and Breu, Ruth},
	month = sep,
	year = {2017}
}

@incollection{tadeusz_tdqmed:_2015,
	address = {Cham},
	title = {{TDQMed}: {Managing} {Collections} of {Complex} {Test} {Data}},
	volume = {9282},
	isbn = {978-3-319-23134-1 978-3-319-23135-8},
	shorttitle = {{TDQMed}},
	url = {http://link.springer.com/10.1007/978-3-319-23135-8_23},
	urldate = {2018-10-31},
	booktitle = {Advances in {Databases} and {Information} {Systems}},
	publisher = {Springer International Publishing},
	author = {Held, Johannes and Lenz, Richard},
	editor = {Tadeusz, Morzy and Valduriez, Patrick and Bellatreche, Ladjel},
	year = {2015},
	doi = {10.1007/978-3-319-23135-8_23},
	pages = {334--347}
}

@incollection{krogstie_methodologies_2012,
	address = {London},
	title = {Methodologies for {Computerised} {Information} {Systems} {Support} in {Organisations}},
	isbn = {978-1-4471-2935-6 978-1-4471-2936-3},
	url = {http://link.springer.com/10.1007/978-1-4471-2936-3_2},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Model-{Based} {Development} and {Evolution} of {Information} {Systems}},
	publisher = {Springer London},
	author = {Krogstie, John},
	collaborator = {Krogstie, John},
	year = {2012},
	doi = {10.1007/978-1-4471-2936-3_2},
	pages = {19--87}
}

@incollection{lassenius_industrial_2015,
	address = {Cham},
	title = {An {Industrial} {Case} {Study} on {Test} {Cases} as {Requirements}},
	volume = {212},
	isbn = {978-3-319-18611-5 978-3-319-18612-2},
	url = {http://link.springer.com/10.1007/978-3-319-18612-2_3},
	urldate = {2018-10-31},
	booktitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	publisher = {Springer International Publishing},
	author = {Bjarnason, Elizabeth and Unterkalmsteiner, Michael and Engstrm, Emelie and Borg, Markus},
	editor = {Lassenius, Casper and Dingsyr, Torgeir and Paasivaara, Maria},
	year = {2015},
	doi = {10.1007/978-3-319-18612-2_3},
	pages = {27--39}
}

@article{kolovos_eugenia:_2017,
	title = {Eugenia: towards disciplined and automated development of {GMF}-based graphical model editors},
	volume = {16},
	issn = {1619-1366, 1619-1374},
	shorttitle = {Eugenia},
	url = {http://link.springer.com/10.1007/s10270-015-0455-3},
	doi = {10.1007/s10270-015-0455-3},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Kolovos, Dimitrios S. and Garca-Domnguez, Antonio and Rose, Louis M. and Paige, Richard F.},
	month = feb,
	year = {2017},
	pages = {229--255},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\BE9SRJW6\\Kolovos et al. - 2017 - Eugenia towards disciplined and automated develop.pdf:application/pdf}
}

@article{neubauer_risk-based_2014,
	title = {Risk-based testing via active continuous quality control},
	volume = {16},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/10.1007/s10009-014-0321-6},
	doi = {10.1007/s10009-014-0321-6},
	language = {en},
	number = {5},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Neubauer, Johannes and Windmller, Stephan and Steffen, Bernhard},
	month = oct,
	year = {2014},
	pages = {569--591}
}

@article{zech_model-based_2017,
	title = {Model-based regression testing by {OCL}},
	volume = {19},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/10.1007/s10009-015-0408-8},
	doi = {10.1007/s10009-015-0408-8},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Zech, Philipp and Kalb, Philipp and Felderer, Michael and Atkinson, Colin and Breu, Ruth},
	month = feb,
	year = {2017},
	pages = {115--131}
}

@incollection{hutchison_specifying_2012,
	address = {Berlin, Heidelberg},
	title = {Specifying a {Testing} {Oracle} for {Train} {Stations}  {Going} beyond with {Product} {Line} {Technology}},
	volume = {7167},
	isbn = {978-3-642-29644-4 978-3-642-29645-1},
	url = {http://link.springer.com/10.1007/978-3-642-29645-1_20},
	urldate = {2018-10-31},
	booktitle = {Models in {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Svendsen, Andreas and Haugen, ystein and Mller-Pedersen, Birger},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Kienzle, Jrg},
	year = {2012},
	doi = {10.1007/978-3-642-29645-1_20},
	pages = {187--201}
}

@incollection{de_lucia_service-oriented_2009,
	address = {Berlin, Heidelberg},
	title = {Service-{Oriented} {Architectures} {Testing}: {A} {Survey}},
	volume = {5413},
	isbn = {978-3-540-95887-1 978-3-540-95888-8},
	shorttitle = {Service-{Oriented} {Architectures} {Testing}},
	url = {http://link.springer.com/10.1007/978-3-540-95888-8_4},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Canfora, Gerardo and Di Penta, Massimiliano},
	editor = {De Lucia, Andrea and Ferrucci, Filomena},
	year = {2009},
	doi = {10.1007/978-3-540-95888-8_4},
	pages = {78--105},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\LZZGCLQE\\Canfora e Di Penta - 2009 - Service-Oriented Architectures Testing A Survey.pdf:application/pdf}
}

@incollection{hutchison_model-based_2008,
	address = {Berlin, Heidelberg},
	title = {A {Model}-{Based} {Framework} for {Security} {Policy} {Specification}, {Deployment} and {Testing}},
	volume = {5301},
	isbn = {978-3-540-87874-2 978-3-540-87875-9},
	url = {http://link.springer.com/10.1007/978-3-540-87875-9_38},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mouelhi, Tejeddine and Fleurey, Franck and Baudry, Benoit and Le Traon, Yves},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Czarnecki, Krzysztof and Ober, Ileana and Bruel, Jean-Michel and Uhl, Axel and Vlter, Markus},
	year = {2008},
	doi = {10.1007/978-3-540-87875-9_38},
	pages = {537--552}
}

@incollection{arnicans_self-management_2016,
	address = {Cham},
	title = {Self-management of {Information} {Systems}},
	volume = {615},
	isbn = {978-3-319-40179-9 978-3-319-40180-5},
	url = {http://link.springer.com/10.1007/978-3-319-40180-5_12},
	urldate = {2018-10-31},
	booktitle = {Databases and {Information} {Systems}},
	publisher = {Springer International Publishing},
	author = {Bicevskis, Janis and Bicevska, Zane and Oditis, Ivo},
	editor = {Arnicans, Guntis and Arnicane, Vineta and Borzovs, Juris and Niedrite, Laila},
	year = {2016},
	doi = {10.1007/978-3-319-40180-5_12},
	pages = {167--180}
}

@incollection{hutchison_model-driven_2012,
	address = {Berlin, Heidelberg},
	title = {Model-{Driven} {V}\&{V} {Processes} for {Computer} {Based} {Control} {Systems}: {A} {Unifying} {Perspective}},
	volume = {7610},
	isbn = {978-3-642-34031-4 978-3-642-34032-1},
	shorttitle = {Model-{Driven} {V}\&{V} {Processes} for {Computer} {Based} {Control} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-642-34032-1_20},
	urldate = {2018-10-31},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}. {Applications} and {Case} {Studies}},
	publisher = {Springer Berlin Heidelberg},
	author = {Flammini, Francesco and Marrone, Stefano and Mazzocca, Nicola and Nardone, Roberto and Vittorini, Valeria},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Margaria, Tiziana and Steffen, Bernhard},
	year = {2012},
	doi = {10.1007/978-3-642-34032-1_20},
	pages = {190--204}
}

@article{lopez-fernandez_example-driven_2015,
	title = {Example-driven meta-model development},
	volume = {14},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-013-0392-y},
	doi = {10.1007/s10270-013-0392-y},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Lpez-Fernndez, Jess J. and Cuadrado, Jess Snchez and Guerra, Esther and de Lara, Juan},
	month = oct,
	year = {2015},
	pages = {1323--1347},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\M6EX8BF5\\Lpez-Fernndez et al. - 2015 - Example-driven meta-model development.pdf:application/pdf}
}

@incollection{meyer_software_2015,
	address = {Cham},
	title = {Software {Mining} {Studies}: {Goals}, {Approaches}, {Artifacts}, and {Replicability}},
	volume = {8987},
	isbn = {978-3-319-28405-7 978-3-319-28406-4},
	shorttitle = {Software {Mining} {Studies}},
	url = {http://link.springer.com/10.1007/978-3-319-28406-4_5},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering}},
	publisher = {Springer International Publishing},
	author = {Amann, Sven and Beyer, Stefanie and Kevic, Katja and Gall, Harald},
	editor = {Meyer, Bertrand and Nordio, Martin},
	year = {2015},
	doi = {10.1007/978-3-319-28406-4_5},
	pages = {121--158}
}

@incollection{hutchison_model-based_2014,
	address = {Cham},
	title = {Model-{Based} {Test} {Case} {Generation} for {Web} {Applications}},
	volume = {8584},
	isbn = {978-3-319-09152-5 978-3-319-09153-2},
	url = {http://link.springer.com/10.1007/978-3-319-09153-2_19},
	urldate = {2018-10-31},
	booktitle = {Computational {Science} and {Its} {Applications}  {ICCSA} 2014},
	publisher = {Springer International Publishing},
	author = {Nabuco, Miguel and Paiva, Ana C. R.},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Murgante, Beniamino and Misra, Sanjay and Rocha, Ana Maria A. C. and Torre, Carmelo and Rocha, Jorge Gustavo and Falco, Maria Irene and Taniar, David and Apduhan, Bernady O. and Gervasi, Osvaldo},
	year = {2014},
	doi = {10.1007/978-3-319-09153-2_19},
	pages = {248--262}
}

@incollection{russo_formal_2018,
	address = {Cham},
	title = {A {Formal} {Framework} for {Incremental} {Model} {Slicing}},
	volume = {10802},
	isbn = {978-3-319-89362-4 978-3-319-89363-1},
	url = {http://link.springer.com/10.1007/978-3-319-89363-1_1},
	urldate = {2018-10-31},
	booktitle = {Fundamental {Approaches} to {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Taentzer, Gabriele and Kehrer, Timo and Pietsch, Christopher and Kelter, Udo},
	editor = {Russo, Alessandra and Schrr, Andy},
	year = {2018},
	doi = {10.1007/978-3-319-89363-1_1},
	pages = {3--20}
}

@InCollection{bishop_case_2011,
  author    = {Hills, Mark and Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  title     = {A {Case} of {Visitor} versus {Interpreter} {Pattern}},
  booktitle = {Objects, {Models}, {Components}, {Patterns}},
  publisher = {Springer Berlin Heidelberg},
  year      = {2011},
  editor    = {Bishop, Judith and Vallecillo, Antonio},
  volume    = {6705},
  pages     = {228--243},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-21951-1 978-3-642-21952-8},
  doi       = {10.1007/978-3-642-21952-8_17},
  file      = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\XJ8F36KD\\Hills et al. - 2011 - A Case of Visitor versus Interpreter Pattern.pdf:application/pdf},
  url       = {http://link.springer.com/10.1007/978-3-642-21952-8_17},
  urldate   = {2018-10-31},
}

@incollection{ugurlu_unit_2013,
	address = {Berkeley, CA},
	title = {Unit {Testing} and {Integration} {Testing}},
	isbn = {978-1-4302-4725-8 978-1-4302-4726-5},
	url = {http://link.springer.com/10.1007/978-1-4302-4726-5_15},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Pro {ASP}.{NET} {Web} {API}},
	publisher = {Apress},
	author = {Ugurlu, Tugberk and Zeitler, Alexander and Kheyrollahi, Ali},
	collaborator = {Ugurlu, Tugberk and Zeitler, Alexander and Kheyrollahi, Ali},
	year = {2013},
	doi = {10.1007/978-1-4302-4726-5_15},
	pages = {431--462}
}

@InCollection{king_dsl_2011,
  author    = {Daz, Oscar and Puente, Gorka},
  title     = {A {DSL} for {Corporate} {Wiki} {Initialization}},
  booktitle = {Active {Flow} and {Combustion} {Control} 2018},
  publisher = {Springer International Publishing},
  year      = {2011},
  editor    = {King, Rudibert},
  volume    = {141},
  pages     = {237--251},
  address   = {Cham},
  isbn      = {978-3-319-98176-5 978-3-319-98177-2},
  doi       = {10.1007/978-3-642-21640-4_19},
  url       = {http://link.springer.com/10.1007/978-3-642-21640-4_19},
  urldate   = {2018-10-31},
}

@article{borle_analyzing_2018,
	title = {Analyzing the effects of test driven development in {GitHub}},
	volume = {23},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-017-9576-3},
	doi = {10.1007/s10664-017-9576-3},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Borle, Neil C. and Feghhi, Meysam and Stroulia, Eleni and Greiner, Russell and Hindle, Abram},
	month = aug,
	year = {2018},
	pages = {1931--1958}
}

@incollection{hutchison_model-based_2013,
	address = {Berlin, Heidelberg},
	title = {Model-{Based} {Testing} for {Verification} {Back}-{Ends}},
	volume = {7942},
	isbn = {978-3-642-38915-3 978-3-642-38916-0},
	url = {http://link.springer.com/10.1007/978-3-642-38916-0_3},
	urldate = {2018-10-31},
	booktitle = {Tests and {Proofs}},
	publisher = {Springer Berlin Heidelberg},
	author = {Artho, Cyrille and Biere, Armin and Seidl, Martina},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Veanes, Margus and Vigan, Luca},
	year = {2013},
	doi = {10.1007/978-3-642-38916-0_3},
	pages = {39--55},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\WWFWXL3S\\Artho et al. - 2013 - Model-Based Testing for Verification Back-Ends.pdf:application/pdf}
}

@incollection{staples_software_2006,
	title = {Software {Product} {Lines}},
	isbn = {978-3-540-28713-1},
	url = {http://link.springer.com/10.1007/3-540-28714-0_9},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Essential {Software} {Architecture}},
	publisher = {Springer Berlin Heidelberg},
	author = {Staples, Mark},
	year = {2006},
	doi = {10.1007/3-540-28714-0_9},
	pages = {155--178}
}

@incollection{gorton_software_2011,
	address = {Berlin, Heidelberg},
	title = {Software {Product} {Lines}},
	isbn = {978-3-642-19175-6 978-3-642-19176-3},
	url = {http://link.springer.com/10.1007/978-3-642-19176-3_15},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Essential {Software} {Architecture}},
	publisher = {Springer Berlin Heidelberg},
	author = {Staples, Mark},
	collaborator = {Gorton, Ian},
	year = {2011},
	doi = {10.1007/978-3-642-19176-3_15},
	pages = {219--237}
}

@incollection{echahed_complexity_2016,
	address = {Cham},
	title = {Complexity is the {Only} {Constant}: {Trends} in {Computing} and {Their} {Relevance} to {Model} {Driven} {Engineering}},
	volume = {9761},
	isbn = {978-3-319-40529-2 978-3-319-40530-8},
	shorttitle = {Complexity is the {Only} {Constant}},
	url = {http://link.springer.com/10.1007/978-3-319-40530-8_1},
	urldate = {2018-10-31},
	booktitle = {Graph {Transformation}},
	publisher = {Springer International Publishing},
	author = {Dingel, Juergen},
	editor = {Echahed, Rachid and Minas, Mark},
	year = {2016},
	doi = {10.1007/978-3-319-40530-8_1},
	pages = {3--18}
}

@incollection{noauthor_book_2007,
	address = {Berkeley, CA},
	title = {Book {Inventory} {Management}},
	isbn = {978-1-59059-736-1},
	url = {http://link.springer.com/10.1007/978-1-4302-0276-9_3},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Ruby} on {Rails} {E}-{Commerce}},
	publisher = {Apress},
	year = {2007},
	doi = {10.1007/978-1-4302-0276-9_3},
	pages = {59--112}
}

@incollection{grabowski_evolving_2016,
	address = {Cham},
	title = {Evolving the {ETSI} {Test} {Description} {Language}},
	volume = {9959},
	isbn = {978-3-319-46612-5 978-3-319-46613-2},
	url = {http://link.springer.com/10.1007/978-3-319-46613-2_8},
	urldate = {2018-10-31},
	booktitle = {System {Analysis} and {Modeling}. {Technology}-{Specific} {Aspects} of {Models}},
	publisher = {Springer International Publishing},
	author = {Makedonski, Philip and Adamis, Gusztv and Krik, Martti and Kristoffersen, Finn and Zeitoun, Xavier},
	editor = {Grabowski, Jens and Herbold, Steffen},
	year = {2016},
	doi = {10.1007/978-3-319-46613-2_8},
	pages = {116--131}
}

@incollection{hutchison_scenarios_2010,
	address = {Berlin, Heidelberg},
	title = {From {Scenarios} to {Test} {Implementations} {Via} {Promela}},
	volume = {6435},
	isbn = {978-3-642-16572-6 978-3-642-16573-3},
	url = {http://link.springer.com/10.1007/978-3-642-16573-3_17},
	urldate = {2018-10-31},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ulrich, Andreas and Alikacem, El-Hachemi and Hallal, Hesham H. and Boroday, Sergiy},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Petrenko, Alexandre and Simo, Adenilso and Maldonado, Jos Carlos},
	year = {2010},
	doi = {10.1007/978-3-642-16573-3_17},
	pages = {236--249},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\9BNZZLGZ\\Ulrich et al. - 2010 - From Scenarios to Test Implementations Via Promela.pdf:application/pdf}
}

@incollection{kordon_model_2009,
	address = {Berlin, Heidelberg},
	title = {Model {Checking} {Techniques} for {Test} {Generation} from {Business} {Process} {Models}},
	volume = {5570},
	isbn = {978-3-642-01923-4 978-3-642-01924-1},
	url = {http://link.springer.com/10.1007/978-3-642-01924-1_5},
	urldate = {2018-10-31},
	booktitle = {Reliable {Software} {Technologies}  {Ada}-{Europe} 2009},
	publisher = {Springer Berlin Heidelberg},
	author = {Buchs, Didier and Lucio, Levi and Chen, Ang},
	editor = {Kordon, Fabrice and Kermarrec, Yvon},
	year = {2009},
	doi = {10.1007/978-3-642-01924-1_5},
	pages = {59--74}
}

@article{bavota_are_2015,
	title = {Are test smells really harmful? {An} empirical study},
	volume = {20},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Are test smells really harmful?},
	url = {http://link.springer.com/10.1007/s10664-014-9313-0},
	doi = {10.1007/s10664-014-9313-0},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Bavota, Gabriele and Qusef, Abdallah and Oliveto, Rocco and De Lucia, Andrea and Binkley, Dave},
	month = aug,
	year = {2015},
	pages = {1052--1094}
}

@incollection{bin_making_2007,
	address = {Berlin, Heidelberg},
	title = {Making {Model}-{Based} {Testing} {More} {Agile}: {A} {Use} {Case} {Driven} {Approach}},
	volume = {4383},
	isbn = {978-3-540-70888-9 978-3-540-70889-6},
	shorttitle = {Making {Model}-{Based} {Testing} {More} {Agile}},
	url = {http://link.springer.com/10.1007/978-3-540-70889-6_17},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Hardware and {Software}, {Verification} and {Testing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Katara, Mika and Kervinen, Antti},
	editor = {Bin, Eyal and Ziv, Avi and Ur, Shmuel},
	year = {2007},
	doi = {10.1007/978-3-540-70889-6_17},
	pages = {219--234},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\6HGA4PPZ\\Katara e Kervinen - 2007 - Making Model-Based Testing More Agile A Use Case .pdf:application/pdf}
}

@incollection{biffl_quantifying_2006,
	address = {Berlin, Heidelberg},
	title = {Quantifying the {Value} of {New} {Technologies} for {Software} {Development}},
	isbn = {978-3-540-25993-0 978-3-540-29263-0},
	url = {http://link.springer.com/10.1007/3-540-29263-2_16},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Value-{Based} {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Atkins, David L. and Mockus, Audris and Siy, Harvey},
	editor = {Biffl, Stefan and Aurum, Aybke and Boehm, Barry and Erdogmus, Hakan and Grnbacher, Paul},
	year = {2006},
	doi = {10.1007/3-540-29263-2_16},
	pages = {327--344},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\KKLW75IZ\\Atkins et al. - 2006 - Quantifying the Value of New Technologies for Soft.pdf:application/pdf}
}

@incollection{zhou_conducting_2015,
	address = {Cham},
	title = {Conducting {Acceptance} {Tests} for {Elderly} {People} on the {Web}},
	volume = {9193},
	isbn = {978-3-319-20891-6 978-3-319-20892-3},
	url = {http://link.springer.com/10.1007/978-3-319-20892-3_32},
	urldate = {2018-10-31},
	booktitle = {Human {Aspects} of {IT} for the {Aged} {Population}. {Design} for {Aging}},
	publisher = {Springer International Publishing},
	author = {Henka, Alexander and Stiegler, Andreas and Zimmermann, Gottfried and Ertl, Thomas},
	editor = {Zhou, Jia and Salvendy, Gavriel},
	year = {2015},
	doi = {10.1007/978-3-319-20892-3_32},
	pages = {325--336}
}

@incollection{hutchison_four_2004,
	address = {Berlin, Heidelberg},
	title = {Four {Mechanisms} for {Adaptable} {Systems}},
	volume = {3154},
	isbn = {978-3-540-22918-6 978-3-540-28630-1},
	url = {http://link.springer.com/10.1007/978-3-540-28630-1_4},
	urldate = {2018-10-31},
	booktitle = {Software {Product} {Lines}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fritsch, Claudia and Renz, Burkhardt},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Nord, Robert L.},
	year = {2004},
	doi = {10.1007/978-3-540-28630-1_4},
	pages = {51--72}
}

@incollection{stephens_tdd_2010,
	address = {Berkeley, CA},
	title = {{TDD} {Using} {Hello} {World}},
	isbn = {978-1-4302-2943-8 978-1-4302-2944-5},
	url = {http://link.springer.com/10.1007/978-1-4302-2944-5_2},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Design {Driven} {Testing}},
	publisher = {Apress},
	author = {Stephens, Matt and Rosenberg, Doug},
	collaborator = {Stephens, Matt and Rosenberg, Doug},
	year = {2010},
	doi = {10.1007/978-1-4302-2944-5_2},
	pages = {17--42}
}

@incollection{yueming_advanced_2015,
	address = {Berlin, Heidelberg},
	title = {Advanced {Test} {Modelling} and {Execution} {Based} on the {International} {Standardized} {Techniques} {TTCN}-3 and {UTP}},
	volume = {520},
	isbn = {978-3-662-47400-6 978-3-662-47401-3},
	url = {http://link.springer.com/10.1007/978-3-662-47401-3_32},
	urldate = {2018-10-31},
	booktitle = {Trustworthy {Computing} and {Services}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rennoch, Axel and Wendland, Marc-Florian and Hoffmann, Andreas and Schneider, Martin},
	editor = {Yueming, Lu and Xu, Wu and Xi, Zhang},
	year = {2015},
	doi = {10.1007/978-3-662-47401-3_32},
	pages = {245--252}
}

@article{white_simplifying_2007,
	title = {Simplifying autonomic enterprise {Java} {Bean} applications via model-driven engineering and simulation},
	volume = {7},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-007-0057-9},
	doi = {10.1007/s10270-007-0057-9},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {White, Jules and Schmidt, Douglas C. and Gokhale, Aniruddha},
	month = nov,
	year = {2007},
	pages = {3--23}
}

@article{sharma_protection_2016,
	title = {Protection of software against various attacks: issues and challenges},
	volume = {4},
	issn = {2277-9078, 2277-9086},
	shorttitle = {Protection of software against various attacks},
	url = {http://link.springer.com/10.1007/s40012-016-0097-9},
	doi = {10.1007/s40012-016-0097-9},
	language = {en},
	number = {2-4},
	urldate = {2018-10-31},
	journal = {CSI Transactions on ICT},
	author = {Sharma, Anshika and Khurana, Palak and Singh, Shailendra Narayan},
	month = dec,
	year = {2016},
	pages = {271--278}
}

@incollection{mendes_web_2006,
	address = {Berlin/Heidelberg},
	title = {Web {System} {Reliability} and {Performance}},
	isbn = {978-3-540-28196-2},
	url = {http://link.springer.com/10.1007/3-540-28218-1_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Web {Engineering}},
	publisher = {Springer-Verlag},
	author = {Andreolini, Mauro and Colajanni, Michele and Lancellotti, Riccardo},
	editor = {Mendes, Emilia and Mosley, Nile},
	year = {2006},
	doi = {10.1007/3-540-28218-1_6},
	pages = {181--218}
}

@incollection{dastani_automated_2015,
	address = {Cham},
	title = {Automated {Integration} of {Service}-{Oriented} {Software} {Systems}},
	volume = {9392},
	isbn = {978-3-319-24643-7 978-3-319-24644-4},
	url = {http://link.springer.com/10.1007/978-3-319-24644-4_2},
	urldate = {2018-10-31},
	booktitle = {Fundamentals of {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Autili, Marco and Inverardi, Paola and Tivoli, Massimo},
	editor = {Dastani, Mehdi and Sirjani, Marjan},
	year = {2015},
	doi = {10.1007/978-3-319-24644-4_2},
	pages = {30--45},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\2CXLY33I\\Autili et al. - 2015 - Automated Integration of Service-Oriented Software.pdf:application/pdf}
}

@incollection{hutchison_automated_2014,
	address = {Berlin, Heidelberg},
	title = {Automated {Multi}-{Language} {Artifact} {Binding} and {Rename} {Refactoring} between {Java} and {DSLs} {Used} by {Java} {Frameworks}},
	volume = {8586},
	isbn = {978-3-662-44201-2 978-3-662-44202-9},
	url = {http://link.springer.com/10.1007/978-3-662-44202-9_18},
	urldate = {2018-10-31},
	booktitle = {{ECOOP} 2014  {Object}-{Oriented} {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mayer, Philip and Schroeder, Andreas},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Jones, Richard},
	year = {2014},
	doi = {10.1007/978-3-662-44202-9_18},
	pages = {437--462}
}

@incollection{noauthor_author_2007,
	address = {Berkeley, CA},
	title = {Author {Management}},
	isbn = {978-1-59059-736-1},
	url = {http://link.springer.com/10.1007/978-1-4302-0276-9_2},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Ruby} on {Rails} {E}-{Commerce}},
	publisher = {Apress},
	year = {2007},
	doi = {10.1007/978-1-4302-0276-9_2},
	pages = {29--57}
}

@incollection{yuan_specifying_2013,
	address = {Berlin, Heidelberg},
	title = {Specifying and {Detecting} {Behavioral} {Changes} in {Source} {Code} {Using} {Abstract} {Syntax} {Tree} {Differencing}},
	volume = {320},
	isbn = {978-3-642-35794-7 978-3-642-35795-4},
	url = {http://link.springer.com/10.1007/978-3-642-35795-4_59},
	urldate = {2018-10-31},
	booktitle = {Trustworthy {Computing} and {Services}},
	publisher = {Springer Berlin Heidelberg},
	author = {Li, Yuankui and Wang, Linzhang},
	editor = {Yuan, Yuyu and Wu, Xu and Lu, Yueming},
	year = {2013},
	doi = {10.1007/978-3-642-35795-4_59},
	pages = {466--473}
}

@incollection{margaria_challenges_2016,
	address = {Cham},
	title = {Challenges in {High}-{Assurance} {Runtime} {Verification}},
	volume = {9952},
	isbn = {978-3-319-47165-5 978-3-319-47166-2},
	url = {http://link.springer.com/10.1007/978-3-319-47166-2_31},
	urldate = {2018-10-31},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}: {Foundational} {Techniques}},
	publisher = {Springer International Publishing},
	author = {Goodloe, Alwyn},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2016},
	doi = {10.1007/978-3-319-47166-2_31},
	pages = {446--460}
}

@incollection{hutchison_requirements_2012,
	address = {Berlin, Heidelberg},
	title = {From {Requirements} to {Web} {Applications} in an {Agile} {Model}-{Driven} {Approach}},
	volume = {7387},
	isbn = {978-3-642-31752-1 978-3-642-31753-8},
	url = {http://link.springer.com/10.1007/978-3-642-31753-8_15},
	urldate = {2018-10-31},
	booktitle = {Web {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Grigera, Julin and Rivero, Jos Matas and Robles Luna, Esteban and Giacosa, Franco and Rossi, Gustavo},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Brambilla, Marco and Tokuda, Takehiro and Tolksdorf, Robert},
	year = {2012},
	doi = {10.1007/978-3-642-31753-8_15},
	pages = {200--214},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\UPYCXEPH\\Grigera et al. - 2012 - From Requirements to Web Applications in an Agile .pdf:application/pdf}
}

@incollection{paige_automatic_2009,
	address = {Berlin, Heidelberg},
	title = {Automatic {Model} {Generation} {Strategies} for {Model} {Transformation} {Testing}},
	volume = {5563},
	isbn = {978-3-642-02407-8 978-3-642-02408-5},
	url = {http://link.springer.com/10.1007/978-3-642-02408-5_11},
	urldate = {2018-10-31},
	booktitle = {Theory and {Practice} of {Model} {Transformations}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sen, Sagar and Baudry, Benoit and Mottu, Jean-Marie},
	editor = {Paige, Richard F.},
	year = {2009},
	doi = {10.1007/978-3-642-02408-5_11},
	pages = {148--164},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\Y8W7KMT9\\Sen et al. - 2009 - Automatic Model Generation Strategies for Model Tr.pdf:application/pdf}
}

@incollection{pathania_elements_2017,
	address = {Berkeley, CA},
	title = {Elements of {Continuous} {Delivery}},
	isbn = {978-1-4842-2912-5 978-1-4842-2913-2},
	url = {http://link.springer.com/10.1007/978-1-4842-2913-2_1},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Pro {Continuous} {Delivery}},
	publisher = {Apress},
	author = {Pathania, Nikhil},
	collaborator = {Pathania, Nikhil},
	year = {2017},
	doi = {10.1007/978-1-4842-2913-2_1},
	pages = {1--21}
}

@article{guerra_engineering_2013,
	title = {Engineering model transformations with {transML}},
	volume = {12},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-011-0211-2},
	doi = {10.1007/s10270-011-0211-2},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Guerra, Esther and de Lara, Juan and Kolovos, Dimitrios S. and Paige, Richard F. and dos Santos, Osmar Marchi},
	month = jul,
	year = {2013},
	pages = {555--577},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\Z6JE24QH\\Guerra et al. - 2013 - Engineering model transformations with transML.pdf:application/pdf}
}

@incollection{bosch_software_2004,
	address = {Berlin, Heidelberg},
	title = {Software {Reuse} and the {Test} {Development} {Process}: {A} {Combined} {Approach}},
	volume = {3107},
	isbn = {978-3-540-22335-1 978-3-540-27799-6},
	shorttitle = {Software {Reuse} and the {Test} {Development} {Process}},
	url = {http://link.springer.com/10.1007/978-3-540-27799-6_6},
	urldate = {2018-10-31},
	booktitle = {Software {Reuse}: {Methods}, {Techniques}, and {Tools}},
	publisher = {Springer Berlin Heidelberg},
	author = {Karinsalo, Mikko and Abrahamsson, Pekka},
	editor = {Bosch, Jan and Krueger, Charles},
	year = {2004},
	doi = {10.1007/978-3-540-27799-6_6},
	pages = {59--68}
}

@article{firmenich_crowdmock:_2018,
	title = {{CrowdMock}: an approach for defining and evolving web augmentation requirements},
	volume = {23},
	issn = {0947-3602, 1432-010X},
	shorttitle = {{CrowdMock}},
	url = {http://link.springer.com/10.1007/s00766-016-0257-3},
	doi = {10.1007/s00766-016-0257-3},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Requirements Engineering},
	author = {Firmenich, Diego and Firmenich, Sergio and Rivero, Jos Matas and Antonelli, Leandro and Rossi, Gustavo},
	month = mar,
	year = {2018},
	pages = {33--61}
}

@incollection{aichernig_combining_2016,
	address = {Cham},
	title = {Combining {Dynamic} and {Static} {Analysis} to {Help} {Develop} {Correct} {Graph} {Transformations}},
	volume = {9762},
	isbn = {978-3-319-41134-7 978-3-319-41135-4},
	url = {http://link.springer.com/10.1007/978-3-319-41135-4_11},
	urldate = {2018-10-31},
	booktitle = {Tests and {Proofs}},
	publisher = {Springer International Publishing},
	author = {Makhlouf, Amani and Tran, Hanh Nhi and Percebois, Christian and Strecker, Martin},
	editor = {Aichernig, Bernhard K. and Furia, Carlo A.},
	year = {2016},
	doi = {10.1007/978-3-319-41135-4_11},
	pages = {183--190}
}

@incollection{pergl_enterprise_2018,
	address = {Cham},
	title = {Enterprise {Ontology}-{Driven} {Development}},
	volume = {332},
	isbn = {978-3-030-00786-7 978-3-030-00787-4},
	url = {http://link.springer.com/10.1007/978-3-030-00787-4_1},
	urldate = {2018-10-31},
	booktitle = {Enterprise and {Organizational} {Modeling} and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Matula, Jiri and Hunka, Frantisek},
	editor = {Pergl, Robert and Babkin, Eduard and Lock, Russell and Malyzhenkov, Pavel and Merunka, Vojtch},
	year = {2018},
	doi = {10.1007/978-3-030-00787-4_1},
	pages = {3--15}
}

@incollection{bosch_scaling_2014,
	address = {Cham},
	title = {Scaling {Agile} {Mechatronics}: {An} {Industrial} {Case} {Study}},
	isbn = {978-3-319-11282-4 978-3-319-11283-1},
	shorttitle = {Scaling {Agile} {Mechatronics}},
	url = {http://link.springer.com/10.1007/978-3-319-11283-1_17},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Continuous {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Lantz, Jonn and Eliasson, Ulf},
	editor = {Bosch, Jan},
	year = {2014},
	doi = {10.1007/978-3-319-11283-1_17},
	pages = {211--222}
}

@incollection{bencomo_modelland:_2014,
	address = {Cham},
	title = {{ModelLAND}: {Where} {Do} {Models} {Come} from?},
	volume = {8378},
	isbn = {978-3-319-08914-0 978-3-319-08915-7},
	shorttitle = {{ModelLAND}},
	url = {http://link.springer.com/10.1007/978-3-319-08915-7_6},
	urldate = {2018-10-31},
	booktitle = {Models@run.time},
	publisher = {Springer International Publishing},
	author = {Autili, Marco and Di Ruscio, Davide and Inverardi, Paola and Pelliccione, Patrizio and Tivoli, Massimo},
	editor = {Bencomo, Nelly and France, Robert and Cheng, Betty H. C. and Amann, Uwe},
	year = {2014},
	doi = {10.1007/978-3-319-08915-7_6},
	pages = {162--187}
}

@incollection{thalheim_integrating_2015,
	address = {Cham},
	title = {Integrating a {Model}-{Driven} {Approach} and {Formal} {Verification} for the {Development} of {Secure} {Service} {Applications}},
	isbn = {978-3-319-17111-1 978-3-319-17112-8},
	url = {http://link.springer.com/10.1007/978-3-319-17112-8_3},
	urldate = {2018-10-31},
	booktitle = {Correct {Software} in {Web} {Applications} and {Web} {Services}},
	publisher = {Springer International Publishing},
	author = {Borek, Marian and Katkalov, Kuzman and Moebius, Nina and Reif, Wolfgang and Schellhorn, Gerhard and Stenzel, Kurt},
	editor = {Thalheim, Bernhard and Schewe, Klaus-Dieter and Prinz, Andreas and Buchberger, Bruno},
	year = {2015},
	doi = {10.1007/978-3-319-17112-8_3},
	pages = {45--81}
}

@incollection{aichernig_testing-based_2016,
	address = {Cham},
	title = {Testing-{Based} {Formal} {Verification} for {Theorems} and {Its} {Application} in {Software} {Specification} {Verification}},
	volume = {9762},
	isbn = {978-3-319-41134-7 978-3-319-41135-4},
	url = {http://link.springer.com/10.1007/978-3-319-41135-4_7},
	urldate = {2018-10-31},
	booktitle = {Tests and {Proofs}},
	publisher = {Springer International Publishing},
	author = {Liu, Shaoying},
	editor = {Aichernig, Bernhard K. and Furia, Carlo A.},
	year = {2016},
	doi = {10.1007/978-3-319-41135-4_7},
	pages = {112--129}
}

@article{kesserwan_use_2017,
	title = {From use case maps to executable test procedures: a scenario-based approach},
	issn = {1619-1366, 1619-1374},
	shorttitle = {From use case maps to executable test procedures},
	url = {http://link.springer.com/10.1007/s10270-017-0620-y},
	doi = {10.1007/s10270-017-0620-y},
	language = {en},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Kesserwan, Nader and Dssouli, Rachida and Bentahar, Jamal and Stepien, Bernard and Labrche, Pierre},
	month = aug,
	year = {2017}
}

@incollection{carneiro_testing_2010,
	address = {Berkeley, CA},
	title = {Testing {Your} {Application}},
	isbn = {978-1-4302-2433-4 978-1-4302-2434-1},
	url = {http://link.springer.com/10.1007/978-1-4302-2434-1_10},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Rails} 3},
	publisher = {Apress},
	editor = {Andres, Clay and Anglin, Steve and Beckner, Mark and Buckingham, Ewan and Cornell, Gary and Gennick, Jonathan and Hassell, Jonathan and Lowman, Michelle and Moodie, Matthew and Parkes, Duncan and Pepper, Jeffrey and Pohlmann, Frank and Pundick, Douglas and Renow-Clarke, Ben and Shakeshaft, Dominic and Wade, Matt and Welsh, Tom and Taylor, Tiffany},
	collaborator = {Carneiro, Cloves and Barazi, Rida Al},
	year = {2010},
	doi = {10.1007/978-1-4302-2434-1_10},
	pages = {233--268}
}

@incollection{abdelwahed_generation_2018,
	address = {Cham},
	title = {Generation of {Behavior}-{Driven} {Development} {C}++ {Tests} from {Abstract} {State} {Machine} {Scenarios}},
	volume = {929},
	isbn = {978-3-030-02851-0 978-3-030-02852-7},
	url = {http://link.springer.com/10.1007/978-3-030-02852-7_13},
	urldate = {2018-10-31},
	booktitle = {New {Trends} in {Model} and {Data} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Bonfanti, Silvia and Gargantini, Angelo and Mashkoor, Atif},
	editor = {Abdelwahed, El Hassan and Bellatreche, Ladjel and Benslimane, Djamal and Golfarelli, Matteo and Jean, Stphane and Mery, Dominique and Nakamatsu, Kazumi and Ordonez, Carlos},
	year = {2018},
	doi = {10.1007/978-3-030-02852-7_13},
	pages = {146--152}
}

@article{yu_alleviating_2018,
	title = {Alleviating patch overfitting with automatic test generation: a study of feasibility and effectiveness for the {Nopol} repair system},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Alleviating patch overfitting with automatic test generation},
	url = {http://link.springer.com/10.1007/s10664-018-9619-4},
	doi = {10.1007/s10664-018-9619-4},
	language = {en},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Yu, Zhongxing and Martinez, Matias and Danglot, Benjamin and Durieux, Thomas and Monperrus, Martin},
	month = may,
	year = {2018},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\V6I2RWU2\\Yu et al. - 2018 - Alleviating patch overfitting with automatic test .pdf:application/pdf}
}

@incollection{preston_integrating_2016,
	address = {Berkeley, CA},
	title = {Integrating {Quality} {Tooling} into the {Chef} {Development} {Life} {Cycle}},
	isbn = {978-1-4842-1477-0 978-1-4842-1476-3},
	url = {http://link.springer.com/10.1007/978-1-4842-1476-3_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Using {Chef} with {Microsoft} {Azure}},
	publisher = {Apress},
	author = {Preston, Stuart},
	collaborator = {Preston, Stuart},
	year = {2016},
	doi = {10.1007/978-1-4842-1476-3_6},
	pages = {131--162}
}

@incollection{noauthor_testing_2007,
	address = {Berkeley, CA},
	title = {Testing {Your} {Application}},
	isbn = {978-1-59059-686-9 978-1-4302-0319-3},
	url = {http://link.springer.com/10.1007/978-1-4302-0319-3_9},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Rails}},
	publisher = {Apress},
	year = {2007},
	doi = {10.1007/978-1-4302-0319-3_9},
	pages = {245--278}
}

@incollection{hutchison_capture_2010,
	address = {Berlin, Heidelberg},
	title = {Capture and {Evolution} of {Web} {Requirements} {Using} {WebSpec}},
	volume = {6189},
	isbn = {978-3-642-13910-9 978-3-642-13911-6},
	url = {http://link.springer.com/10.1007/978-3-642-13911-6_12},
	urldate = {2018-10-31},
	booktitle = {Web {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Luna, Esteban Robles and Garrigs, Irene and Grigera, Julin and Winckler, Marco},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Benatallah, Boualem and Casati, Fabio and Kappel, Gerti and Rossi, Gustavo},
	year = {2010},
	doi = {10.1007/978-3-642-13911-6_12},
	pages = {173--188},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\8G7LMSL3\\Luna et al. - 2010 - Capture and Evolution of Web Requirements Using We.pdf:application/pdf}
}

@article{szvetits_systematic_2016,
	title = {Systematic literature review of the objectives, techniques, kinds, and architectures of models at runtime},
	volume = {15},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-013-0394-9},
	doi = {10.1007/s10270-013-0394-9},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Szvetits, Michael and Zdun, Uwe},
	month = feb,
	year = {2016},
	pages = {31--69}
}

@article{bergmann_change-driven_2012,
	title = {Change-driven model transformations: {Change} (in) the rule to rule the change},
	volume = {11},
	issn = {1619-1366, 1619-1374},
	shorttitle = {Change-driven model transformations},
	url = {http://link.springer.com/10.1007/s10270-011-0197-9},
	doi = {10.1007/s10270-011-0197-9},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Bergmann, Gbor and Rth, Istvn and Varr, Gergely and Varr, Dniel},
	month = jul,
	year = {2012},
	pages = {431--461}
}

@incollection{gamble_testing_2013,
	address = {Berkeley, CA},
	title = {Testing {Your} {Application}},
	isbn = {978-1-4302-6034-9 978-1-4302-6035-6},
	url = {http://link.springer.com/10.1007/978-1-4302-6035-6_11},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Rails} 4},
	publisher = {Apress},
	author = {Gamble, Adam and Carneiro, Cloves and Barazi, Rida Al},
	collaborator = {Gamble, Adam and Carneiro, Cloves and Barazi, Rida Al},
	year = {2013},
	doi = {10.1007/978-1-4302-6035-6_11},
	pages = {219--250}
}

@incollection{dregvaite_information_2014,
	address = {Cham},
	title = {Information {Systems} {Requirements} {Specification} and {Usage} in {Test} {Case} {Generation}},
	volume = {465},
	isbn = {978-3-319-11957-1 978-3-319-11958-8},
	url = {http://link.springer.com/10.1007/978-3-319-11958-8_3},
	urldate = {2018-10-31},
	booktitle = {Information and {Software} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Sipaviien, Neringa and Smilgyt, Kristina and Butleris, Rimantas},
	editor = {Dregvaite, Giedre and Damasevicius, Robertas},
	year = {2014},
	doi = {10.1007/978-3-319-11958-8_3},
	pages = {24--34}
}

@incollection{whittle_modeling_2011,
	address = {Berlin, Heidelberg},
	title = {Modeling {Model} {Slicers}},
	volume = {6981},
	isbn = {978-3-642-24484-1 978-3-642-24485-8},
	url = {http://link.springer.com/10.1007/978-3-642-24485-8_6},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blouin, Arnaud and Combemale, Benot and Baudry, Benoit and Beaudoux, Olivier},
	editor = {Whittle, Jon and Clark, Tony and Khne, Thomas},
	year = {2011},
	doi = {10.1007/978-3-642-24485-8_6},
	pages = {62--76}
}

@article{marrone_towards_2014,
	title = {Towards {Model}-{Driven} {V}\&{V} assessment of railway control systems},
	volume = {16},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/10.1007/s10009-014-0320-7},
	doi = {10.1007/s10009-014-0320-7},
	language = {en},
	number = {6},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Marrone, Stefano and Flammini, Francesco and Mazzocca, Nicola and Nardone, Roberto and Vittorini, Valeria},
	month = nov,
	year = {2014},
	pages = {669--683}
}

@article{aichernig_property-based_2017,
	title = {Property-based testing of web services by deriving properties from business-rule models},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-017-0647-0},
	doi = {10.1007/s10270-017-0647-0},
	language = {en},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Aichernig, Bernhard K. and Schumi, Richard},
	month = dec,
	year = {2017},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\8K3XI5MZ\\Aichernig e Schumi - 2017 - Property-based testing of web services by deriving.pdf:application/pdf}
}

@incollection{bosch_ttcn-3_2004,
	address = {Berlin, Heidelberg},
	title = {{TTCN}-3 {Language} {Characteristics} in {Producing} {Reusable} {Test} {Software}},
	volume = {3107},
	isbn = {978-3-540-22335-1 978-3-540-27799-6},
	url = {http://link.springer.com/10.1007/978-3-540-27799-6_5},
	urldate = {2018-10-31},
	booktitle = {Software {Reuse}: {Methods}, {Techniques}, and {Tools}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ruuska, Pekka and Krki, Matti},
	editor = {Bosch, Jan and Krueger, Charles},
	year = {2004},
	doi = {10.1007/978-3-540-27799-6_5},
	pages = {49--58}
}

@incollection{pierantonio_property-aware_2018,
	address = {Cham},
	title = {Property-{Aware} {Unit} {Testing} of {UML}-{RT} {Models} in the {Context} of {MDE}},
	volume = {10890},
	isbn = {978-3-319-92996-5 978-3-319-92997-2},
	url = {http://link.springer.com/10.1007/978-3-319-92997-2_10},
	urldate = {2018-10-31},
	booktitle = {Modelling {Foundations} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Ahmadi, Reza and Hili, Nicolas and Dingel, Juergen},
	editor = {Pierantonio, Alfonso and Trujillo, Salvador},
	year = {2018},
	doi = {10.1007/978-3-319-92997-2_10},
	pages = {147--163}
}

@incollection{seehusen_improving_2015,
	address = {Cham},
	title = {Improving {Security} {Testing} with {Usage}-{Based} {Fuzz} {Testing}},
	volume = {9488},
	isbn = {978-3-319-26415-8 978-3-319-26416-5},
	url = {http://link.springer.com/10.1007/978-3-319-26416-5_8},
	urldate = {2018-10-31},
	booktitle = {Risk {Assessment} and {Risk}-{Driven} {Testing}},
	publisher = {Springer International Publishing},
	author = {Schneider, Martin A. and Herbold, Steffen and Wendland, Marc-Florian and Grabowski, Jens},
	editor = {Seehusen, Fredrik and Felderer, Michael and Gromann, Jrgen and Wendland, Marc-Florian},
	year = {2015},
	doi = {10.1007/978-3-319-26416-5_8},
	pages = {110--119}
}

@article{nogueira_test_2014,
	title = {Test generation from state based use case models},
	volume = {26},
	issn = {0934-5043, 1433-299X},
	url = {http://link.springer.com/10.1007/s00165-012-0258-z},
	doi = {10.1007/s00165-012-0258-z},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Formal Aspects of Computing},
	author = {Nogueira, Sidney and Sampaio, Augusto and Mota, Alexandre},
	month = may,
	year = {2014},
	pages = {441--490}
}

@incollection{garcia-alfaro_enforcing_2014,
	address = {Berlin, Heidelberg},
	title = {Enforcing {Input} {Validation} through {Aspect} {Oriented} {Programming}},
	volume = {8247},
	isbn = {978-3-642-54567-2 978-3-642-54568-9},
	url = {http://link.springer.com/10.1007/978-3-642-54568-9_20},
	urldate = {2018-10-31},
	booktitle = {Data {Privacy} {Management} and {Autonomous} {Spontaneous} {Security}},
	publisher = {Springer Berlin Heidelberg},
	author = {Serme, Gabriel and Scholte, Theodoor and de Oliveira, Anderson Santana},
	editor = {Garcia-Alfaro, Joaquin and Lioudakis, Georgios and Cuppens-Boulahia, Nora and Foley, Simon and Fitzgerald, William M.},
	year = {2014},
	doi = {10.1007/978-3-642-54568-9_20},
	pages = {316--332}
}

@article{guldali_torc:_2011,
	title = {{TORC}: test plan optimization by requirements clustering},
	volume = {19},
	issn = {0963-9314, 1573-1367},
	shorttitle = {{TORC}},
	url = {http://link.springer.com/10.1007/s11219-011-9149-4},
	doi = {10.1007/s11219-011-9149-4},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software Quality Journal},
	author = {Gldali, Baris and Funke, Holger and Sauer, Stefan and Engels, Gregor},
	month = dec,
	year = {2011},
	pages = {771--799}
}

@article{liu_identification_2013,
	title = {Identification of generalization refactoring opportunities},
	volume = {20},
	issn = {0928-8910, 1573-7535},
	url = {http://link.springer.com/10.1007/s10515-012-0100-0},
	doi = {10.1007/s10515-012-0100-0},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Automated Software Engineering},
	author = {Liu, Hui and Niu, Zhendong and Ma, Zhiyi and Shao, Weizhong},
	month = mar,
	year = {2013},
	pages = {81--110}
}

@incollection{seehusen_risk-driven_2015,
	address = {Cham},
	title = {Risk-{Driven} {Vulnerability} {Testing}: {Results} from {eHealth} {Experiments} {Using} {Patterns} and {Model}-{Based} {Approach}},
	volume = {9488},
	isbn = {978-3-319-26415-8 978-3-319-26416-5},
	shorttitle = {Risk-{Driven} {Vulnerability} {Testing}},
	url = {http://link.springer.com/10.1007/978-3-319-26416-5_7},
	urldate = {2018-10-31},
	booktitle = {Risk {Assessment} and {Risk}-{Driven} {Testing}},
	publisher = {Springer International Publishing},
	author = {Vernotte, Alexandre and Botea, Cornel and Legeard, Bruno and Molnar, Arthur and Peureux, Fabien},
	editor = {Seehusen, Fredrik and Felderer, Michael and Gromann, Jrgen and Wendland, Marc-Florian},
	year = {2015},
	doi = {10.1007/978-3-319-26416-5_7},
	pages = {93--109}
}

@incollection{hinze_advances_2013,
	address = {Berlin, Heidelberg},
	title = {Advances in {Lazy} {SmallCheck}},
	volume = {8241},
	isbn = {978-3-642-41581-4 978-3-642-41582-1},
	url = {http://link.springer.com/10.1007/978-3-642-41582-1_4},
	urldate = {2018-10-31},
	booktitle = {Implementation and {Application} of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Reich, Jason S. and Naylor, Matthew and Runciman, Colin},
	editor = {Hinze, Ralf},
	year = {2013},
	doi = {10.1007/978-3-642-41582-1_4},
	pages = {53--70}
}

@incollection{cuesta_crunch:_2018,
	address = {Cham},
	title = {Crunch: {Automated} {Assessment} of {Microservice} {Architecture} {Assignments} with {Formative} {Feedback}},
	volume = {11048},
	isbn = {978-3-030-00760-7 978-3-030-00761-4},
	shorttitle = {Crunch},
	url = {http://link.springer.com/10.1007/978-3-030-00761-4_12},
	urldate = {2018-10-31},
	booktitle = {Software {Architecture}},
	publisher = {Springer International Publishing},
	author = {Christensen, Henrik Brbak},
	editor = {Cuesta, Carlos E. and Garlan, David and Prez, Jennifer},
	year = {2018},
	doi = {10.1007/978-3-030-00761-4_12},
	pages = {175--190}
}

@incollection{li_overview_2009,
	address = {Berlin, Heidelberg},
	title = {Overview of {Software} {Processes} and {Software} {Evolution}},
	isbn = {978-3-540-79463-9 978-3-540-79464-6},
	url = {http://link.springer.com/10.1007/978-3-540-79464-6_2},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {An {Approach} to {Modelling} {Software} {Evolution} {Processes}},
	publisher = {Springer Berlin Heidelberg},
	author = {Li, Tong},
	year = {2009},
	doi = {10.1007/978-3-540-79464-6_2},
	pages = {8--33}
}

@incollection{cimatti_validating_2017,
	address = {Cham},
	title = {Validating the {Meta}-{Theory} of {Programming} {Languages} ({Short} {Paper})},
	volume = {10469},
	isbn = {978-3-319-66196-4 978-3-319-66197-1},
	url = {http://link.springer.com/10.1007/978-3-319-66197-1_23},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering} and {Formal} {Methods}},
	publisher = {Springer International Publishing},
	author = {Fachini, Guglielmo and Momigliano, Alberto},
	editor = {Cimatti, Alessandro and Sirjani, Marjan},
	year = {2017},
	doi = {10.1007/978-3-319-66197-1_23},
	pages = {367--374}
}

@article{prout_code_2012,
	title = {Code generation for a family of executable modelling notations},
	volume = {11},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-010-0176-6},
	doi = {10.1007/s10270-010-0176-6},
	language = {en},
	number = {2},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Prout, Adam and Atlee, Joanne M. and Day, Nancy A. and Shaker, Pourya},
	month = may,
	year = {2012},
	pages = {251--272}
}

@incollection{abramowicz_towards_2007,
	address = {Berlin, Heidelberg},
	title = {Towards {Operationalizing} {Strategic} {Alignment} of {IT} by {Usage} of {Software} {Engineering} {Methods}},
	volume = {4439},
	isbn = {978-3-540-72034-8 978-3-540-72035-5},
	url = {http://link.springer.com/10.1007/978-3-540-72035-5_48},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Business {Information} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Tilg, Bernd and Chimiak-Opoka, Joanna and Lenz, Chris and Breu, Ruth},
	editor = {Abramowicz, Witold},
	year = {2007},
	doi = {10.1007/978-3-540-72035-5_48},
	pages = {610--625}
}

@incollection{steffen_issues_2016,
	address = {Cham},
	title = {Issues on {Software} {Quality} {Models} for {Mastering} {Change}},
	volume = {9960},
	isbn = {978-3-319-46507-4 978-3-319-46508-1},
	url = {http://link.springer.com/10.1007/978-3-319-46508-1_12},
	urldate = {2018-10-31},
	booktitle = {Transactions on {Foundations} for {Mastering} {Change} {I}},
	publisher = {Springer International Publishing},
	author = {Felderer, Michael},
	editor = {Steffen, Bernhard},
	year = {2016},
	doi = {10.1007/978-3-319-46508-1_12},
	pages = {225--241}
}

@incollection{hutchison_model_2012,
	address = {Berlin, Heidelberg},
	title = {Model {Transformations} for {Migrating} {Legacy} {Models}: {An} {Industrial} {Case} {Study}},
	volume = {7349},
	isbn = {978-3-642-31490-2 978-3-642-31491-9},
	shorttitle = {Model {Transformations} for {Migrating} {Legacy} {Models}},
	url = {http://link.springer.com/10.1007/978-3-642-31491-9_9},
	urldate = {2018-10-31},
	booktitle = {Modelling {Foundations} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Selim, Gehan M. K. and Wang, Shige and Cordy, James R. and Dingel, Juergen},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Vallecillo, Antonio and Tolvanen, Juha-Pekka and Kindler, Ekkart and Strrle, Harald and Kolovos, Dimitris},
	year = {2012},
	doi = {10.1007/978-3-642-31491-9_9},
	pages = {90--101}
}

@incollection{murgante_ddat:_2011,
	address = {Berlin, Heidelberg},
	title = {{DDAT}: {Data} {Dependency} {Analysis} {Tool} for {Web} {Service} {Business} {Processes}},
	volume = {6786},
	isbn = {978-3-642-21933-7 978-3-642-21934-4},
	shorttitle = {{DDAT}},
	url = {http://link.springer.com/10.1007/978-3-642-21934-4_20},
	urldate = {2018-10-31},
	booktitle = {Computational {Science} and {Its} {Applications} - {ICCSA} 2011},
	publisher = {Springer Berlin Heidelberg},
	author = {Spassov, Ivaylo and Pavlov, Valentin and Petrova-Antonova, Dessislava and Ilieva, Sylvia},
	editor = {Murgante, Beniamino and Gervasi, Osvaldo and Iglesias, Andrs and Taniar, David and Apduhan, Bernady O.},
	year = {2011},
	doi = {10.1007/978-3-642-21934-4_20},
	pages = {232--243}
}

@incollection{giese_8_2010,
	address = {Berlin, Heidelberg},
	title = {8 {UML} for {Software} {Safety} and {Certification}},
	volume = {6100},
	isbn = {978-3-642-16276-3 978-3-642-16277-0},
	url = {http://link.springer.com/10.1007/978-3-642-16277-0_8},
	urldate = {2018-10-31},
	booktitle = {Model-{Based} {Engineering} of {Embedded} {Real}-{Time} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Huhn, Michaela and Hungar, Hardi},
	editor = {Giese, Holger and Karsai, Gabor and Lee, Edward and Rumpe, Bernhard and Schtz, Bernhard},
	year = {2010},
	doi = {10.1007/978-3-642-16277-0_8},
	pages = {201--237}
}

@incollection{hutchison_generating_2004,
	address = {Berlin, Heidelberg},
	title = {Generating {AspectJ} {Programs} with {Meta}-{AspectJ}},
	volume = {3286},
	isbn = {978-3-540-23580-4 978-3-540-30175-2},
	url = {http://link.springer.com/10.1007/978-3-540-30175-2_1},
	urldate = {2018-10-31},
	booktitle = {Generative {Programming} and {Component} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Zook, David and Huang, Shan Shan and Smaragdakis, Yannis},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Karsai, Gabor and Visser, Eelco},
	year = {2004},
	doi = {10.1007/978-3-540-30175-2_1},
	pages = {1--18}
}

@incollection{schurr_code_2008,
	address = {Berlin, Heidelberg},
	title = {Code {Graph} {Transformations} for {Verifiable} {Generation} of {SIMD}-{Parallel} {Assembly} {Code}},
	volume = {5088},
	isbn = {978-3-540-89019-5 978-3-540-89020-1},
	url = {http://link.springer.com/10.1007/978-3-540-89020-1_16},
	urldate = {2018-10-31},
	booktitle = {Applications of {Graph} {Transformations} with {Industrial} {Relevance}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kumar Anand, Christopher and Kahl, Wolfram},
	editor = {Schrr, Andy and Nagl, Manfred and Zndorf, Albert},
	year = {2008},
	doi = {10.1007/978-3-540-89020-1_16},
	pages = {217--232}
}

@incollection{lammel_compilation_2013,
	address = {Berlin, Heidelberg},
	title = {Compilation of {Legacy} {Languages} in the 21st {Century}},
	volume = {7680},
	isbn = {978-3-642-35991-0 978-3-642-35992-7},
	url = {http://link.springer.com/10.1007/978-3-642-35992-7_1},
	urldate = {2018-10-31},
	booktitle = {Generative and {Transformational} {Techniques} in {Software} {Engineering} {IV}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blasband, Darius},
	editor = {Lmmel, Ralf and Saraiva, Joo and Visser, Joost},
	year = {2013},
	doi = {10.1007/978-3-642-35992-7_1},
	pages = {1--54}
}

@incollection{hutchison_automated_2013,
	address = {Berlin, Heidelberg},
	title = {Automated {Analysis} in {Feature} {Modelling} and {Product} {Configuration}},
	volume = {7925},
	isbn = {978-3-642-38976-4 978-3-642-38977-1},
	url = {http://link.springer.com/10.1007/978-3-642-38977-1_11},
	urldate = {2018-10-31},
	booktitle = {Safe and {Secure} {Software} {Reuse}},
	publisher = {Springer Berlin Heidelberg},
	author = {Benavides, David and Felfernig, Alexander and Galindo, Jos A. and Reinfrank, Florian},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Favaro, John and Morisio, Maurizio},
	year = {2013},
	doi = {10.1007/978-3-642-38977-1_11},
	pages = {160--175},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\TKG2XJCY\\Benavides et al. - 2013 - Automated Analysis in Feature Modelling and Produc.pdf:application/pdf}
}

@article{freeman_lifting_2010,
	title = {Lifting transformational models of product lines: a case study},
	volume = {9},
	issn = {1619-1366, 1619-1374},
	shorttitle = {Lifting transformational models of product lines},
	url = {http://link.springer.com/10.1007/s10270-009-0131-6},
	doi = {10.1007/s10270-009-0131-6},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Freeman, Greg and Batory, Don and Lavender, Greg and Sarvela, Jacob Neal},
	month = jun,
	year = {2010},
	pages = {359--373},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\92XJAG2P\\Freeman et al. - 2010 - Lifting transformational models of product lines .pdf:application/pdf}
}

@incollection{kamsties_understanding_2018,
	address = {Cham},
	title = {On the {Understanding} of {BDD} {Scenarios} {Quality}: {Preliminary} {Practitioners} {Opinions}},
	volume = {10753},
	isbn = {978-3-319-77242-4 978-3-319-77243-1},
	shorttitle = {On the {Understanding} of {BDD} {Scenarios} {Quality}},
	url = {http://link.springer.com/10.1007/978-3-319-77243-1_18},
	urldate = {2018-10-31},
	booktitle = {Requirements {Engineering}: {Foundation} for {Software} {Quality}},
	publisher = {Springer International Publishing},
	author = {Oliveira, Gabriel and Marczak, Sabrina},
	editor = {Kamsties, Erik and Horkoff, Jennifer and Dalpiaz, Fabiano},
	year = {2018},
	doi = {10.1007/978-3-319-77243-1_18},
	pages = {290--296}
}

@incollection{gervasi_mobile_2015,
	address = {Cham},
	title = {Mobile {Application} {Verification}: {A} {Systematic} {Mapping} {Study}},
	volume = {9159},
	isbn = {978-3-319-21412-2 978-3-319-21413-9},
	shorttitle = {Mobile {Application} {Verification}},
	url = {http://link.springer.com/10.1007/978-3-319-21413-9_11},
	urldate = {2018-10-31},
	booktitle = {Computational {Science} and {Its} {Applications} -- {ICCSA} 2015},
	publisher = {Springer International Publishing},
	author = {Sahinoglu, Mehmet and Incki, Koray and Aktas, Mehmet S.},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Gavrilova, Marina L. and Rocha, Ana Maria Alves Coutinho and Torre, Carmelo and Taniar, David and Apduhan, Bernady O.},
	year = {2015},
	doi = {10.1007/978-3-319-21413-9_11},
	pages = {147--163}
}

@InCollection{bishop_case_2011,
  author    = {Hills, Mark and Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  title     = {A {Case} of {Visitor} versus {Interpreter} {Pattern}},
  booktitle = {Objects, {Models}, {Components}, {Patterns}},
  publisher = {Springer Berlin Heidelberg},
  year      = {2011},
  editor    = {Bishop, Judith and Vallecillo, Antonio},
  volume    = {6705},
  pages     = {228--243},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-21951-1 978-3-642-21952-8},
  doi       = {10.1007/978-3-642-21952-8_17},
  file      = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\XJ8F36KD\\Hills et al. - 2011 - A Case of Visitor versus Interpreter Pattern.pdf:application/pdf},
  url       = {http://link.springer.com/10.1007/978-3-642-21952-8_17},
  urldate   = {2018-10-31},
}

@incollection{bernardi_dependability_2013,
	address = {Berlin, Heidelberg},
	title = {Dependability {Domain} {Model}},
	isbn = {978-3-642-39511-6 978-3-642-39512-3},
	url = {http://link.springer.com/10.1007/978-3-642-39512-3_4},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Model-{Driven} {Dependability} {Assessment} of {Software} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bernardi, Simona and Merseguer, Jos and Petriu, Dorina Corina},
	collaborator = {Bernardi, Simona and Merseguer, Jos and Petriu, Dorina Corina},
	year = {2013},
	doi = {10.1007/978-3-642-39512-3_4},
	pages = {41--50}
}

@incollection{munch_empirical_2013,
	address = {Berlin, Heidelberg},
	title = {An {Empirical} {Investigation} of the {Component}-{Based} {Performance} {Prediction} {Method} {Palladio}},
	isbn = {978-3-642-37394-7 978-3-642-37395-4},
	url = {http://link.springer.com/10.1007/978-3-642-37395-4_13},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Perspectives on the {Future} of {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Reussner, Ralf and Becker, Steffen and Koziolek, Anne and Koziolek, Heiko},
	editor = {Mnch, Jrgen and Schmid, Klaus},
	year = {2013},
	doi = {10.1007/978-3-642-37395-4_13},
	pages = {191--207}
}

@incollection{noauthor_introducing_2006,
	title = {Introducing {SQLite}},
	isbn = {978-1-59059-673-9},
	url = {http://link.springer.com/10.1007/978-1-4302-0172-4_1},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {The {Definitive} {Guide} to {SQLite}},
	publisher = {Apress},
	year = {2006},
	doi = {10.1007/978-1-4302-0172-4_1},
	pages = {1--16}
}

@incollection{krcmar_sensorcloud:_2014,
	address = {Cham},
	title = {{SensorCloud}: {Towards} the {Interdisciplinary} {Development} of a {Trustworthy} {Platform} for {Globally} {Interconnected} {Sensors} and {Actuators}},
	isbn = {978-3-319-12717-0 978-3-319-12718-7},
	shorttitle = {{SensorCloud}},
	url = {http://link.springer.com/10.1007/978-3-319-12718-7_13},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Trusted {Cloud} {Computing}},
	publisher = {Springer International Publishing},
	author = {Eggert, Michael and Huling, Roger and Henze, Martin and Hermerschmidt, Lars and Hummen, Ren and Kerpen, Daniel and Prez, Antonio Navarro and Rumpe, Bernhard and Thien, Dirk and Wehrle, Klaus},
	editor = {Krcmar, Helmut and Reussner, Ralf and Rumpe, Bernhard},
	year = {2014},
	doi = {10.1007/978-3-319-12718-7_13},
	pages = {203--218},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\B4GX8WYA\\Eggert et al. - 2014 - SensorCloud Towards the Interdisciplinary Develop.pdf:application/pdf}
}

@incollection{kramer_service_2007,
	address = {Berlin, Heidelberg},
	title = {Service {Design} {Process} for {Reusable} {Services}: {Financial} {Services} {Case} {Study}},
	volume = {4749},
	isbn = {978-3-540-74973-8 978-3-540-74974-5},
	shorttitle = {Service {Design} {Process} for {Reusable} {Services}},
	url = {http://link.springer.com/10.1007/978-3-540-74974-5_56},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Service-{Oriented} {Computing}  {ICSOC} 2007},
	publisher = {Springer Berlin Heidelberg},
	author = {Erradi, Abdelkarim and Kulkarni, Naveen and Maheshwari, Piyush},
	editor = {Krmer, Bernd J. and Lin, Kwei-Jay and Narasimhan, Priya},
	year = {2007},
	doi = {10.1007/978-3-540-74974-5_56},
	pages = {606--617}
}

@incollection{reussner_trustworthy_2006,
	address = {Berlin, Heidelberg},
	title = {Trustworthy {Instantiation} of {Frameworks}},
	volume = {3938},
	isbn = {978-3-540-35800-8 978-3-540-35833-6},
	url = {http://link.springer.com/10.1007/11786160_9},
	urldate = {2018-10-31},
	booktitle = {Architecting {Systems} with {Trustworthy} {Components}},
	publisher = {Springer Berlin Heidelberg},
	author = {Amann, Uwe and Bartho, Andreas and Hartmann, Falk and Savga, Ilie and Wittek, Barbara},
	editor = {Reussner, Ralf H. and Stafford, Judith A. and Szyperski, Clemens A.},
	year = {2006},
	doi = {10.1007/11786160_9},
	pages = {152--168}
}

@incollection{doss_specification-based_2015,
	address = {Cham},
	title = {Specification-{Based} {Intrusion} {Detection} {Using} {Sequence} {Alignment} and {Data} {Clustering}},
	volume = {523},
	isbn = {978-3-319-19209-3 978-3-319-19210-9},
	url = {http://link.springer.com/10.1007/978-3-319-19210-9_3},
	urldate = {2018-10-31},
	booktitle = {Future {Network} {Systems} and {Security}},
	publisher = {Springer International Publishing},
	author = {Amadou Kountch, Djibrilla and Gombault, Sylvain},
	editor = {Doss, Robin and Piramuthu, Selwyn and Zhou, Wei},
	year = {2015},
	doi = {10.1007/978-3-319-19210-9_3},
	pages = {31--46}
}

@incollection{cossentino_adelfe_2014,
	address = {Berlin, Heidelberg},
	title = {{ADELFE} 2.0},
	isbn = {978-3-642-39974-9 978-3-642-39975-6},
	url = {http://link.springer.com/10.1007/978-3-642-39975-6_3},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Handbook on {Agent}-{Oriented} {Design} {Processes}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bonjean, N. and Mefteh, W. and Gleizes, M. P. and Maurel, C. and Migeon, F.},
	editor = {Cossentino, Massimo and Hilaire, Vincent and Molesini, Ambra and Seidita, Valeria},
	year = {2014},
	doi = {10.1007/978-3-642-39975-6_3},
	pages = {19--63}
}

@incollection{hendren_design_2008,
	address = {Berlin, Heidelberg},
	title = {Design {Choices} in a {Compiler} {Course} or {How} to {Make} {Undergraduates} {Love} {Formal} {Notation}},
	volume = {4959},
	isbn = {978-3-540-78790-7 978-3-540-78791-4},
	url = {http://link.springer.com/10.1007/978-3-540-78791-4_1},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Compiler {Construction}},
	publisher = {Springer Berlin Heidelberg},
	author = {Schwartzbach, Michael I.},
	editor = {Hendren, Laurie},
	year = {2008},
	doi = {10.1007/978-3-540-78791-4_1},
	pages = {1--15},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\A67WTFAS\\Schwartzbach - 2008 - Design Choices in a Compiler Course or How to Make.pdf:application/pdf}
}

@incollection{el_abbadi_comparative_2017,
	address = {Cham},
	title = {A {Comparative} {Study} of {Software} {Testing} {Techniques}},
	volume = {10299},
	isbn = {978-3-319-59646-4 978-3-319-59647-1},
	url = {http://link.springer.com/10.1007/978-3-319-59647-1_27},
	urldate = {2018-10-31},
	booktitle = {Networked {Systems}},
	publisher = {Springer International Publishing},
	author = {Atifi, Meriem and Mamouni, Abdelaziz and Marzak, Abdelaziz},
	editor = {El Abbadi, Amr and Garbinato, Benot},
	year = {2017},
	doi = {10.1007/978-3-319-59647-1_27},
	pages = {373--390}
}

@incollection{moreno-diaz_separation_2009,
	address = {Berlin, Heidelberg},
	title = {Separation of {Transitions}, {Actions}, and {Exceptions} in {Model}-{Based} {Testing}},
	volume = {5717},
	isbn = {978-3-642-04771-8 978-3-642-04772-5},
	url = {http://link.springer.com/10.1007/978-3-642-04772-5_37},
	urldate = {2018-10-31},
	booktitle = {Computer {Aided} {Systems} {Theory} - {EUROCAST} 2009},
	publisher = {Springer Berlin Heidelberg},
	author = {Artho, Cyrille},
	editor = {Moreno-Daz, Roberto and Pichler, Franz and Quesada-Arencibia, Alexis},
	year = {2009},
	doi = {10.1007/978-3-642-04772-5_37},
	pages = {279--286},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\MX26QBCK\\Artho - 2009 - Separation of Transitions, Actions, and Exceptions.pdf:application/pdf}
}

@incollection{itsykson_survey_2018,
	address = {Cham},
	title = {A {Survey} on {Model}-{Based} {Testing} {Tools} for {Test} {Case} {Generation}},
	volume = {779},
	isbn = {978-3-319-71733-3 978-3-319-71734-0},
	url = {http://link.springer.com/10.1007/978-3-319-71734-0_7},
	urldate = {2018-10-31},
	booktitle = {Tools and {Methods} of {Program} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Li, Wenbin and Le Gall, Franck and Spaseski, Naum},
	editor = {Itsykson, Vladimir and Scedrov, Andre and Zakharov, Victor},
	year = {2018},
	doi = {10.1007/978-3-319-71734-0_7},
	pages = {77--89}
}

@incollection{margaria_towards_2014,
	address = {Berlin, Heidelberg},
	title = {Towards {Performance}-{Aware} {Engineering} of {Autonomic} {Component} {Ensembles}},
	volume = {8802},
	isbn = {978-3-662-45233-2 978-3-662-45234-9},
	url = {http://link.springer.com/10.1007/978-3-662-45234-9_10},
	urldate = {2018-10-31},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}. {Technologies} for {Mastering} {Change}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bure, Tom and Hork, Vojtch and Kit, Micha and Marek, Luk and Tma, Petr},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2014},
	doi = {10.1007/978-3-662-45234-9_10},
	pages = {131--146},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\YZT3VWTW\\Bure et al. - 2014 - Towards Performance-Aware Engineering of Autonomic.pdf:application/pdf}
}

@incollection{cuesta_enabling_2018,
	address = {Cham},
	title = {Enabling {Continuous} {Software} {Engineering} for {Embedded} {Systems} {Architectures} with {Virtual} {Prototypes}},
	volume = {11048},
	isbn = {978-3-030-00760-7 978-3-030-00761-4},
	url = {http://link.springer.com/10.1007/978-3-030-00761-4_8},
	urldate = {2018-10-31},
	booktitle = {Software {Architecture}},
	publisher = {Springer International Publishing},
	author = {Antonino, Pablo Oliveira and Jung, Matthias and Morgenstern, Andreas and Fanacht, Florian and Bauer, Thomas and Bachorek, Adam and Kuhn, Thomas and Nakagawa, Elisa Yumi},
	editor = {Cuesta, Carlos E. and Garlan, David and Prez, Jennifer},
	year = {2018},
	doi = {10.1007/978-3-030-00761-4_8},
	pages = {115--130}
}

@incollection{hutchison_run-time_2014,
	address = {Berlin, Heidelberg},
	title = {Run-{Time} {Assertion} {Checking} of {Data}- and {Protocol}-{Oriented} {Properties} of {Java} {Programs}: {An} {Industrial} {Case} {Study}},
	volume = {8400},
	isbn = {978-3-642-55098-0 978-3-642-55099-7},
	shorttitle = {Run-{Time} {Assertion} {Checking} of {Data}- and {Protocol}-{Oriented} {Properties} of {Java} {Programs}},
	url = {http://link.springer.com/10.1007/978-3-642-55099-7_1},
	urldate = {2018-10-31},
	booktitle = {Transactions on {Aspect}-{Oriented} {Software} {Development} {XI}},
	publisher = {Springer Berlin Heidelberg},
	author = {de Boer, Frank S. and de Gouw, Stijn and Johnsen, Einar Broch and Kohn, Andreas and Wong, Peter Y. H.},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Chiba, Shigeru and Tanter, ric and Bodden, Eric and Maoz, Shahar and Kienzle, Jrg},
	year = {2014},
	doi = {10.1007/978-3-642-55099-7_1},
	pages = {1--26},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\MB2LWWTT\\de Boer et al. - 2014 - Run-Time Assertion Checking of Data- and Protocol-.pdf:application/pdf}
}

@incollection{hutchison_formal_2006,
	address = {Berlin, Heidelberg},
	title = {Formal {Test} {Generation} from {UML} {Models}},
	volume = {4028},
	isbn = {978-3-540-36821-2 978-3-540-36823-6},
	url = {http://link.springer.com/10.1007/11808107_7},
	urldate = {2018-10-31},
	booktitle = {Dependable {Systems}: {Software}, {Computing}, {Networks}},
	publisher = {Springer Berlin Heidelberg},
	author = {Buchs, Didier and Pedro, Luis and Lcio, Levi},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Kohlas, Jrg and Meyer, Bertrand and Schiper, Andr},
	year = {2006},
	doi = {10.1007/11808107_7},
	pages = {145--171},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\2AUWPF2E\\Buchs et al. - 2006 - Formal Test Generation from UML Models.pdf:application/pdf}
}

@incollection{hutchison_catching_2006,
	address = {Berlin, Heidelberg},
	title = {Catching and {Identifying} {Bugs} in {Register} {Allocation}},
	volume = {4134},
	isbn = {978-3-540-37756-6 978-3-540-37758-0},
	url = {http://link.springer.com/10.1007/11823230_19},
	urldate = {2018-10-31},
	booktitle = {Static {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Huang, Yuqiang and Childers, Bruce R. and Soffa, Mary Lou},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Yi, Kwangkeun},
	year = {2006},
	doi = {10.1007/11823230_19},
	pages = {281--300},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\JV79K5FR\\Huang et al. - 2006 - Catching and Identifying Bugs in Register Allocati.pdf:application/pdf}
}

@incollection{milazzo_spatio-temporal_2016,
	address = {Cham},
	title = {Spatio-{Temporal} {Models} for {Formal} {Analysis} and {Property}-{Based} {Testing}},
	volume = {9946},
	isbn = {978-3-319-50229-8 978-3-319-50230-4},
	url = {http://link.springer.com/10.1007/978-3-319-50230-4_14},
	urldate = {2018-10-31},
	booktitle = {Software {Technologies}: {Applications} and {Foundations}},
	publisher = {Springer International Publishing},
	author = {Alzahrani, Nasser and Spichkova, Maria and Blech, Jan Olaf},
	editor = {Milazzo, Paolo and Varr, Dniel and Wimmer, Manuel},
	year = {2016},
	doi = {10.1007/978-3-319-50230-4_14},
	pages = {196--206},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\LWPMQVDX\\Alzahrani et al. - 2016 - Spatio-Temporal Models for Formal Analysis and Pro.pdf:application/pdf}
}

@incollection{hutchison_experiences_2007,
	address = {Berlin, Heidelberg},
	title = {Experiences in {Deploying} {Model}-{Driven} {Engineering}},
	volume = {4745},
	isbn = {978-3-540-74983-7 978-3-540-74984-4},
	url = {http://link.springer.com/10.1007/978-3-540-74984-4_3},
	urldate = {2018-10-31},
	booktitle = {{SDL} 2007: {Design} for {Dependable} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Weigert, Thomas and Weil, Frank and Marth, Kevin and Baker, Paul and Jervis, Clive and Dietz, Paul and Gui, Yexuan and van den Berg, Aswin and Fleer, Kim and Nelson, David and Wells, Michael and Mastenbrook, Brian},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gaudin, Emmanuel and Najm, Elie and Reed, Rick},
	year = {2007},
	doi = {10.1007/978-3-540-74984-4_3},
	pages = {35--53}
}

@article{halin_test_2018,
	title = {Test them all, is it worth it? {Assessing} configuration sampling on the {JHipster} {Web} development stack},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Test them all, is it worth it?},
	url = {http://link.springer.com/10.1007/s10664-018-9635-4},
	doi = {10.1007/s10664-018-9635-4},
	language = {en},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Baudry, Benoit},
	month = jul,
	year = {2018},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\95U8E6JN\\Halin et al. - 2018 - Test them all, is it worth it Assessing configura.pdf:application/pdf}
}

@article{iqbal_applying_2015,
	title = {Applying {UML}/{MARTE} on industrial projects: challenges, experiences, and guidelines},
	volume = {14},
	issn = {1619-1366, 1619-1374},
	shorttitle = {Applying {UML}/{MARTE} on industrial projects},
	url = {http://link.springer.com/10.1007/s10270-014-0405-5},
	doi = {10.1007/s10270-014-0405-5},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Iqbal, Muhammad Zohaib and Ali, Shaukat and Yue, Tao and Briand, Lionel},
	month = oct,
	year = {2015},
	pages = {1367--1385}
}

@incollection{hutchison_automatic_2010,
	address = {Berlin, Heidelberg},
	title = {Automatic {Test}-{Case} {Generation}},
	volume = {6153},
	isbn = {978-3-642-14334-2 978-3-642-14335-9},
	url = {http://link.springer.com/10.1007/978-3-642-14335-9_3},
	urldate = {2018-10-31},
	booktitle = {Testing {Techniques} in {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Machado, Patrcia and Sampaio, Augusto},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Borba, Paulo and Cavalcanti, Ana and Sampaio, Augusto and Woodcook, Jim},
	year = {2010},
	doi = {10.1007/978-3-642-14335-9_3},
	pages = {59--103}
}

@incollection{dumke_measuring_2008,
	address = {Berlin, Heidelberg},
	title = {Measuring 75 {Million} {Lines} of {Code}},
	volume = {5338},
	isbn = {978-3-540-89402-5 978-3-540-89403-2},
	url = {http://link.springer.com/10.1007/978-3-540-89403-2_23},
	urldate = {2018-10-31},
	booktitle = {Software {Process} and {Product} {Measurement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sneed, Harry M.},
	editor = {Dumke, Reiner R. and Braungarten, Ren and Bren, Gnter and Abran, Alain and Cuadrado-Gallego, Juan J.},
	year = {2008},
	doi = {10.1007/978-3-540-89403-2_23},
	pages = {271--286}
}

@InCollection{bishop_case_2011,
  author    = {Hills, Mark and Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  title     = {A {Case} of {Visitor} versus {Interpreter} {Pattern}},
  booktitle = {Objects, {Models}, {Components}, {Patterns}},
  publisher = {Springer Berlin Heidelberg},
  year      = {2011},
  editor    = {Bishop, Judith and Vallecillo, Antonio},
  volume    = {6705},
  pages     = {228--243},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-21951-1 978-3-642-21952-8},
  doi       = {10.1007/978-3-642-21952-8_17},
  file      = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\XJ8F36KD\\Hills et al. - 2011 - A Case of Visitor versus Interpreter Pattern.pdf:application/pdf},
  url       = {http://link.springer.com/10.1007/978-3-642-21952-8_17},
  urldate   = {2018-10-31},
}

@incollection{hutchison_prologcheck_2014,
	address = {Cham},
	title = {{PrologCheck}  {Property}-{Based} {Testing} in {Prolog}},
	volume = {8475},
	isbn = {978-3-319-07150-3 978-3-319-07151-0},
	url = {http://link.springer.com/10.1007/978-3-319-07151-0_1},
	urldate = {2018-10-31},
	booktitle = {Functional and {Logic} {Programming}},
	publisher = {Springer International Publishing},
	author = {Amaral, Cludio and Florido, Mrio and Santos Costa, Vtor},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Codish, Michael and Sumii, Eijiro},
	year = {2014},
	doi = {10.1007/978-3-319-07151-0_1},
	pages = {1--17}
}

@incollection{gaedke_higher_2009,
	address = {Berlin, Heidelberg},
	title = {A {Higher} {Order} {Generative} {Framework} for {Weaving} {Traceability} {Links} into a {Code} {Generator} for {Web} {Application} {Testing}},
	volume = {5648},
	isbn = {978-3-642-02817-5 978-3-642-02818-2},
	url = {http://link.springer.com/10.1007/978-3-642-02818-2_28},
	urldate = {2018-10-31},
	booktitle = {Web {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fraternali, Piero and Tisi, Massimo},
	editor = {Gaedke, Martin and Grossniklaus, Michael and Daz, Oscar},
	year = {2009},
	doi = {10.1007/978-3-642-02818-2_28},
	pages = {340--354},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\WH44W3W7\\Fraternali e Tisi - 2009 - A Higher Order Generative Framework for Weaving Tr.pdf:application/pdf}
}

@incollection{hinchey_service-orientation:_2012,
	address = {London},
	title = {Service-{Orientation}: {Conquering} {Complexity} with {XMDD}},
	isbn = {978-1-4471-2296-8 978-1-4471-2297-5},
	shorttitle = {Service-{Orientation}},
	url = {http://link.springer.com/10.1007/978-1-4471-2297-5_10},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Conquering {Complexity}},
	publisher = {Springer London},
	author = {Margaria, Tiziana and Steffen, Bernhard},
	editor = {Hinchey, Mike and Coyle, Lorcan},
	year = {2012},
	doi = {10.1007/978-1-4471-2297-5_10},
	pages = {217--236}
}

@incollection{galmiche_exploring_2018,
	address = {Cham},
	title = {Exploring {Approximations} for {Floating}-{Point} {Arithmetic} {Using} {UppSAT}},
	volume = {10900},
	isbn = {978-3-319-94204-9 978-3-319-94205-6},
	url = {http://link.springer.com/10.1007/978-3-319-94205-6_17},
	urldate = {2018-10-31},
	booktitle = {Automated {Reasoning}},
	publisher = {Springer International Publishing},
	author = {Zelji, Aleksandar and Backeman, Peter and Wintersteiger, Christoph M. and Rmmer, Philipp},
	editor = {Galmiche, Didier and Schulz, Stephan and Sebastiani, Roberto},
	year = {2018},
	doi = {10.1007/978-3-319-94205-6_17},
	pages = {246--262},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\FDPR6CQE\\Zelji et al. - 2018 - Exploring Approximations for Floating-Point Arithm.pdf:application/pdf}
}

@incollection{hutchison_linking_2010,
	address = {Berlin, Heidelberg},
	title = {Linking {Feature} {Models} to {Code} {Artifacts} {Using} {Executable} {Acceptance} {Tests}},
	volume = {6287},
	isbn = {978-3-642-15578-9 978-3-642-15579-6},
	url = {http://link.springer.com/10.1007/978-3-642-15579-6_15},
	urldate = {2018-10-31},
	booktitle = {Software {Product} {Lines}: {Going} {Beyond}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ghanam, Yaser and Maurer, Frank},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bosch, Jan and Lee, Jaejoon},
	year = {2010},
	doi = {10.1007/978-3-642-15579-6_15},
	pages = {211--225},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\LHS2X3EY\\Ghanam e Maurer - 2010 - Linking Feature Models to Code Artifacts Using Exe.pdf:application/pdf}
}

@incollection{bernardo_combining_2014,
	address = {Cham},
	title = {Combining {Monitoring} with {Run}-{Time} {Assertion} {Checking}},
	volume = {8483},
	isbn = {978-3-319-07316-3 978-3-319-07317-0},
	url = {http://link.springer.com/10.1007/978-3-319-07317-0_6},
	urldate = {2018-10-31},
	booktitle = {Formal {Methods} for {Executable} {Software} {Models}},
	publisher = {Springer International Publishing},
	author = {de Boer, Frank S. and de Gouw, Stijn},
	editor = {Bernardo, Marco and Damiani, Ferruccio and Hhnle, Reiner and Johnsen, Einar Broch and Schaefer, Ina},
	year = {2014},
	doi = {10.1007/978-3-319-07317-0_6},
	pages = {217--262},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\7XC4G2LT\\de Boer e de Gouw - 2014 - Combining Monitoring with Run-Time Assertion Check.pdf:application/pdf}
}

@article{bernardi_dependability_2011,
	title = {A dependability profile within {MARTE}},
	volume = {10},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-009-0128-1},
	doi = {10.1007/s10270-009-0128-1},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Bernardi, Simona and Merseguer, Jos and Petriu, Dorina C.},
	month = jul,
	year = {2011},
	pages = {313--336},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\IGKAAFEU\\Bernardi et al. - 2011 - A dependability profile within MARTE.pdf:application/pdf}
}

@incollection{dams_integrated_2010,
	address = {Berlin, Heidelberg},
	title = {Integrated and {Automated} {Abstract} {Interpretation}, {Verification} and {Testing} of {C}/{C}++ {Modules}},
	volume = {5930},
	isbn = {978-3-642-11511-0 978-3-642-11512-7},
	url = {http://link.springer.com/10.1007/978-3-642-11512-7_18},
	urldate = {2018-10-31},
	booktitle = {Concurrency, {Compositionality}, and {Correctness}},
	publisher = {Springer Berlin Heidelberg},
	author = {Peleska, Jan},
	editor = {Dams, Dennis and Hannemann, Ulrich and Steffen, Martin},
	year = {2010},
	doi = {10.1007/978-3-642-11512-7_18},
	pages = {277--299},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\MVADY4CV\\Peleska - 2010 - Integrated and Automated Abstract Interpretation, .pdf:application/pdf}
}

@incollection{tryfonas_security_2016,
	address = {Cham},
	title = {Security {Middleware} {Programming} {Using} {P}4},
	volume = {9750},
	isbn = {978-3-319-39380-3 978-3-319-39381-0},
	url = {http://link.springer.com/10.1007/978-3-319-39381-0_25},
	urldate = {2018-10-31},
	booktitle = {Human {Aspects} of {Information} {Security}, {Privacy}, and {Trust}},
	publisher = {Springer International Publishing},
	author = {Vrs, Pter and Kiss, Attila},
	editor = {Tryfonas, Theo},
	year = {2016},
	doi = {10.1007/978-3-319-39381-0_25},
	pages = {277--287}
}

@incollection{hutchison_smock_2013,
	address = {Berlin, Heidelberg},
	title = {{SMock}  {A} {Test} {Platform} for {Monitoring} {Tools}},
	volume = {8174},
	isbn = {978-3-642-40786-4 978-3-642-40787-1},
	url = {http://link.springer.com/10.1007/978-3-642-40787-1_24},
	urldate = {2018-10-31},
	booktitle = {Runtime {Verification}},
	publisher = {Springer Berlin Heidelberg},
	author = {Colombo, Christian and Mizzi, Ruth and Pace, Gordon J.},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Legay, Axel and Bensalem, Saddek},
	year = {2013},
	doi = {10.1007/978-3-642-40787-1_24},
	pages = {352--357}
}

@incollection{mandler_acumen:_2016,
	address = {Cham},
	title = {Acumen: {An} {Open}-{Source} {Testbed} for {Cyber}-{Physical} {Systems} {Research}},
	volume = {169},
	isbn = {978-3-319-47062-7 978-3-319-47063-4},
	shorttitle = {Acumen},
	url = {http://link.springer.com/10.1007/978-3-319-47063-4_11},
	urldate = {2018-10-31},
	booktitle = {Internet of {Things}. {IoT} {Infrastructures}},
	publisher = {Springer International Publishing},
	author = {Taha, Walid and Duracz, Adam and Zeng, Yingfu and Atkinson, Kevin and Bartha, Ferenc A. and Brauner, Paul and Duracz, Jan and Xu, Fei and Cartwright, Robert and Konen, Michal and Moggi, Eugenio and Masood, Jawad and Andreasson, Pererik and Inoue, Jun and SantAnna, Anita and Philippsen, Roland and Chapoutot, Alexandre and OMalley, Marcia and Ames, Aaron and Gaspes, Veronica and Hvatum, Lise and Mehta, Shyam and Eriksson, Henrik and Grante, Christian},
	editor = {Mandler, Benny and Marquez-Barja, Johann and Mitre Campista, Miguel Elias and Cagov, Dagmar and Chaouchi, Hakima and Zeadally, Sherali and Badra, Mohamad and Giordano, Stefano and Fazio, Maria and Somov, Andrey and Vieriu, Radu-Laurentiu},
	year = {2016},
	doi = {10.1007/978-3-319-47063-4_11},
	pages = {118--130},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\FZBZVXE9\\Taha et al. - 2016 - Acumen An Open-Source Testbed for Cyber-Physical .pdf:application/pdf}
}

@incollection{hutchison_architecting_2010,
	address = {Berlin, Heidelberg},
	title = {Architecting {Dependable} {Systems} {Using} {Reflective} {Computing}: {Lessons} {Learnt} and {Some} {Challenges}},
	volume = {6420},
	isbn = {978-3-642-17244-1 978-3-642-17245-8},
	shorttitle = {Architecting {Dependable} {Systems} {Using} {Reflective} {Computing}},
	url = {http://link.springer.com/10.1007/978-3-642-17245-8_12},
	urldate = {2018-10-31},
	booktitle = {Architecting {Dependable} {Systems} {VII}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fabre, Jean-Charles},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Casimiro, Antonio and de Lemos, Rogrio and Gacek, Cristina},
	year = {2010},
	doi = {10.1007/978-3-642-17245-8_12},
	pages = {273--296}
}

@incollection{suzuki_model-based_2008,
	address = {Berlin, Heidelberg},
	title = {Model-{Based} {Generation} of {Testbeds} for {Web} {Services}},
	volume = {5047},
	isbn = {978-3-540-68514-2 978-3-540-68524-1},
	url = {http://link.springer.com/10.1007/978-3-540-68524-1_19},
	urldate = {2018-10-31},
	booktitle = {Testing of {Software} and {Communicating} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bertolino, Antonia and De Angelis, Guglielmo and Frantzen, Lars and Polini, Andrea},
	editor = {Suzuki, Kenji and Higashino, Teruo and Ulrich, Andreas and Hasegawa, Toru},
	year = {2008},
	doi = {10.1007/978-3-540-68524-1_19},
	pages = {266--282},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\PRUUE65M\\Bertolino et al. - 2008 - Model-Based Generation of Testbeds for Web Service.pdf:application/pdf}
}

@article{gario_fail-safe_2018,
	title = {Fail-safe testing of safety-critical systems: a case study and efficiency analysis},
	volume = {26},
	issn = {0963-9314, 1573-1367},
	shorttitle = {Fail-safe testing of safety-critical systems},
	url = {http://link.springer.com/10.1007/s11219-015-9283-5},
	doi = {10.1007/s11219-015-9283-5},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Software Quality Journal},
	author = {Gario, Ahmed and Andrews, Anneliese and Hagerman, Seana},
	month = mar,
	year = {2018},
	pages = {3--48}
}

@article{he_template-based_2017,
	title = {Template-based model generation},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-017-0634-5},
	doi = {10.1007/s10270-017-0634-5},
	language = {en},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {He, Xiao and Zhang, Tian and Pan, Minxue and Ma, Zhiyi and Hu, Chang-Jun},
	month = nov,
	year = {2017}
}

@incollection{murgante_inferring_2014,
	address = {Cham},
	title = {Inferring {User} {Interface} {Patterns} from {Execution} {Traces} of {Web} {Applications}},
	volume = {8583},
	isbn = {978-3-319-09155-6 978-3-319-09156-3},
	url = {http://link.springer.com/10.1007/978-3-319-09156-3_23},
	urldate = {2018-10-31},
	booktitle = {Computational {Science} and {Its} {Applications}  {ICCSA} 2014},
	publisher = {Springer International Publishing},
	author = {Nabuco, Miguel and Paiva, Ana C. R. and Faria, Joo Pascoal},
	editor = {Murgante, Beniamino and Misra, Sanjay and Rocha, Ana Maria A. C. and Torre, Carmelo and Rocha, Jorge Gustavo and Falco, Maria Irene and Taniar, David and Apduhan, Bernady O. and Gervasi, Osvaldo},
	year = {2014},
	doi = {10.1007/978-3-319-09156-3_23},
	pages = {311--326}
}

@incollection{de_lucia_plastic_2009,
	address = {Berlin, Heidelberg},
	title = {The {PLASTIC} {Framework} and {Tools} for {Testing} {Service}-{Oriented} {Applications}},
	volume = {5413},
	isbn = {978-3-540-95887-1 978-3-540-95888-8},
	url = {http://link.springer.com/10.1007/978-3-540-95888-8_5},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bertolino, Antonia and De Angelis, Guglielmo and Frantzen, Lars and Polini, Andrea},
	editor = {De Lucia, Andrea and Ferrucci, Filomena},
	year = {2009},
	doi = {10.1007/978-3-540-95888-8_5},
	pages = {106--139},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\Q7PMBPBX\\Bertolino et al. - 2009 - The PLASTIC Framework and Tools for Testing Servic.pdf:application/pdf}
}

@article{sprinkle_model-based_2009,
	title = {Model-based design: a report from the trenches of the {DARPA} {Urban} {Challenge}},
	volume = {8},
	issn = {1619-1366, 1619-1374},
	shorttitle = {Model-based design},
	url = {http://link.springer.com/10.1007/s10270-009-0116-5},
	doi = {10.1007/s10270-009-0116-5},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Sprinkle, Jonathan and Eklund, J. Mikael and Gonzalez, Humberto and Grtli, Esten Ingar and Upcroft, Ben and Makarenko, Alex and Uther, Will and Moser, Michael and Fitch, Robert and Durrant-Whyte, Hugh and Sastry, S. Shankar},
	month = sep,
	year = {2009},
	pages = {551--566},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\I68SJHVR\\Sprinkle et al. - 2009 - Model-based design a report from the trenches of .pdf:application/pdf}
}

@incollection{kuhne_validation_2007,
	address = {Berlin, Heidelberg},
	title = {Validation of {Model} {Transformations}  {First} {Experiences} {Using} a {White} {Box} {Approach}},
	volume = {4364},
	isbn = {978-3-540-69488-5 978-3-540-69489-2},
	url = {http://link.springer.com/10.1007/978-3-540-69489-2_24},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Models in {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kster, Jochen M. and Abd-El-Razik, Mohamed},
	editor = {Khne, Thomas},
	year = {2007},
	doi = {10.1007/978-3-540-69489-2_24},
	pages = {193--204}
}

@incollection{hutchison_hermes:_2005,
	address = {Berlin, Heidelberg},
	title = {Hermes: {Agent}-{Based} {Middleware} for {Mobile} {Computing}},
	volume = {3465},
	isbn = {978-3-540-25697-7 978-3-540-32021-0},
	shorttitle = {Hermes},
	url = {http://link.springer.com/10.1007/11419822_8},
	urldate = {2018-10-31},
	booktitle = {Formal {Methods} for {Mobile} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Corradini, Flavio and Merelli, Emanuela},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Bernardo, Marco and Bogliolo, Alessandro},
	year = {2005},
	doi = {10.1007/11419822_8},
	pages = {234--270},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\LSNDLX9L\\Corradini e Merelli - 2005 - Hermes Agent-Based Middleware for Mobile Computin.pdf:application/pdf}
}

@incollection{whittle_vision_2011,
	address = {Berlin, Heidelberg},
	title = {Vision {Paper}: {Towards} {Model}-{Based} {Energy} {Testing}},
	volume = {6981},
	isbn = {978-3-642-24484-1 978-3-642-24485-8},
	shorttitle = {Vision {Paper}},
	url = {http://link.springer.com/10.1007/978-3-642-24485-8_35},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wilke, Claas and Gtz, Sebastian and Reimann, Jan and Amann, Uwe},
	editor = {Whittle, Jon and Clark, Tony and Khne, Thomas},
	year = {2011},
	doi = {10.1007/978-3-642-24485-8_35},
	pages = {480--489}
}

@incollection{bennaceur_active_2018,
	address = {Cham},
	title = {Active {Automata} {Learning} in {Practice}: {An} {Annotated} {Bibliography} of the {Years} 2011 to 2016},
	volume = {11026},
	isbn = {978-3-319-96561-1 978-3-319-96562-8},
	shorttitle = {Active {Automata} {Learning} in {Practice}},
	url = {http://link.springer.com/10.1007/978-3-319-96562-8_5},
	urldate = {2018-10-31},
	booktitle = {Machine {Learning} for {Dynamic} {Software} {Analysis}: {Potentials} and {Limits}},
	publisher = {Springer International Publishing},
	author = {Howar, Falk and Steffen, Bernhard},
	editor = {Bennaceur, Amel and Hhnle, Reiner and Meinke, Karl},
	year = {2018},
	doi = {10.1007/978-3-319-96562-8_5},
	pages = {123--148}
}

@incollection{wirsing_engineering_2008,
	address = {Berlin, Heidelberg},
	title = {Engineering of {Software}-{Intensive} {Systems}: {State} of the {Art} and {Research} {Challenges}},
	volume = {5380},
	isbn = {978-3-540-89436-0 978-3-540-89437-7},
	shorttitle = {Engineering of {Software}-{Intensive} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-540-89437-7_1},
	urldate = {2018-10-31},
	booktitle = {Software-{Intensive} {Systems} and {New} {Computing} {Paradigms}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hlzl, Matthias and Rauschmayer, Axel and Wirsing, Martin},
	editor = {Wirsing, Martin and Bantre, Jean-Pierre and Hlzl, Matthias and Rauschmayer, Axel},
	year = {2008},
	doi = {10.1007/978-3-540-89437-7_1},
	pages = {1--44}
}

@incollection{hutchison_modularity_2014,
	address = {Berlin, Heidelberg},
	title = {Modularity and {Dynamic} {Adaptation} of {Flexibly} {Secure} {Systems}: {Model}-{Driven} {Adaptive} {Delegation} in {Access} {Control} {Management}},
	volume = {8400},
	isbn = {978-3-642-55098-0 978-3-642-55099-7},
	shorttitle = {Modularity and {Dynamic} {Adaptation} of {Flexibly} {Secure} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-642-55099-7_4},
	urldate = {2018-10-31},
	booktitle = {Transactions on {Aspect}-{Oriented} {Software} {Development} {XI}},
	publisher = {Springer Berlin Heidelberg},
	author = {Nguyen, Phu H. and Nain, Gregory and Klein, Jacques and Mouelhi, Tejeddine and Le Traon, Yves},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Chiba, Shigeru and Tanter, ric and Bodden, Eric and Maoz, Shahar and Kienzle, Jrg},
	year = {2014},
	doi = {10.1007/978-3-642-55099-7_4},
	pages = {109--144},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\SKRHWN2S\\Nguyen et al. - 2014 - Modularity and Dynamic Adaptation of Flexibly Secu.pdf:application/pdf}
}

@incollection{huttermann_introducing_2012,
	address = {Berkeley, CA},
	title = {Introducing {DevOps}},
	isbn = {978-1-4302-4569-8 978-1-4302-4570-4},
	url = {http://link.springer.com/10.1007/978-1-4302-4570-4_2},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {{DevOps} for {Developers}},
	publisher = {Apress},
	author = {Httermann, Michael},
	collaborator = {Httermann, Michael},
	year = {2012},
	doi = {10.1007/978-1-4302-4570-4_2},
	pages = {15--31}
}

@incollection{fujita_modeling_2015,
	address = {Cham},
	title = {Modeling {Tools} for {Social} {Coding}},
	volume = {532},
	isbn = {978-3-319-22688-0 978-3-319-22689-7},
	url = {http://link.springer.com/10.1007/978-3-319-22689-7_31},
	urldate = {2018-10-31},
	booktitle = {Intelligent {Software} {Methodologies}, {Tools} and {Techniques}},
	publisher = {Springer International Publishing},
	author = {Watanabe, Mirai and Watanobe, Yutaka and Vazhenin, Alexander},
	editor = {Fujita, Hamido and Guizzi, Guido},
	year = {2015},
	doi = {10.1007/978-3-319-22689-7_31},
	pages = {399--410}
}

@incollection{martellini_selected_2017,
	address = {Cham},
	title = {Selected {Issues} of {Cyber} {Security} {Practices} in {CBRNeCy} {Critical} {Infrastructure}},
	isbn = {978-3-319-62107-4 978-3-319-62108-1},
	url = {http://link.springer.com/10.1007/978-3-319-62108-1_2},
	urldate = {2018-10-31},
	booktitle = {Cyber and {Chemical}, {Biological}, {Radiological}, {Nuclear}, {Explosives} {Challenges}},
	publisher = {Springer International Publishing},
	author = {Abaimov, Stanislav and Martellini, Maurizio},
	editor = {Martellini, Maurizio and Malizia, Andrea},
	year = {2017},
	doi = {10.1007/978-3-319-62108-1_2},
	pages = {11--34}
}

@incollection{gasevic_neon:_2009,
	address = {Berlin, Heidelberg},
	title = {Neon: {A} {Library} for {Language} {Usage} {Analysis}},
	volume = {5452},
	isbn = {978-3-642-00433-9 978-3-642-00434-6},
	shorttitle = {Neon},
	url = {http://link.springer.com/10.1007/978-3-642-00434-6_4},
	urldate = {2018-10-31},
	booktitle = {Software {Language} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hage, Jurriaan and van Keeken, Peter},
	editor = {Gaevi, Dragan and Lmmel, Ralf and Van Wyk, Eric},
	year = {2009},
	doi = {10.1007/978-3-642-00434-6_4},
	pages = {35--53}
}

@article{guerra_specification-driven_2015,
	title = {Specification-driven model transformation testing},
	volume = {14},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-013-0369-x},
	doi = {10.1007/s10270-013-0369-x},
	language = {en},
	number = {2},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Guerra, Esther and Soeken, Mathias},
	month = may,
	year = {2015},
	pages = {623--644},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\YVZGSHFB\\Guerra e Soeken - 2015 - Specification-driven model transformation testing.pdf:application/pdf}
}

@incollection{abrahamsson_choreography_2016,
	address = {Cham},
	title = {Choreography {Modelling} {Language} for the {Embedded} {Systems} {Domain}},
	volume = {10027},
	isbn = {978-3-319-49093-9 978-3-319-49094-6},
	url = {http://link.springer.com/10.1007/978-3-319-49094-6_10},
	urldate = {2018-10-31},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer International Publishing},
	author = {Tauan, Neboja and Lehto, Jari and Markkula, Jouni and Kuvaja, Pasi and Oivo, Markku},
	editor = {Abrahamsson, Pekka and Jedlitschka, Andreas and Nguyen Duc, Anh and Felderer, Michael and Amasaki, Sousuke and Mikkonen, Tommi},
	year = {2016},
	doi = {10.1007/978-3-319-49094-6_10},
	pages = {144--159}
}

@incollection{wehrle_omnet++_2010,
	address = {Berlin, Heidelberg},
	title = {{OMNeT}++},
	isbn = {978-3-642-12330-6 978-3-642-12331-3},
	url = {http://link.springer.com/10.1007/978-3-642-12331-3_3},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Modeling and {Tools} for {Network} {Simulation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Varga, Andras},
	editor = {Wehrle, Klaus and Gne, Mesut and Gross, James},
	year = {2010},
	doi = {10.1007/978-3-642-12331-3_3},
	pages = {35--59}
}

@incollection{erlingsson_authorization_2011,
	address = {Berlin, Heidelberg},
	title = {Authorization {Enforcement} {Usability} {Case} {Study}},
	volume = {6542},
	isbn = {978-3-642-19124-4 978-3-642-19125-1},
	url = {http://link.springer.com/10.1007/978-3-642-19125-1_16},
	urldate = {2018-10-31},
	booktitle = {Engineering {Secure} {Software} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bartsch, Steffen},
	editor = {Erlingsson, lfar and Wieringa, Roel and Zannone, Nicola},
	year = {2011},
	doi = {10.1007/978-3-642-19125-1_16},
	pages = {209--220}
}

@incollection{hutchison_model_2012-1,
	address = {Berlin, Heidelberg},
	title = {Model {Based} {Testing} with {Logical} {Properties} versus {State} {Machines}},
	volume = {7257},
	isbn = {978-3-642-34406-0 978-3-642-34407-7},
	url = {http://link.springer.com/10.1007/978-3-642-34407-7_8},
	urldate = {2018-10-31},
	booktitle = {Implementation and {Application} of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Koopman, Pieter and Achten, Peter and Plasmeijer, Rinus},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gill, Andy and Hage, Jurriaan},
	year = {2012},
	doi = {10.1007/978-3-642-34407-7_8},
	pages = {116--133}
}

@incollection{bernardo_formal_2012,
	address = {Berlin, Heidelberg},
	title = {Formal {Specification} and {Testing} of {Model} {Transformations}},
	volume = {7320},
	isbn = {978-3-642-30981-6 978-3-642-30982-3},
	url = {http://link.springer.com/10.1007/978-3-642-30982-3_11},
	urldate = {2018-10-31},
	booktitle = {Formal {Methods} for {Model}-{Driven} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vallecillo, Antonio and Gogolla, Martin and Burgueo, Loli and Wimmer, Manuel and Hamann, Lars},
	editor = {Bernardo, Marco and Cortellessa, Vittorio and Pierantonio, Alfonso},
	year = {2012},
	doi = {10.1007/978-3-642-30982-3_11},
	pages = {399--437},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\IH36B2P4\\Vallecillo et al. - 2012 - Formal Specification and Testing of Model Transfor.pdf:application/pdf}
}

@incollection{svensson_identifying_2016,
	address = {Berkeley, CA},
	title = {Identifying {Vulnerabilities}},
	isbn = {978-1-4842-2282-9 978-1-4842-2283-6},
	url = {http://link.springer.com/10.1007/978-1-4842-2283-6_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {From {Hacking} to {Report} {Writing}},
	publisher = {Apress},
	author = {Svensson, Robert},
	collaborator = {Svensson, Robert},
	year = {2016},
	doi = {10.1007/978-1-4842-2283-6_6},
	pages = {59--87}
}

@InCollection{king_dsl_2011,
  author    = {Daz, Oscar and Puente, Gorka},
  title     = {A {DSL} for {Corporate} {Wiki} {Initialization}},
  booktitle = {Active {Flow} and {Combustion} {Control} 2018},
  publisher = {Springer International Publishing},
  year      = {2011},
  editor    = {King, Rudibert},
  volume    = {141},
  pages     = {237--251},
  address   = {Cham},
  isbn      = {978-3-319-98176-5 978-3-319-98177-2},
  doi       = {10.1007/978-3-642-21640-4_19},
  url       = {http://link.springer.com/10.1007/978-3-642-21640-4_19},
  urldate   = {2018-10-31},
}

@incollection{abramowicz_orchestrating_2011,
	address = {Berlin, Heidelberg},
	title = {Orchestrating {Security} and {System} {Engineering} for {Evolving} {Systems}},
	volume = {6994},
	isbn = {978-3-642-24754-5 978-3-642-24755-2},
	url = {http://link.springer.com/10.1007/978-3-642-24755-2_12},
	urldate = {2018-10-31},
	booktitle = {Towards a {Service}-{Based} {Internet}},
	publisher = {Springer Berlin Heidelberg},
	author = {Massacci, Fabio and Bouquet, Fabrice and Fourneret, Elizabeta and Jurjens, Jan and Lund, Mass S. and Madelnat, Sbastien and Muehlberg, JanTobias and Paci, Federica and Paul, Stphane and Piessens, Frank and Solhaug, Bjornar and Wenzel, Sven},
	editor = {Abramowicz, Witold and Llorente, Ignacio M. and Surridge, Mike and Zisman, Andrea and Vayssire, Julien},
	year = {2011},
	doi = {10.1007/978-3-642-24755-2_12},
	pages = {134--143}
}

@incollection{gruhn_data-driven_2018,
	address = {Cham},
	title = {Data-{Driven} {Decisions} and {Actions} in {Today}s {Software} {Development}},
	isbn = {978-3-319-73896-3 978-3-319-73897-0},
	url = {http://link.springer.com/10.1007/978-3-319-73897-0_9},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {The {Essence} of {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Gall, Harald and Alexandru, Carol and Ciurumelea, Adelina and Grano, Giovanni and Laaber, Christoph and Panichella, Sebastiano and Proksch, Sebastian and Schermann, Gerald and Vassallo, Carmine and Zhao, Jitong},
	editor = {Gruhn, Volker and Striemer, Rdiger},
	year = {2018},
	doi = {10.1007/978-3-319-73897-0_9},
	pages = {137--168}
}

@incollection{grieskamp_simulated_2006,
	address = {Berlin, Heidelberg},
	title = {Simulated {Time} for {Testing} {Railway} {Interlockings} with {TTCN}-3},
	volume = {3997},
	isbn = {978-3-540-34454-4 978-3-540-34455-1},
	url = {http://link.springer.com/10.1007/11759744_1},
	urldate = {2018-10-31},
	booktitle = {Formal {Approaches} to {Software} {Testing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blom, Stefan and Ioustinova, Natalia and van de Pol, Jaco and Rennoch, Axel and Sidorova, Natalia},
	editor = {Grieskamp, Wolfgang and Weise, Carsten},
	year = {2006},
	doi = {10.1007/11759744_1},
	pages = {1--15},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\Z2BPA7F4\\Blom et al. - 2006 - Simulated Time for Testing Railway Interlockings w.pdf:application/pdf}
}

@incollection{silva_new_2014,
	address = {Cham},
	title = {A {New} {GCC} {Plugin}-{Based} {Compiler} {Pass} to {Add} {Support} for {Thread}-{Level} {Speculation} into {OpenMP}},
	volume = {8632},
	isbn = {978-3-319-09872-2 978-3-319-09873-9},
	url = {http://link.springer.com/10.1007/978-3-319-09873-9_20},
	urldate = {2018-10-31},
	booktitle = {Euro-{Par} 2014 {Parallel} {Processing}},
	publisher = {Springer International Publishing},
	author = {Aldea, Sergio and Estebanez, Alvaro and Llanos, Diego R. and Gonzalez-Escribano, Arturo},
	editor = {Silva, Fernando and Dutra, Ins and Santos Costa, Vtor},
	year = {2014},
	doi = {10.1007/978-3-319-09873-9_20},
	pages = {234--245}
}

@incollection{margaria_back--back_2014,
	address = {Berlin, Heidelberg},
	title = {Back-{To}-{Back} {Testing} of {Model}-{Based} {Code} {Generators}},
	volume = {8802},
	isbn = {978-3-662-45233-2 978-3-662-45234-9},
	url = {http://link.springer.com/10.1007/978-3-662-45234-9_30},
	urldate = {2018-10-31},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}. {Technologies} for {Mastering} {Change}},
	publisher = {Springer Berlin Heidelberg},
	author = {Jrges, Sven and Steffen, Bernhard},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2014},
	doi = {10.1007/978-3-662-45234-9_30},
	pages = {425--444}
}

@incollection{tsai_introduction_2017,
	address = {Singapore},
	title = {Introduction},
	isbn = {978-981-10-4480-9 978-981-10-4481-6},
	url = {http://link.springer.com/10.1007/978-981-10-4481-6_1},
	urldate = {2018-10-31},
	booktitle = {Combinatorial {Testing} in {Cloud} {Computing}},
	publisher = {Springer Singapore},
	author = {Tsai, Wei-Tek and Qi, Guanqiu},
	collaborator = {Tsai, Wei-Tek and Qi, Guanqiu},
	year = {2017},
	doi = {10.1007/978-981-10-4481-6_1},
	pages = {1--13}
}

@incollection{benzmuller_chr.js:_2018,
	address = {Cham},
	title = {{CHR}.js: {A} {CHR} {Implementation} in {JavaScript}},
	volume = {11092},
	isbn = {978-3-319-99905-0 978-3-319-99906-7},
	shorttitle = {{CHR}.js},
	url = {http://link.springer.com/10.1007/978-3-319-99906-7_9},
	urldate = {2018-10-31},
	booktitle = {Rules and {Reasoning}},
	publisher = {Springer International Publishing},
	author = {Nogatz, Falco and Frhwirth, Thom and Seipel, Dietmar},
	editor = {Benzmller, Christoph and Ricca, Francesco and Parent, Xavier and Roman, Dumitru},
	year = {2018},
	doi = {10.1007/978-3-319-99906-7_9},
	pages = {131--146}
}

@article{chowdhury_greenscaler:_2018,
	title = {{GreenScaler}: training software energy models with automatic test generation},
	issn = {1382-3256, 1573-7616},
	shorttitle = {{GreenScaler}},
	url = {http://link.springer.com/10.1007/s10664-018-9640-7},
	doi = {10.1007/s10664-018-9640-7},
	language = {en},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Chowdhury, Shaiful and Borle, Stephanie and Romansky, Stephen and Hindle, Abram},
	month = jul,
	year = {2018}
}

@article{ali_modeling_2012,
	title = {Modeling robustness behavior using aspect-oriented modeling to support robustness testing of industrial systems},
	volume = {11},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-011-0206-z},
	doi = {10.1007/s10270-011-0206-z},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Ali, Shaukat and Briand, Lionel C. and Hemmati, Hadi},
	month = oct,
	year = {2012},
	pages = {633--670}
}

@article{sojer_deriving_2015,
	title = {Deriving fault-detection mechanisms from safety requirements},
	volume = {30},
	issn = {1865-2034, 1865-2042},
	url = {http://link.springer.com/10.1007/s00450-011-0203-z},
	doi = {10.1007/s00450-011-0203-z},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Computer Science - Research and Development},
	author = {Sojer, Dominik and Buckl, Christian and Knoll, Alois},
	month = feb,
	year = {2015},
	pages = {21--34},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\7NZVATTV\\Sojer et al. - 2015 - Deriving fault-detection mechanisms from safety re.pdf:application/pdf}
}

@incollection{cordeiro_simultaneously_2014,
	address = {Berlin, Heidelberg},
	title = {Simultaneously {Improving} {Quality} and {Time}-to-{Market} in {Agile} {Development}},
	volume = {457},
	isbn = {978-3-662-44919-6 978-3-662-44920-2},
	url = {http://link.springer.com/10.1007/978-3-662-44920-2_6},
	urldate = {2018-10-31},
	booktitle = {Software {Technologies}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dra, Pryscilla Marcilli and Oliveira, Ana Cristina and Moura, J. Anto B.},
	editor = {Cordeiro, Jos and van Sinderen, Marten},
	year = {2014},
	doi = {10.1007/978-3-662-44920-2_6},
	pages = {84--98}
}

@incollection{kuncak_modeling_2012,
	address = {Berlin, Heidelberg},
	title = {Modeling {Asynchronous} {Message} {Passing} for {C} {Programs}},
	volume = {7148},
	isbn = {978-3-642-27939-3 978-3-642-27940-9},
	url = {http://link.springer.com/10.1007/978-3-642-27940-9_22},
	urldate = {2018-10-31},
	booktitle = {Verification, {Model} {Checking}, and {Abstract} {Interpretation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Morse, Everett and Vrvilo, Nick and Mercer, Eric and McCarthy, Jay},
	editor = {Kuncak, Viktor and Rybalchenko, Andrey},
	year = {2012},
	doi = {10.1007/978-3-642-27940-9_22},
	pages = {332--347},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\UEWRUY85\\Morse et al. - 2012 - Modeling Asynchronous Message Passing for C Progra.pdf:application/pdf}
}

@incollection{hirschfeld_sbcl:_2008,
	address = {Berlin, Heidelberg},
	title = {{SBCL}: {A} {Sanely}-{Bootstrappable} {Common} {Lisp}},
	volume = {5146},
	isbn = {978-3-540-89274-8 978-3-540-89275-5},
	shorttitle = {{SBCL}},
	url = {http://link.springer.com/10.1007/978-3-540-89275-5_5},
	urldate = {2018-10-31},
	booktitle = {Self-{Sustaining} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rhodes, Christophe},
	editor = {Hirschfeld, Robert and Rose, Kim},
	year = {2008},
	doi = {10.1007/978-3-540-89275-5_5},
	pages = {74--86},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\YJ72QW57\\Rhodes - 2008 - SBCL A Sanely-Bootstrappable Common Lisp.pdf:application/pdf}
}

@incollection{buchberger_information_2010,
	address = {Berlin, Heidelberg},
	title = {Information and {Semantics} in {Databases} and on the {Web}},
	isbn = {978-3-642-02126-8 978-3-642-02127-5},
	url = {http://link.springer.com/10.1007/978-3-642-02127-5_7},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Hagenberg {Research}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wagner, Roland and Kng, Josef and Prll, Birgit and Buttinger, Christina and Feilmayr, Christina and Freudenthaler, Bernhard and Guttenbrunner, Michael and Hawel, Christian and Himsl, Melanie and Jabornig, Daniel and Leithner, Werner and Parzer, Stefan and Stumptner, Reinhard and Wagner, Stefan and W, Wolfram},
	editor = {Buchberger, Bruno and Affenzeller, Michael and Ferscha, Alois and Haller, Michael and Jebelean, Tudor and Klement, Erich Peter and Paule, Peter and Pomberger, Gustav and Schreiner, Wolfgang and Stubenrauch, Robert and Wagner, Roland and Wei, Gerhard and Windsteiger, Wolfgang},
	year = {2010},
	doi = {10.1007/978-3-642-02127-5_7},
	pages = {281--331}
}

@article{binalialhag_static_2018,
	title = {Static slicing of {Use} {Case} {Maps} requirements models},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-018-0680-7},
	doi = {10.1007/s10270-018-0680-7},
	language = {en},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Binalialhag, Taha and Hassine, Jameleddine and Amyot, Daniel},
	month = jun,
	year = {2018}
}

@incollection{fischer_simulating_2015,
	address = {Cham},
	title = {Simulating {Distributed} {Systems} with {SDL} and {Hardware}-in-the-{Loop}},
	volume = {9369},
	isbn = {978-3-319-24911-7 978-3-319-24912-4},
	url = {http://link.springer.com/10.1007/978-3-319-24912-4_5},
	urldate = {2018-10-31},
	booktitle = {{SDL} 2015: {Model}-{Driven} {Engineering} for {Smart} {Cities}},
	publisher = {Springer International Publishing},
	author = {Braun, Tobias and Christmann, Dennis},
	editor = {Fischer, Joachim and Scheidgen, Markus and Schieferdecker, Ina and Reed, Rick},
	year = {2015},
	doi = {10.1007/978-3-319-24912-4_5},
	pages = {49--64}
}

@article{fernandez-saez_industrial_2018,
	title = {An industrial case study on the use of {UML} in software maintenance and its perceived benefits and hurdles},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-018-9599-4},
	doi = {10.1007/s10664-018-9599-4},
	language = {en},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Fernndez-Sez, Ana M. and Chaudron, Michel R. V. and Genero, Marcela},
	month = mar,
	year = {2018}
}

@incollection{cherifi_dealing_2011,
	address = {Berlin, Heidelberg},
	title = {Dealing with {Stateful} {Firewall} {Checking}},
	volume = {166},
	isbn = {978-3-642-21983-2 978-3-642-21984-9},
	url = {http://link.springer.com/10.1007/978-3-642-21984-9_42},
	urldate = {2018-10-31},
	booktitle = {Digital {Information} and {Communication} {Technology} and {Its} {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ben Youssef, Nihel and Bouhoula, Adel},
	editor = {Cherifi, Hocine and Zain, Jasni Mohamad and El-Qawasmeh, Eyas},
	year = {2011},
	doi = {10.1007/978-3-642-21984-9_42},
	pages = {493--507}
}

@incollection{hutchison_functional_2012,
	address = {Berlin, Heidelberg},
	title = {Functional {Instrumentation} of {ActionScript} {Programs} with {Asil}},
	volume = {7257},
	isbn = {978-3-642-34406-0 978-3-642-34407-7},
	url = {http://link.springer.com/10.1007/978-3-642-34407-7_1},
	urldate = {2018-10-31},
	booktitle = {Implementation and {Application} of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Middelkoop, Arie and Elyasov, Alexander B. and Prasetya, Wishnu},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gill, Andy and Hage, Jurriaan},
	year = {2012},
	doi = {10.1007/978-3-642-34407-7_1},
	pages = {1--16}
}

@incollection{margaria_feasibility_2016,
	address = {Cham},
	title = {On the {Feasibility} of a {Unified} {Modelling} and {Programming} {Paradigm}},
	volume = {9953},
	isbn = {978-3-319-47168-6 978-3-319-47169-3},
	url = {http://link.springer.com/10.1007/978-3-319-47169-3_4},
	urldate = {2018-10-31},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}: {Discussion}, {Dissemination}, {Applications}},
	publisher = {Springer International Publishing},
	author = {Haxthausen, Anne E. and Peleska, Jan},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2016},
	doi = {10.1007/978-3-319-47169-3_4},
	pages = {32--49}
}

@incollection{bayro-corrochano_requirements_2014,
	address = {Cham},
	title = {Requirements {Engineering} for {Cyber}-{Physical} {Systems}: {Challenges} in the {Context} of {Industrie} 4.0},
	volume = {8827},
	isbn = {978-3-319-12567-1 978-3-319-12568-8},
	shorttitle = {Requirements {Engineering} for {Cyber}-{Physical} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-662-44739-0_35},
	urldate = {2018-10-31},
	booktitle = {Progress in {Pattern} {Recognition}, {Image} {Analysis}, {Computer} {Vision}, and {Applications}},
	publisher = {Springer International Publishing},
	author = {Wiesner, Stefan and Gorldt, Christian and Soeken, Mathias and Thoben, Klaus-Dieter and Drechsler, Rolf},
	editor = {Bayro-Corrochano, Eduardo and Hancock, Edwin},
	year = {2014},
	doi = {10.1007/978-3-662-44739-0_35},
	pages = {281--288},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\KY8GXC7N\\Wiesner et al. - 2014 - Requirements Engineering for Cyber-Physical System.pdf:application/pdf}
}

@incollection{nolan_test_2017,
	address = {Berkeley, CA},
	title = {Test {Driven} {Development}},
	isbn = {978-1-4842-2101-3 978-1-4842-2102-0},
	url = {http://link.springer.com/10.1007/978-1-4842-2102-0_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Agile {Swift}},
	publisher = {Apress},
	author = {Nolan, Godfrey},
	collaborator = {Nolan, Godfrey},
	year = {2017},
	doi = {10.1007/978-1-4842-2102-0_6},
	pages = {131--167}
}

@incollection{rossi_overview_2008,
	address = {London},
	title = {Overview of {Design} {Issues} for {Web} {Applications} {Development}},
	isbn = {978-1-84628-922-4},
	url = {http://link.springer.com/10.1007/978-1-84628-923-1_4},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Web {Engineering}: {Modelling} and {Implementing} {Web} {Applications}},
	publisher = {Springer London},
	author = {Rossi, Gustavo and Schwabe, Daniel and Olsina, Luis and Pastor, Oscar},
	editor = {Rossi, Gustavo and Pastor, Oscar and Schwabe, Daniel and Olsina, Luis},
	year = {2008},
	doi = {10.1007/978-1-84628-923-1_4},
	pages = {49--63}
}

@article{ciancone_klapersuite_2014,
	title = {The {KlaperSuite} framework for model-driven reliability analysis of component-based systems},
	volume = {13},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-013-0334-8},
	doi = {10.1007/s10270-013-0334-8},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Ciancone, Andrea and Drago, Mauro Luigi and Filieri, Antonio and Grassi, Vincenzo and Koziolek, Heiko and Mirandola, Raffaela},
	month = oct,
	year = {2014},
	pages = {1269--1290},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\ULUCILVP\\Ciancone et al. - 2014 - The KlaperSuite framework for model-driven reliabi.pdf:application/pdf}
}

@incollection{goos_reliability_1974,
	address = {Berlin, Heidelberg},
	title = {Reliability and integrity of large computer programs},
	volume = {12},
	isbn = {978-3-540-06786-3 978-3-540-38389-5},
	url = {http://link.springer.com/10.1007/3-540-06786-8_260},
	urldate = {2018-10-31},
	booktitle = {{GFK}-{GI}-{GMR} {Fachtagung} {Prozessrechner} 1974},
	publisher = {Springer Berlin Heidelberg},
	author = {Ramamoorthy, C. V. and Cheung, R. C. and Kim, K. H.},
	editor = {Goos, G. and Hartmanis, J. and Brinch Hansen, P. and Gries, D. and Moler, C. and Seegmller, G. and Wirth, N. and Krger, Gerhard and Friehmelt, Rdiger},
	year = {1974},
	doi = {10.1007/3-540-06786-8_260},
	pages = {86--161}
}

@article{bozga_using_2003,
	title = {Using static analysis to improve automatic test generation},
	volume = {4},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/10.1007/s10009-002-0098-x},
	doi = {10.1007/s10009-002-0098-x},
	number = {2},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer (STTT)},
	author = {Bozga, Marius and Fernandez, Jean-Claude and Ghirvu, Lucian},
	month = feb,
	year = {2003},
	pages = {142--152},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\AWH89PHD\\Bozga et al. - 2003 - Using static analysis to improve automatic test ge.pdf:application/pdf}
}

@incollection{chaudron_empirical_2008,
	address = {Berlin, Heidelberg},
	title = {An {Empirical} {Investigation} of the {Effort} of {Creating} {Reusable}, {Component}-{Based} {Models} for {Performance} {Prediction}},
	volume = {5282},
	isbn = {978-3-540-87890-2 978-3-540-87891-9},
	url = {http://link.springer.com/10.1007/978-3-540-87891-9_2},
	urldate = {2018-10-31},
	booktitle = {Component-{Based} {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Martens, Anne and Becker, Steffen and Koziolek, Heiko and Reussner, Ralf},
	editor = {Chaudron, Michel R. V. and Szyperski, Clemens and Reussner, Ralf},
	year = {2008},
	doi = {10.1007/978-3-540-87891-9_2},
	pages = {16--31}
}

@incollection{thomas_empirical_2008,
	address = {Berlin, Heidelberg},
	title = {An {Empirical} {Investigation} of the {Applicability} of a {Component}-{Based} {Performance} {Prediction} {Method}},
	volume = {5261},
	isbn = {978-3-540-87411-9 978-3-540-87412-6},
	url = {http://link.springer.com/10.1007/978-3-540-87412-6_3},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Computer {Performance} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Martens, Anne and Becker, Steffen and Koziolek, Heiko and Reussner, Ralf},
	editor = {Thomas, Nigel and Juiz, Carlos},
	year = {2008},
	doi = {10.1007/978-3-540-87412-6_3},
	pages = {17--31},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\P8REDLKI\\Martens et al. - 2008 - An Empirical Investigation of the Applicability of.pdf:application/pdf}
}

@incollection{bouguettaya_wsdarwin:_2014,
	address = {New York, NY},
	title = {{WSDarwin}: {Studying} the {Evolution} of {Web} {Service} {Systems}},
	isbn = {978-1-4614-7534-7 978-1-4614-7535-4},
	shorttitle = {{WSDarwin}},
	url = {http://link.springer.com/10.1007/978-1-4614-7535-4_9},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Advanced {Web} {Services}},
	publisher = {Springer New York},
	author = {Fokaefs, Marios and Stroulia, Eleni},
	editor = {Bouguettaya, Athman and Sheng, Quan Z. and Daniel, Florian},
	year = {2014},
	doi = {10.1007/978-1-4614-7535-4_9},
	pages = {199--223}
}

@incollection{kneuper_software_2018,
	address = {Cham},
	title = {Software {Processes} in the {Software} {Product} {Life} {Cycle}},
	isbn = {978-3-319-98844-3 978-3-319-98845-0},
	url = {http://link.springer.com/10.1007/978-3-319-98845-0_3},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Software {Processes} and {Life} {Cycle} {Models}},
	publisher = {Springer International Publishing},
	author = {Kneuper, Ralf},
	collaborator = {Kneuper, Ralf},
	year = {2018},
	doi = {10.1007/978-3-319-98845-0_3},
	pages = {69--157}
}

@incollection{fisher_effective_2016,
	address = {Berkeley, CA},
	title = {Effective {Testing}},
	isbn = {978-1-4842-0269-2 978-1-4842-0268-5},
	url = {http://link.springer.com/10.1007/978-1-4842-0268-5_7},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Spring {Persistence} with {Hibernate}},
	publisher = {Apress},
	author = {Fisher, Paul and Murphy, Brian D.},
	collaborator = {Fisher, Paul and Murphy, Brian D.},
	year = {2016},
	doi = {10.1007/978-1-4842-0268-5_7},
	pages = {127--139}
}

@incollection{hutchison_handling_2009,
	address = {Berlin, Heidelberg},
	title = {Handling {Software} {Faults} with {Redundancy}},
	volume = {5835},
	isbn = {978-3-642-10247-9 978-3-642-10248-6},
	url = {http://link.springer.com/10.1007/978-3-642-10248-6_7},
	urldate = {2018-10-31},
	booktitle = {Architecting {Dependable} {Systems} {VI}},
	publisher = {Springer Berlin Heidelberg},
	author = {Carzaniga, Antonio and Gorla, Alessandra and Pezz, Mauro},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and de Lemos, Rogrio and Fabre, Jean-Charles and Gacek, Cristina and Gadducci, Fabio and ter Beek, Maurice},
	year = {2009},
	doi = {10.1007/978-3-642-10248-6_7},
	pages = {148--171},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\3FIYXPUF\\Carzaniga et al. - 2009 - Handling Software Faults with Redundancy.pdf:application/pdf}
}

@incollection{fisher_effective_2010,
	address = {Berkeley, CA},
	title = {Effective {Testing}},
	isbn = {978-1-4302-2632-1 978-1-4302-2633-8},
	url = {http://link.springer.com/10.1007/978-1-4302-2633-8_8},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Spring {Persistence} with {Hibernate}},
	publisher = {Apress},
	author = {Fisher, Paul Tepper and Murphy, Brian D.},
	editor = {Anglin, Steve and Welsh, Tom and Beckner, Mark and Buckingham, Ewan and Cornell, Gary and Gennick, Jonathan and Hassell, Jonathan and Lowman, Michelle and Moodie, Matthew and Parkes, Duncan and Pepper, Jeffrey and Pohlmann, Frank and Pundick, Douglas and Renow-Clarke, Ben and Shakeshaft, Dominic and Wade, Matt and Tobin, Mary and Smith, Marilyn},
	collaborator = {Fisher, Paul Tepper and Murphy, Brian D.},
	year = {2010},
	doi = {10.1007/978-1-4302-2633-8_8},
	pages = {125--136}
}

@incollection{margaria_evaluation_2016,
	address = {Cham},
	title = {Evaluation and {Reproducibility} of {Program} {Analysis} and {Verification} ({Track} {Introduction})},
	volume = {9952},
	isbn = {978-3-319-47165-5 978-3-319-47166-2},
	url = {http://link.springer.com/10.1007/978-3-319-47166-2_13},
	urldate = {2018-10-31},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}: {Foundational} {Techniques}},
	publisher = {Springer International Publishing},
	author = {Schordan, Markus and Beyer, Dirk and Lundberg, Jonas},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2016},
	doi = {10.1007/978-3-319-47166-2_13},
	pages = {191--194}
}

@incollection{hutchison_managing_2012,
	address = {Berlin, Heidelberg},
	title = {Managing {Execution} {Environment} {Variability} during {Software} {Testing}: {An} {Industrial} {Experience}},
	volume = {7641},
	isbn = {978-3-642-34690-3 978-3-642-34691-0},
	shorttitle = {Managing {Execution} {Environment} {Variability} during {Software} {Testing}},
	url = {http://link.springer.com/10.1007/978-3-642-34691-0_4},
	urldate = {2018-10-31},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hervieu, Aymeric and Baudry, Benoit and Gotlieb, Arnaud},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Nielsen, Brian and Weise, Carsten},
	year = {2012},
	doi = {10.1007/978-3-642-34691-0_4},
	pages = {24--38},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\4JQAIFR4\\Hervieu et al. - 2012 - Managing Execution Environment Variability during .pdf:application/pdf}
}

@incollection{hutchison_asm_2005,
	address = {Berlin, Heidelberg},
	title = {The {ASM} {Method} for {System} {Design} and {Analysis}. {A} {Tutorial} {Introduction}},
	volume = {3717},
	isbn = {978-3-540-29051-3 978-3-540-31730-2},
	url = {http://link.springer.com/10.1007/11559306_15},
	urldate = {2018-10-31},
	booktitle = {Frontiers of {Combining} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Brger, Egon},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Gramlich, Bernhard},
	year = {2005},
	doi = {10.1007/11559306_15},
	pages = {264--283},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\27NPPGLI\\Brger - 2005 - The ASM Method for System Design and Analysis. A T.pdf:application/pdf}
}

@article{marchetto_objects_2009,
	title = {From objects to services: toward a stepwise migration approach for {Java} applications},
	volume = {11},
	issn = {1433-2779, 1433-2787},
	shorttitle = {From objects to services},
	url = {http://link.springer.com/10.1007/s10009-009-0123-4},
	doi = {10.1007/s10009-009-0123-4},
	language = {en},
	number = {6},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Marchetto, Alessandro and Ricca, Filippo},
	month = dec,
	year = {2009},
	pages = {427--440}
}

@incollection{de_lucia_open_2013,
	address = {Berlin, Heidelberg},
	title = {Open {Source} {Practices} in {Software} {Product} {Line} {Engineering}},
	volume = {7171},
	isbn = {978-3-642-36053-4 978-3-642-36054-1},
	url = {http://link.springer.com/10.1007/978-3-642-36054-1_8},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {van der Linden, Frank},
	editor = {De Lucia, Andrea and Ferrucci, Filomena},
	year = {2013},
	doi = {10.1007/978-3-642-36054-1_8},
	pages = {216--235}
}

@incollection{kapitsaki_feature_2016,
	address = {Cham},
	title = {Feature {Location} {Benchmark} for {Software} {Families} {Using} {Eclipse} {Community} {Releases}},
	volume = {9679},
	isbn = {978-3-319-35121-6 978-3-319-35122-3},
	url = {http://link.springer.com/10.1007/978-3-319-35122-3_18},
	urldate = {2018-10-31},
	booktitle = {Software {Reuse}: {Bridging} with {Social}-{Awareness}},
	publisher = {Springer International Publishing},
	author = {Martinez, Jabier and Ziadi, Tewfik and Papadakis, Mike and Bissyand, Tegawend F. and Klein, Jacques and Le Traon, Yves},
	editor = {Kapitsaki, Georgia M. and Santana de Almeida, Eduardo},
	year = {2016},
	doi = {10.1007/978-3-319-35122-3_18},
	pages = {267--283},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\S62HBFAP\\Martinez et al. - 2016 - Feature Location Benchmark for Software Families U.pdf:application/pdf}
}

@incollection{hutchison_weaving_2014,
	address = {Berlin, Heidelberg},
	title = {Weaving {Aspects} and {Business} {Processes} through {Model} {Transformation}},
	volume = {7908},
	isbn = {978-3-642-38708-1 978-3-642-38709-8},
	url = {http://link.springer.com/10.1007/978-3-662-44879-3_4},
	urldate = {2018-10-31},
	booktitle = {Advanced {Information} {Systems} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Witteborg, Heiko and Charfi, Anis and Colomer Collell, Daniel and Mezini, Mira},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Salinesi, Camille and Norrie, Moira C. and Pastor, scar},
	year = {2014},
	doi = {10.1007/978-3-662-44879-3_4},
	pages = {47--61}
}

@incollection{ahrendt_quo_2016,
	address = {Cham},
	title = {Quo {Vadis} {Formal} {Verification}?},
	volume = {10001},
	isbn = {978-3-319-49811-9 978-3-319-49812-6},
	url = {http://link.springer.com/10.1007/978-3-319-49812-6_1},
	urldate = {2018-10-31},
	booktitle = {Deductive {Software} {Verification}  {The} {KeY} {Book}},
	publisher = {Springer International Publishing},
	author = {Hhnle, Reiner},
	editor = {Ahrendt, Wolfgang and Beckert, Bernhard and Bubel, Richard and Hhnle, Reiner and Schmitt, Peter H. and Ulbrich, Mattias},
	year = {2016},
	doi = {10.1007/978-3-319-49812-6_1},
	pages = {1--19}
}

@incollection{borger_technologies_2008,
	address = {Berlin, Heidelberg},
	title = {Technologies for {Evolvable} {Software} {Products}: {The} {Conflict} between {Customizations} and {Evolution}},
	volume = {5316},
	isbn = {978-3-540-89761-3 978-3-540-89762-0},
	shorttitle = {Technologies for {Evolvable} {Software} {Products}},
	url = {http://link.springer.com/10.1007/978-3-540-89762-0_8},
	urldate = {2018-10-31},
	booktitle = {Advances in {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sestoft, Peter and Vaucouleur, Sebastien},
	editor = {Brger, Egon and Cisternino, Antonio},
	year = {2008},
	doi = {10.1007/978-3-540-89762-0_8},
	pages = {216--253},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\KHIXX3YB\\Sestoft e Vaucouleur - 2008 - Technologies for Evolvable Software Products The .pdf:application/pdf}
}

@incollection{cappello_virtual_2007,
	address = {Berlin, Heidelberg},
	title = {A {Virtual} {Test} {Environment} for {MPI} {Development}: {Quick} {Answers} to {Many} {Small} {Questions}},
	volume = {4757},
	isbn = {978-3-540-75415-2 978-3-540-75416-9},
	shorttitle = {A {Virtual} {Test} {Environment} for {MPI} {Development}},
	url = {http://link.springer.com/10.1007/978-3-540-75416-9_16},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Recent {Advances} in {Parallel} {Virtual} {Machine} and {Message} {Passing} {Interface}},
	publisher = {Springer Berlin Heidelberg},
	author = {Schnerring, Wolfgang and Kauhaus, Christian and Fey, Dietmar},
	editor = {Cappello, Franck and Herault, Thomas and Dongarra, Jack},
	year = {2007},
	doi = {10.1007/978-3-540-75416-9_16},
	pages = {73--80}
}

@incollection{lopez_behavioral_1998,
	address = {Boston, MA},
	title = {Behavioral {Fault} {Simulation}},
	isbn = {978-1-4419-5031-4 978-1-4757-4419-4},
	url = {http://link.springer.com/10.1007/978-1-4757-4419-4_11},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Advanced {Techniques} for {Embedded} {Systems} {Design} and {Test}},
	publisher = {Springer US},
	author = {Santucci, Jean-Franois and Bisgambiglia, Paul and Federici, Dominique},
	editor = {Lpez, Juan Carlos and Hermida, Romn and Geisselhardt, Walter},
	year = {1998},
	doi = {10.1007/978-1-4757-4419-4_11},
	pages = {261--284}
}

@incollection{auger_linear_2018,
	address = {Cham},
	title = {Linear {Combination} of {Distance} {Measures} for {Surrogate} {Models} in {Genetic} {Programming}},
	volume = {11102},
	isbn = {978-3-319-99258-7 978-3-319-99259-4},
	url = {http://link.springer.com/10.1007/978-3-319-99259-4_18},
	urldate = {2018-10-31},
	booktitle = {Parallel {Problem} {Solving} from {Nature}  {PPSN} {XV}},
	publisher = {Springer International Publishing},
	author = {Zaefferer, Martin and Stork, Jrg and Flasch, Oliver and Bartz-Beielstein, Thomas},
	editor = {Auger, Anne and Fonseca, Carlos M. and Loureno, Nuno and Machado, Penousal and Paquete, Lus and Whitley, Darrell},
	year = {2018},
	doi = {10.1007/978-3-319-99259-4_18},
	pages = {220--231},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\QUZVTDNL\\Zaefferer et al. - 2018 - Linear Combination of Distance Measures for Surrog.pdf:application/pdf}
}

@incollection{nurcan_facilitating_2015,
	address = {Cham},
	title = {Facilitating {Effective} {Stakeholder} {Communication} in {Software} {Development} {Processes}},
	volume = {204},
	isbn = {978-3-319-19269-7 978-3-319-19270-3},
	url = {http://link.springer.com/10.1007/978-3-319-19270-3_8},
	urldate = {2018-10-31},
	booktitle = {Information {Systems} {Engineering} in {Complex} {Environments}},
	publisher = {Springer International Publishing},
	author = {Shekhovtsov, Vladimir A. and Mayr, Heinrich C. and Kop, Christian},
	editor = {Nurcan, Selmin and Pimenidis, Elias},
	year = {2015},
	doi = {10.1007/978-3-319-19270-3_8},
	pages = {116--132}
}

@incollection{huttermann_infrastructure_2012,
	address = {Berkeley, CA},
	title = {Infrastructure as {Code}},
	isbn = {978-1-4302-4569-8 978-1-4302-4570-4},
	url = {http://link.springer.com/10.1007/978-1-4302-4570-4_9},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {{DevOps} for {Developers}},
	publisher = {Apress},
	author = {Httermann, Michael},
	collaborator = {Httermann, Michael},
	year = {2012},
	doi = {10.1007/978-1-4302-4570-4_9},
	pages = {135--156}
}

@incollection{goos_integrated_2002,
	address = {Berlin, Heidelberg},
	title = {Integrated {Multi}-purposed {Testbed} to {Characterize} the {Performance} of {Internet} {Access} over {Hybrid} {Fiber} {Coaxial} {Access} {Networks}},
	volume = {2345},
	isbn = {978-3-540-43709-3 978-3-540-47906-2},
	url = {http://link.springer.com/10.1007/3-540-47906-6_81},
	urldate = {2018-10-31},
	booktitle = {{NETWORKING} 2002: {Networking} {Technologies}, {Services}, and {Protocols}; {Performance} of {Computer} and {Communication} {Networks}; {Mobile} and {Wireless} {Communications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chan, Hung Nguyen and Martinez, Belen Carro and Gomez, Rafa Mompo and Granados, Judith Redoli},
	editor = {Goos, G. and Hartmanis, J. and van Leeuwen, J. and Gregori, Enrico and Conti, Marco and Campbell, Andrew T. and Omidyar, Guy and Zukerman, Moshe},
	year = {2002},
	doi = {10.1007/3-540-47906-6_81},
	pages = {996--1007},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\XS8IESST\\Chan et al. - 2002 - Integrated Multi-purposed Testbed to Characterize .pdf:application/pdf}
}

@incollection{avgeriou_how_2014,
	address = {Cham},
	title = {How {Do} {Software} {Architects} {Specify} and {Validate} {Quality} {Requirements}?},
	volume = {8627},
	isbn = {978-3-319-09969-9 978-3-319-09970-5},
	url = {http://link.springer.com/10.1007/978-3-319-09970-5_32},
	urldate = {2018-10-31},
	booktitle = {Software {Architecture}},
	publisher = {Springer International Publishing},
	author = {Caracciolo, Andrea and Lungu, Mircea Filip and Nierstrasz, Oscar},
	editor = {Avgeriou, Paris and Zdun, Uwe},
	year = {2014},
	doi = {10.1007/978-3-319-09970-5_32},
	pages = {374--389}
}

@incollection{vitek_type_2015,
	address = {Berlin, Heidelberg},
	title = {Type {Targeted} {Testing}},
	volume = {9032},
	isbn = {978-3-662-46668-1 978-3-662-46669-8},
	url = {http://link.springer.com/10.1007/978-3-662-46669-8_33},
	urldate = {2018-10-31},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Seidel, Eric L. and Vazou, Niki and Jhala, Ranjit},
	editor = {Vitek, Jan},
	year = {2015},
	doi = {10.1007/978-3-662-46669-8_33},
	pages = {812--836},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\WEYEVAG4\\Seidel et al. - 2015 - Type Targeted Testing.pdf:application/pdf}
}

@incollection{carneiro_polyglot_2016,
	address = {Berkeley, CA},
	title = {Polyglot {Services}},
	isbn = {978-1-4842-1936-2 978-1-4842-1937-9},
	url = {http://link.springer.com/10.1007/978-1-4842-1937-9_11},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Microservices {From} {Day} {One}},
	publisher = {Apress},
	author = {Carneiro, Cloves and Schmelmer, Tim},
	collaborator = {Carneiro, Cloves and Schmelmer, Tim},
	year = {2016},
	doi = {10.1007/978-1-4842-1937-9_11},
	pages = {177--184}
}

@incollection{brown_understanding_2013,
	address = {Berkeley, CA},
	title = {Understanding {Domain} {Classes}},
	isbn = {978-1-4302-4377-9 978-1-4302-4378-6},
	url = {http://link.springer.com/10.1007/978-1-4302-4378-6_3},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {The {Definitive} {Guide} to {Grails} 2},
	publisher = {Apress},
	author = {Brown, Jeff Scott and Rocher, Graeme},
	collaborator = {Brown, Jeff Scott and Rocher, Graeme},
	year = {2013},
	doi = {10.1007/978-1-4302-4378-6_3},
	pages = {41--61}
}

@incollection{yenigun_automatic_2013,
	address = {Berlin, Heidelberg},
	title = {Automatic {Grammar}-{Based} {Test} {Generation}},
	volume = {8254},
	isbn = {978-3-642-41706-1 978-3-642-41707-8},
	url = {http://link.springer.com/10.1007/978-3-642-41707-8_2},
	urldate = {2018-10-31},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Guo, Hai-Feng and Qiu, Zongyan},
	editor = {Yenign, Hsn and Yilmaz, Cemal and Ulrich, Andreas},
	year = {2013},
	doi = {10.1007/978-3-642-41707-8_2},
	pages = {17--32},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\WZD7VHFA\\Guo e Qiu - 2013 - Automatic Grammar-Based Test Generation.pdf:application/pdf}
}

@incollection{goos_verification_2000,
	address = {Berlin, Heidelberg},
	title = {Verification by {Testing} for {Recursive} {Program} {Schemes}},
	volume = {1817},
	isbn = {978-3-540-67628-7 978-3-540-45148-8},
	url = {http://link.springer.com/10.1007/10720327_15},
	urldate = {2018-10-31},
	booktitle = {Logic-{Based} {Program} {Synthesis} and {Transformation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Le Mtayer, Daniel and Nicolas, Valrie-Anne and Ridoux, Olivier},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Bossi, Annalisa},
	year = {2000},
	doi = {10.1007/10720327_15},
	pages = {255--272},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\CNIYKUMD\\Le Mtayer et al. - 2000 - Verification by Testing for Recursive Program Sche.pdf:application/pdf}
}

@incollection{igarashi_probabilistic_2016,
	address = {Cham},
	title = {Probabilistic {Programming} {Language} and its {Incremental} {Evaluation}},
	volume = {10017},
	isbn = {978-3-319-47957-6 978-3-319-47958-3},
	url = {http://link.springer.com/10.1007/978-3-319-47958-3_19},
	urldate = {2018-10-31},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Kiselyov, Oleg},
	editor = {Igarashi, Atsushi},
	year = {2016},
	doi = {10.1007/978-3-319-47958-3_19},
	pages = {357--376}
}

@article{mussbacher_aourn-based_2012,
	title = {{AoURN}-based modeling and analysis of software product lines},
	volume = {20},
	issn = {0963-9314, 1573-1367},
	url = {http://link.springer.com/10.1007/s11219-011-9153-8},
	doi = {10.1007/s11219-011-9153-8},
	language = {en},
	number = {3-4},
	urldate = {2018-10-31},
	journal = {Software Quality Journal},
	author = {Mussbacher, Gunter and Arajo, Joo and Moreira, Ana and Amyot, Daniel},
	month = sep,
	year = {2012},
	pages = {645--687}
}

@incollection{medina-bulo_validation_2018,
	address = {Cham},
	title = {Validation of {Transformation} from {Abstract} {State} {Machine} {Models} to {C}++ {Code}},
	volume = {11146},
	isbn = {978-3-319-99926-5 978-3-319-99927-2},
	url = {http://link.springer.com/10.1007/978-3-319-99927-2_2},
	urldate = {2018-10-31},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Bonfanti, Silvia and Gargantini, Angelo and Mashkoor, Atif},
	editor = {Medina-Bulo, Inmaculada and Merayo, Mercedes G. and Hierons, Robert},
	year = {2018},
	doi = {10.1007/978-3-319-99927-2_2},
	pages = {17--32}
}

@incollection{prakash_efficient_2014,
	address = {Cham},
	title = {Efficient {Detection} of {Multi}-step {Cross}-{Site} {Scripting} {Vulnerabilities}},
	volume = {8880},
	isbn = {978-3-319-13840-4 978-3-319-13841-1},
	url = {http://link.springer.com/10.1007/978-3-319-13841-1_20},
	urldate = {2018-10-31},
	booktitle = {Information {Systems} {Security}},
	publisher = {Springer International Publishing},
	author = {Vernotte, Alexandre and Dadeau, Frdric and Lebeau, Franck and Legeard, Bruno and Peureux, Fabien and Piat, Franois},
	editor = {Prakash, Atul and Shyamasundar, Rudrapatna},
	year = {2014},
	doi = {10.1007/978-3-319-13841-1_20},
	pages = {358--377}
}

@article{fernandes_integration_2006,
	title = {Integration of {DFDs} into a {UML}-based {Model}-driven {Engineering} {Approach}},
	volume = {5},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-006-0013-0},
	doi = {10.1007/s10270-006-0013-0},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Fernandes, Joo M. and Lilius, Johan and Truscan, Dragos},
	month = nov,
	year = {2006},
	pages = {403--428}
}

@incollection{cordeiro_modelling_2013,
	address = {Berlin, Heidelberg},
	title = {Modelling the {Requirements} of {Rich} {Internet} {Applications} in {WebRe}},
	volume = {170},
	isbn = {978-3-642-29577-5 978-3-642-29578-2},
	url = {http://link.springer.com/10.1007/978-3-642-29578-2_2},
	urldate = {2018-10-31},
	booktitle = {Software and {Data} {Technologies}},
	publisher = {Springer Berlin Heidelberg},
	author = {Robles Luna, Esteban and Escalona, M. J. and Rossi, G.},
	editor = {Cordeiro, Jos and Virvou, Maria and Shishkov, Boris},
	year = {2013},
	doi = {10.1007/978-3-642-29578-2_2},
	pages = {27--41}
}

@InCollection{bishop_case_2011,
  author    = {Hills, Mark and Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  title     = {A {Case} of {Visitor} versus {Interpreter} {Pattern}},
  booktitle = {Objects, {Models}, {Components}, {Patterns}},
  publisher = {Springer Berlin Heidelberg},
  year      = {2011},
  editor    = {Bishop, Judith and Vallecillo, Antonio},
  volume    = {6705},
  pages     = {228--243},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-21951-1 978-3-642-21952-8},
  doi       = {10.1007/978-3-642-21952-8_17},
  file      = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\XJ8F36KD\\Hills et al. - 2011 - A Case of Visitor versus Interpreter Pattern.pdf:application/pdf},
  url       = {http://link.springer.com/10.1007/978-3-642-21952-8_17},
  urldate   = {2018-10-31},
}

@article{demakov_using_2008,
	title = {Using abstract models for the generation of test data with a complex structure},
	volume = {34},
	issn = {0361-7688, 1608-3261},
	url = {http://link.springer.com/10.1134/S0361768808060054},
	doi = {10.1134/S0361768808060054},
	language = {en},
	number = {6},
	urldate = {2018-10-31},
	journal = {Programming and Computer Software},
	author = {Demakov, A. V. and Zelenov, S. V. and Zelenova, S. A.},
	month = nov,
	year = {2008},
	pages = {341--350}
}

@incollection{maalej_dufice:_2013,
	address = {Berlin, Heidelberg},
	title = {{DUFICE}: {Guidelines} for a {Lightweight} {Management} of {Requirements} {Knowledge}},
	isbn = {978-3-642-34418-3 978-3-642-34419-0},
	shorttitle = {{DUFICE}},
	url = {http://link.springer.com/10.1007/978-3-642-34419-0_4},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Managing {Requirements} {Knowledge}},
	publisher = {Springer Berlin Heidelberg},
	author = {Maalej, W. and Thurimella, A. K.},
	editor = {Maalej, Walid and Thurimella, Anil Kumar},
	year = {2013},
	doi = {10.1007/978-3-642-34419-0_4},
	pages = {75--91}
}

@incollection{hutchison_architecture_2005,
	address = {Berlin, Heidelberg},
	title = {Architecture and {Protocols} for the {Seamless} and {Integrated} {Next} {Generation} {IP} {Networks}},
	volume = {3375},
	isbn = {978-3-540-24557-5 978-3-540-30573-6},
	url = {http://link.springer.com/10.1007/978-3-540-30573-6_33},
	urldate = {2018-10-31},
	booktitle = {Quality of {Service} in {Multiservice} {IP} {Networks}},
	publisher = {Springer Berlin Heidelberg},
	author = {Carrozzo, Gino and Ciulli, Nicola and Giordano, Stefano and Giorgi, Giodi and Listanti, Marco and Monaco, Ugo and Mustacchio, Fabio and Procissi, Gregorio and Ricciato, Fabio},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Ajmone Marsan, Marco and Bianchi, Giuseppe and Listanti, Marco and Meo, Michela},
	year = {2005},
	doi = {10.1007/978-3-540-30573-6_33},
	pages = {419--432},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\5BAYNA9L\\Carrozzo et al. - 2005 - Architecture and Protocols for the Seamless and In.pdf:application/pdf}
}

@incollection{hutchison_automated_2007,
	address = {Berlin, Heidelberg},
	title = {Automated {Semantic} {Analysis} of {Design} {Models}},
	volume = {4735},
	isbn = {978-3-540-75208-0 978-3-540-75209-7},
	url = {http://link.springer.com/10.1007/978-3-540-75209-7_12},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Weil, Frank and Mastenbrook, Brian and Nelson, David and Dietz, Paul and van den Berg, Aswin},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Engels, Gregor and Opdyke, Bill and Schmidt, Douglas C. and Weil, Frank},
	year = {2007},
	doi = {10.1007/978-3-540-75209-7_12},
	pages = {166--180}
}

@article{ricca_impact_2018,
	title = {On the impact of state-based model-driven development on maintainability: a family of experiments using {UniMod}},
	volume = {23},
	issn = {1382-3256, 1573-7616},
	shorttitle = {On the impact of state-based model-driven development on maintainability},
	url = {http://link.springer.com/10.1007/s10664-017-9563-8},
	doi = {10.1007/s10664-017-9563-8},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Ricca, Filippo and Torchiano, Marco and Leotta, Maurizio and Tiso, Alessandro and Guerrini, Giovanna and Reggio, Gianna},
	month = jun,
	year = {2018},
	pages = {1743--1790}
}

@incollection{margaria_agile_2008,
	address = {Berlin, Heidelberg},
	title = {Agile {IT}: {Thinking} in {User}-{Centric} {Models}},
	volume = {17},
	isbn = {978-3-540-88478-1 978-3-540-88479-8},
	shorttitle = {Agile {IT}},
	url = {http://link.springer.com/10.1007/978-3-540-88479-8_35},
	urldate = {2018-10-31},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Margaria, Tiziana and Steffen, Bernhard},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2008},
	doi = {10.1007/978-3-540-88479-8_35},
	pages = {490--502}
}

@incollection{noauthor_forum_2007,
	address = {Berkeley, CA},
	title = {Forum {Implementation}},
	isbn = {978-1-59059-736-1},
	url = {http://link.springer.com/10.1007/978-1-4302-0276-9_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Ruby} on {Rails} {E}-{Commerce}},
	publisher = {Apress},
	year = {2007},
	doi = {10.1007/978-1-4302-0276-9_6},
	pages = {169--195}
}

@incollection{oconnor_mapping_2009,
	address = {Berlin, Heidelberg},
	title = {Mapping {CMMI} {Level} 2 to {Scrum} {Practices}: {An} {Experience} {Report}},
	volume = {42},
	isbn = {978-3-642-04132-7 978-3-642-04133-4},
	shorttitle = {Mapping {CMMI} {Level} 2 to {Scrum} {Practices}},
	url = {http://link.springer.com/10.1007/978-3-642-04133-4_8},
	urldate = {2018-10-31},
	booktitle = {Software {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Diaz, Jessica and Garbajosa, Juan and Calvo-Manzano, Jose A.},
	editor = {OConnor, Rory V. and Baddoo, Nathan and Cuadrago Gallego, Juan and Rejas Muslera, Ricardo and Smolander, Kari and Messnarz, Richard},
	year = {2009},
	doi = {10.1007/978-3-642-04133-4_8},
	pages = {93--104}
}

@incollection{bishop_case_2011,
	address = {Berlin, Heidelberg},
	title = {A {Case} of {Visitor} versus {Interpreter} {Pattern}},
	volume = {6705},
	isbn = {978-3-642-21951-1 978-3-642-21952-8},
	url = {http://link.springer.com/10.1007/978-3-642-21952-8_17},
	urldate = {2018-10-31},
	booktitle = {Objects, {Models}, {Components}, {Patterns}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hills, Mark and Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
	editor = {Bishop, Judith and Vallecillo, Antonio},
	year = {2011},
	doi = {10.1007/978-3-642-21952-8_17},
	pages = {228--243},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\XJ8F36KD\\Hills et al. - 2011 - A Case of Visitor versus Interpreter Pattern.pdf:application/pdf}
}

@article{guo_data-efficient_2018,
	title = {Data-efficient performance learning for configurable systems},
	volume = {23},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-017-9573-6},
	doi = {10.1007/s10664-017-9573-6},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
	month = jun,
	year = {2018},
	pages = {1826--1867}
}

@incollection{czarnecki_adding_2008,
	address = {Berlin, Heidelberg},
	title = {Adding {Dependability} {Analysis} {Capabilities} to the {MARTE} {Profile}},
	volume = {5301},
	isbn = {978-3-540-87874-2 978-3-540-87875-9},
	url = {http://link.springer.com/10.1007/978-3-540-87875-9_51},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bernardi, Simona and Merseguer, Jos and Petriu, Dorina C.},
	editor = {Czarnecki, Krzysztof and Ober, Ileana and Bruel, Jean-Michel and Uhl, Axel and Vlter, Markus},
	year = {2008},
	doi = {10.1007/978-3-540-87875-9_51},
	pages = {736--750},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\98IMB5JX\\Bernardi et al. - 2008 - Adding Dependability Analysis Capabilities to the .pdf:application/pdf}
}

@incollection{pires_surveying_2018,
	address = {Cham},
	title = {Surveying {Co}-evolution in {Modeling} {Ecosystems}},
	volume = {880},
	isbn = {978-3-319-94763-1 978-3-319-94764-8},
	url = {http://link.springer.com/10.1007/978-3-319-94764-8_15},
	urldate = {2018-10-31},
	booktitle = {Model-{Driven} {Engineering} and {Software} {Development}},
	publisher = {Springer International Publishing},
	author = {Etzlstorfer, Jrgen and Kapsammer, Elisabeth and Schwinger, Wieland and Schnbck, Johannes},
	editor = {Pires, Lus Ferreira and Hammoudi, Slimane and Selic, Bran},
	year = {2018},
	doi = {10.1007/978-3-319-94764-8_15},
	pages = {354--376}
}

@InCollection{bishop_case_2011,
  author    = {Hills, Mark and Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  title     = {A {Case} of {Visitor} versus {Interpreter} {Pattern}},
  booktitle = {Objects, {Models}, {Components}, {Patterns}},
  publisher = {Springer Berlin Heidelberg},
  year      = {2011},
  editor    = {Bishop, Judith and Vallecillo, Antonio},
  volume    = {6705},
  pages     = {228--243},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-21951-1 978-3-642-21952-8},
  doi       = {10.1007/978-3-642-21952-8_17},
  file      = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\XJ8F36KD\\Hills et al. - 2011 - A Case of Visitor versus Interpreter Pattern.pdf:application/pdf},
  url       = {http://link.springer.com/10.1007/978-3-642-21952-8_17},
  urldate   = {2018-10-31},
}

@article{herbold_model-based_2017,
	title = {Model-based testing as a service},
	volume = {19},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/10.1007/s10009-017-0449-2},
	doi = {10.1007/s10009-017-0449-2},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Herbold, Steffen and Hoffmann, Andreas},
	month = jun,
	year = {2017},
	pages = {271--279},
	file = {Full Text:C\:\\Users\\Juliana\\Zotero\\storage\\ERKZJJET\\Herbold e Hoffmann - 2017 - Model-based testing as a service.pdf:application/pdf}
}

@incollection{king_goal-based_2011,
	address = {Cham},
	title = {Goal-{Based} {Behavioral} {Customization} of {Information} {Systems}},
	volume = {141},
	isbn = {978-3-319-98176-5 978-3-319-98177-2},
	url = {http://link.springer.com/10.1007/978-3-642-21640-4_8},
	urldate = {2018-10-31},
	booktitle = {Active {Flow} and {Combustion} {Control} 2018},
	publisher = {Springer International Publishing},
	author = {Liaskos, Sotirios and Litoiu, Marin and Jungblut, Marina Daoud and Mylopoulos, John},
	editor = {King, Rudibert},
	year = {2011},
	doi = {10.1007/978-3-642-21640-4_8},
	pages = {77--92}
}

@article{baresi_formal_2015,
	title = {Formal verification and validation of embedded systems: the {UML}-based {MADES} approach},
	volume = {14},
	issn = {1619-1366, 1619-1374},
	shorttitle = {Formal verification and validation of embedded systems},
	url = {http://link.springer.com/10.1007/s10270-013-0330-z},
	doi = {10.1007/s10270-013-0330-z},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Baresi, Luciano and Blohm, Gundula and Kolovos, Dimitrios S. and Matragkas, Nicholas and Motta, Alfredo and Paige, Richard F. and Radjenovic, Alek and Rossi, Matteo},
	month = feb,
	year = {2015},
	pages = {343--363}
}

@incollection{noauthor_tagging_2007,
	address = {Berkeley, CA},
	title = {Tagging {Support}},
	isbn = {978-1-59059-736-1},
	url = {http://link.springer.com/10.1007/978-1-4302-0276-9_7},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Ruby} on {Rails} {E}-{Commerce}},
	publisher = {Apress},
	year = {2007},
	doi = {10.1007/978-1-4302-0276-9_7},
	pages = {197--221}
}

@incollection{thomas_model-based_2011,
	address = {Berlin, Heidelberg},
	title = {Model-{Based} {Evaluation} and {Improvement} of {PTP} {Syntonisation} {Accuracy} in {Packet}-{Switched} {Backhaul} {Networks} for {Mobile} {Applications}},
	volume = {6977},
	isbn = {978-3-642-24748-4 978-3-642-24749-1},
	url = {http://link.springer.com/10.1007/978-3-642-24749-1_17},
	urldate = {2018-10-31},
	booktitle = {Computer {Performance} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wolter, Katinka and Reinecke, Philipp and Mittermaier, Alfons},
	editor = {Thomas, Nigel},
	year = {2011},
	doi = {10.1007/978-3-642-24749-1_17},
	pages = {219--234}
}

@incollection{edge_mass_2015,
	address = {Berkeley, CA},
	title = {Mass {Deployment}},
	isbn = {978-1-4842-1705-4 978-1-4842-1706-1},
	url = {http://link.springer.com/10.1007/978-1-4842-1706-1_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Enterprise {Mac} {Administrator}s {Guide}},
	publisher = {Apress},
	author = {Edge, Charles S. and Smith, William},
	collaborator = {Edge, Charles S. and Smith, William},
	year = {2015},
	doi = {10.1007/978-1-4842-1706-1_6},
	pages = {213--281}
}

@incollection{sutcliffe_crews-savre:_1998,
	address = {Boston, MA},
	title = {{CREWS}-{SAVRE}: {Scenarios} for {Acquiring} and {Validating} {Requirements}},
	isbn = {978-1-4613-7568-5 978-1-4615-5613-8},
	shorttitle = {{CREWS}-{SAVRE}},
	url = {http://link.springer.com/10.1007/978-1-4615-5613-8_3},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Domain {Modelling} for {Interactive} {Systems} {Design}},
	publisher = {Springer US},
	author = {Maiden, N. A. M},
	editor = {Sutcliffe, Alistair and Benyon, David},
	year = {1998},
	doi = {10.1007/978-1-4615-5613-8_3},
	pages = {39--66},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\MRLS5D9Z\\Maiden - 1998 - CREWS-SAVRE Scenarios for Acquiring and Validatin.pdf:application/pdf}
}

@incollection{barner_feedback-based_2011,
	address = {Berlin, Heidelberg},
	title = {Feedback-{Based} {Coverage} {Directed} {Test} {Generation}: {An} {Industrial} {Evaluation}},
	volume = {6504},
	isbn = {978-3-642-19582-2 978-3-642-19583-9},
	shorttitle = {Feedback-{Based} {Coverage} {Directed} {Test} {Generation}},
	url = {http://link.springer.com/10.1007/978-3-642-19583-9_13},
	urldate = {2018-10-31},
	booktitle = {Hardware and {Software}: {Verification} and {Testing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ioannides, Charalambos and Barrett, Geoff and Eder, Kerstin},
	editor = {Barner, Sharon and Harris, Ian and Kroening, Daniel and Raz, Orna},
	year = {2011},
	doi = {10.1007/978-3-642-19583-9_13},
	pages = {112--128},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\BNZXLX6F\\Ioannides et al. - 2011 - Feedback-Based Coverage Directed Test Generation .pdf:application/pdf}
}

@incollection{edge_mass_2009,
	address = {Berkeley, CA},
	title = {Mass {Deployment}},
	isbn = {978-1-4302-2443-3 978-1-4302-2444-0},
	url = {http://link.springer.com/10.1007/978-1-4302-2444-0_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Enterprise {Mac} {Administrator}s {Guide}},
	publisher = {Apress},
	author = {Edge, Charles S. and Hunter, Beau and Smith, Zack},
	collaborator = {Edge, Charles S. and Hunter, Beau and Smith, Zack},
	year = {2009},
	doi = {10.1007/978-1-4302-2444-0_6},
	pages = {287--354}
}

@article{baur_interoperable_2009,
	title = {An {Interoperable} {Grid} {Information} {System} for {Integrated} {Resource} {Monitoring} {Based} on {Virtual} {Organizations}},
	volume = {7},
	issn = {1570-7873, 1572-9184},
	url = {http://link.springer.com/10.1007/s10723-009-9134-3},
	doi = {10.1007/s10723-009-9134-3},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Journal of Grid Computing},
	author = {Baur, Timo and Breu, Rebecca and Klmn, Tibor and Lindinger, Tobias and Milbert, Anne and Poghosyan, Gevorg and Reiser, Helmut and Romberg, Mathilde},
	month = sep,
	year = {2009},
	pages = {319--333}
}

@article{choetkiertikul_predicting_2017,
	title = {Predicting the delay of issues with due dates in software projects},
	volume = {22},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-016-9496-7},
	doi = {10.1007/s10664-016-9496-7},
	language = {en},
	number = {3},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
	month = jun,
	year = {2017},
	pages = {1223--1263}
}

@incollection{noauthor_adding_2007,
	address = {Berkeley, CA},
	title = {Adding {Users} and {Groups}},
	isbn = {978-1-59059-841-2},
	url = {http://link.springer.com/10.1007/978-1-4302-0273-8_3},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Practical {Rails} {Social} {Networking} {Sites}},
	publisher = {Apress},
	year = {2007},
	doi = {10.1007/978-1-4302-0273-8_3},
	pages = {47--81}
}

@incollection{davis_grails_2016,
	address = {Berkeley, CA},
	title = {Grails},
	isbn = {978-1-4842-2116-7 978-1-4842-2117-4},
	url = {http://link.springer.com/10.1007/978-1-4842-2117-4_13},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Learning {Groovy}},
	publisher = {Apress},
	author = {Davis, Adam L.},
	collaborator = {Davis, Adam L.},
	year = {2016},
	doi = {10.1007/978-1-4842-2117-4_13},
	pages = {71--78}
}

@incollection{noauthor_d_2004,
	address = {Boston},
	title = {D},
	volume = {767},
	isbn = {978-1-4020-7889-7},
	url = {http://link.springer.com/10.1007/1-4020-7927-3_4},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {The {Information} {Security} {Dictionary}},
	publisher = {Kluwer Academic Publishers},
	year = {2004},
	doi = {10.1007/1-4020-7927-3_4},
	pages = {77--105}
}

@incollection{noauthor_s_2004,
	address = {Boston},
	title = {S},
	volume = {767},
	isbn = {978-1-4020-7889-7},
	url = {http://link.springer.com/10.1007/1-4020-7927-3_19},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {The {Information} {Security} {Dictionary}},
	publisher = {Kluwer Academic Publishers},
	year = {2004},
	doi = {10.1007/1-4020-7927-3_19},
	pages = {282--318}
}

@incollection{kalmanek_capacity_2010,
	address = {London},
	title = {Capacity and {Performance} {Engineering} for {Networked} {Application} {Servers}: {A} {Case} {Study} in {E}-mail {Platform} {Planning}},
	isbn = {978-1-84882-827-8 978-1-84882-828-5},
	shorttitle = {Capacity and {Performance} {Engineering} for {Networked} {Application} {Servers}},
	url = {http://link.springer.com/10.1007/978-1-84882-828-5_16},
	urldate = {2018-10-31},
	booktitle = {Guide to {Reliable} {Internet} {Services} and {Applications}},
	publisher = {Springer London},
	author = {Reeser, Paul},
	editor = {Kalmanek, Charles R. and Misra, Sudip and Yang, Yang},
	year = {2010},
	doi = {10.1007/978-1-84882-828-5_16},
	pages = {581--627}
}

@incollection{rumpe_refactoring_2017,
	address = {Cham},
	title = {Refactoring as a {Model} {Transformation}},
	isbn = {978-3-319-58861-2 978-3-319-58862-9},
	url = {http://link.springer.com/10.1007/978-3-319-58862-9_9},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Agile {Modeling} with {UML}},
	publisher = {Springer International Publishing},
	author = {Rumpe, Bernhard},
	collaborator = {Rumpe, Bernhard},
	year = {2017},
	doi = {10.1007/978-3-319-58862-9_9},
	pages = {255--283}
}

@incollection{fitzgerald_unified_2008,
	address = {Berlin, Heidelberg},
	title = {A {Unified} {Approach} to {Abstract} {Interpretation}, {Formal} {Verification} and {Testing} of {C}/{C}++ {Modules}},
	volume = {5160},
	isbn = {978-3-540-85761-7 978-3-540-85762-4},
	url = {http://link.springer.com/10.1007/978-3-540-85762-4_2},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Theoretical {Aspects} of {Computing} - {ICTAC} 2008},
	publisher = {Springer Berlin Heidelberg},
	author = {Peleska, Jan},
	editor = {Fitzgerald, John S. and Haxthausen, Anne E. and Yenigun, Husnu},
	year = {2008},
	doi = {10.1007/978-3-540-85762-4_2},
	pages = {3--22}
}

@incollection{lipp_zurich_2008,
	address = {Berlin, Heidelberg},
	title = {The {Zurich} {Trusted} {Information} {Channel}  {An} {Efficient} {Defence} {Against} {Man}-in-the-{Middle} and {Malicious} {Software} {Attacks}},
	volume = {4968},
	isbn = {978-3-540-68978-2 978-3-540-68979-9},
	url = {http://link.springer.com/10.1007/978-3-540-68979-9_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Trusted {Computing} - {Challenges} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Weigold, Thomas and Kramp, Thorsten and Hermann, Reto and Hring, Frank and Buhler, Peter and Baentsch, Michael},
	editor = {Lipp, Peter and Sadeghi, Ahmad-Reza and Koch, Klaus-Michael},
	year = {2008},
	doi = {10.1007/978-3-540-68979-9_6},
	pages = {75--91}
}

@incollection{rocher_integrating_2009,
	address = {Berkeley, CA},
	title = {Integrating {Grails}},
	isbn = {978-1-59059-995-2 978-1-4302-0871-6},
	url = {http://link.springer.com/10.1007/978-1-4302-0871-6_12},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {The {Definitive} {Guide} to {Grails}},
	publisher = {Apress},
	author = {Rocher, Graeme and Brown, Jeff},
	collaborator = {Rocher, Graeme and Brown, Jeff},
	year = {2009},
	doi = {10.1007/978-1-4302-0871-6_12},
	pages = {305--365}
}

@incollection{brown_integration_2013,
	address = {Berkeley, CA},
	title = {Integration and {Dependency} {Management}},
	isbn = {978-1-4302-4377-9 978-1-4302-4378-6},
	url = {http://link.springer.com/10.1007/978-1-4302-4378-6_11},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {The {Definitive} {Guide} to {Grails} 2},
	publisher = {Apress},
	author = {Brown, Jeff Scott and Rocher, Graeme},
	collaborator = {Brown, Jeff Scott and Rocher, Graeme},
	year = {2013},
	doi = {10.1007/978-1-4302-4378-6_11},
	pages = {249--291}
}

@incollection{leue_integration_2008,
	address = {Berlin, Heidelberg},
	title = {Integration of {Formal} {Analysis} into a {Model}-{Based} {Software} {Development} {Process}},
	volume = {4916},
	isbn = {978-3-540-79706-7 978-3-540-79707-4},
	url = {http://link.springer.com/10.1007/978-3-540-79707-4_7},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Formal {Methods} for {Industrial} {Critical} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Whalen, Michael and Cofer, Darren and Miller, Steven and Krogh, Bruce H. and Storm, Walter},
	editor = {Leue, Stefan and Merino, Pedro},
	year = {2008},
	doi = {10.1007/978-3-540-79707-4_7},
	pages = {68--84}
}

@incollection{jain_developing_2003,
	address = {Boston, MA},
	title = {Developing an {OSS} {Strategy}},
	isbn = {978-1-4613-4859-7 978-1-4419-9252-9},
	url = {http://link.springer.com/10.1007/978-1-4419-9252-9_4},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Broadband {Infrastructure}},
	publisher = {Springer US},
	author = {Jain, Shailendra and Hayward, Mark and Kumar, Sharad},
	collaborator = {Jain, Shailendra and Hayward, Mark and Kumar, Sharad},
	year = {2003},
	doi = {10.1007/978-1-4419-9252-9_4},
	pages = {91--112}
}

@incollection{burge_rationale_2008,
	address = {Berlin, Heidelberg},
	title = {Rationale and the {Software} {Lifecycle}},
	isbn = {978-3-540-77582-9 978-3-540-77583-6},
	url = {http://link.springer.com/10.1007/978-3-540-77583-6_10},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Rationale-{Based} {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	collaborator = {Burge, Janet E. and Carroll, John M. and McCall, Raymond and Mistrik, Ivan},
	year = {2008},
	doi = {10.1007/978-3-540-77583-6_10},
	pages = {125--138}
}

@article{groce_how_2018,
	title = {How verified (or tested) is my code? {Falsification}-driven verification and testing},
	volume = {25},
	issn = {0928-8910, 1573-7535},
	shorttitle = {How verified (or tested) is my code?},
	url = {http://link.springer.com/10.1007/s10515-018-0240-y},
	doi = {10.1007/s10515-018-0240-y},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Automated Software Engineering},
	author = {Groce, Alex and Ahmed, Iftekhar and Jensen, Carlos and McKenney, Paul E. and Holmes, Josie},
	month = dec,
	year = {2018},
	pages = {917--960}
}

@incollection{magedanz_novel_2011,
	address = {Berlin, Heidelberg},
	title = {A {Novel} {Testbed} for {P}2P {Networks}},
	volume = {46},
	isbn = {978-3-642-17850-4 978-3-642-17851-1},
	url = {http://link.springer.com/10.1007/978-3-642-17851-1_5},
	urldate = {2018-10-31},
	booktitle = {Testbeds and {Research} {Infrastructures}. {Development} of {Networks} and {Communities}},
	publisher = {Springer Berlin Heidelberg},
	author = {Perl, Pekka H. J. and Paananen, Jori P. and Mukhopadhyay, Milton and Laulajainen, Jukka-Pekka},
	editor = {Magedanz, Thomas and Gavras, Anastasius and Thanh, Nguyen Huu and Chase, Jeffry S.},
	year = {2011},
	doi = {10.1007/978-3-642-17851-1_5},
	pages = {69--83}
}

@incollection{noauthor_introduction_2008,
	address = {Berkeley, CA},
	title = {Introduction to {Grails}},
	isbn = {978-1-4302-1045-0 978-1-4302-1046-7},
	url = {http://link.springer.com/10.1007/978-1-4302-1046-7_4},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Groovy} and {Grails}},
	publisher = {Apress},
	year = {2008},
	doi = {10.1007/978-1-4302-1046-7_4},
	pages = {63--103}
}

@incollection{cuesta_abstraction_2018,
	address = {Cham},
	title = {Abstraction {Layered} {Architecture}: {Writing} {Maintainable} {Embedded} {Code}},
	volume = {11048},
	isbn = {978-3-030-00760-7 978-3-030-00761-4},
	shorttitle = {Abstraction {Layered} {Architecture}},
	url = {http://link.springer.com/10.1007/978-3-030-00761-4_9},
	urldate = {2018-10-31},
	booktitle = {Software {Architecture}},
	publisher = {Springer International Publishing},
	author = {Spray, John and Sinha, Roopak},
	editor = {Cuesta, Carlos E. and Garlan, David and Prez, Jennifer},
	year = {2018},
	doi = {10.1007/978-3-030-00761-4_9},
	pages = {131--146}
}

@incollection{goos_influence_1992,
	address = {Berlin, Heidelberg},
	title = {The influence of software engineering paradigms on individual and team project results},
	volume = {640},
	isbn = {978-3-540-55963-4 978-3-540-47330-5},
	url = {http://link.springer.com/10.1007/3-540-55963-9_67},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering} {Education}},
	publisher = {Springer Berlin Heidelberg},
	author = {Junk, William and Oman, Paul},
	editor = {Goos, Gerhard and Hartmanis, Juris and Sledge, Carol},
	year = {1992},
	doi = {10.1007/3-540-55963-9_67},
	pages = {417--436}
}

@incollection{hasan_stress_2003,
	address = {Berkeley, CA},
	title = {Stress {Testing} and {Monitoring} {ASP}.{NET} {Applications}},
	isbn = {978-1-59059-072-0 978-1-4302-0758-0},
	url = {http://link.springer.com/10.1007/978-1-4302-0758-0_7},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Performance {Tuning} and {Optimizing} {ASP}.{NET} {Applications}},
	publisher = {Apress},
	author = {Hasan, Jeffrey and Tu, Kenneth},
	collaborator = {Hasan, Jeffrey and Tu, Kenneth},
	year = {2003},
	doi = {10.1007/978-1-4302-0758-0_7},
	pages = {253--296}
}

@incollection{cerone_towards_2018,
	address = {Cham},
	title = {Towards a {Taxonomy} of {Microservices} {Architectures}},
	volume = {10729},
	isbn = {978-3-319-74780-4 978-3-319-74781-1},
	url = {http://link.springer.com/10.1007/978-3-319-74781-1_15},
	urldate = {2018-10-31},
	booktitle = {Software {Engineering} and {Formal} {Methods}},
	publisher = {Springer International Publishing},
	author = {Garriga, Martin},
	editor = {Cerone, Antonio and Roveri, Marco},
	year = {2018},
	doi = {10.1007/978-3-319-74781-1_15},
	pages = {203--218}
}

@incollection{noauthor_overview_2006,
	title = {Overview of .{NET} {Application} {Architecture}},
	isbn = {978-1-59059-522-0},
	url = {http://link.springer.com/10.1007/978-1-4302-0073-4_1},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Expert {ASP}.{NET} 2.0},
	publisher = {Apress},
	year = {2006},
	doi = {10.1007/978-1-4302-0073-4_1},
	pages = {3--28}
}

@article{bill_local_2017,
	title = {A local and global tour on {MOMoT}},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-017-0644-3},
	doi = {10.1007/s10270-017-0644-3},
	language = {en},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Bill, Robert and Fleck, Martin and Troya, Javier and Mayerhofer, Tanja and Wimmer, Manuel},
	month = dec,
	year = {2017}
}

@incollection{hammoudi_automated_2017,
	address = {Cham},
	title = {Automated {Web} {Service} {Composition} {Testing} as a {Service}},
	volume = {692},
	isbn = {978-3-319-66301-2 978-3-319-66302-9},
	url = {http://link.springer.com/10.1007/978-3-319-66302-9_6},
	urldate = {2018-10-31},
	booktitle = {Model-{Driven} {Engineering} and {Software} {Development}},
	publisher = {Springer International Publishing},
	author = {Petrova-Antonova, Dessislava and Ilieva, Sylvia and Manova, Denitsa},
	editor = {Hammoudi, Slimane and Pires, Lus Ferreira and Selic, Bran and Desfray, Philippe},
	year = {2017},
	doi = {10.1007/978-3-319-66302-9_6},
	pages = {114--131}
}

@article{winkler_survey_2010,
	title = {A survey of traceability in requirements engineering and model-driven development},
	volume = {9},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-009-0145-0},
	doi = {10.1007/s10270-009-0145-0},
	language = {en},
	number = {4},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Winkler, Stefan and von Pilgrim, Jens},
	month = sep,
	year = {2010},
	pages = {529--565}
}

@incollection{mena_documenting_2014,
	address = {Berkeley, CA},
	title = {Documenting, {Testing}, and {Verifying}},
	isbn = {978-1-4302-6250-3 978-1-4302-6251-0},
	url = {http://link.springer.com/10.1007/978-1-4302-6251-0_15},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Haskell}},
	publisher = {Apress},
	author = {Mena, Alejandro Serrano},
	collaborator = {Mena, Alejandro Serrano},
	year = {2014},
	doi = {10.1007/978-1-4302-6251-0_15},
	pages = {355--371}
}

@incollection{smialek_applying_2015,
	address = {Cham},
	title = {Applying {MDRE} in {Practice}},
	isbn = {978-3-319-12837-5 978-3-319-12838-2},
	url = {http://link.springer.com/10.1007/978-3-319-12838-2_7},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {From {Requirements} to {Java} in a {Snap}},
	publisher = {Springer International Publishing},
	author = {miaek, Micha and Nowakowski, Wiktor},
	collaborator = {miaek, Micha and Nowakowski, Wiktor},
	year = {2015},
	doi = {10.1007/978-3-319-12838-2_7},
	pages = {225--256}
}

@incollection{cooper_web_2009,
	address = {Berkeley, CA},
	title = {Web {Application} {Frameworks}: {Rails}, {Sinatra}, and {Ramaze}},
	isbn = {978-1-4302-2363-4 978-1-4302-2364-1},
	shorttitle = {Web {Application} {Frameworks}},
	url = {http://link.springer.com/10.1007/978-1-4302-2364-1_13},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Beginning {Ruby}},
	publisher = {Apress},
	author = {Cooper, Peter},
	collaborator = {Cooper, Peter},
	year = {2009},
	doi = {10.1007/978-1-4302-2364-1_13},
	pages = {349--411}
}

@incollection{cherifi_lifelong_2011,
	address = {Berlin, Heidelberg},
	title = {Lifelong {Automated} {Testing}: {A} {Collaborative} {Framework} for {Checking} {Industrial} {Products} {Along} {Their} {Lifecycle}},
	volume = {167},
	isbn = {978-3-642-22026-5 978-3-642-22027-2},
	shorttitle = {Lifelong {Automated} {Testing}},
	url = {http://link.springer.com/10.1007/978-3-642-22027-2_8},
	urldate = {2018-10-31},
	booktitle = {Digital {Information} and {Communication} {Technology} and {Its} {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Tamisier, Thomas and Feltz, Fernand},
	editor = {Cherifi, Hocine and Zain, Jasni Mohamad and El-Qawasmeh, Eyas},
	year = {2011},
	doi = {10.1007/978-3-642-22027-2_8},
	pages = {80--86}
}

@article{tran_vbtrace:_2011,
	title = {{VbTrace}: using view-based and model-driven development to support traceability in process-driven {SOAs}},
	volume = {10},
	issn = {1619-1366, 1619-1374},
	shorttitle = {{VbTrace}},
	url = {http://link.springer.com/10.1007/s10270-009-0137-0},
	doi = {10.1007/s10270-009-0137-0},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Software \& Systems Modeling},
	author = {Tran, Huy and Zdun, Uwe and Dustdar, Schahram},
	month = feb,
	year = {2011},
	pages = {5--29},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\J5JSLNNS\\Tran et al. - 2011 - VbTrace using view-based and model-driven develop.pdf:application/pdf}
}

@incollection{noauthor_developing_2007,
	address = {Berkeley, CA},
	title = {Developing a {Content} {Management} {System}},
	isbn = {978-1-59059-841-2},
	url = {http://link.springer.com/10.1007/978-1-4302-0273-8_2},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Practical {Rails} {Social} {Networking} {Sites}},
	publisher = {Apress},
	year = {2007},
	doi = {10.1007/978-1-4302-0273-8_2},
	pages = {21--45}
}

@incollection{noauthor_software_2006,
	title = {Software {Factory} {Template}: {Application} {Core} {Assets}},
	isbn = {978-1-59059-665-4},
	shorttitle = {Software {Factory} {Template}},
	url = {http://link.springer.com/10.1007/978-1-4302-0181-6_6},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Practical {Software} {Factories} in .{NET}},
	publisher = {Apress},
	year = {2006},
	doi = {10.1007/978-1-4302-0181-6_6},
	pages = {121--146}
}

@incollection{jain_enterprise_2003,
	address = {Boston, MA},
	title = {The {Enterprise} {OSS} {Platform}},
	isbn = {978-1-4613-4859-7 978-1-4419-9252-9},
	url = {http://link.springer.com/10.1007/978-1-4419-9252-9_5},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Broadband {Infrastructure}},
	publisher = {Springer US},
	author = {Jain, Shailendra and Hayward, Mark and Kumar, Sharad},
	collaborator = {Jain, Shailendra and Hayward, Mark and Kumar, Sharad},
	year = {2003},
	doi = {10.1007/978-1-4419-9252-9_5},
	pages = {113--150}
}

@incollection{hutchison_multi-stage_2005,
	address = {Berlin, Heidelberg},
	title = {Multi-stage {Programming} with {Functors} and {Monads}: {Eliminating} {Abstraction} {Overhead} from {Generic} {Code}},
	volume = {3676},
	isbn = {978-3-540-29138-1 978-3-540-31977-1},
	shorttitle = {Multi-stage {Programming} with {Functors} and {Monads}},
	url = {http://link.springer.com/10.1007/11561347_18},
	urldate = {2018-10-31},
	booktitle = {Generative {Programming} and {Component} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Carette, Jacques and Kiselyov, Oleg},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Glck, Robert and Lowry, Michael},
	year = {2005},
	doi = {10.1007/11561347_18},
	pages = {256--274},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\V3RU8ZWX\\Carette e Kiselyov - 2005 - Multi-stage Programming with Functors and Monads .pdf:application/pdf}
}

@incollection{hutchison_workshops_2005,
	address = {Berlin, Heidelberg},
	title = {Workshops at the {MODELS} 2005 {Conference}},
	volume = {3713},
	isbn = {978-3-540-29010-0 978-3-540-32057-9},
	url = {http://link.springer.com/10.1007/11557432_52},
	urldate = {2018-10-31},
	booktitle = {Model {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bruel, Jean-Michel},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Briand, Lionel and Williams, Clay},
	year = {2005},
	doi = {10.1007/11557432_52},
	pages = {706--714}
}

@incollection{abdelwahed_automatic_2018,
	address = {Cham},
	title = {Automatic {Planning}: {From} {Event}-{B} to {PDDL}},
	volume = {929},
	isbn = {978-3-030-02851-0 978-3-030-02852-7},
	shorttitle = {Automatic {Planning}},
	url = {http://link.springer.com/10.1007/978-3-030-02852-7_21},
	urldate = {2018-10-31},
	booktitle = {New {Trends} in {Model} and {Data} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Ammar, Sabrine and Bhiri, Mohamed Tahar},
	editor = {Abdelwahed, El Hassan and Bellatreche, Ladjel and Benslimane, Djamal and Golfarelli, Matteo and Jean, Stphane and Mery, Dominique and Nakamatsu, Kazumi and Ordonez, Carlos},
	year = {2018},
	doi = {10.1007/978-3-030-02852-7_21},
	pages = {247--254}
}

@incollection{kirikova_success_2002,
	address = {Boston, MA},
	title = {Success {Factors} for {Outsourced} {Information} {System} {Development}},
	isbn = {978-1-4613-4950-1 978-1-4615-0167-1},
	url = {http://link.springer.com/10.1007/978-1-4615-0167-1_11},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Information {Systems} {Development}},
	publisher = {Springer US},
	author = {Jennex, Murray E. and Adelakun, Olayele},
	editor = {Kirikova, Marite and Grundspenkis, Janis and Wojtkowski, Wita and Wojtkowski, W. Gregory and Wrycza, Stanisaw and Zupani, Joe},
	year = {2002},
	doi = {10.1007/978-1-4615-0167-1_11},
	pages = {123--134}
}

@incollection{hutchison_fingerprint_2005,
	address = {Berlin, Heidelberg},
	title = {A {Fingerprint} {Authentication} {System} {Based} on {Mobile} {Phone}},
	volume = {3546},
	isbn = {978-3-540-27887-0 978-3-540-31638-1},
	url = {http://link.springer.com/10.1007/11527923_16},
	urldate = {2018-10-31},
	booktitle = {Audio- and {Video}-{Based} {Biometric} {Person} {Authentication}},
	publisher = {Springer Berlin Heidelberg},
	author = {Su, Qi and Tian, Jie and Chen, Xinjian and Yang, Xin},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Kanade, Takeo and Jain, Anil and Ratha, Nalini K.},
	year = {2005},
	doi = {10.1007/11527923_16},
	pages = {151--159}
}

@article{smite_empirically_2014,
	title = {An empirically based terminology and taxonomy for global software engineering},
	volume = {19},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-012-9217-9},
	doi = {10.1007/s10664-012-9217-9},
	language = {en},
	number = {1},
	urldate = {2018-10-31},
	journal = {Empirical Software Engineering},
	author = {mite, Darja and Wohlin, Claes and Galvia, Zane and Prikladnicki, Rafael},
	month = feb,
	year = {2014},
	pages = {105--153},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\IZQ6LK4U\\mite et al. - 2014 - An empirically based terminology and taxonomy for .pdf:application/pdf}
}

@incollection{haritsa_efficient_2008,
	address = {Berlin, Heidelberg},
	title = {Efficient {Mining} of {Recurrent} {Rules} from a {Sequence} {Database}},
	volume = {4947},
	isbn = {978-3-540-78567-5 978-3-540-78568-2},
	url = {http://link.springer.com/10.1007/978-3-540-78568-2_8},
	language = {en},
	urldate = {2018-10-31},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lo, David and Khoo, Siau-Cheng and Liu, Chao},
	editor = {Haritsa, Jayant R. and Kotagiri, Ramamohanarao and Pudi, Vikram},
	year = {2008},
	doi = {10.1007/978-3-540-78568-2_8},
	pages = {67--83},
	file = {Submitted Version:C\:\\Users\\Juliana\\Zotero\\storage\\DEGLTEEW\\Lo et al. - 2008 - Efficient Mining of Recurrent Rules from a Sequenc.pdf:application/pdf}
}
@Article{,
  author  = {Dale ParsonBryan SchliederPaul Beatty},
  title   = {Extension Language Automation of Embedded System Debugging},
  journal = {Automated Software Engineering},
  year    = {2002},
  pages   = {9},
  doi     = {10.1023/A:1013276002164},
  url     = {http://link.springer.com/article/10.1023/A%3A1013276002164},
}

@Article{Porter1997,
  author    = {Adam A. Porter},
  title     = {Fundamental Laws and Assumptions of Software Maintenance},
  journal   = {Empirical Software Engineering},
  year      = {1997},
  volume    = {2},
  number    = {2},
  pages     = {119--131},
  doi       = {10.1023/a:1009793015685},
  publisher = {Springer Nature},
  url       = {http://link.springer.com/article/10.1023/A%3A1009793015685},
}

@Comment{jabref-meta: databaseType:bibtex;}
