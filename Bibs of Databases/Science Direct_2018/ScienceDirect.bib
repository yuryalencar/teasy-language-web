% Encoding: UTF-8
@article{OTADUY2017212,
title = "User acceptance testing for Agile-developed web-based applications: Empowering customers through wikis and mind maps",
journal = "Journal of Systems and Software",
volume = "133",
pages = "212 - 229",
year = "2017",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.01.002",
url = "http://www.sciencedirect.com/science/article/pii/S016412121730002X",
author = "I. Otaduy and O. Diaz",
keywords = "Agile development, User acceptance testing, Test automation",
abstract = "User Acceptance Testing (UAT) involves validating software in a real setting by the intended audience. The aim is not so much to check the defined requirements but to ensure that the software satisfies the customer’s needs. Agile methodologies put stringent demands on UAT, if only for the frequency at which it needs to be conducted due to the iterative development of small product releases. In this setting, traditional in-person meetings might not scale up well. Complementary ways are needed to reduce the costs of developer-customer collaboration during UAT. This work introduces a wiki-based approach where customers and developers asynchronously collaborate: developers set the UAT scaffolding that will later shepherd customers when testing. To facilitate understanding, mind maps are used to represent UAT sessions. To facilitate engagement, a popular mind map editor, FreeMind, is turned into an editor for FitNesse, the wiki engine in which these ideas are borne out. The approach is evaluated through a case study involving three real customers. First evaluations are promising. Though at different levels of completeness, the three customers were able to complete a UAT. Customers valued asynchronicity, mind map structuredness, and the transparent generation of documentation out of the UAT session."
}
@article{TICHY2017159,
title = "Rapid Continuous Software Engineering",
journal = "Journal of Systems and Software",
volume = "133",
pages = "159",
year = "2017",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.08.046",
url = "http://www.sciencedirect.com/science/article/pii/S0164121217301887",
author = "Matthias Tichy and Michael Goedicke and Jan Bosch and Brian Fitzgerald"
}
@article{BRIENZA2014124,
title = "3T diffusion tensor imaging and electroneurography of peripheral nerve: A morphofunctional analysis in carpal tunnel syndrome",
journal = "Journal of Neuroradiology",
volume = "41",
number = "2",
pages = "124 - 130",
year = "2014",
issn = "0150-9861",
doi = "https://doi.org/10.1016/j.neurad.2013.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S0150986113000540",
author = "Marianna Brienza and Francesco Pujia and M. Chiara Colaiacomo and M. Grazia Anastasio and Francesco Pierelli and Claudio Di Biasi and Chiara Andreoli and Gianfranco Gualdi and Gabriele O.R. Valente",
keywords = "Diffusion tensor imaging, Median nerve, Fractional anisotropy, Apparent diffusion coefficient, Electroneurography",
abstract = "Summary
Objective
The aim of the study was to assess the diagnostic potential of diffusion tensor imaging (DTI) for pathologies of the peripheral nervous system (PNS) through clinical, electrophysiological and morphological evaluation of the median nerve.
Methods
The present work was a multilevel prospective study involving 30 subjects, 15 of whom had carpal tunnel syndrome (CTS) and 15 healthy controls. All subjects underwent clinical evaluation through administration of the Boston Carpal Tunnel Questionnaire (BCTQ), electroneurography (ENG), 3-Tesla magnetic resonance imaging with DTI, and calculation of fractional anisotropy (FA) and the apparent diffusion coefficient (ADC) at the flexor retinaculum. Tractography was also performed for three-dimensional reconstruction of the route of the median nerve through the carpal tunnel. The degree of functional impairment was compared with the anatomical damage to the median nerve according to ENG and DTI.
Results
FA and ADC were significantly correlated with ENG parameters of CTS and BCTQ data. Mean FA and ADC values in the CTS patients were 0.359±0.06 and 1.866±0.050×10−3mm2/s, respectively, vs 0.59±0.014 and 1.395±0.035×10−3mm2/s, respectively, in the controls. FA was decreased and ADC increased in patients with CTS compared with healthy controls (P<0.05).
Conclusion
DTI parameters were clearly confirmed by both clinical and ENG data and, therefore, may be used for the diagnosis of CTS."
}
@incollection{BIALY201739,
title = "3 - Software Engineering for Model-Based Development by Domain Experts",
editor = "Edward Griffor",
booktitle = "Handbook of System Safety and Security",
publisher = "Syngress",
address = "Boston",
pages = "39 - 64",
year = "2017",
isbn = "978-0-12-803773-7",
doi = "https://doi.org/10.1016/B978-0-12-803773-7.00003-6",
url = "http://www.sciencedirect.com/science/article/pii/B9780128037737000036",
author = "M. Bialy and V. Pantelic and J. Jaskolka and A. Schaap and L. Patcas and M. Lawford and A. Wassyng",
keywords = "Software engineering, domain experts, functional safety, embedded software, model-based development, Simulink, automotive",
abstract = "Model-Based Development (MBD) has been gaining traction in the development of embedded software in many industries, especially in safety-critical domains. The models are typically described using domain-specific languages and tools that are readily accessible to the domain experts. Consequently, domain experts, despite not having formal software engineering training, find themselves creating models (designs) from which code is generated, thus effectively contributing to the design and coding activities of software development. This new role for domain experts as software developers can have a direct impact on the system safety if the domain experts do not follow software engineering best practices. In this chapter, we describe our experiences as software engineers in multiyear collaborations with domain experts from the automotive industry, who are developing embedded software with the MBD approach. We provide guidelines that strengthen the collaboration between domain experts and software engineers and improve the quality, and hence safety, of embedded software systems developed using MBD. We clarify the role of some of the most commonly used software engineering principles and artefacts, while also addressing issues and misconceptions encountered in adopting software engineering practices in MBD. Although this chapter focuses on the MBD of automotive embedded software in Matlab Simulink, the guidelines we provide are applicable to the MBD of software in general."
}
@article{ZHU2015447,
title = "Serum DHEAS levels are associated with the development of depression",
journal = "Psychiatry Research",
volume = "229",
number = "1",
pages = "447 - 453",
year = "2015",
issn = "0165-1781",
doi = "https://doi.org/10.1016/j.psychres.2015.05.093",
url = "http://www.sciencedirect.com/science/article/pii/S0165178115004084",
author = "Guang Zhu and You Yin and Chun-Lan Xiao and Rong-Jie Mao and Bo-Hai Shi and Yong Jie and Zuo-Wei Wang",
keywords = "Dehydroepiandrosterone sulfate, Depression, Serum levels, Case–control study, Meta-analysis",
abstract = "The aim of study was to evaluate the association between serum DHEAS levels and depression with a case–control study together with a meta-analysis. Radioimmunoassay (RIA) was performed to measure the serum DHEAS levels of all participants before and after treatment. Depression Patients were divided into mild depression and severe depression based on Hamilton depression scale (HAMD24) and received 5-hydroxytryptamine (5-HT) and citalopram (20mg/d) for 8 weeks. Case–control studies related to our study theme were enrolled for meta-analysis and Comprehensive Meta-analysis 2.0 (CMA 2.0) was used for statistical analysis. After treatment, DHEAS levels in depression patients were significantly increased, while before and after treatment, DHEAS levels were all lower in depression patients than in controls (all P<0.001); further analysis on age revealed that DHEAS levels were decreased with the rising of age. Meta-analysis results suggested that serum DHEAS levels (ng/mL) were significantly higher in healthy controls compared to depression patients (SMD=0.777, 95%CI=0.156–1.399, P=0.014). In conclusion, our study suggests that serum DHEAS levels are associated with the development of depression and it decreased with the rising of age."
}
@incollection{UTTING201653,
title = "Chapter Two - Recent Advances in Model-Based Testing",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "101",
pages = "53 - 120",
year = "2016",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2015.11.004",
url = "http://www.sciencedirect.com/science/article/pii/S0065245815000650",
author = "Mark Utting and Bruno Legeard and Fabrice Bouquet and Elizabeta Fourneret and Fabien Peureux and Alexandre Vernotte",
keywords = "Model-based testing, Modeling approaches, Test generation Technology, Security testing",
abstract = "This chapter gives an overview of the field of model-based testing (MBT), particularly the recent advances in the last decade. It gives a summary of the MBT process, the modeling languages that are currently used by the various communities who practice MBT, the technologies used to generate tests from models, and discusses best practices, such as traceability between models and tests. It also briefly describes several findings from a recent survey of MBT users in industry, outlines the increasingly popular use of MBT for security testing, and discusses future challenges for MBT."
}
@article{GRAUPP2011175,
title = "Association of genetic variants in the two isoforms of 5α-reductase, SRD5A1 and SRD5A2, in lean patients with polycystic ovary syndrome",
journal = "European Journal of Obstetrics & Gynecology and Reproductive Biology",
volume = "157",
number = "2",
pages = "175 - 179",
year = "2011",
issn = "0301-2115",
doi = "https://doi.org/10.1016/j.ejogrb.2011.03.026",
url = "http://www.sciencedirect.com/science/article/pii/S0301211511001849",
author = "M. Graupp and E. Wehr and N. Schweighofer and T.R. Pieber and B. Obermayer-Pietsch",
keywords = "Polycystic ovary syndrome, 5α-Reductase, Polymorphisms, Hyperandrogenemia",
abstract = "Objective
Given its role in converting testosterone to dihydrotestosterone and cortisol to dihydrocortisol, 5α-reductase may be important in the pathophysiology of the polycystic ovary syndrome (PCOS). Increased activity of this enzyme has already been demonstrated in ovaries of affected women, and might be caused by genetic alterations. The aim of this study was to analyze representative genetic variants of both isoforms of 5α-reductase with regard to PCOS parameters in lean and obese women.
Study design
We analyzed one single nucleotide polymorphism (SNP) (rs523349) of the isoform 2 (SRD5A2) and one haplotype of the isoform 1 (SRD5A1), consisting of the two SNPs rs39848 and rs3797179, in 249 women with PCOS and 226 healthy women using a 5′-exonuclease-assay. The genotypes were associated with anthropometric, metabolic and hormonal as well as functional tests in these women.
Results
In the investigated haplotype of SRD5A1, the TA variant was associated with an increased frequency of PCOS (P=0.022) and an increased Ferriman–Gallwey Score (hirsutism) (P=0.016) in women with normal weight. The G allele at the examined position of the SRD5A2 showed a decreased frequency of PCOS (P=0.03) in women with normal weight.
Conclusion
One of the keys in the development of the PCOS is hyperandrogenism, which might be caused by an increased 5α-reductase activity, as it is often seen in obesity. This mechanism might therefore be of importance in lean PCOS patients and contribute to the clinical findings."
}
@incollection{FELDERER20161,
title = "Chapter One - Security Testing: A Survey",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "101",
pages = "1 - 51",
year = "2016",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2015.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S0065245815000649",
author = "Michael Felderer and Matthias Büchler and Martin Johns and Achim D. Brucker and Ruth Breu and Alexander Pretschner",
keywords = "Security testing, Security testing techniques, Model-based security testing, White-box security testing, Black-box security testing, Penetration testing, Security regression testing, Security engineering, Software testing, Survey",
abstract = "Identifying vulnerabilities and ensuring security functionality by security testing is a widely applied measure to evaluate and improve the security of software. Due to the openness of modern software-based systems, applying appropriate security testing techniques is of growing importance and essential to perform effective and efficient security testing. Therefore, an overview of actual security testing techniques is of high value both for researchers to evaluate and refine the techniques and for practitioners to apply and disseminate them. This chapter fulfills this need and provides an overview of recent security testing techniques. For this purpose, it first summarize the required background of testing and security engineering. Then, basics and recent developments of security testing techniques applied during the secure software development life cycle, ie, model-based security testing, code-based testing and static analysis, penetration testing and dynamic analysis, as well as security regression testing are discussed. Finally, the security testing techniques are illustrated by adopting them for an example three-tiered web-based business application."
}
@article{MOHIYIDDEEN2012677,
title = "Follicle-stimulating hormone receptor gene polymorphisms are not associated with ovarian reserve markers",
journal = "Fertility and Sterility",
volume = "97",
number = "3",
pages = "677 - 681",
year = "2012",
issn = "0015-0282",
doi = "https://doi.org/10.1016/j.fertnstert.2011.12.040",
url = "http://www.sciencedirect.com/science/article/pii/S0015028211029244",
author = "Lamiya Mohiyiddeen and William G. Newman and Helen McBurney and Betselot Mulugeta and Stephen A. Roberts and Luciano G. Nardo",
keywords = "Single nucleotide polymorphism, Ser680Asn, Thr307Ala FSH receptor, antimüllerian hormone, antral follicle count, follicle-stimulating hormone",
abstract = "Objective
To evaluate the association between variants in the FSHR receptor (FSHR) gene and current markers of ovarian reserve (antimüllerian hormone, antral follicle count, FSH).
Design
Prospective observational study.
Setting
Tertiary referral center for reproductive medicine.
Patient(s)
Women (n = 421) undergoing their first cycle of controlled ovarian stimulation for IVF.
Intervention(s)
Baseline pelvic ultrasound and blood tests were taken on day 2–3 of the cycle for assessment of baseline hormones and for DNA extraction. Genotypes for FSHR p.Asn680Ser and p.Thr307Ala variants were determined using TaqMan allelic discrimination assays.
Main Outcome Measure(s)
Association of FSHR single nucleotide polymorphisms with markers of ovarian reserve.
Result(s)
There was no evidence of any difference in basal FSH, antimüllerian hormone, or antral follicle count between the patients with different genotypes, with or without an adjustment for age or body mass index.
Conclusion(s)
No associations of FSHR genotypes with markers of ovarian reserve were detected in our cohort."
}
@article{WANG2011181,
title = "A Modeling Language Based on UML for Modeling Simulation Testing System of Avionic Software",
journal = "Chinese Journal of Aeronautics",
volume = "24",
number = "2",
pages = "181 - 194",
year = "2011",
issn = "1000-9361",
doi = "https://doi.org/10.1016/S1000-9361(11)60022-8",
url = "http://www.sciencedirect.com/science/article/pii/S1000936111600228",
author = "Lize WANG and Bin LIU and Minyan LU",
keywords = "avionics, hardware-in-the-loop, test facilities, meta-model, UML profile, domain-specific modeling language, abstract state machine",
abstract = "With direct expression of individual application domain patterns and ideas, domain-specific modeling language (DSML) is more and more frequently used to build models instead of using a combination of one or more general constructs. Based on the profile mechanism of unified modeling language (UML) 2.2, a kind of DSML is presented to model simulation testing systems of avionic software (STSAS). To define the syntax, semantics and notions of the DSML, the domain model of the STSAS from which we generalize the domain concepts and relationships among these concepts is given, and then, the domain model is mapped into a UML meta-model, named UML-STSAS profile. Assuming a flight control system (FCS) as system under test (SUT), we design the relevant STSAS. The results indicate that extending UML to the simulation testing domain can effectively and precisely model STSAS."
}
@article{SYRIANI201843,
title = "Systematic mapping study of template-based code generation",
journal = "Computer Languages, Systems & Structures",
volume = "52",
pages = "43 - 62",
year = "2018",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2017.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S1477842417301239",
author = "Eugene Syriani and Lechanceux Luhunu and Houari Sahraoui",
keywords = "Code generation, Systematic mapping study, Model-driven engineering",
abstract = "Context: Template-based code generation (TBCG) is a synthesis technique that produces code from high-level specifications, called templates. TBCG is a popular technique in model-driven engineering (MDE) given that they both emphasize abstraction and automation. Given the diversity of tools and approaches, it is necessary to classify existing TBCG techniques to better guide developers in their choices. Objective: The goal of this article is to better understand the characteristics of TBCG techniques and associated tools, identify research trends, and assess the importance of the role of MDE in this code synthesis approach. Method: We survey the literature to paint an interesting picture about the trends and uses of TBCG in research. To this end, we follow a systematic mapping study process. Results: Our study shows, among other observations, that the research community has been diversely using TBCG over the past 16 years. An important observation is that TBCG has greatly benefited from MDE. It has favored a template style that is output-based and high-level modeling languages as input. TBCG is mainly used to generate source code and has been applied to many domains. Conclusion: TBCG is now a mature technique and much research work is still conducted in this area. However, some issues remain to be addressed, such as support for template definition and assessment of the correctness and quality of the generated code."
}
@article{STURM20141390,
title = "Evaluating the productivity of a reference-based programming approach: A controlled experiment",
journal = "Information and Software Technology",
volume = "56",
number = "10",
pages = "1390 - 1402",
year = "2014",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.05.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914001062",
author = "Arnon Sturm and Oded Kramer",
keywords = "Productivity, Programming, Software reusability, Software quality, Domain engineering",
abstract = "Context
Domain engineering aims at facilitating software development in an efficient and economical way. One way to measure that is through productivity indicators, which refer to the ability of creating a quality software product in a limited period and with limited resources. Many approaches have been devised to increase productivity; however, these approaches seem to suffer from a tension between expressiveness on the one hand, and applicability (or the lack of it) in providing guidance for developers.
Objective
This paper evaluates the applicability and efficiency of adopting a domain engineering approach, called Application-based DOmain Modeling (ADOM), in the context of the programming task with Java, and thus termed ADOM-Java, for improving productivity in terms of code quality and development time.
Method
To achieve that objective we have qualitatively evaluate the approach using questionnaires and following a text analysis procedure. We also set a controlled experiment in which 50 undergraduate students performed a Java-based programming task using either ADOM-Java or Java alone.
Results
The qualitative evaluation reveal that the approach is easy to uses and provides valuable guidance. Nevertheless, it requires training. The outcomes of the experiment indicate that the approach is applicable and that the students that used ADOM-Java achieved better code quality, as well as better functionality and within less time than the students who used only Java.
Conclusion
The results of the experiments imply that by providing a code base equipped with reuse guidelines for programmers can increase programming productivity in terms of quality and development time. These guidelines may also enforce coding standards and architectural design."
}
@article{BASSO2016612,
title = "Automated design of multi-layered web information systems",
journal = "Journal of Systems and Software",
volume = "117",
pages = "612 - 637",
year = "2016",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2016.04.060",
url = "http://www.sciencedirect.com/science/article/pii/S0164121216300358",
author = "Fábio Paulo Basso and Raquel Mainardi Pillat and Toacy Cavalcante Oliveira and Fabricia Roos-Frantz and Rafael Z. Frantz",
keywords = "Model-driven web engineering, Rapid application prototype, Domain-specific language, Prototyping, Automated design, Mockup, Experience report",
abstract = "In the development of web information systems, design tasks are commonly used in approaches for Model-Driven Web Engineering (MDWE) to represent models. To generate fully implemented prototypes, these models require a rich representation of the semantics for actions (e.g., database persistence operations). In the development of some use case scenarios for the multi-layered development of web information systems, these design tasks may consume weeks of work even for experienced designers. The literature pointed out that the impossibility for executing a software project with short iterations hampers the adoption of some approaches for design in some contexts, such as start-up companies. A possible solution to introduce design tasks in short iterations is the use of automated design techniques, which assist the production of models by means of transformation tasks and refinements. This paper details our methodology for MDWE, which is supported by automated design techniques strictly associated with use case patterns of type CRUD. The novelty relies on iterations that are possible for execution with short time-scales. This is a benefit from automated design techniques not observed in MDWE approaches based on manual design tasks. We also report on previous experiences and address open questions relevant for the theory and practice of MDWE."
}
@article{GAROUSI20102251,
title = "A replicated survey of software testing practices in the Canadian province of Alberta: What has changed from 2004 to 2009?",
journal = "Journal of Systems and Software",
volume = "83",
number = "11",
pages = "2251 - 2262",
year = "2010",
note = "Interplay between Usability Evaluation and Software Development",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2010.07.012",
url = "http://www.sciencedirect.com/science/article/pii/S0164121210001834",
author = "Vahid Garousi and Tan Varma",
keywords = "Survey, Software testing practices, Canada",
abstract = "Software organizations have typically de-emphasized the importance of software testing. In an earlier study in 2004, our colleagues reported the results of an Alberta-wide regional survey of software testing techniques in practice. Five years after that first study, the authors felt it is time to replicate the survey and analyze what has changed and what not from 2004 to 2009. This study was conducted during the summer of 2009 by surveying software organizations in the Canadian province of Alberta. The survey results reveal important and interesting findings about software testing practices in Alberta, and point out what has changed from 2004 to 2009 and what not. Note that although our study is conducted in the province of Alberta, we have compared the results to few international similar studies, such as the ones conducted in the US, Turkey, Hong Kong and Australia, The study should thus be of interest to all testing professionals world-wide. Among the findings are the followings: (1) almost all companies perform unit and system testing with a slight increase since 2004, (2) automation of unit, integration and systems tests has increased sharply since 2004, (3) more organization are using observations and expert opinion to conduct usability testing, (4) the choices of test-case generation mechanisms have not changed much from 2004, (5) JUnit and IBM Rational tools are the most widely used test tools, (6) Alberta companies still face approximately the same defect-related economic issues as do companies in other jurisdictions, (7) Alberta software firms have improved their test automation capability since 2004, but there is still some room for improvement, and (8) compared to 2004, more companies are spending more effort on pre-release testing."
}
@article{CORONA2010574,
title = "Liquid chromatography tandem mass spectrometry assay for fast and sensitive quantification of estrone-sulfate",
journal = "Clinica Chimica Acta",
volume = "411",
number = "7",
pages = "574 - 580",
year = "2010",
issn = "0009-8981",
doi = "https://doi.org/10.1016/j.cca.2010.01.019",
url = "http://www.sciencedirect.com/science/article/pii/S000989811000032X",
author = "Giuseppe Corona and Caterina Elia and Bruno Casetta and Alessandro Da Ponte and Lino Del Pup and Enzo Ottavian and Giuseppe Toffoli",
keywords = "Estrone-sulfate, LC–MS/MS, Estrogen, Menopausal, Estrogen suppression, Aromatase inhibitors",
abstract = "Background
The circulating pool of estrone-sulfate is considered as a “reservoir” of slowly-metabolized estrogen that can be exploited for assessing overall individual estrogenicity. The aim of this study was to develop a rapid and sensitive liquid chromatography–tandem mass spectrometry assay for the determination of estrone-sulfate, suitable for routine clinical investigations.
Methods
The proposed assay is based on a simple protein precipitation procedure and on a fast measurement with a triple–quadrupole mass spectrometer operating in negative ion mode and in multiple reaction monitoring. The method was assessed for intra- and inter-day precision, accuracy, recovery, and clinical suitability. A comparison with available radioimmunoassay was also performed.
Results
The LC–MS/MS method is able to detect estrone-sulfate concentrations ≤1pg/mL and has a low limit of quantification of 7.8pg/mL. Intra- and inter-day precision and accuracy were less than 10.5% and 5.0% respectively. The recovery was in the range of 93%–110%. When compared with radioimmunoassay the method resulted more accurate and therefore more suitable for quantifying the estrone-sulfate in different clinical settings, including patients treated with aromatase inhibitors.
Conclusions
The proposed LC–MS/MS method represents a convincing alternative to the immunoassay for a fast, cost-effective and reliable measurement of estrone-sulfate in routine clinical investigations and in large epidemiological studies. It may contribute in shedding a new light on the diagnostic value of estrone-sulfate in normal and pathological conditions."
}
@article{DAMOTASILVEIRANETO2011407,
title = "A systematic mapping study of software product lines testing",
journal = "Information and Software Technology",
volume = "53",
number = "5",
pages = "407 - 423",
year = "2011",
note = "Special Section on Best Papers from XP2010",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2010.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584910002193",
author = "Paulo Anselmo da Mota Silveira Neto and Ivan do Carmo Machado and John D. McGregor and Eduardo Santana de Almeida and Silvio Romero de Lemos Meira",
keywords = "Software product lines, Software testing, Mapping study",
abstract = "Context
In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development.
Objective
This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature.
Method
A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated.
Results
Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed.
Conclusion
The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet."
}
@article{ELALLAOUI2016221,
title = "Automated Model Driven Testing Using AndroMDA and UML2 Testing Profile in Scrum Process",
journal = "Procedia Computer Science",
volume = "83",
pages = "221 - 228",
year = "2016",
note = "The 7th International Conference on Ambient Systems, Networks and Technologies (ANT 2016) / The 6th International Conference on Sustainable Energy Information Technology (SEIT-2016) / Affiliated Workshops",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2016.04.119",
url = "http://www.sciencedirect.com/science/article/pii/S1877050916301429",
author = "Meryem Elallaoui and Khalid Nafil and Raja Touahni and Rochdi Messoussi",
keywords = "Testing, Model-Driven Testing, UML2, UML2 Testing Profile (U2TP), MDA",
abstract = "Software testing is an important step in the life cycle of agile development; it represents an efficient way to ensure the good functioning of the product. But as the complexity of a system increases, the effort and expertise to test it also increases. To significantly reduce these efforts, and reduce the cost and time; several studies have been carried out and various tools and test automation techniques have been proposed. In this paper, we present an approach to automatic generation of test cases from UML 2 Models at the Scrum agile process. This approach automates two important steps: the transformation of design models into test models and generating test cases, based on an open source MDA framework."
}
@incollection{WILLIAMS2014201,
title = "Chapter 10 - Change Management",
editor = "Timothy J. Shimeall and Jonathan M. Spring",
booktitle = "Introduction to Information Security",
publisher = "Syngress",
address = "Boston",
pages = "201 - 231",
year = "2014",
isbn = "978-1-59749-969-9",
doi = "https://doi.org/10.1016/B978-1-59749-969-9.00010-9",
url = "http://www.sciencedirect.com/science/article/pii/B9781597499699000109",
author = "James G. Williams",
keywords = "change management, configuration management, configuration management database (CMDB), change automation, configuration automation, information assurance, Alan Turing, SNMP, Chef, CFEngine, automation systems, change process, patch management, system maintenance, certifications, change types, critical change, major change, minor change, insignificant change",
abstract = "This chapter covers the related but distinct fields of change and configuration management. The chapter introduces the two topics, why they are useful and important, and how they fit in the project management life cycle. The two fields are distinguished, and then each is described in detail. Change management topics discussed are phases of the process, differential processes for different severities of changes (e.g., critical, minor, or insignificant), and how change management directly bears on security. Automation and documentation of change management processes are also discussed. Configuration management begins with an item that straddles change and configuration management: software patch management. Configuration management is then discussed in the context of management systems, configuring software, configuring network devices, configuration information-assurance critical items, configuration and system maintenance, and finally configuration management databases for tracking the various configurations. The chapter concludes with the relevant change and configuration management certification bodies."
}
@incollection{EELES20141,
title = "Chapter 1 - Relating System Quality and Software Architecture: Foundations and Approaches",
editor = "Ivan Mistrik and Rami Bahsoon and Peter Eeles and Roshanak Roshandel and Michael Stal",
booktitle = "Relating System Quality and Software Architecture",
publisher = "Morgan Kaufmann",
address = "Boston",
pages = "1 - 20",
year = "2014",
isbn = "978-0-12-417009-4",
doi = "https://doi.org/10.1016/B978-0-12-417009-4.00001-6",
url = "http://www.sciencedirect.com/science/article/pii/B9780124170094000016",
author = "Peter Eeles and Rami Bahsoon and Ivan Mistrik and Roshanak Roshandel and Michael Stal",
keywords = "Quality Attributes, System quality, Architecture, Assessment, Documentation, Design, Development Process, Lifecycle Approach, Scenario",
abstract = "The field of software architecture has gone through significant evolution over the past two decades. Early research in software architecture focused on technological contributions such as the modeling of structural and behavioral properties of software systems. Automated analysis of these models resulted in the development of tools and approaches aimed at ensuring a system’s functional and nonfunctional properties such as performance, interoperability, and schedulability. More recently, however, software architecture research has shifted in fundamental ways. The emphasis on capturing design decisions and their relationship to both a software system’s requirements and its implementation is predominant. The synergy between the design decisions captured in the software architecture and system quality is the primary motivation behind this book."
}
@article{HAVEMAN2013293,
title = "Requirements for High Level Models Supporting Design Space Exploration in Model-based Systems Engineering",
journal = "Procedia Computer Science",
volume = "16",
pages = "293 - 302",
year = "2013",
note = "2013 Conference on Systems Engineering Research",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.01.031",
url = "http://www.sciencedirect.com/science/article/pii/S187705091300032X",
author = "Steven P. Haveman and G. Maarten Bonnema",
keywords = "Model-based systems engineering, systems engineering challenges, high level models, communication, design space exploration, design trade-offs",
abstract = "Most formal models are used in detailed design and focus on a single domain. Few effective approaches exist that can effectively tie these lower level models to a high level system model during design space exploration. This complicates the validation of high level system requirements during detailed design. In this paper, we define requirements for a high level model that is firstly driven by key systems engineering challenges present in industry and secondly connects to several formal and domain specific models used in model-based design. We analysed part of the systems engineering process at a company developing complex systems, by observing the design process and by analysing design documentation and development databases. By generalizing these observations, we identified several high level systems engineering challenges. They are compared to literature, focusing on reported systems engineering challenges and on existing approaches that incorporate high level models in model-based systems engineering. Finally, we argue that high level system models supporting design space exploration should be able to communicate information regarding design trade-offs (e.g. safety versus ease of use) effectively in a multidisciplinary setting. In our outlook, we propose how to continue our research, by recommending further research and defining a research question."
}
@incollection{SAMPATH2016155,
title = "Chapter Four - Advances in Web Application Testing, 2010–2014",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "101",
pages = "155 - 191",
year = "2016",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2015.11.006",
url = "http://www.sciencedirect.com/science/article/pii/S0065245815000674",
author = "Sreedevi Sampath and Sara Sprenkle",
keywords = "web applications, Software testing, Web testing, Test case generation, Oracles, Test effectiveness, Regression testing",
abstract = "As web applications increase in popularity, complexity, and size, approaches and tools to automate testing the correctness of web applications must continually evolve. In this chapter, we provide a broad background on web applications and the challenges in testing these distributed, dynamic applications made up of heterogeneous components. We then focus on the recent advances in web application testing that were published between 2010 and 2014, including work on test-case generation, oracles, testing evaluation, and regression testing. Through this targeted survey, we identify trends in web application testing and open problems that still need to be addressed."
}
@article{VANDENBRAND201575,
title = "Software engineering: Redundancy is key",
journal = "Science of Computer Programming",
volume = "97",
pages = "75 - 81",
year = "2015",
note = "Special Issue on New Ideas and Emerging Results in Understanding Software",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2013.11.020",
url = "http://www.sciencedirect.com/science/article/pii/S0167642313003043",
author = "Mark van den Brand and Jan Friso Groote",
keywords = "Software engineering, Software quality, Redundancy",
abstract = "Software engineers are humans and so they make lots of mistakes. Typically 1 out of 10 to 100 tasks go wrong. The only way to avoid these mistakes is to introduce redundancy in the software engineering process. This article is a plea to consciously introduce several levels of redundancy for each programming task. Depending on the required level of correctness, expressed in a residual error probability (typically 10−3 to 10−10), each programming task must be carried out redundantly 4 to 8 times. This number is hardly influenced by the size of a programming endeavour. Training software engineers do have some effect as non-trained software engineers require a double amount of redundant tasks to deliver software of a desired quality. More compact programming, for instance by using domain specific languages, only reduces the number of redundant tasks by a small constant."
}
@article{TOKMAKOFF2016537,
title = "AusPlots Rangelands field data collection and publication: Infrastructure for ecological monitoring",
journal = "Future Generation Computer Systems",
volume = "56",
pages = "537 - 549",
year = "2016",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2015.08.016",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X15002782",
author = "Andrew Tokmakoff and Ben Sparrow and David Turner and Andrew Lowe",
keywords = "Ecological data, Mobile, Data collection, Data publishing",
abstract = "The TERN AusPlots Rangelands field data collection system has been developed to facilitate simple and efficient data collection by ecologists operating in the Australian outback. The infrastructure provides tooling for ‘clean’ data collection on mobile (tablet) devices, associated data storage in a cloud-based server infrastructure, facilities for data curation and management and interfaces with the Australian Ecological Knowledge and Observation System (ÆKOS) data repository for long-term data management and semantic enrichment. In this paper, we introduce the AusPlots Rangelands field data collection solution, providing a systems-level view and motivating its development through the discussion of key functional requirements. We provide an outline of the ÆKOS data repository and demonstrate that the combined system provides a unique end-to-end data collection, curation, archiving and publishing mechanism for ecological data."
}
@incollection{2007185,
title = "Index",
editor = "Michael Guttman and John Parodi",
booktitle = "Real-Life MDA",
publisher = "Morgan Kaufmann",
address = "San Francisco",
pages = "185 - 200",
year = "2007",
series = "The MK/OMG Press",
isbn = "978-0-12-370592-1",
doi = "https://doi.org/10.1016/B978-012370592-1/50014-3",
url = "http://www.sciencedirect.com/science/article/pii/B9780123705921500143"
}
@article{KORAKAKIS2018235,
title = "Blood Flow Restriction induces hypoalgesia in recreationally active adult male anterior knee pain patients allowing therapeutic exercise loading",
journal = "Physical Therapy in Sport",
volume = "32",
pages = "235 - 243",
year = "2018",
issn = "1466-853X",
doi = "https://doi.org/10.1016/j.ptsp.2018.05.021",
url = "http://www.sciencedirect.com/science/article/pii/S1466853X17305035",
author = "Vasileios Korakakis and Rodney Whiteley and Konstantinos Epameinontidis",
keywords = "Blood flow restriction, Occlusion, Resistance training, Ischaemia, Rehabilitation",
abstract = "Objective
To evaluate if a single blood flow restriction (BFR)-exercise bout would induce hypoalgaesia in patients with anterior knee pain (AKP) and allow painless application of therapeutic exercise.
Design
Cross-sectional repeated measures design.
Setting
Institutional out-patients physiotherapy clinic.
Patients
Convenience sample of 30 AKP patients.
Intervention
BFR was applied at 80% of complete vascular occlusion. Four sets of low-load open kinetic chain knee extensions were implemented using a pain monitoring model.
Main outcome measurements
Pain (0–10) was assessed immediately after BFR application and after a physiotherapy session (45 min) during shallow and deep single-leg squat (SSLS, DSLS), and step-down test (SDT). To estimate the patient rating of clinical effectiveness, previously described thresholds for pain change (≥40%) were used, with appropriate adjustments for baseline pain levels.
Results
Significant effects were found with greater pain relief immediate after BFR in SSLS (d = 0.61, p < 0.001), DSLS (d = 0.61, p < 0.001), and SDT (d = 0.60, p < 0.001). Time analysis revealed that pain reduction was sustained after the physiotherapy session for all tests (d(SSLS) = 0.60, d(DSLS) = 0.60, d(SDT) = 0.58, all p < 0.001). The reduction in pain effect size was found to be clinically significant in both post-BFR assessments.
Conclusion
A single BFR-exercise bout immediately reduced AKP with the effect sustained for at least 45 min."
}
@article{RUTLE2015545,
title = "Model-driven Software Engineering in Practice: A Content Analysis Software for Health Reform Agreements",
journal = "Procedia Computer Science",
volume = "63",
pages = "545 - 552",
year = "2015",
note = "The 6th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2015)/ The 5th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2015)/ Affiliated Workshops",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.08.383",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915025181",
author = "Adrian Rutle and Kent Inge Fagerland Simonsen and Hans Georg Schaathun and Ralf Kirchhoff",
keywords = "Model-driven Software Engineering, Coordination Reforms in healthcare, Metamodelling, MVCore",
abstract = "The Coordination Reform of 2012 requires Norwegian municipalities and regional health authorities to enter into legally binding service agreements. Although several research projects have been undertaken to analyse the implications of this reform, there is no central database where researches can be given access and analyse the service agreements. In this paper we present how we use model-driven software engineering and user-centric design in an initial development of an information system designed to allow researches to access and analyse service agreements. For this project, it was crucial to discuss the requirements of the system with domain-experts at a high level of abstraction in order to elicit feedback so that the development could proceed at a fast pace and in the right direction. Furthermore, given time and resource constraints, we elected to use a model driven approach using automatic code generation coupled with high-productivity frameworks. In this way we were able to create prototypes so that the developers could get fast feedback from the domain-experts and improvements could be implemented with minimal effort."
}
@article{HAIDER201765,
title = "Notch signalling in placental development and gestational diseases",
journal = "Placenta",
volume = "56",
pages = "65 - 72",
year = "2017",
note = "Exploring common mechanisms between placental and tumour growth",
issn = "0143-4004",
doi = "https://doi.org/10.1016/j.placenta.2017.01.117",
url = "http://www.sciencedirect.com/science/article/pii/S0143400417301194",
author = "S. Haider and J. Pollheimer and M. Knöfler",
keywords = "Placenta, Trophoblast, Notch signalling, Gestational diseases",
abstract = "Activation of Notch signalling upon cell-cell contact of neighbouring cells controls a plethora of cellular processes such as stem cell maintenance, cell lineage determination, cell proliferation, and survival. Accumulating evidence suggests that the pathway also critically regulates these events during placental development and differentiation. Herein, we summarize our present knowledge about Notch signalling in murine and human placentation and discuss its potential role in the pathophysiology of gestational disorders. Studies in mice suggest that Notch controls trophectoderm formation, decidualization, placental branching morphogenesis and endovascular trophoblast invasion. In humans, the particular signalling cascade promotes formation of the extravillous trophoblast lineage and regulates trophoblast proliferation, survival and differentiation. Expression patterns as well as functional analyses indicate distinct roles of Notch receptors in different trophoblast subtypes. Altered effects of Notch signalling have been detected in choriocarcinoma cells, consistent with its role in cancer development and progression. Moreover, deregulation of Notch signalling components were observed in pregnancy disorders such as preeclampsia and fetal growth restriction. In summary, Notch plays fundamental roles in different developmental processes of the placenta. Abnormal signalling through this pathway could contribute to the pathogenesis of gestational diseases with aberrant placentation and trophoblast function."
}
@article{RYSSEL201283,
title = "Automatic library migration for the generation of hardware-in-the-loop models",
journal = "Science of Computer Programming",
volume = "77",
number = "2",
pages = "83 - 95",
year = "2012",
note = "Special Issue on Automatic Program Generation for Embedded Systems",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2010.06.005",
url = "http://www.sciencedirect.com/science/article/pii/S0167642310001115",
author = "Uwe Ryssel and Joern Ploennigs and Klaus Kabitzsch",
keywords = "Generative programming, Function-block-based design, Library migration, Structural comparison",
abstract = "Embedded systems are widely used in several applications nowadays. As they integrate hard- and software elements, their functionality and reliability are often tested by hardware-in-the-loop methods, in which the system under test runs in a simulated environment. Due to the rising complexity of the embedded functions, performance limitations and practicability reasons, the simulations are often specialized to test specific aspects of the embedded system and develop a high diversity by themselves. This diversity is difficult to manage for a user and results in erroneously selected test components and compatibility problems in the test configuration. This paper presents a generative programming approach that handles the diversity of test libraries. Compatibility issues are explicitly evaluated by a new interface concept. Furthermore, a novel model analyzer facilitates the efficient application in practice by migrating existing libraries. The approach is evaluated for an example from the automotive domain using MATLAB/Simulink."
}
@incollection{2010525,
editor = "Eric Conrad and Seth Misenar and Joshua Feldman",
booktitle = "CISSP Study Guide",
publisher = "Syngress",
address = "Boston",
pages = "525 - 567",
year = "2010",
isbn = "978-1-59749-563-9",
doi = "https://doi.org/10.1016/B978-1-59749-563-9.00022-6",
url = "http://www.sciencedirect.com/science/article/pii/B9781597495639000226"
}
@incollection{2017207,
title = "Index",
editor = "Eric Conrad and Seth Misenar and Joshua Feldman",
booktitle = "Eleventh Hour CISSP® (Third Edition)",
publisher = "Syngress",
edition = "Third Edition",
pages = "207 - 221",
year = "2017",
isbn = "978-0-12-811248-9",
doi = "https://doi.org/10.1016/B978-0-12-811248-9.09992-7",
url = "http://www.sciencedirect.com/science/article/pii/B9780128112489099927"
}
@article{JIMENEZ20153,
title = "MeTAGeM-Trace: Improving trace generation in model transformation by leveraging the role of transformation models",
journal = "Science of Computer Programming",
volume = "98",
pages = "3 - 27",
year = "2015",
note = "Fifth issue of Experimental Software and Toolkits (EST): A special issue on Academics Modelling with Eclipse (ACME2012)",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2014.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167642314003700",
author = "Álvaro Jiménez and Juan M. Vara and Verónica A. Bollati and Esperanza Marcos",
keywords = "Model-driven engineering, Traceability, Model transformation",
abstract = "In the context of Model-Driven Engineering (MDE), generation of traces can be automated using the implicit traceability relationships contained in any model transformation. Besides, if transformations are developed adopting a Model-Driven Engineering (MDE) approach, i.e. promoting the role of models and the level of automation, model transformation will benefit from the promised advantages of MDE in terms of less costly software development while reducing the inherent complexity of coding model transformations. To put these ideas into practice, this work introduces MeTAGeM-Trace, the first prototype of an EMF-based toolkit for the MDD of model-to-model transformations which supports trace generation, i.e. it allows developing model transformations that produce not only the corresponding target models, but also a trace model between the elements of the source and target models involved in the transformation."
}
@article{WOMELDORFF2017555,
title = "Taking Lessons Learned from a Proxy Application to a Full Application for SNAP and PARTISN",
journal = "Procedia Computer Science",
volume = "108",
pages = "555 - 565",
year = "2017",
note = "International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2017.05.243",
url = "http://www.sciencedirect.com/science/article/pii/S1877050917308700",
author = "Geoff Womeldorff and Joshua Payne and Ben Bergen",
abstract = "SNAP is a proxy application which simulates the computational motion of a neutral particle transport code, PARTISN. In this work, we have adapted parts of SNAP separately; we have re-implemented the iterative shell of SNAP in the task-model runtime Legion, showing an improvement to the original schedule, and we have created multiple Kokkos implementations of the computational kernel of SNAP, displaying similar performance to the native Fortran. We then translate our Kokkos experiments in SNAP to PARTISN, necessitating engineering development, regression testing, and further thought."
}
@article{HASER201652,
title = "Is business domain language support beneficial for creating test case specifications: A controlled experiment",
journal = "Information and Software Technology",
volume = "79",
pages = "52 - 62",
year = "2016",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2016.07.001",
url = "http://www.sciencedirect.com/science/article/pii/S095058491630115X",
author = "Florian Häser and Michael Felderer and Ruth Breu",
keywords = "Domain Specific Languages (DSL), Behavior driven development, Controlled experiment, Software testing, Student experiment",
abstract = "Context: Behavior Driven Development (BDD), widely used in modern software development, enables easy creation of acceptance test case specifications and serves as a communication basis between business- and technical-oriented stakeholders. BDD is largely facilitated through simple domain specific languages (DSL) and usually restricted to technical test domain concepts. Integrating business domain concepts to implement a ubiquitous language for all members of the development team is an appealing test language improvement issue. But the integration of business domain concepts into BDD toolkits has so far not been investigated. Objective: The objective of the study presented in this paper is to examine whether supporting the ubiquitous language features inside a DSL, by extending a DSL with business domain concepts, is beneficial over using a DSL without those concepts. In the context of the study, benefit is measured in terms of perceived quality, creation time and length of the created test case specifications. In addition, we analyze if participants feel supported when using predefined business domain concepts. Method: We investigate the creation of test case specifications, similar to BDD, in a controlled student experiment performed with graduate students based on a novel platform for DSL experimentation. The experiment was carried out by two groups, each solving a similar comparable test case, one with the simple DSL, the other one with the DSL that includes business domain concepts. A crossover design was chosen for evaluating the perceived quality of the resulting specifications. Results: Our experiment indicates that a business domain aware language allows significant faster creation of documents without lowering the perceived quality. Subjects felt better supported by the DSL with business concepts. Conclusion: Based on our findings we propose that existing BDD toolkits could be further improved by integrating business domain concepts."
}
@article{ZHU2007265,
title = "MDABench: Customized benchmark generation using MDA",
journal = "Journal of Systems and Software",
volume = "80",
number = "2",
pages = "265 - 282",
year = "2007",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2006.10.052",
url = "http://www.sciencedirect.com/science/article/pii/S0164121206003384",
author = "Liming Zhu and Ngoc Bao Bui and Yan Liu and Ian Gorton",
keywords = "MDA, Model-driven development, Performance, Testing, Code generation",
abstract = "This paper describes an approach for generating customized benchmark suites from a software architecture description following a Model Driven Architecture (MDA) approach. The benchmark generation and performance data capture tool implementation (MDABench) is based on widely used open source MDA frameworks. The benchmark application is modeled in UML and generated by taking advantage of the existing community-maintained code generation “cartridges” so that current component technology can be exploited. We have also tailored the UML 2.0 Testing Profile so architects can model the performance testing and data collection architecture in a standards compatible way. We then extended the MDA framework to generate a load testing suite and automatic performance measurement infrastructure. This greatly reduces the effort and expertise needed for benchmarking with complex component and Web service technologies while being fully MDA standard compatible. The approach complements current model-based performance prediction and analysis methods by generating the benchmark application from the same application architecture that the performance models are derived from. We illustrate the approach using two case studies based on Enterprise JavaBean component technology and Web services."
}
@article{ZITKO20112259,
title = "SNEG – Mathematica package for symbolic calculations with second-quantization-operator expressions",
journal = "Computer Physics Communications",
volume = "182",
number = "10",
pages = "2259 - 2264",
year = "2011",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2011.05.013",
url = "http://www.sciencedirect.com/science/article/pii/S0010465511001792",
author = "Rok Žitko",
keywords = "Symbolic manipulation, Second-quantization operators, Wickʼs theorem, Occupation-number representation, Bra–ket notation",
abstract = "In many-particle problems involving interacting fermions or bosons, the most natural language for expressing the Hamiltonian, the observables, and the basis states is the language of the second-quantization operators. It thus appears advantageous to write numerical computer codes which allow the user to define the problem and the quantities of interest directly in terms of operator strings, rather than in some low-level programming language. Here I describe a Mathematica package which provides a flexible framework for performing the required translations between several different representations of operator expressions: condensed notation using pure ASCII character strings, traditional notation (“pretty printing”), internal Mathematica representation using nested lists (used for automatic symbolic manipulations), and various higher-level (“macro”) expressions. The package consists of a collection of transformation rules that define the algebra of operators and a comprehensive library of utility functions. While the emphasis is given on the problems from solid-state and atomic physics, the package can be easily adapted to any given problem involving non-commuting operators. It can be used for educational and demonstration purposes, but also for direct calculations of problems of moderate size.
Program summary
Program title: SNEG Catalogue identifier: AEJL_vl_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEJL_vl_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: GNU General Public License No. of lines in distributed program, including test data, etc.: 319 808 No. of bytes in distributed program, including test data, etc.: 1 081 247 Distribution format: tar.gz Programming language: Mathematica Computer: Any computer which runs Mathematica Operating system: Any OS which runs Mathematica RAM: Problem dependent Classification: 2.9, 5, 6.2 Nature of problem: Manipulation of expressions involving second-quantization operators and other non-commuting objects. Calculation of commutators, anticommutators, expectation values. Generation of matrix representations of the Hamiltonians expressed in the second-quantization language. Solution method: Automatic reordering of operator strings in some well specified canonical order; (anti)commutation rules are used where needed. States may be represented in occupation-number representation. Dirac bra–ket notation may be intermixed with non-commuting operator expressions. Restrictions: For very long operator strings, the brute-force automatic reordering becomes slow, but it can be turned off. In such cases, the expectation values may still be evaluated using Wickʼs theorem. Unusual features: SNEG provides the natural notation of second-quantization operators (dagger for creation operators, etc.) when used interactively using the Mathematica notebook interface. Running time: Problem dependent"
}
@article{SANCHEZGORDON2017162,
title = "A standard-based framework to integrate software work in small settings",
journal = "Computer Standards & Interfaces",
volume = "54",
pages = "162 - 175",
year = "2017",
note = "Standards in Software Process Improvement and Capability Determination",
issn = "0920-5489",
doi = "https://doi.org/10.1016/j.csi.2016.11.009",
url = "http://www.sciencedirect.com/science/article/pii/S0920548916301891",
author = "Mary-Luz Sanchez-Gordon and Antonio de Amescua and Rory V. O’Connor and Xabier Larrucea",
keywords = "Small companies, VSE, Small settings, Software process improvement, Human factors, Socio-technical system",
abstract = "Small software companies have to work hard in order to survive. They usually find it challenging to spend time and effort on improving their operations and processes. Therefore, it is important to address such needs by the introduction of a proposed framework that specifies ways of getting things done while consciously encourage them to enhance their ability to improve. Although there are many software process improvement approaches, none of them address the human factors of small companies in a comprehensive and holistic way. Samay is a proposed framework to integrate human factors in the daily work as a way to deal with that challenge. This study suggests managing human factors but pointing out the software process life cycle. The purpose is to converge toward a continuous improvement by means of alternative mechanisms that impact on people. This framework was developed based upon reviews of relevant standards (such as ISO/IEC 29110, ISO 10018, OMG Essence and ISO/IEC 33014) and previously published studies in this field. Moreover, an expert review and validation findings supported the view that Samay could support practitioners when small software companies want to start improving their ways of work."
}
@article{LUCENA2013890,
title = "Contributions to the emergence and consolidation of Agent-oriented Software Engineering",
journal = "Journal of Systems and Software",
volume = "86",
number = "4",
pages = "890 - 904",
year = "2013",
note = "SI : Software Engineering in Brazil: Retrospective and Prospective Views",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2012.09.016",
url = "http://www.sciencedirect.com/science/article/pii/S0164121212002567",
author = "Carlos Lucena and Ingrid Nunes",
keywords = "Multiagent systems, Agent-oriented Software Engineering, LES, PUC-Rio, SBES 25 years",
abstract = "Many of the issues addressed with multi-agent approaches, such as distributed coordination and self-organization, are now becoming part of industrial and business systems. However, Multiagent Systems (MASs) are still not widely adopted in industry owing to the lack of a connection between MAS and software engineering. Since 2000, there is an effort to bridge this gap and to produce software engineering techniques for agent-based systems that guide the processes of design, development and maintenance. In Brazil, Agent-oriented Software Engineering (AOSE) was first investigated by the research group in the Software Engineering Laboratory (LES) at PUC-Rio, which after one decade of study in this area has built an AOSE community. This paper presents the history of AOSE at LES by discussing the sub-areas of MAS Software Engineering research and development that have been focus of the LES research group. We give examples of relevant results and present a subset of the extensive literature the group has produced during the last decade. We also report how we faced the challenges that emerged from our research by organizing and developing a research community at the intersection of software engineering, programming and MASs with a concern for scalability of solutions."
}
@incollection{2007371,
title = "Chapter 11 - Putting it into practice",
editor = "Mark Utting and Bruno Legeard",
booktitle = "Practical Model-Based Testing",
publisher = "Morgan Kaufmann",
address = "San Francisco",
pages = "371 - 389",
year = "2007",
isbn = "978-0-12-372501-1",
doi = "https://doi.org/10.1016/B978-012372501-1/50012-0",
url = "http://www.sciencedirect.com/science/article/pii/B9780123725011500120",
abstract = "Publisher Summary
This chapter discusses several practical issues and techniques for adopting model-based testing. It starts with the prerequisites of model-based testing and then goes through taxonomy of the possible approaches with the goal of helping one to choose a good approach for one's needs. It deals with people and training issues and discusses how model based testing can fit into agile development processes and the Unified Modeling Language (UML) unified process. One benefit of model-based testing is that it identifies faults in the analysis model and in the requirements earlier in the design process than is usually the case. This can prevent those faults from flowing into the design model and the SUT implementation. This happens because the modeling and model-validation stages of model-based testing raise questions about the requirements and can expose faults well before any tests are executed."
}
@article{TYUGASHEV20181457,
title = "Verification and online updating of decision making control logic for onboard real-time control systems",
journal = "Procedia Computer Science",
volume = "126",
pages = "1457 - 1466",
year = "2018",
note = "Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2018.08.118",
url = "http://www.sciencedirect.com/science/article/pii/S1877050918313966",
author = "Andrey Tyugashev and Dmitrii Zheleznov",
keywords = "Control logic, real-time control algorithm, complex technical system, flight control software, intelligent control",
abstract = "The errors during the decision making process in the control system of a modern complex technical system such as a ship, a plane, a spaceship or a power station can lead to unacceptable consequences. Meanwhile, decision making is based on so-named “control logic” described in dedicated specification documents and then implemented by the hardware and software in the real time mode. There are some problems in this process, caused by contradictions and incompletenesses in the specification documents written in natural language, and misunderstanding between specialists in onboard systems, operational engineers, and programmers. In this paper, two practical examples of verification and online updating of spacecraft control logic are described. The approaches we used allow avoiding the mentioned problems. The theoretical basis for verification is Real-Time Control Algorithms Logic RTCAL. Using this, we have developed and successfully applied software tools and domain-specific languages used at the design and operational stages of spacecraft control. The ongoing work includes introducing SMT solvers into our approach, and automatic generation of valid control logic."
}
@article{LOCHAU201463,
title = "Delta-oriented model-based integration testing of large-scale systems",
journal = "Journal of Systems and Software",
volume = "91",
pages = "63 - 84",
year = "2014",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2013.11.1096",
url = "http://www.sciencedirect.com/science/article/pii/S0164121213002781",
author = "Malte Lochau and Sascha Lity and Remo Lachmann and Ina Schaefer and Ursula Goltz",
keywords = "Large-scale systems, Model-based testing, Regression testing, Variable software architectures",
abstract = "Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain."
}
@article{HUMMER20131884,
title = "Enforcement of entailment constraints in distributed service-based business processes",
journal = "Information and Software Technology",
volume = "55",
number = "11",
pages = "1884 - 1903",
year = "2013",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2013.05.001",
url = "http://www.sciencedirect.com/science/article/pii/S0950584913001006",
author = "Waldemar Hummer and Patrick Gaubatz and Mark Strembeck and Uwe Zdun and Schahram Dustdar",
keywords = "Identity and access management, Business process management, Entailment constraints, Service-Oriented Architecture (SOA), WS-BPEL",
abstract = "Context
A distributed business process is executed in a distributed computing environment. The service-oriented architecture (SOA) paradigm is a popular option for the integration of software services and execution of distributed business processes. Entailment constraints, such as mutual exclusion and binding constraints, are important means to control process execution. Mutually exclusive tasks result from the division of powerful rights and responsibilities to prevent fraud and abuse. In contrast, binding constraints define that a subject who performed one task must also perform the corresponding bound task(s).
Objective
We aim to provide a model-driven approach for the specification and enforcement of task-based entailment constraints in distributed service-based business processes.
Method
Based on a generic metamodel, we define a domain-specific language (DSL) that maps the different modeling-level artifacts to the implementation-level. The DSL integrates elements from role-based access control (RBAC) with the tasks that are performed in a business process. Process definitions are annotated using the DSL, and our software platform uses automated model transformations to produce executable WS-BPEL specifications which enforce the entailment constraints. We evaluate the impact of constraint enforcement on runtime performance for five selected service-based processes from existing literature.
Results
Our evaluation demonstrates that the approach correctly enforces task-based entailment constraints at runtime. The performance experiments illustrate that the runtime enforcement operates with an overhead that scales well up to the order of several ten thousand logged invocations. Using our DSL annotations, the user-defined process definition remains declarative and clean of security enforcement code.
Conclusion
Our approach decouples the concerns of (non-technical) domain experts from technical details of entailment constraint enforcement. The developed framework integrates seamlessly with WS-BPEL and the Web services technology stack. Our prototype implementation shows the feasibility of the approach, and the evaluation points to future work and further performance optimizations."
}
@article{SLAWIK2018846,
title = "Establishing User-centric Cloud Service Registries",
journal = "Future Generation Computer Systems",
volume = "87",
pages = "846 - 867",
year = "2018",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2018.03.010",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X18304813",
author = "Mathias Slawik and Begüm İlke Zilci and Axel Küpper",
keywords = "Cloud service registry, Cloud computing, Service matchmaking, Cloud brokering",
abstract = "Many potential cloud consumers are overburdened by the challenges persisting when discovering, assessing, and selecting contemporary Cloud Service offerings: the cloud market is vast and fast-moving, the selection criteria are ambiguous, service knowledge is scattered through the Internet, and features as well as prices are complex and incomparable. Much research has been carried out to create cloud service registries to help users select cloud services for eventual consumption, especially within the field of semantic web services. Through analyzing real-world requirements of six use cases we identified a gap in research for user-centric technologies. We fill this gap by creating a business vocabulary reflecting common service selection criteria, defining a textual domain specific language to let any user describe services easily, and implementing a novel brokering and matchmaking component to support users in their selection process. As a combination of those technologies, we create the Open Service Compendium (OSC), a crowd-sourced cloud service registry. Our evaluation activities highlight how these developments solve real-world challenges in diverse near-production settings. All of this implies that a substantial benefit for service registry users can be created by following a simple architecture that is focused on their concrete needs — instead of aiming for highest sophistication and broadest applicability as observed in many of the related works."
}
@article{KUMAR2015859,
title = "Model Based Distributed Testing of Object Oriented Programs",
journal = "Procedia Computer Science",
volume = "46",
pages = "859 - 866",
year = "2015",
note = "Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace & Island Resort, Kochi, India",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.02.155",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915002197",
author = "K.S. Vipin Kumar and Sheena Mathew",
keywords = "Distributed Testing, SDG, FCFS, Object Oriented Program Testing ;",
abstract = "In recent times the software systems have evolved in size and complexity. This has resulted in usage of object oriented programming in the development of such systems. Though object oriented programs are helpful in programming large systems, testing of such systems requires much more effort and time. For this the program is analyzed to create a model based on System Dependence Graph(SDG) which is then used to find locations within the program where the state of the program can be freezed and reused while executing other test cases."
}
@incollection{OSIS201753,
title = "Chapter 2 - Software Designing With Unified Modeling Language Driven Approaches",
editor = "Janis Osis and Uldis Donins",
booktitle = "Topological UML Modeling",
publisher = "Elsevier",
address = "Boston",
pages = "53 - 82",
year = "2017",
series = "Computer Science Reviews and Trends",
isbn = "978-0-12-805476-5",
doi = "https://doi.org/10.1016/B978-0-12-805476-5.00002-2",
url = "http://www.sciencedirect.com/science/article/pii/B9780128054765000022",
author = "Janis Osis and Uldis Donins",
keywords = "Software modeling methods and approaches, UML application, benefits and limitations of UML modeling driven approaches",
abstract = "The Unified Modeling Language (UML) is a notation and as such its specification does not contain any guidelines for software development process. Despite that UML is independent of particular methods and approaches, most of the UML modeling driven methods uses use case driven approach thus raising incomplete analysis of the problem domain functioning. Since UML modeling driven approaches are elaborated by different authors, their prescriptions differ. There is also difference in the use of use case narratives across various methods due to the lack of guidance on narrative format in the UML specification. The UML specification only states that “use cases are typically specified in various idiosyncratic formats such as natural language, tables, trees, etc. Therefore, it is not easy to capture its structure accurately or generally by a formal model.” This chapter discusses the current state of the art of UML-based software development approaches. Most attention is paid on the artifacts created by using the UML."
}
@article{DIAZ2012737,
title = "Wiki Scaffolding: Aligning wikis with the corporate strategy",
journal = "Information Systems",
volume = "37",
number = "8",
pages = "737 - 752",
year = "2012",
note = "Special Issue: Advanced Information Systems Engineering (CAiSE'11)",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2012.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0306437912000695",
author = "Oscar Díaz and Gorka Puente",
keywords = "Wiki, Wiki management, DSL, Mind map, FreeMind",
abstract = "Wikis are main exponents of collaborative development by user communities. This community may be created around the wiki itself (e.g., community of contributors in Wikipedia) or already exist (e.g., company employees in corporate wikis). In the latter case, the wiki is not created in a vacuum but as part of the information ecosystem of the hosting organization. As any other Information System resource, wiki success highly depends on the interplay of technology, work practice and the organization. Thus, wiki contributions should be framed along the concerns already in use in the hosting organization in terms of glossaries, schedules, policies, organigrams and the like. The question is then, how can corporate strategies permeate wiki construction while preserving wiki openness and accessibility? We advocate for the use of “Wiki Scaffoldings”, i.e., a wiki installation that is provided at the onset to mimic these corporate concerns: categories, users, templates, articles initialized with boilerplate text, are all introduced in the wiki before any contribution is made. To retain wikis' friendliness and engage layman participation, we propose scaffoldings to be described as mind maps. Mind maps are next “exported” as wiki installations. We show the feasibility of the approach introducing a Wiki Scaffolding Language (WSL). WSL is realized as a plugin for FreeMind, a popular tool for mind mapping. Finally, we validate the expressiveness of WSL in four case studies. WSL is available for download."
}
@incollection{AHMAD20181,
title = "Chapter One - Model-Based Testing for Internet of Things Systems",
editor = "Atif M. Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "108",
pages = "1 - 58",
year = "2018",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2017.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S0065245817300517",
author = "Abbas Ahmad and Fabrice Bouquet and Elizabeta Fourneret and Bruno Legeard",
keywords = "Model-based testing, Modeling approaches, Test generation technology, Security testing",
abstract = "The Internet of Things (IoT) is nowadays globally a mean of innovation and transformation for many companies. Applications extend to a large number of domains, such as smart cities, smart homes, and health care. The Gartner Group estimates an increase up to 21 billion connected things by 2020. The large span of “things” introduces problematic aspects, such as interoperability due to the heterogeneity of communication protocols and the lack of a globally accepted standard. The large span of usages introduces problems regarding secure deployments and scalability of the network over large-scale infrastructures. This chapter describes the challenges for the IoT testing, includes state-of-the-art testing of IoT systems using models, and presents a model-based testing as a service approach to respond to its challenges through demonstrations with real use cases involving two of the most accepted standards worldwide: FIWARE and oneM2M."
}
@incollection{KAWAMOTO2014819,
title = "Chapter 29 - Integration of Knowledge Resources into Applications to Enable CDS: Architectural Considerations",
editor = "Robert A. Greenes",
booktitle = "Clinical Decision Support (Second Edition)",
publisher = "Academic Press",
edition = "Second Edition",
address = "Oxford",
pages = "819 - 849",
year = "2014",
isbn = "978-0-12-398476-0",
doi = "https://doi.org/10.1016/B978-0-12-398476-0.00029-4",
url = "http://www.sciencedirect.com/science/article/pii/B9780123984760000294",
author = "Kensaku Kawamoto and Emory Fry and Robert Greenes",
keywords = "Clinical decision support, scalable CDS, Health eDecisions, OpenCDS, Service-Oriented Architecture (SOA)",
abstract = "This chapter examines in detail the bridge between the creation and delivery of CDS content – in other words, how knowledge resources can be integrated with clinical information systems (CISs) to enable CDS. While such integration can be relatively straightforward for a single instance – that is, the integration of a specific knowledge resource in a specific clinical information system – the challenge lies in the fact that both knowledge resources and clinical information systems are quite diverse. Consequently, there is no single knowledge integration architecture that can address all circumstances. However, there are several architectural patterns for knowledge integration that can, taken together, enable the effective integration of knowledge resources into applications. The primary purpose of this chapter, then, is to outline the main CDS knowledge integration architectures that are available and to detail the pros and cons of each approach. The appropriateness of a given architecture for a particular organization depends on a variety of factors, including the existing clinical information system infrastructure and the type of CDS capability involved (e.g. real-time vs. non-real-time applications. These various approaches are outlined here, with special attention being placed on knowledge integration architectures aligned with broad trends in the IT landscape, such as service-oriented architectures, cloud-based computing, and app-based software ecosystems. The chapter also discusses how CDS architectures must align with larger changes in the health care industry as a whole, as shifting health care reimbursement models are requiring continuity of care across multiple organizations and health IT systems centered around patients and populations rather than care episodes at individual care settings."
}
@incollection{2017267,
title = "Index",
editor = "Edward Griffor",
booktitle = "Handbook of System Safety and Security",
publisher = "Syngress",
address = "Boston",
pages = "267 - 273",
year = "2017",
isbn = "978-0-12-803773-7",
doi = "https://doi.org/10.1016/B978-0-12-803773-7.00022-X",
url = "http://www.sciencedirect.com/science/article/pii/B978012803773700022X"
}
@article{BJARNASON201661,
title = "A multi-case study of agile requirements engineering and the use of test cases as requirements",
journal = "Information and Software Technology",
volume = "77",
pages = "61 - 79",
year = "2016",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2016.03.008",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916300544",
author = "Elizabeth Bjarnason and Michael Unterkalmsteiner and Markus Borg and Emelie Engström",
keywords = "Agile development, Requirements, Testing, Test-first development, Test-driven development, Behaviour-driven development, Acceptance test, Case study, Empirical software engineering",
abstract = "Context
It is an enigma that agile projects can succeed ‘without requirements’ when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases.
Objective
We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies.
Method
We performed an iterative case study at three companies and collected data through 14 interviews and two focus groups.
Results
The use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements.
Conclusions
The findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change."
}
@article{CHAVARRIAGA2017133,
title = "An approach to build XML-based domain specific languages solutions for client-side web applications",
journal = "Computer Languages, Systems & Structures",
volume = "49",
pages = "133 - 151",
year = "2017",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2017.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S1477842416301634",
author = "Enrique Chavarriaga and Francisco Jurado and Fernando Díez",
keywords = "Domain-Specific Languages, XML interpreter, JavaScript, Web Application, XML programing language",
abstract = "Summary
Domain-Specific Languages (DSLs) allow for the building of applications that ease the labour of both software engineers and domain experts thanks to the level of abstraction they provide. In cases where the domain is restricted to Client-Side Web Applications (CSWA), XML-based languages, frameworks and widgets are commonly combined in order to provide fast, robust and flexible solutions. This article presents an approach designed to create XML-based DSL solutions for CSWA that includes an evaluation engine, a programming model and a lightweight development environment. The approach is able to evaluate multiple XML-based DSL programs simultaneously to provide solutions to those Domain Specific Problems for CSWAs. To better demonstrate the capabilities and potential of this novel approach, we will employ a couple of case studies, namely Anisha and FeedPsi."
}
@incollection{BUCHGEHER2014161,
title = "Chapter 7 - Continuous Software Architecture Analysis",
editor = "Muhammad Ali Babar and Alan W. Brown and Ivan Mistrik",
booktitle = "Agile Software Architecture",
publisher = "Morgan Kaufmann",
address = "Boston",
pages = "161 - 188",
year = "2014",
isbn = "978-0-12-407772-0",
doi = "https://doi.org/10.1016/B978-0-12-407772-0.00006-X",
url = "http://www.sciencedirect.com/science/article/pii/B978012407772000006X",
author = "Georg Buchgeher and Rainer Weinreich",
keywords = "Agile development, Continuous quality control, Continuous software architecture analysis, Software architecture, Software architecture analysis",
abstract = "This chapter discusses software architecture analysis in the context of agile processes. Agile processes are characterized by incremental and interleaved activities and by a focus on continuous improvement and delivery. Most approaches to software architecture analysis, however, have been developed to be performed at dedicated points in the development process or as external evaluation activities and not as continuous activities throughout the development process. This chapter discusses continuous software architecture analysis (CSAA). It reviews important requirements for CSAA and outlines how CSAA is supported by current software architecture analysis approaches. It further presents experiences with an approach for continuous structural and conformance analysis and identifies future research challenges."
}
@article{LAZAR201091,
title = "Behaviour-Driven Development of Foundational UML Components",
journal = "Electronic Notes in Theoretical Computer Science",
volume = "264",
number = "1",
pages = "91 - 105",
year = "2010",
note = "Proceedings of the 7th International Workshop on Formal Engineering approaches to Software Components and Architectures (FESCA 2010)",
issn = "1571-0661",
doi = "https://doi.org/10.1016/j.entcs.2010.07.007",
url = "http://www.sciencedirect.com/science/article/pii/S1571066110000666",
author = "Ioan Lazăr and Simona Motogna and Bazil Pârv",
keywords = "behaviour-driven development, executable UML, user story, executable specification, acceptance criteria",
abstract = "Behaviour-Driven Development (BDD) focuses all development activities on the delivery of behaviours – what a system should do, described such that developers and domain experts speak the same language. BDD frameworks allow users to represent the required system behaviour as executable user stories and the acceptance criteria as executable scenarios attached to user stories. In this paper we define a UML profile that allows users to create executable Foundational UML (fUML) stories and scenarios. In order to easily construct scenarios we introduce a BDD model library which contains fUML activities for testing equalities and inclusions. We also present an Eclipse-based development tool that supports a BDD approach for developing fUML components. The tool provides developers a concrete syntax for defining executable scenarios, and automatically updates the project status based on verified delivered behaviorus."
}
@article{KOS201674,
title = "Test automation of a measurement system using a domain-specific modelling language",
journal = "Journal of Systems and Software",
volume = "111",
pages = "74 - 88",
year = "2016",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2015.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S0164121215002058",
author = "Tomaž Kos and Marjan Mernik and Tomaž Kosar",
keywords = "Test automation, Domain-specific modelling languages, Usage experience",
abstract = "The construction of domain-specific modelling languages (DSMLs) is only the first step within the needed toolchain. Models need to be maintained, modified or functional errors searched for. Therefore, tool support is vital for the DSML end-user’s efficiency. This paper presents SeTT, a simple but very useful tool for DSML end-users, a testing framework integrated within a DSML Sequencer. This Sequencer, part of the DEWESoft data acquisition system, supports the development of model-based tests using a high-level abstraction. The tests are used during the whole data acquisition process and able to test different systems’ parts. This paper shows how high-level specifications can be extended to describe a testing infrastructure for a specific DSML. In this manner, the Sequencer and SeTT were combined at the metamodel level. The contribution of the paper is to show that one can leverage on the DSML to build a testing framework with relatively little effort, by implementing assertions to it."
}
@article{CALDERON2018238,
title = "MEdit4CEP-Gam: A model-driven approach for user-friendly gamification design, monitoring and code generation in CEP-based systems",
journal = "Information and Software Technology",
volume = "95",
pages = "238 - 264",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.11.009",
url = "http://www.sciencedirect.com/science/article/pii/S0950584917303464",
author = "Alejandro Calderón and Juan Boubeta-Puig and Mercedes Ruiz",
keywords = "Gamification, Model-Driven Engineering, Complex event processing, Strategy design, Monitoring, Graphical modelling editor",
abstract = "Context
Gamification has been proven to increase engagement and motivation in multiple and different non-game contexts such as healthcare, education, workplace, and marketing, among others. However, many of these applications fail to achieve the desired benefits of gamification, mainly because of a poor design.
Objective
This paper explores the conceptualization, implementation and monitoring phases of meaningful gamification strategies and proposes a solution for strategy experts that hides the implementation details and helps them focus only on what is crucial for the success of the strategy. The solution makes use of Model-Driven Engineering (MDE) and Complex Event Processing (CEP) technology.
Method
An easy-to-use graphical editor is used to provide the high-level models that represent the design of the gamification strategy and its deployment and monitoring. These models contain the event pattern definitions to be automatically transformed into code. This code is then deployed both in a CEP engine to detect the conditions expressed in such patterns and in an enterprise service bus to execute the corresponding pattern actions.
Results
The paper reports on the use of both a graphical modeling editor for gamification domain definition and a graphical modeling editor for gamification strategy design, monitoring and code generation in event-based systems. It also shows how the proposal can be used to design and automate the implementation and monitoring of a gamification strategy in an educational domain supported by a well-known Learning Management System (LMS) such as Moodle.
Conclusion
It can be concluded that this unprecedented model-driven approach leveraging gamification and CEP technology provides strategy experts with the ability to graphically define gamification strategies, which can be directly transformed into code executable by event-based systems. Therefore, this is a novel solution for bringing CEP closer to any strategy expert, positively influencing the gamification strategy design, implementation and real-time monitoring processes."
}
@article{TYUGASHEV2016120,
title = "Language and Toolset for Visual Construction of Programs for Intelligent Autonomous Spacecraft Control",
journal = "IFAC-PapersOnLine",
volume = "49",
number = "5",
pages = "120 - 125",
year = "2016",
note = "4th IFAC Conference on Intelligent Control and Automation SciencesICONS 2016",
issn = "2405-8963",
doi = "https://doi.org/10.1016/j.ifacol.2016.07.100",
url = "http://www.sciencedirect.com/science/article/pii/S2405896316302968",
author = "Andrey A. Tyugashev",
keywords = "Autonomous Control, Satellite Control, Diagnostic Programs, Software Tools, Software Engineering, Domain Specific Language, Visual Software Construction",
abstract = "The paper describes approach to Autonomous Fault Tolerant Intelligent Control of Spacecraft based on usage of Onboard Real-Time Interpreter of Integrated Control Programs, including special Diagnostic Routine. Rules of the Autonomous Control Program could be added or refined from Earth in operative manner by radio channel. Specially designed Visual Domain Specific Language allowing Control Logic Designers check, analyze and construct Rules in user friendly graphical environment excluding necessity to involve Software Developers. Proposed approach allows reducing of costs and labor consuming of Space Mission because of reducing of efforts needed for common-style Flight Control Software coding, multi-stage testing and support. Special Software Engineering Toolset that including Visualizer and Graphical Constructor of these autonomous control programs presented as well as the principles of its design and development. The Prototype of the Toolset has been successfully introduced at JSC Information Satellite Systems, Krasnoyarsk Region, Russia."
}
@article{ALTURJMAN2017,
title = "5G-enabled devices and smart-spaces in social-IoT: An overview",
journal = "Future Generation Computer Systems",
year = "2017",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2017.11.035",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X17311962",
author = "Fadi Al-Turjman",
keywords = "Internet of Things (IoT), Smart environments, Sensors, 5G, Smartphones",
abstract = "The abundance of smartphones, with their growing capabilities potentiates applications in numerous domains. A typical smartphone nowadays is equipped with an array of embedded sensors (e.g., GPS, accelerometers, gyroscopes, RFID readers, cameras, and microphones) along with different communication interfaces (e.g. Cellular, WiFi, Bluetooth, etc.). Thus, a smartphone is a significant provider for sensory data that awaits the utilization in many critical applications. Primers of this vision have demonstrated success, both in the literature and application’s market. In this literature review, we present the main motivations in carrying these smart devices, and the correlation between the user surrounding context and the application usage. We focus on context-awareness in smart systems and space discovery paradigms; online versus offline, the femtocell usage and energy aspects to be considered, and about the ongoing social IoT applications. Moreover, we highlight the most up-to-date open research issues in this area."
}
@article{BRYCE2006960,
title = "Prioritized interaction testing for pair-wise coverage with seeding and constraints",
journal = "Information and Software Technology",
volume = "48",
number = "10",
pages = "960 - 970",
year = "2006",
note = "Advances in Model-based Testing",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2006.03.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584906000401",
author = "Renée C. Bryce and Charles J. Colbourn",
keywords = "Biased covering arrays, Covering arrays, Greedy algorithm, Mixed-level covering arrays, Pair-wise interaction coverage, Software interaction testing, Test prioritization",
abstract = "Interaction testing is widely used in screening for faults. In software testing, it provides a natural mechanism for testing systems to be deployed on a variety of hardware and software configurations. In many applications where interaction testing is needed, the entire test suite is not run as a result of time or budget constraints. In these situations, it is essential to prioritize the tests. Here, we adapt a “one-test-at-a-time” greedy method to take importance of pairs into account. The method can be used to generate a set of tests in order, so that when run to completion all pair-wise interactions are tested, but when terminated after any intermediate number of tests, those deemed most important are tested. In addition, practical concerns of seeding and avoids are addressed. Computational results are reported."
}
@article{DOGAN2014174,
title = "Web application testing: A systematic literature review",
journal = "Journal of Systems and Software",
volume = "91",
pages = "174 - 201",
year = "2014",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2014.01.010",
url = "http://www.sciencedirect.com/science/article/pii/S0164121214000223",
author = "Serdar Doğan and Aysu Betin-Can and Vahid Garousi",
keywords = "Systematic literature review, Web application, Testing",
abstract = "Context
The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013.
Objective
As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field.
Methods
We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area.
Results
Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance.
Conclusion
We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community."
}
@article{SMITH2016404,
title = "A Document-Driven Method for Certifying Scientific Computing Software for Use in Nuclear Safety Analysis",
journal = "Nuclear Engineering and Technology",
volume = "48",
number = "2",
pages = "404 - 418",
year = "2016",
issn = "1738-5733",
doi = "https://doi.org/10.1016/j.net.2015.11.008",
url = "http://www.sciencedirect.com/science/article/pii/S1738573315002582",
author = "W. Spencer Smith and Nirmitha Koothoor",
keywords = "Literate Programming, Nuclear Safety Analysis, Numerical Simulation, Requirements Specification, Software Engineering, Software Quality",
abstract = "This paper presents a documentation and development method to facilitate the certification of scientific computing software used in the safety analysis of nuclear facilities. To study the problems faced during quality assurance and certification activities, a case study was performed on legacy software used for thermal analysis of a fuelpin in a nuclear reactor. Although no errors were uncovered in the code, 27 issues of incompleteness and inconsistency were found with the documentation. This work proposes that software documentation follow a rational process, which includes a software requirements specification following a template that is reusable, maintainable, and understandable. To develop the design and implementation, this paper suggests literate programming as an alternative to traditional structured programming. Literate programming allows for documenting of numerical algorithms and code together in what is termed the literate programmer's manual. This manual is developed with explicit traceability to the software requirements specification. The traceability between the theory, numerical algorithms, and implementation facilitates achieving completeness and consistency, as well as simplifies the process of verification and the associated certification."
}
@article{REBAHI201839,
title = "Towards a next generation 112 testbed: The EMYNOS ESInet",
journal = "International Journal of Critical Infrastructure Protection",
volume = "22",
pages = "39 - 50",
year = "2018",
issn = "1874-5482",
doi = "https://doi.org/10.1016/j.ijcip.2018.05.001",
url = "http://www.sciencedirect.com/science/article/pii/S1874548217302081",
author = "Yacine Rebahi and Kin Tsun Chiu and Nikolay Tcholtchev and Simon Hohberg and Evangelos Pallis and Evangelos Markakis",
keywords = "Emergency services, 112, 911, EMYNOS, SIP, IMS, ESInet, Location, i3 architecture, RTT",
abstract = "This paper belongs to a series of research documents describing the progress in the specification and development of the EMYNOS framework offering an IP based platform for emergency services. EMYNOS is an international research project funded by the European Commission. Although migrating to Next Generation 112 and 911 is not new as a topic, no real testbed that can be used for evaluating the relevant standards exists so far. In this paper we discuss the EMYNOS approach and in particular some mechanisms that have been developed in this context. Special attention is paid to the EMYNOS testbed that was assessed during the ETSI NG112 plugtests in 2016 and 2017. Some experiments and test results are provided as well."
}
@article{IZADI2018191,
title = "Tolerance induction by surface immobilization of Jagged-1 for immunoprotection of pancreatic islets",
journal = "Biomaterials",
volume = "182",
pages = "191 - 201",
year = "2018",
issn = "0142-9612",
doi = "https://doi.org/10.1016/j.biomaterials.2018.08.017",
url = "http://www.sciencedirect.com/science/article/pii/S0142961218305714",
author = "Zhila Izadi and Ensiyeh Hajizadeh-Saffar and Jamshid Hadjati and Mahdi Habibi-Anbouhi and Mohammad Hossein Ghanian and Hamid Sadeghi-Abandansari and Mohammad Kazemi Ashtiani and Zakieh Samsonchi and Mohammad Raoufi and Maedeh Moazenchi and Mahmoud Izadi and Anava sadat Sadr Hashemi Nejad and Haideh Namdari and Yaser Tahamtani and Seyed Nasser Ostad and Hamid Akbari-Javar and Hossein Baharvand",
keywords = "Type 1 diabetes, Immunomodulation, Surface immobilization, Jagged-1, Islet PEGylation",
abstract = "Although transplantation of pancreatic islets is a promising approach for treatment of type 1 diabetes mellitus, the engraftment efficiency of these islets is limited by host immune responses. Extensive efforts have been made to immunoisolate these islets by introducing barriers on the islet surface. To date, these barriers have not successfully protected islets from attack by the immune system. In addition, the inevitable permeability of an islet capsule cannot prevent filtration by proinflammatory cytokines and islet self-antigens. Thus, we have developed a surface engineering approach for localized immonumodulation of the islet microenvironment. Jagged-1 (JAG-1), as a potent immunomodulatory factor, was immobilized on the islet surface by mediation of a double-layer of heterobifunctional poly (ethylene glycol) (PEG). Immobilization and functionality of JAG-1 on PEGylated islet surfaces were established. When co-cultured with splenocytes, the JAG-1 conjugated islets induced a significant increase in regulatory T cells and regulated the cytokine levels produced by immune cells. The results demonstrated that JAG-1 immobilization could improve immunoprotection of pancreatic islets by localized modulation of the immune milieu from an inflammatory to an anti-inflammatory state. We also evaluated the effects of surface modification of these islets by JAG-1 in a xenotransplantation model. The transplanted JAG-1/PEG/islets group showed a significantly reduced blood glucose levels compared with the control group of diabetic mice during the acute phase of the immune response to the transplanted islets. Our results demonstrated that surface modification has the potential to shift the immune system from an inflammatory to anti-inflammatory milieu and may offer a new prospective for immunoprotection of pancreatic islets."
}
@article{PONCELET2016143,
title = "Model-based testing for building reliable realtime interactive music systems",
journal = "Science of Computer Programming",
volume = "132",
pages = "143 - 172",
year = "2016",
note = "Special Issue on Software Verification and Testing (SAC-SVT'15)",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2016.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S0167642316301022",
author = "Clement Poncelet and Florent Jacquemard",
keywords = "Model based testing, Interactive music systems, Timed automata",
abstract = "The role of an Interactive Music System (IMS) is to accompany musicians during live performances, acting like a real musician. It must react in realtime to audio signals from musicians, according to a timed high-level requirement called mixed score, written in a domain specific language. Such goals imply strong requirements of temporal reliability and robustness to unforeseen errors in input, yet not much addressed by the computer music community. We present the application of Model-Based Testing techniques and tools to a state-of-the-art IMS, including in particular: offline and on-the-fly approaches for the generation of relevant input data for testing (including timing values), with coverage criteria, the computation of the corresponding expected output, according to the semantics of a given mixed score, the black-box execution of the test data on the System Under Test and the production of a verdict. Our method is based on formal models in a dedicated intermediate representation, compiled directly from mixed scores (high-level requirements), and either passed, to the model-checker Uppaal (after conversion to Timed Automata) in the offline approach, or executed by a virtual machine in the online approach. Our fully automatic framework has been applied to real mixed scores used in concerts and the results obtained have permitted to identify bugs in the target IMS."
}
@incollection{KHAN2012141,
title = "Chapter 4 - Pragmatic Directions in Engineering Secure Dependable Systems",
editor = "Ali Hurson and Sahra Sedigh",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "84",
pages = "141 - 167",
year = "2012",
booktitle = "Dependable and Secure Systems Engineering",
issn = "0065-2458",
doi = "https://doi.org/10.1016/B978-0-12-396525-7.00005-8",
url = "http://www.sciencedirect.com/science/article/pii/B9780123965257000058",
author = "M. Farrukh Khan and Raymond A. Paul",
keywords = "Engineering dependable systems, Engineering complex systems, Dependable software/hardware systems, Dependable cloud infrastructure, Secure and reliable systems",
abstract = "All large and complex computer and communications systems have an intrinsic requirement to be dependable since their failure can cause significant losses in terms of life or treasure. Such the systems are expected to have the attributes of reliability, availability, safety, confidentiality, survivability, integrity, and maintainability. Current software and hardware systems continue to evolve in complexity at rapid rates. Although the increase in the complexity of single artifact (such as number of logical decision points in a software package) can often be tracked with Moore's Law like approximations, systems constructed out of larger number of smaller subsystems defy such classifications. The reason for this added complexity is that interactions between the subsystems explode exponentially in the size of the parent system. Yet all component interactions must be addressed exhaustively to predict accurate behavior of the whole system. The challenge that we face is that it is seldom possible to model or test all such interactions in a given system. As a result, building dependable complex systems with realistic assessment of risks of failure is an extremely difficult endeavor. Attempts have been made to ameliorate the difficulty in the engineering of dependable complex systems using lessons from engineering methodologies in other domains. We discuss key attributes of dependable complex systems, with a special emphasis on security where information is involved. We review classical approaches to designing, building, and maintaining dependable complex systems. We present promising features and novel ideas applicable to the lifecycle of dependable complex systems. Most of our discussion is focused within the domain of hardware and software systems. Over time, practitioners in dependable engineering have learned lessons from previous experience and continue to present prescriptive approaches discovered through research and analysis. These lessons and approaches are often applicable to other engineering domains such as construction, transportation, and industrial control. We look at specific engineering challenges and proposed solutions pertaining to the following general domains, with occasional examples from any branch of engineering:•dependable hardware/software systems;•secure dependable systems;•dependable cloud infrastructure and applications. Finally, we conclude with the observation that several approaches are applicable across all these domains and identify accessible techniques that have good potential to increase the dependability of systems. These approaches can be considered as axiomatic in building any future complex systems with a high degree of dependability."
}
@article{JAMRO20181,
title = "Agile and hierarchical round-trip engineering of IEC 61131-3 control software",
journal = "Computers in Industry",
volume = "96",
pages = "1 - 9",
year = "2018",
issn = "0166-3615",
doi = "https://doi.org/10.1016/j.compind.2018.01.004",
url = "http://www.sciencedirect.com/science/article/pii/S0166361517304165",
author = "Marcin Jamro and Dariusz Rzonca",
keywords = "Control software, IEC 61131-3, Implementation, Modeling, Round-trip engineering, Synchronization, Testing",
abstract = "The control software often performs complex, important, and responsible operations in industrial manufacturing systems. The size and complexity of such software are still increasing, thus it is important to provide engineers with methods and tools that simplify the development process. The situation can be improved by modeling using the Model-Driven Development (MDD) approach, standardized implementation process, as well as various testing methods. In the paper, the authors propose the further step, which consists of the comprehensive agile hierarchical round-trip engineering approach dedicated to the IEC 61131-3 control software. It divides the project into four connected parts – model, configuration, implementation, and tests. Such a solution allows developers to work independently and iteratively on various project parts, because changes are discovered automatically and are propagated to suitable views inside the project. The synchronization mechanism has been introduced into the CPDev engineering environment for programming industrial controllers."
}
@article{HE2018109,
title = "Testing bidirectional model transformation using metamorphic testing",
journal = "Information and Software Technology",
volume = "104",
pages = "109 - 129",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.07.010",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301538",
author = "Xiao He and Xing Chen and Sibo Cai and Ying Zhang and Gang Huang",
keywords = "Metamorphic testing, Bidirectional model transformation, Model driven engineering",
abstract = "Context
In model-based software development, bidirectional model transformation (BX) provides a fundamental solution to model synchronization that can retain the consistency among models. Similar to conventional programs, a BX program may also contain bugs. Accordingly, a BX program must be tested prior to being used in practice. A challenging problem of testing BX programs is to construct test oracles (e.g., assertions and expected output models), which are usually difficult and/or expensive to manually specify.
Objective
In we paper, we investigate how to alleviate the oracle problem in BX testing via reducing the costs of developing test oracles.
Method
We propose a metamorphic testing approach for BX. First, we identify three generic metamorphic relations for BX. Afterwards, we define a metamodel MT4MT to establish metamorphic test groups and test scripts. We also propose a testing framework to support metamorphic testing based on MT4MT.
Results
We conducted an experimental study of mutation analysis and a case study on three ATL-based ad-hoc BXs. The results of the experimental study and the case study showed that our approach killed 79.38% mutants and enabled us to test real-world ATL-based ad-hoc BXs. We also demonstrated that MT4MT can be used to test the semantics properties of BXs.
Conclusion
Our approach is an effective and practical approach with lower costs of developing test oracles."
}
@article{AYNUTDINOV2009227,
title = "The prototype string for the km3-scale Baikal neutrino telescope",
journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
volume = "602",
number = "1",
pages = "227 - 234",
year = "2009",
note = "Proceedings of the 3rd International Workshop on a Very Large Volume Neutrino Telescope for the Mediterranean Sea",
issn = "0168-9002",
doi = "https://doi.org/10.1016/j.nima.2008.12.149",
url = "http://www.sciencedirect.com/science/article/pii/S0168900208018998",
author = "V. Aynutdinov and A. Avrorin and V. Balkanov and I. Belolaptikov and D. Bogorodsky and N. Budnev and I. Danilchenko and G. Domogatsky and A. Doroshenko and A. Dyachok and Zh.-A. Dzhilkibaev and S. Fialkovsky and O. Gaponenko and K. Golubkov and O. Gress and T. Gress and O. Grishin and A. Klabukov and A. Klimov and A. Kochanov and K. Konischev and A. Koshechkin and V. Kulepov and D. Kuleshov and L. Kuzmichev and E. Middell and S. Mikheyev and M. Milenin and R. Mirgazov and E. Osipova and G. Pan’kov and L. Pan’kov and A. Panfilov and D. Petukhov and E. Pliskovsky and P. Pokhil and V. Poleschuk and E. Popova and V. Prosin and M. Rozanov and V. Rubtzov and A. Sheifler and A. Shirokov and B. Shoibonov and Ch. Spiering and O. Suvorova and B. Tarashansky and R. Wischnewski and I. Yashin and V. Zhukov",
keywords = "Neutrino telescopes, BAIKAL",
abstract = "A prototype string for the future km3-scale Baikal neutrino telescope has been deployed in April, 2008, and is fully integrated into the NT200+ telescope. All basic string elements–optical modules (with 12″/13″ hemispherical photomultipliers), 200MHz FADC readout and calibration system–have been redesigned following experience with NT200+. First results of in-situ operation of this prototype string are presented."
}
@article{TSAI20091578,
title = "Experience on knowledge-based software engineering: A logic-based requirements language and its industrial applications",
journal = "Journal of Systems and Software",
volume = "82",
number = "10",
pages = "1578 - 1587",
year = "2009",
note = "SI: YAU",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2009.03.019",
url = "http://www.sciencedirect.com/science/article/pii/S0164121209000521",
author = "Jeffrey J.P. Tsai and Alan Liu",
keywords = "Formal specification language, Nonmonotonic logic, Formal verification, Automatic code generation, Knowledge-based software engineering",
abstract = "A formal requirements specification language plays an important role in software development. Not only can such language be used for stating requirements specification, but also can be used in many phases of software development life cycle. The FRORL project started from constructing a language with a solid logical foundation and further expanded to research in verification, validation, requirements analysis, debugging, and transformation. Research in this project aided in some industrial applications in which a code generation tool produced software for embedded systems. This article reports the experiences gained from this project and states the value of research in knowledge-based software engineering."
}
@article{AFZAL201886,
title = "The MegaM@Rt2 ECSEL project: MegaModelling at Runtime – Scalable model-based framework for continuous development and runtime validation of complex systems",
journal = "Microprocessors and Microsystems",
volume = "61",
pages = "86 - 95",
year = "2018",
issn = "0141-9331",
doi = "https://doi.org/10.1016/j.micpro.2018.05.010",
url = "http://www.sciencedirect.com/science/article/pii/S014193311830022X",
author = "Wasif Afzal and Hugo Bruneliere and Davide Di Ruscio and Andrey Sadovykh and Silvia Mazzini and Eric Cariou and Dragos Truscan and Jordi Cabot and Abel Gómez and Jesús Gorroñogoitia and Luigi Pomante and Pavel Smrz",
keywords = "Model-driven engineering, Design time, Runtime, Megamodelling",
abstract = "A major challenge for the European electronic industry is to enhance productivity by ensuring quality of development, integration and maintenance while reducing the associated costs. Model-Driven Engineering (MDE) principles and techniques have already shown promising capabilities, but they still need to scale up to support real-world scenarios implied by the full deployment and use of complex electronic components and systems. Moreover, maintaining efficient traceability, integration, and communication between two fundamental system life cycle phases (design time and runtime) is another challenge requiring the scalability of MDE. This paper presents an overview of the ECSEL 11http://www.ecsel-ju.eu/web/index.php. project entitled “MegaModelling at runtime – Scalable model-based framework for continuous development and runtime validation of complex systems” (MegaM@Rt2), whose aim is to address the above mentioned challenges facing MDE. Driven by both large and small industrial enterprises, with the support of research partners and technology providers, MegaM@Rt2 aims to deliver a framework of tools and methods for: 1) system engineering/design and continuous development, 2) related runtime analysis and 3) global models and traceability management. Diverse industrial use cases (covering strategic domains such as aeronautics, railway, construction and telecommunications) will integrate and demonstrate the validity of the MegaM@Rt2 solution. This paper provides an overview of the MegaM@Rt2 project with respect to its approach, mission, objectives as well as to its implementation details. It further introduces the consortium as well as describes the work packages and few already produced deliverables."
}
@article{ALMENDROSJIMENEZ2016332,
title = "PTL: A model transformation language based on logic programming",
journal = "Journal of Logical and Algebraic Methods in Programming",
volume = "85",
number = "2",
pages = "332 - 366",
year = "2016",
issn = "2352-2208",
doi = "https://doi.org/10.1016/j.jlamp.2015.06.006",
url = "http://www.sciencedirect.com/science/article/pii/S2352220815000565",
author = "Jesús M. Almendros-Jiménez and Luis Iribarne and Jesús López-Fernández and Ángel Mora-Segura",
keywords = "Logic programming, Model transformation, Software engineering, Model driven engineering, Domain specific languages",
abstract = "In this paper we present a model transformation language based on logic programming. The language, called PTL (Prolog based Transformation Language), can be considered as a hybrid language in which ATL (Atlas Transformation Language)-style rules are combined with logic rules for defining transformations. ATL-style rules are used to define mappings from source models to target models while logic rules are used as helpers. The implementation of PTL is based on the encoding of the ATL-style rules by Prolog rules. Thus, PTL makes use of Prolog as a transformation engine. We have provided a declarative semantics to PTL and proved the semantics equivalent to the encoded program. We have studied an encoding of OCL (Object Constraint Language) with Prolog goals in order to map ATL to PTL. Thus a subset of PTL can be considered equivalent to a subset of ATL. The proposed language can be also used for model validation, that is, for checking constraints on models and transformations. We have equipped our language with debugging and tracing capabilities which help developers to detect programming errors in PTL rules. Additionally, we have developed an Eclipse plugin for editing PTL programs, as well as for debugging, tracing and validation. Finally, we have evaluated the language with several transformation examples as well as tested the performance with large models."
}
@article{STRUBER2017196,
title = "A text-based visual notation for the unit testing of model-driven tools",
journal = "Computer Languages, Systems & Structures",
volume = "49",
pages = "196 - 215",
year = "2017",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2016.08.004",
url = "http://www.sciencedirect.com/science/article/pii/S1477842416300276",
author = "Daniel Strüber and Felix Rieger and Gabriele Taentzer",
keywords = "Model-driven engineering, Tools, Model notation, Flexible modeling, Testing",
abstract = "During the unit testing of model-driven tools, a large number of models and test classes needs to be managed and maintained. Typically, some of these artifacts are specified manually, some are generated automatically. Existing approaches to test management rely on the available visual and textual modeling notations. As these notations are not tailored to unit testing, distinct maintainability trade-offs arise. In this paper, we propose a notation that aims to combine the benefits of visual and text-based approaches. The notation is at the same time visual and text-based, as it uses ASCII characters to emulate the familiar graphical notations. In our evaluation based on real models, we identify problematic model shapes challenging the scalability our notation, while finding that it is well-suited to capture typical test models."
}
@article{HUTCHESSON2013525,
title = "Trusted Product Lines",
journal = "Information and Software Technology",
volume = "55",
number = "3",
pages = "525 - 540",
year = "2013",
note = "Special Issue on Software Reuse and Product Lines",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2012.06.005",
url = "http://www.sciencedirect.com/science/article/pii/S0950584912001085",
author = "Stuart Hutchesson and John McDermid",
keywords = "Software Product Lines, High-integrity software, DO-178B/ED-12B, SPARK, Model transformation, GSN",
abstract = "Context
The paper addresses the use of a Software Product Line approach in the context of developing software for a high-integrity, regulated domain such as civil aerospace. The success of a Software Product Line approach must be judged on whether useful products can be developed more effectively (lower cost, reduced schedule) than with traditional single-system approaches. When developing products for regulated domains, the usefulness of the product is critically dependent on the ability of the development process to provide approval evidence for scrutiny by the regulating authority.
Objective
The objective of the work described is to propose a framework for arguing that a product instantiated using a Software Product Line approach can be approved and used within a regulated domain, such that the development cost of that product would be less than if it had been developed in isolation.
Method
The paper identifies and surveys the issues relating the adoption of Software Product Lines as currently understood (including related technologies such as feature modelling, component-based development and model transformation) when applied to high-integrity software development. We develop an argument framework using Goal Structuring Notation to structure the claims made and the evidence required to support the approval of an instantiated product in such domains. Any unsubstantiated claims or missing/sub-standard evidence is identified, and we propose potential approaches or pose research questions to help address this.
Results
The paper provides an argument framework supporting the use of a Software Product Line approach within a high-integrity regulated domain. It shows how lifecycle evidence can be collected, managed and used to credibly support a regulatory approval process, and provides a detailed example showing how claims regarding model transformation may be supported. Any attempt to use a Software Product Line approach in a regulated domain will need to provide evidence to support their approach in accordance with the argument outlined in the paper.
Conclusion
Product Line practices may complicate the generation of convincing evidence for approval of instantiated products, but it is possible to define a credible Trusted Product Line approach."
}
@incollection{MARIANI2015157,
title = "Chapter Four - Recent Advances in Automatic Black-Box Testing",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "99",
pages = "157 - 193",
year = "2015",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2015.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S0065245815000315",
author = "Leonardo Mariani and Mauro Pezzè and Daniele Zuddas",
keywords = "Black-box testing, Model-based testing, Random testing, Testing with complex inputs, Combinatorial interaction testing, Test automation",
abstract = "Research in black-box testing has produced impressive results in the past 40 years, addressing many aspects of the problem that span from integration with the development process, to test case generation and execution. In the past few years, the research in this area has focused mostly on the automation of black-box approaches to improve applicability and scalability. This chapter surveys the recent advances in automatic black-box testing, covering contributions from 2010 to 2014, presenting the main research results and discussing the research trends."
}
@article{HABERMAIER201544,
title = "Executable Specifications of Safety-Critical Systems with S#",
journal = "IFAC-PapersOnLine",
volume = "48",
number = "7",
pages = "44 - 49",
year = "2015",
note = "5th IFAC International Workshop on Dependable Control of Discrete Systems",
issn = "2405-8963",
doi = "https://doi.org/10.1016/j.ifacol.2015.06.471",
url = "http://www.sciencedirect.com/science/article/pii/S2405896315007107",
author = "Axel Habermaier and Johannes Leupolz and Wolfgang Reif",
keywords = "safety analysis, executable specification, design tools and techniques, formal methods, model checking, deductive cause consequence analysis, software engineering",
abstract = "Abstract:
Model-based safety analysis techniques use formal methods to rigorously assess the risks associated with safety-critical systems. The adequacy of the results obtained from those formal techniques, however, is greatly influenced by the quality and comprehensibility of the underlying formal models. We introduce our S# modeling framework (pronounced "safety sharp"), an executable, systematic, high-level specification language and tool suite specifically designed for the convenient modeling and formal analysis of safety-critical systems. This paper shows how S# facilitates and improves model simulation, debugging, and testing during all stages of the development of such systems."
}
@article{LAUCIELLO2016310,
title = "A high yield optimized method for the production of acylated ACPs enabling the analysis of enzymes involved in P. falciparum fatty acid biosynthesis",
journal = "Biochemistry and Biophysics Reports",
volume = "8",
pages = "310 - 317",
year = "2016",
issn = "2405-5808",
doi = "https://doi.org/10.1016/j.bbrep.2016.09.017",
url = "http://www.sciencedirect.com/science/article/pii/S2405580816302114",
author = "Leonardo Lauciello and Gabriela Lack and Leonardo Scapozza and Remo Perozzo",
keywords = "Acyl carrier protein, Acylation, Fatty acid biosynthesis, , Enzyme kinetics, Natural substrates",
abstract = "The natural substrates of the enzymes involved in type-II fatty acid biosynthesis (FAS-II) are acylated acyl carrier proteins (acyl-ACPs). The state of the art method to produce acyl-ACPs involves the transfer of a phosphopantetheine moiety from CoA to apo-ACP by E. coli holo-ACP synthase (EcACPS), yielding holo-ACP which subsequently becomes thioesterified with free fatty acids by the E. coli acyl-ACP synthase (EcAAS). Alternatively, acyl-ACPs can be synthesized by direct transfer of acylated phosphopantetheine moieties from acyl-CoA to apo-ACP by means of EcACPS. The need for native substrates to characterize the FAS-II enzymes of P. falciparum prompted us to investigate the potential and limit of the two methods to efficiently acylate P. falciparum ACP (PfACP) with respect to chain length and β-modification and in preparative amounts. The EcAAS activity is found to be independent from the oxidation state at the β-position and accepts fatty acids as substrates with chain lengths starting from C8 to C20, whereas EcACPS accepts very efficiently acyl-CoAs with chain lengths up to C16, and with decreasing activity also longer chains (C18 to C20). Methods were developed to synthesize and purify preparative amounts of high quality natural substrates that are fully functional for the enzymes of the P. falciparum FAS-II system."
}
@article{PRASETYA2018223,
title = "Temporal algebraic query of test sequences",
journal = "Journal of Systems and Software",
volume = "136",
pages = "223 - 236",
year = "2018",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.07.014",
url = "http://www.sciencedirect.com/science/article/pii/S016412121730153X",
author = "I.S.W.B. Prasetya",
keywords = "Query based testing, Property based testing, LTL Test oracles, Algebraic test oracles, Dynamic analysis",
abstract = "Nowadays tools can generate test suites consisting of large number of test sequences. The used algorithms are typically random-based. Although more advanced variations may incorporate an advanced search algorithm to cover difficult scenarios, many decisions still have to be made randomly simply because no information is available to calculate the best decision. Because of this, many of the generated sequences may be redundant, while some others may be rare and hard to get. This paper presents a rich formalism that is based on a mix of algebraic relations and Linear Temporal Logic (LTL) to query test suites, and an efficient algorithm to execute such queries. Queries can be used as correctness specifications (oracles) to validate a test suite. They are however more general as they can be used to filter out test sequences with interesting properties, e.g. to archive them for future use. The proposed formalism is quite expressive: it can express algebraic equations with logical variables, Hoare triples, class invariants, as well as their temporal modalities. An evaluation of the query algorithm’s performance is included in this paper. The whole query framework has been implemented in a testing tool for Java called T3i."
}
@incollection{JILANI2018,
title = "Advances in Applications of Object Constraint Language for Software Engineering",
series = "Advances in Computers",
publisher = "Elsevier",
year = "2018",
issn = "0065-2458",
doi = "https://doi.org/10.1016/bs.adcom.2017.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0065245817300554",
author = "Atif A. Jilani and Muhammad Z. Iqbal and Muhammad U. Khan and Muhammad Usman",
keywords = "Object Constraint Language, Model-driven engineering, Secondary study, Software engineering",
abstract = "Object Constraint Language (OCL) is a standard language defined by Object Management Group for specifying constraints on models. Since its introduction as part of Unified Modeling Language, OCL has received significant attention by researchers with works in the literature ranging from temporal extensions of OCL to automated test generation by solving OCL constraints. In this chapter, we provide a survey of the various works discussed in literature related to OCL with the aim of highlighting the advances made in the field. We classify the literature into five broad categories and provide summaries for various works in the literature. The chapter also provides insights and highlights the potentials areas of further research in the field."
}
@incollection{2011183,
editor = "Eric Conrad",
booktitle = "Eleventh Hour CISSP",
publisher = "Syngress",
address = "Boston",
pages = "183 - 196",
year = "2011",
isbn = "978-1-59749-566-0",
doi = "https://doi.org/10.1016/B978-1-59749-566-0.00016-3",
url = "http://www.sciencedirect.com/science/article/pii/B9781597495660000163"
}
@article{RAMLER2018248,
title = "Adapting automated test generation to GUI testing of industry applications",
journal = "Information and Software Technology",
volume = "93",
pages = "248 - 263",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916303676",
author = "Rudolf Ramler and Georg Buchgeher and Claus Klammer",
abstract = "Context
Automated test generation promises to improve the effectiveness of software testing and to reduce the involved manual effort. While automated test generation has been successfully applied for code-level API testing, it has not found widespread adoption in practice for testing of graphical user interfaces. Tools for test generation do not support GUI testing out-of-the-box but require dedicated extensions.
Objective
This paper explores the applicability of automated test generation for testing GUIs of industry applications. We propose a test adapter approach to bridge the gap between automated test generation tools and industry applications.
Method
A multiple case study was conducted in which automated test generation with test adapters has been applied at the unit, integration, and system test level in three industry projects from two different companies.
Results
Automated test generation via test adapters could be applied at all test levels. It has led to an increase of coverage as well as the detection of new defects that were not found by preceding testing activities in the projects. While test adapters can easily be implemented at the unit test level, their complexity and the corresponding effort for providing adapter implementations rises at higher test levels.
Conclusion
Test adapters can be used for applying automated test generation for testing GUIs of industry applications. They bridge the gap between automated test generation tools and industry applications. The development of test adapters requires no tool-specific knowledge and can be performed by members of the development team."
}
@incollection{ALEXANDER2007109,
title = "Chapter 5 - Protocol testing",
editor = "Tom Alexander",
booktitle = "Optimizing and Testing WLANs",
publisher = "Newnes",
address = "Burlington",
pages = "109 - 135",
year = "2007",
isbn = "978-0-7506-7986-2",
doi = "https://doi.org/10.1016/B978-075067986-2/50006-9",
url = "http://www.sciencedirect.com/science/article/pii/B9780750679862500069",
author = "Tom Alexander",
abstract = "Publisher Summary
This chapter describes the metrics and measurements pertinent to the wireless LAN (WLAN) Medium Access Control (MAC), as well as the Transmission Control Protocol (TCP)/Internet Protocol (IP) stack. A functional test is concerned with verifying that a device or system functions properly. A performance test is much more broad-based and generic, and is aimed at characterizing a device or system according to some well-defined metric. Indeed, a single performance metric may be applicable to a wide variety of devices. WLAN protocol tests, almost without exception, inject packet traffic into the device under test (DUT) as test stimuli, and measure the DUT response in terms of the number or rate of specific types of packets that it generates in turn. As tests are being performed on the MAC layer, the RF characteristics of the packet traffic are rarely of much interest, beyond ensuring that these characteristics do not skew the test results by causing unexpected issues at the physical (PHY) layer. Conformance tests are almost exclusively performed in a highly controlled and isolated environment, as external interference at the wrong moment can invalidate an entire test. It is found that IEEE 802.11 protocol conformance tests are performed on the different aspects of the WLAN-MAC and PHY protocol specifications."
}
@incollection{FRIEDRICH2014139,
title = "Chapter 11 - Knowledge Engineering for Configuration Systems",
editor = "Alexander Felfernig and Lothar Hotz and Claire Bagley and Juha Tiihonen",
booktitle = "Knowledge-Based Configuration",
publisher = "Morgan Kaufmann",
address = "Boston",
pages = "139 - 155",
year = "2014",
isbn = "978-0-12-415817-7",
doi = "https://doi.org/10.1016/B978-0-12-415817-7.00011-6",
url = "http://www.sciencedirect.com/science/article/pii/B9780124158177000116",
author = "Gerhard Friedrich and Dietmar Jannach and Markus Stumptner and Markus Zanker",
keywords = "Knowledge-based Configuration, Development Life Cycle of Configurators, Debugging of Configuration Knowledge Bases, Organizational Challenges",
abstract = "Developing a product configuration system is a nontrivial and challenging task for various reasons. First, the domain knowledge that has to be encoded into the system is often spread over several departments or functions within a company. Besides that, in many cases data from existing information systems have to be integrated into the configurator. Finally, the business rules or technical constraints that define the space of possible configurations can be relatively complex and also subject to frequent changes. This makes acquiring and encoding domain knowledge as well as testing and debugging particularly demanding tasks. In this chapter, we give an overview of the challenges when developing a knowledge-based configuration system. We will particularly focus on questions related to the knowledge acquisition process and will additionally show how model-based debugging techniques can be applied to support the knowledge engineer in the testing and debugging process."
}
@article{KAMEYAMA2015120,
title = "Combinators for impure yet hygienic code generation",
journal = "Science of Computer Programming",
volume = "112",
pages = "120 - 144",
year = "2015",
note = "Selected and extended papers from Partial Evaluation and Program Manipulation 2014",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2015.08.007",
url = "http://www.sciencedirect.com/science/article/pii/S016764231500249X",
author = "Yukiyoshi Kameyama and Oleg Kiselyov and Chung-chieh Shan",
keywords = "Multi-stage programming, Mutable state and control effects, CPS, Higher-order abstract syntax, Lexical scope",
abstract = "Code generation is the leading approach to making high-performance software reusable. Effects are indispensable in code generators, whether to report failures or to insert let-statements and if-guards. Extensive painful experience shows that unrestricted effects interact with generated binders in undesirable ways to produce unexpectedly unbound variables, or worse, unexpectedly bound ones. These subtleties hinder domain experts in using and extending the generator. A pressing problem is thus to express the desired effects while regulating them so that the generated code is correct, or at least correctly scoped, by construction. We present a code-combinator framework that lets us express arbitrary monadic effects, including mutable references and delimited control, that move open code across generated binders. The static types of our generator expressions not only ensure that a well-typed generator produces well-typed and well-scoped code. They also express the lexical scopes of generated binders and prevent mixing up variables with different scopes. For the first time ever we demonstrate statically safe and well-scoped loop interchange and constant factoring from arbitrarily nested loops. Our framework is implemented as a Haskell library that embeds an extensible typed higher-order domain-specific language. It may be regarded as ‘staged Haskell.’ To become practical, the library relies on higher-order abstract syntax and polymorphism over generated type environments, and is written in a mature language."
}
@article{MARTINEZ201846,
title = "Feature location benchmark for extractive software product line adoption research using realistic and synthetic Eclipse variants",
journal = "Information and Software Technology",
volume = "104",
pages = "46 - 59",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301472",
author = "Jabier Martinez and Tewfik Ziadi and Mike Papadakis and Tegawendé F. Bissyandé and Jacques Klein and Yves le Traon",
keywords = "Feature location, Software families, Eclipse, Benchmark, Software product lines, Static analysis, Information retrieval",
abstract = "Context: It is common belief that high impact research in software reuse requires assessment in non-trivial, comparable, and reproducible settings. However, software artefacts and common representations are usually unavailable. Also, establishing a representative ground truth is a challenging and debatable subject. Feature location in the context of software families, which is key for software product line adoption, is a research field that is becoming more mature with a high proliferation of techniques. Objective: We present EFLBench, a benchmark and a framework to provide a common ground for the evaluation of feature location techniques in families of systems. Method: EFLBench leverages the efforts made by the Eclipse Community which provides feature-based family artefacts and their plugin-based implementations. Eclipse is an active and non-trivial project and thus, it establishes an unbiased ground truth which is realistic and challenging. Results: EFLBench is publicly available and supports all tasks for feature location techniques integration, benchmark construction and benchmark usage. We demonstrate its usage, simplicity and reproducibility by comparing four techniques in Eclipse releases. As an extension of our previously published work, we consider a decade of Eclipse releases and we also contribute an approach to automatically generate synthetic Eclipse variants to benchmark feature location techniques in tailored settings. We present and discuss three strategies for this automatic generation and we present the results using different settings. Conclusion: EFLBench is a contribution to foster the research in feature location in families of systems providing a common framework and a set of baseline techniques and results."
}
@incollection{MEHTA2015479,
title = "Chapter 16 - Asset management systems",
editor = "B.R. Mehta and Y.J. Reddy",
booktitle = "Industrial Process Automation Systems",
publisher = "Butterworth-Heinemann",
address = "Oxford",
pages = "479 - 506",
year = "2015",
isbn = "978-0-12-800939-0",
doi = "https://doi.org/10.1016/B978-0-12-800939-0.00016-4",
url = "http://www.sciencedirect.com/science/article/pii/B9780128009390000164",
author = "B.R. Mehta and Y.J. Reddy",
keywords = "Asset, smart instrument, fault models, EDDL, diagnostics, predictive, preventive, FDI",
abstract = "Asset management systems are a class of software and hardware applications used in the process plants for the efficient and optimum utilization of the equipment. An asset management system helps the plant to drive the operational and maintenance excellence. There are various systems developed for various applications; they together as a suite help to monitor, analyze, predict, and report the asset performance. This chapter deals with plant asset management systems, instrument asset management systems (IAMS), and some of the key rendering technologies and standards. The key drivers for the deployment of such systems and role-based access to the systems are discussed at length. Some of the communication technologies mentioned in this chapter are dealt separately as individual chapters. At the end of the topic, the key features from the asset management systems from an instrumentation perspective are dealt in much more length with little overview on enterprise asset management systems. Some of the rendering technologies such as DD/EDDL and FDT are discussed in more detail for the instrumentation engineers to get more familiar with these technologies and hence help them to choose the same selectively, considering the needs of the plants. The plant asset management systems and the modeling and underlying benefits of such systems are also discussed to create awareness on the possibilities of asset management in a plant context extending the limits from instrumentation and control to assets such as pumps, electrical machinery, and rotating equipment."
}
@article{RODRIGUESDASILVA2015139,
title = "Model-driven engineering: A survey supported by the unified conceptual model",
journal = "Computer Languages, Systems & Structures",
volume = "43",
pages = "139 - 155",
year = "2015",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2015.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S1477842415000408",
author = "Alberto Rodrigues da Silva",
keywords = "Model, Metamodel, Modeling language, Software system, Model-driven engineering, Model-driven approaches",
abstract = "During the last decade a new trend of approaches has emerged, which considers models not just documentation artefacts, but also central artefacts in the software engineering field, allowing the creation or automatic execution of software systems starting from those models. These proposals have been classified generically as Model-Driven Engineering (MDE) and share common concepts and terms that need to be abstracted, discussed and understood. This paper presents a survey on MDE based on a unified conceptual model that clearly identifies and relates these essential concepts, namely the concepts of system, model, metamodel, modeling language, transformations, software platform, and software product. In addition, this paper discusses the terminologies relating MDE, MDD, MDA and others. This survey is based on earlier work, however, contrary to those, it intends to give a simple, broader and integrated view of the essential concepts and respective terminology commonly involved in the MDE, answering to key questions such as: What is a model? What is the relation between a model and a metamodel? What are the key facets of a modeling language? How can I use models in the context of a software development process? What are the relations between models and source code artefacts and software platforms? and What are the relations between MDE, MDD, MDA and other MD approaches?"
}
@article{GEORG2015109,
title = "Synergy between Activity Theory and goal/scenario modeling for requirements elicitation, analysis, and evolution",
journal = "Information and Software Technology",
volume = "59",
pages = "109 - 135",
year = "2015",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914002419",
author = "Geri Georg and Gunter Mussbacher and Daniel Amyot and Dorina Petriu and Lucy Troup and Saul Lozano-Fuentes and Robert France",
keywords = "Requirements engineering, Activity Theory, User Requirements Notation, Goal modeling, Scenario modeling",
abstract = "Context
It is challenging to develop comprehensive, consistent, analyzable requirements models for evolving requirements. This is particularly critical for certain highly interactive types of socio-technical systems that involve a wide range of stakeholders with disparate backgrounds; system success is often dependent on how well local social constraints are addressed in system design.
Objective
This paper describes feasibility research, combining a holistic social system perspective provided by Activity Theory (AT), a psychological paradigm, with existing system development methodologies and tools, specifically goal and scenario modeling.
Method
AT is used to understand the relationships between a system, its stakeholders, and the system’s evolving context. The User Requirements Notation (URN) is used to produce rigorous, analyzable specifications combining goal and scenario models. First, an AT language was developed constraining the framework for automation, second consistency heuristics were developed for constructing and analyzing combined AT/URN models, third a combined AT/URN methodology was developed, and consequently applied to a proof-of-concept system.
Results
An AT language with limited tool support was developed, as was a combined AT/URN methodology. This methodology was applied to an evolving disease management system to demonstrate the feasibility of adapting AT for use in system development with existing methodologies and tools. Bi-directional transformations between the languages allow proposed changes in system design to be propagated to AT models for use in stakeholder discussions regarding system evolution.
Conclusions
The AT framework can be constrained for use in requirements elicitation and combined with URN tools to provide system designs that include social system perspectives. The developed AT/URN methodology can help engineers to track the impact on system design due to requirement changes triggered by changes in the system’s social context. The methodology also allows engineers to assess the impact of proposed system design changes on the social elements of the system context."
}
@article{KESSENTINI2018,
title = "Automated metamodel/model co-evolution: A search-based approach",
journal = "Information and Software Technology",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301915",
author = "Wael Kessentini and Houari Sahraoui and Manuel Wimmer",
keywords = "Metamodel/model co-evolution, Model migration, Coupled evolution, Search based software engineering",
abstract = "Context: Metamodels evolve over time to accommodate new features, improve existing designs, and fix errors identified in previous releases. One of the obstacles that may limit the adaptation of new metamodels by developers is the extensive manual changes that have been applied to migrate existing models. Recent studies addressed the problem of automating the metamodel/model co-evolution based on manually defined migration rules. The definition of these rules requires the list of changes at the metamodel level which are difficult to fully identify. Furthermore, different possible alternatives may be available to translate a metamodel change to a model change. Thus, it is hard to generalize these co-evolution rules. Objective: We propose an alternative automated approach for the metamodel/model co-evolution. The proposed approach refines an initial model instantiated from the previous metamodel version to make it as conformant as possible to the new metamodel version by finding the best compromise between three objectives, namely minimizing (i) the non-conformities with new metamodel version, (ii) the changes to existing models, and (iii) the textual and structural dissimilarities between the initial and revised models. Method: We formulated the metamodel/model co-evolution as a multi-objective optimization problem to handle the different conflicting objectives using the Non-dominated Sorting Genetic Algorithm II (NSGA-II) and the Multi-Objective Particle Swarm Optimization (MOPSO). Results: We evaluated our approach on several evolution scenarios extracted from different widely used metamodels. The results confirm the effectiveness of our approach with average manual correctness, precision and recall respectively higher than 91%, 88% and 89% on the different co-evolution scenarios. Conclusion: A comparison with our previous work confirms the out-performance of our multi-objective formulation."
}
@article{OHARA201473,
title = "Glycan receptors of the Polyomaviridae: structure, function, and pathogenesis",
journal = "Current Opinion in Virology",
volume = "7",
pages = "73 - 78",
year = "2014",
note = "Virus-glycan interactions and pathogenesis / Viruses and RNA interference",
issn = "1879-6257",
doi = "https://doi.org/10.1016/j.coviro.2014.05.004",
url = "http://www.sciencedirect.com/science/article/pii/S1879625714001242",
author = "Samantha D O’Hara and Thilo Stehle and Robert Garcea",
abstract = "Multiple glycans have been identified as potential cell surface binding motifs for polyomaviruses (PyVs) using both crystallographic structural determinations and in vitro binding assays. However, binding alone does not necessarily imply that a glycan is a functional receptor, and confirmation that specific glycans are important for infection has proved challenging. In vivo analysis of murine polyomavirus (MPyV) infection has shown that subtle alterations in PyV–glycan interactions alone can result in dramatic changes in pathogenicity, implying that similar effects will be found for other PyVs. Our discussion will review the assays used for determining virus–glycan binding, and how these relate to known PyV tropism and pathogenesis."
}
@article{GONZALEZALONSO2012889,
title = "Towards a new open communication standard between homes and service robots, the DHCompliant case",
journal = "Robotics and Autonomous Systems",
volume = "60",
number = "6",
pages = "889 - 900",
year = "2012",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2012.01.006",
url = "http://www.sciencedirect.com/science/article/pii/S0921889012000188",
author = "Ignacio González Alonso and Omar Álvarez Fres and Alberto Alonso Fernández and Pablo Gómez del Torno and J.M. Maestre and M.d.P. Almudena García Fuente",
keywords = "Home&Building automation, Interoperability, Service robot, UPnP, SysML, DHCompliant",
abstract = "The interoperability of service robots and digital home was a user demand from the past years. In response to that necessity, the researchers from the Infobotica Research Group, in cooperation with a group of companies and universities, have proposed a new open standard and architecture. It is composed of different virtual services, protocols as well as an open adapters’ architecture, on top of the UPnP protocol stack. The proposed application protocols and the general architecture provide a communication environment for positioning devices, rules compliance checks, the collaboration between devices and managing energy efficiently. The different tools, adapters, and protocols, developed within the DHCompliant architecture, have defined a new level of application protocol that has allowed increased integration of those modules into home automation, improving their interoperability, and allowing the addition of new services to the same standard and commercial hardware."
}
@article{DABHOLKAR2009756,
title = "The role of perceived control and gender in consumer reactions to download delays",
journal = "Journal of Business Research",
volume = "62",
number = "7",
pages = "756 - 760",
year = "2009",
issn = "0148-2963",
doi = "https://doi.org/10.1016/j.jbusres.2008.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S0148296308001380",
author = "Pratibha A. Dabholkar and Xiaojing Sheng",
keywords = "Perceived control, Gender differences, Download delays, Attitudes, Intentions, Online marketing",
abstract = "An empirical study finds that perceived control strongly mediates the effects of perceived speed of a Web site download on consumers' attitudes and intentions to use the Web site. Moreover, results show that men are more likely to react positively to the perceived speed of a Web site download, whereas women are more likely to base their reactions on perceptions of control in the context of download delays. In contrast to past online research, the gender differences are intrinsic in two ways—they are context independent, and they are not caused by length of Internet experience, extent of Internet usage, or type of Internet connections."
}
@incollection{AERTS2017287,
title = "Chapter 19 - Model-Based Testing of Cyber-Physical Systems",
editor = "Houbing Song and Danda B. Rawat and Sabina Jeschke and Christian Brecher",
booktitle = "Cyber-Physical Systems",
publisher = "Academic Press",
address = "Boston",
pages = "287 - 304",
year = "2017",
series = "Intelligent Data-Centric Systems",
isbn = "978-0-12-803801-7",
doi = "https://doi.org/10.1016/B978-0-12-803801-7.00019-5",
url = "http://www.sciencedirect.com/science/article/pii/B9780128038017000195",
author = "A. Aerts and M. Reniers and M.R. Mousavi",
keywords = "Cyber-physical systems, V-model, Model-based testing, Conformance, Test-case generation, Test coverage",
abstract = "Cyber-physical systems (CPSs) are the result of the integration of connected computer systems with the physical world. They feature complex interactions that go beyond traditional communication schemes and protocols in computer systems. One distinguished feature of such complex interactions is the tight coupling between discrete and continuous interactions, captured by hybrid system models. Due to the complexity of CPSs, providing rigorous and model-based analysis methods and tools for verifying correctness of such systems is of the utmost importance. Model-based testing (MBT) is one such verification technique that can be used for checking the conformance of an implementation of a system to its specification (model). In this chapter, we first review the main concepts and techniques in MBT. Subsequently, we review the most common modeling formalisms for CPSs, with focus on hybrid system models. Subsequently, we provide a brief overview of conformance relations and conformance testing techniques for CPSs."
}
@incollection{SLATTEN2013119,
title = "Chapter 4 - Model-Driven Engineering of Reliable Fault-Tolerant Systems—A State-of-the-Art Survey",
editor = "Atif Memon",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "91",
pages = "119 - 205",
year = "2013",
booktitle = "Advances in Computers",
issn = "0065-2458",
doi = "https://doi.org/10.1016/B978-0-12-408089-8.00004-5",
url = "http://www.sciencedirect.com/science/article/pii/B9780124080898000045",
author = "Vidar Slåtten and Peter Herrmann and Frank Alexander Kraemer",
keywords = "Model-driven engineering, Fault tolerance, Reliability, Verification, Survey",
abstract = "To improve the reliability of a system, we can add fault-tolerance mechanisms. This, however, leads to a rise of complexity that increases the probability of software faults being introduced. Hence, unless the process is handled carefully, adding fault tolerance may even lead to a less reliable system. As a way to deal with the inherently high level of complexity of fault-tolerant systems, some research groups have turned to the paradigm of model-driven engineering. This results in a research field that crosscuts the established fields of software engineering, system verification, fault-tolerant systems and distributed systems. Many works are presented in the context of one of these traditional fields, making it difficult to get a good overview of what is presently offered. We survey 10 approaches for model-driven engineering of reliable fault-tolerant systems and present 13 characteristics classifying the approaches in a manner useful for both users and developers of such approaches. We further discuss the state of the field and what the future may bring."
}
@article{ZHANG2006209,
title = "Using source transformation to test and model check implicit-invocation systems",
journal = "Science of Computer Programming",
volume = "62",
number = "3",
pages = "209 - 227",
year = "2006",
note = "Special issue on Source code analysis and manipulation (SCAM 2005)",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2006.04.008",
url = "http://www.sciencedirect.com/science/article/pii/S0167642306000955",
author = "Hongyu Zhang and Jeremy S. Bradbury and James R. Cordy and Juergen Dingel",
keywords = "Source transformation, Domain-specific language, Verification, Testing, Model checking, Implicit invocation",
abstract = "In this paper we present a source transformation-based framework to support uniform testing and model checking of implicit-invocation software systems. The framework includes a new domain-specific programming language, the Implicit-Invocation Language (IIL), explicitly designed for directly expressing implicit-invocation software systems, and a set of formal rule-based source transformation tools that allow automatic generation of both executable and formal verification artifacts. We provide details of these transformation tools, evaluate the framework in practice, and discuss the benefits of formal automatic transformation in this context. Our approach is designed not only to advance the state-of-the-art in validating implicit-invocation systems, but also to further explore the use of automated source transformation as a uniform vehicle to assist in the implementation, validation and verification of programming languages and software systems in general."
}
@article{BELETE201749,
title = "An overview of the model integration process: From pre-integration assessment to testing",
journal = "Environmental Modelling & Software",
volume = "87",
pages = "49 - 63",
year = "2017",
issn = "1364-8152",
doi = "https://doi.org/10.1016/j.envsoft.2016.10.013",
url = "http://www.sciencedirect.com/science/article/pii/S1364815216308805",
author = "Getachew F. Belete and Alexey Voinov and Gerard F. Laniak",
keywords = "Integrated modeling, Interoperability, Interfaces, Wrapping, Components, Web services",
abstract = "Integration of models requires linking models which can be developed using different tools, methodologies, and assumptions. We performed a literature review with the aim of improving our understanding of model integration process, and also presenting better strategies for building integrated modeling systems. We identified five different phases to characterize integration process: pre-integration assessment, preparation of models for integration, orchestration of models during simulation, data interoperability, and testing. Commonly, there is little reuse of existing frameworks beyond the development teams and not much sharing of science components across frameworks. We believe this must change to enable researchers and assessors to form complex workflows that leverage the current environmental science available. In this paper, we characterize the model integration process and compare integration practices of different groups. We highlight key strategies, features, standards, and practices that can be employed by developers to increase reuse and interoperability of science software components and systems."
}
@article{DASILVA2015527,
title = "Using a multi-method approach to understand Agile software product lines",
journal = "Information and Software Technology",
volume = "57",
pages = "527 - 542",
year = "2015",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914001438",
author = "Ivonei Freitas da Silva and Paulo Anselmo da Mota Silveira Neto and Pádraig O’Leary and Eduardo Santana de Almeida and Silvio Romero de Lemos Meira",
keywords = "Agile, Software product lines, Multi-method approach, Case study, Mapping study, Expert opinion",
abstract = "Context
Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method.
Objective
This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach.
Method
Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence.
Results
This combination results in 23 findings that provide evidence on how Agile and SPL could be combined.
Conclusion
Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies."
}
@incollection{2012549,
editor = "Eric Conrad and Seth Misenar and Joshua Feldman",
booktitle = "CISSP Study Guide (Second Edition)",
publisher = "Syngress",
edition = "Second Edition",
address = "Boston",
pages = "549 - 577",
year = "2012",
isbn = "978-1-59749-961-3",
doi = "https://doi.org/10.1016/B978-1-59749-961-3.09984-2",
url = "http://www.sciencedirect.com/science/article/pii/B9781597499613099842"
}
@article{PENG201633,
title = "Reusing simulation experiment specifications to support developing models by successive extension",
journal = "Simulation Modelling Practice and Theory",
volume = "68",
pages = "33 - 53",
year = "2016",
issn = "1569-190X",
doi = "https://doi.org/10.1016/j.simpat.2016.07.006",
url = "http://www.sciencedirect.com/science/article/pii/S1569190X16302118",
author = "Danhua Peng and Tom Warnke and Fiete Haack and Adelinde M. Uhrmacher",
keywords = "Model extension, Stochastic modeling, Simulation experiments, Experiment specification, Experiment generation and execution",
abstract = "Model development is a successive process of validating, revising, and extending models, and requires iterative execution of simulation experiments. While developing a model by extension, executing similar simulation experiments to those performed with the original model reveals important behavioral insights into the extended model. An automatic generation and execution of these simulation experiments can provide valuable support in the process of developing models. A prerequisite is an explicit specification of simulation experiments. Therefore, we annotate models with simulation experiments that are specified in a declarative domain specific language SESSL (Simulation Experiment Specification via a Scala Layer). Based on experiment specifications of the original model, we introduce a mechanism to automatically generate and execute simulation experiments for the extended model with necessary adaptations. Furthermore, as we experiment with stochastic models, we exploit statistical model checking and specify the expected model behavioral properties, against which the simulation results are checked. Thereby, when a model is extended, the original experiment specifications are reused, adapted, and applied to the extended model. Accordingly, the generated simulation trajectories are probed to check whether the expected properties hold with a certain probability or not. Thus, more fast and frequent feedback during model development can be provided to the modeler. Based on a model of membrane related dynamics, we show how the developed approach can be used in successively extending models."
}
@incollection{2015481,
title = "Index",
editor = "Carl S. Young",
booktitle = "The Science and Technology of Counterterrorism",
publisher = "Butterworth-Heinemann",
address = "Boston",
pages = "481 - 492",
year = "2015",
isbn = "978-0-12-420056-2",
doi = "https://doi.org/10.1016/B978-0-12-420056-2.09981-4",
url = "http://www.sciencedirect.com/science/article/pii/B9780124200562099814"
}
@article{LOPEZFERNANDEZ2016104,
title = "Combining unit and specification-based testing for meta-model validation and verification",
journal = "Information Systems",
volume = "62",
pages = "104 - 135",
year = "2016",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2016.06.008",
url = "http://www.sciencedirect.com/science/article/pii/S0306437916301934",
author = "Jesús J. López-Fernández and Esther Guerra and Juan de Lara",
keywords = "Model-driven engineering, Meta-modelling, Domain-specific modelling languages, Validation & verification, Meta-model quality",
abstract = "Meta-models play a cornerstone role in Model-Driven Engineering as they are used to define the abstract syntax of modelling languages, and so models and all sorts of model transformations depend on them. However, there are scarce tools and methods supporting their Validation and Verification (V&V), which are essential activities for the proper engineering of meta-models. In order to fill this gap, we propose two complementary meta-model V&V languages. The first one has similar philosophy to the xUnit framework, as it enables the definition of meta-model unit test suites comprising model fragments and assertions on their (in-)correctness. The second one is directed to express and verify expected properties of a meta-model, including domain and design properties, quality criteria and platform-specific requirements. As a proof of concept, we have developed tooling for both languages in the Eclipse platform, and illustrate its use within an example-driven approach for meta-model construction. The expressiveness of our languages is demonstrated by their application to build a library of meta-model quality issues, which has been evaluated over the ATL zoo of meta-models and some OMG specifications. The results show that integrated support for meta-model V&V (as the one we propose here) is urgently needed in meta-modelling environments."
}
@article{JIANG2006584,
title = "Modeling real-time communication systems: Practices and experiences in Motorola",
journal = "Journal of Visual Languages & Computing",
volume = "17",
number = "6",
pages = "584 - 605",
year = "2006",
note = "Visual Modeling for Software Intensive Systems",
issn = "1045-926X",
doi = "https://doi.org/10.1016/j.jvlc.2006.10.003",
url = "http://www.sciencedirect.com/science/article/pii/S1045926X06000607",
author = "Michael Jiang and Michael Groble and Andrij Neczwid and Allan Willey",
keywords = "UML modeling, SDL modeling, MDE code generation, Model validation, Real-time communication systems, TTCN, Structured methods",
abstract = "Visual modeling languages and techniques have been increasingly adopted for software specification, design, development, and testing. With the major improvements of UML 2.0 and tools support, visual modeling technologies have significant potential for simplifying design, facilitating collaborations, and reducing development cost. In this paper, we describe our practices and experiences of applying visual modeling techniques to the design and development of real-time wireless communication systems within Motorola. A model-driven engineering approach of integrating visual modeling with development and validation is described. Results, issues, and our viewpoints are also discussed."
}
@article{VIANA20133123,
title = "Domain-Specific Modeling Languages to improve framework instantiation",
journal = "Journal of Systems and Software",
volume = "86",
number = "12",
pages = "3123 - 3139",
year = "2013",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2013.07.030",
url = "http://www.sciencedirect.com/science/article/pii/S0164121213001775",
author = "Matheus C. Viana and Rosângela A.D. Penteado and Antônio F. do Prado",
keywords = "Framework, Domain-Specific Modeling Language, Reuse",
abstract = "Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The DSML of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the DSML into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach."
}
@InCollection{Sturm2017,
  author        = {Rick Sturm and Carol Pollard and Julie Craig},
  title         = {Chapter 16 - The Case for Standards},
  booktitle     = {Application Performance Management (APM) in the Digital Enterprise},
  publisher     = {Morgan Kaufmann},
  year          = {2017},
  editor        = {Rick Sturm and Carol Pollard and Julie Craig},
  pages         = {211 - 235},
  address       = {Boston},
  isbn          = {978-0-12-804018-8},
  __markedentry = {[Juliana:]},
  abstract      = {In response to customer demands for increased efficiency and effectiveness in application manageability, portability, and interoperability, several organizations have stepped up to develop standards to guide deployment and management of software and hardware components. These organizations recognize the importance of creating standards to enable the development of standardized technologies to instrument applications. They include the Internet Engineering Task Force, Desktop Management Task Force, Institute of Electronic Engineers, Tivoli, the ASL-BiSL Foundation, and the International Organization of Standards. In this chapter, these various standards organizations are introduced and the standards developed by these influential organizations are described vis-à-vis the different aspects of the application management lifecycle. Finally, the pros and cons of using standards to facilitate the management of applications are presented. The primary utility of this chapter is that it shows how application management standards have evolved over the past 25-plus years and provides an overview of the various standards in one concise resource.},
  doi           = {https://doi.org/10.1016/B978-0-12-804018-8.00016-4},
  keywords      = {Application description files (ADFs), Application response measurement (ARM), Cloud application management for platforms (CAMP), Cloud auditing data federation (CADF), Common information model (CIM), Component description files (CDFs), Desktop and mobile architecture for system hardware (DASH), Global description file (GDF), IEEE 1220, ISO/IEC 16350, ISO/IEC 17023:2011, ISO/IEC 17963:2013, Object identifier (OID), Organization for advancing open standards for the information society (OASIS), POSIX 1387.2, System application MIB (sysApplMIB), System management architecture for server management (SMASH), Tivoli application management specification (AMS), Web services management (WS-MAN)},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128040188000164},
}

@Article{Catala2013,
  author        = {Alejandro Catala and Patricia Pons and Javier Jaen and Jose A. Mocholi and Elena Navarro},
  title         = {A meta-model for dataflow-based rules in smart environments: Evaluating user comprehension and performance},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {10},
  pages         = {1930 - 1950},
  issn          = {0167-6423},
  note          = {Special section on Language Descriptions Tools and Applications (LDTA’08 \& ’09) \& Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
  __markedentry = {[Juliana:]},
  abstract      = {A considerable part of the behavior in smart environments relies on event-driven and rule specification. Rules are the mechanism most often used to enable user customization of the environment. However, the expressiveness of the rules available to users in editing and other tools is usually either limited or the available rule editing interfaces are not designed for end-users with low skills in programming. This means we have to look for interaction techniques and new ways to define user customization rules. This paper describes a generic and flexible meta-model to support expressive rules enhanced with data flow expressions that will graphically support the definition of rules without writing code. An empirical study was conducted on the ease of understanding of the visual data flow expressions, which are the key elements in our rule proposal. The visual dataflow language was compared to its corresponding textual version in terms of comprehension and ease of learning by teenagers in exercises involving calculations, modifications, writing and detecting equivalences in expressions in both languages. Although the subjects had some previous experience in editing mathematical expressions on spreadsheets, the study found their performance with visual dataflows to be significantly better in calculation and modification exercises. This makes our dataflow approach a promising mechanism for expressing user-customized reactive behavior in Ambient Intelligence (AmI) environments. The performance of the rule matching processor was validated by means of two stress tests to ensure that the meta-model approach adopted would be able to scale up with the number of types and instances in the space.},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.010},
  keywords      = {Ambient intelligence, Customization, Dataflow, Visual language, Rule, Event based, Non-expert programmer, Smart home},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001232},
}

@InCollection{Zelkowitz1999,
  title         = {Subject Index},
  publisher     = {Elsevier},
  year          = {1999},
  editor        = {Marvin V. Zelkowitz},
  volume        = {49},
  series        = {Advances in Computers},
  pages         = {365 - 373},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S0065-2458(08)60290-9},
  issn          = {0065-2458},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245808602909},
}

@Article{Jozwiak1992,
  author        = {Lech Jóźwiak and Hein Mijland},
  title         = {On the use of OR-BDDs for test generation},
  journal       = {Microprocessing and Microprogramming},
  year          = {1992},
  volume        = {35},
  number        = {1},
  pages         = {159 - 166},
  issn          = {0165-6074},
  note          = {Software and Hardware: Specification and Design},
  __markedentry = {[Juliana:]},
  abstract      = {The binary decision diagrams (BDDs) have recently been recognized as efficient means for modelling the Boolean functions for the purpose of testability analysis and test pattern generation; however, they have too low modelling power in order to model the logical implementation structure accurately and, therefore, there is no direct correspondence between the BDD's fault models and the more realistic structural fault models. In this paper, we propose an extension to BDDs, referred to as OR-BDDs, that enables the accurate modelling of the circuit structure in a compact manner. We show how to construct the minimal OR-BDDs. We introduce the fault model for OR-BDDs and show it's one-to-one correspondence with the structural stuck-at-value model. Finally, we present an algorithm for test pattern generation that uses OR-BDDs and their fault model. This algorithm discovers efficiently circuit redundancy and enables 100% fault coverage for all detectable faults by a compact set of test vectors.},
  doi           = {https://doi.org/10.1016/0165-6074(92)90310-4},
  url           = {http://www.sciencedirect.com/science/article/pii/0165607492903104},
}

@Article{Rodriguez2017,
  author        = {Pilar Rodríguez and Alireza Haghighatkhah and Lucy Ellen Lwakatare and Susanna Teppola and Tanja Suomalainen and Juho Eskeli and Teemu Karvonen and Pasi Kuvaja and June M. Verner and Markku Oivo},
  title         = {Continuous deployment of software intensive products and services: A systematic mapping study},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {123},
  pages         = {263 - 291},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {The software intensive industry is moving towards the adoption of a value-driven and adaptive real-time business paradigm. The traditional view of software as an item that evolves through releases every few months is being replaced by the continuous evolution of software functionality. This study aims to classify and analyse the literature related to continuous deployment in the software domain in order to scope the phenomenon, provide an overview of the state-of-the-art, investigate the scientific evidence in the reported results and identify areas suitable for further research. We conducted a systematic mapping study and classified the continuous deployment literature. The benefits and challenges related to continuous deployment were also analysed. RESULTS: The systematic mapping study includes 50 primary studies published between 2001 and 2014. An in-depth analysis of the primary studies revealed ten recurrent themes that characterize continuous deployment and provide researchers with directions for future work. In addition, a set of benefits and challenges of which practitioners may take advantage were identified. CONCLUSION: Overall, although the topic area is very promising, it is still in its infancy, thus offering a plethora of new opportunities for both researchers and software intensive companies.},
  doi           = {https://doi.org/10.1016/j.jss.2015.12.015},
  keywords      = {Continuous deployment, Software development, Systematic mapping study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002812},
}

@InCollection{Betz2007,
  author        = {Charles T. Betz},
  title         = {Chapter 4 - A Supporting Systems Architecture},
  booktitle     = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance},
  publisher     = {Morgan Kaufmann},
  year          = {2007},
  editor        = {Charles T. Betz},
  pages         = {227 - 305},
  address       = {Burlington},
  isbn          = {978-0-12-370593-8},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
As discussed at the outset of this chapter, companies often turn to the research organizations, the vendors, or both. However, the vendors are clearly self-interested, and the research organizations receive much of their funding from the vendors and are not articulating a comprehensive vision for how IT enablement tooling must interoperate. Hence, this chapter. Enablement tools and architectures are important and must be given their due. This chapter has several goals. First, it discusses the generic categories of internal IT enablement systems and potential interactions and overlaps. One issue in particular in this area is the proliferation of single-point systems. Providing a framework to support such efforts is a primary goal of this chapter. Finally, the use of a common logical process and data model can assist in the integration of the vendor packages available, and in particular the master data subjects be clearly established with defined systems of record so that the diverse systems can be aligned.},
  doi           = {https://doi.org/10.1016/B978-012370593-8/50031-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123705938500319},
}

@Article{1977,
  title         = {Useful list of abbreviations commonly used in reliability},
  journal       = {Microelectronics Reliability},
  year          = {1977},
  volume        = {16},
  number        = {3},
  pages         = {197 - 206},
  issn          = {0026-2714},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/0026-2714(77)90757-0},
  url           = {http://www.sciencedirect.com/science/article/pii/0026271477907570},
}

@InCollection{Noergaard2010,
  title         = {Appendix B - Embedded Systems Glossary},
  booktitle     = {Demystifying Embedded Systems Middleware},
  publisher     = {Newnes},
  year          = {2010},
  editor        = {Tammy Noergaard},
  pages         = {367 - 387},
  address       = {Burlington},
  isbn          = {978-0-7506-8455-2},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-7506-8455-2.00011-X},
  url           = {http://www.sciencedirect.com/science/article/pii/B978075068455200011X},
}

@Article{Gratacos2010,
  author        = {M. Gratacos and N. Raguer and J. Gamez and J.L. Seoane and M. Benito},
  title         = {P12-16 Neurophysiological characterization of Miller Fisher syndrome patients: Report of 10 patients},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S175 - S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60720-1},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607201},
}

@InCollection{Sittig2017,
  author        = {Dean F. Sittig},
  title         = {1 - Category Definitions},
  booktitle     = {Clinical Informatics Literacy},
  publisher     = {Academic Press},
  year          = {2017},
  editor        = {Dean F. Sittig},
  pages         = {1 - 170},
  isbn          = {978-0-12-803206-0},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-803206-0.00001-8},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128032060000018},
}

@InCollection{Cho1991,
  author        = {Chang H. Cho and James R. Armstrong},
  title         = {VHDL Semantics for Behavioral Test Generation},
  booktitle     = {Computer Hardware Description Languages and their Applications},
  publisher     = {North-Holland},
  year          = {1991},
  editor        = {DOMINIQUE BORRIONE and RONALD WAXMAN},
  pages         = {427 - 444},
  address       = {Amsterdam},
  isbn          = {978-0-444-89208-9},
  __markedentry = {[Juliana:]},
  abstract      = {In this paper, we discuss how the VHDL semantics which represent the concepts of event-driven simulation and bus resolution function affect the test generation algorithm, and present methods of generating realistic tests without being affected by the VHDL semantics. A formal representation of the VHDL process statement is described and the concept of event-driven simulation and its impact on test generation are discussed using the formal representation. The new test generation method generates realistic tests by ignoring the sensitivity list of a process statement and identifying the type of the behavior described by the statements inside the process statement (two types of behavior - synchronous and asynchronous, are defined.). A systematic way of converting a VHDL model to one suitable for checking the validity of the generated tests is presented. A method of further compacting the generated tests is also presented. Finally, an approach to generating tests in the presence of different types of bus resolution functions is discussed.},
  doi           = {https://doi.org/10.1016/B978-0-444-89208-9.50030-2},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780444892089500302},
}

@Article{Mosterman2009,
  author        = {Pieter J. Mosterman and Justyna Zander and Gregoire Hamon and Ben Denckla},
  title         = {Towards Computational Hybrid System Semantics for Time-Based Block Diagrams},
  journal       = {IFAC Proceedings Volumes},
  year          = {2009},
  volume        = {42},
  number        = {17},
  pages         = {376 - 385},
  issn          = {1474-6670},
  note          = {3rd IFAC Conference on Analysis and Design of Hybrid Systems},
  __markedentry = {[Juliana:]},
  abstract      = {At the core of Model-Based Design, computational models have caused an autocatalytic trend to use computation in design by unlocking the potential of model transformations. Precisely specifying a computational transformation requires well-defined semantics of the source and target representations. In this regard, continuous-time behavior is an essential aspect of time-based block diagrams that is typically approximated by numerical integration. The corresponding theory, however, is mostly concerned with local error and the mathematical semantics of long time behavior fails to be sufficiently precise from a computational perspective. In this work, first a computational semantics is developed based on a multi-stage variablestep solver. Next, the computational semantics of the discrete and continuous parts of hybrid systems and their interaction are formalized in a unifying framework. The framework exploits a successful functional approach to defining discrete-time and discrete-event behavior established in other work. Unification is then achieved by developing a computational representation of the continuous-time behavior as pure functions on streams.},
  doi           = {https://doi.org/10.3182/20090916-3-ES-3003.00065},
  keywords      = {Computational methods, Computer simulation, Computer-aided control system design, Embedded systems, Numerical simulation, Synchronous data flow, Systems design, Variable-structure systems, Verification, Zero crossings},
  url           = {http://www.sciencedirect.com/science/article/pii/S1474667015307916},
}

@Article{Lopez-Sanchez2011,
  author        = {Almudena López-Sánchez and Alejandra Sáenz and Cristina Casals},
  title         = {Surfactant protein A (SP-A)-tacrolimus complexes have a greater anti-inflammatory effect than either SP-A or tacrolimus alone on human macrophage-like U937 cells},
  journal       = {European Journal of Pharmaceutics and Biopharmaceutics},
  year          = {2011},
  volume        = {77},
  number        = {3},
  pages         = {384 - 391},
  issn          = {0939-6411},
  note          = {Biological Barriers},
  __markedentry = {[Juliana:]},
  abstract      = {Intratracheal administration of immunosuppressive agents to the lung is a novel treatment after lung transplantation. Nanoparticles of tacrolimus (FK506) might interact with human SP-A, which is the most abundant lipoprotein in the alveolar fluid. This study was undertaken to determine whether the formation of FK506/SP-A complexes interferes with FK506 immunosuppressive actions on stimulated human macrophage-like U937 cells. We found that SP-A was avidly bound to FK506 (Kd=35±4nM), as determined by solid phase–binding assays and dynamic light scattering. Free FK506, at concentrations ⩽1μM, had no effect on the inflammatory response of LPS-stimulated U937 macrophages. However, coincubation of FK506 and SP-A, at concentrations where each component alone did not affect LPS-stimulated macrophage response, significantly inhibited LPS-induced NF-κB activation and TNF-alpha secretion. Free FK506, but not FK506/SP-A, functioned as substrate for the efflux transporter P-glycoprotein. FK506 bound to SP-A was delivered to macrophages by endocytosis, since several endocytosis inhibitors blocked FK506/SP-A anti-inflammatory effects. This process depended partly on SP-A binding to its receptor, SP-R210. These results indicate that FK506/SP-A complexes have a greater anti-inflammatory effect than either FK506 or SP-A alone and suggest that SP-A strengthened FK506 anti-inflammatory activity by facilitating FK506 entrance into the cell, overcoming P-glycoprotein.},
  doi           = {https://doi.org/10.1016/j.ejpb.2010.12.013},
  keywords      = {Lung, Tacrolimus, Surfactant protein A, Inflammation, Macrophages, P-glycoprotein},
  url           = {http://www.sciencedirect.com/science/article/pii/S0939641110003310},
}

@Article{Verhoef2005,
  author        = {C. Verhoef},
  title         = {Quantitative aspects of outsourcing deals},
  journal       = {Science of Computer Programming},
  year          = {2005},
  volume        = {56},
  number        = {3},
  pages         = {275 - 313},
  issn          = {0167-6423},
  __markedentry = {[Juliana:]},
  abstract      = {There are many goals for outsourcing information technology: for instance, cost reduction, speed to market, quality improvement, or new business opportunities. Based on our real-world experience in advising organizations with goal-driven outsourcing deals, we identified the most prominent quantitative input needed to close such deals. These comprise what we named the five executive issues enabling rational decision making. They concern cost, duration, risk, return, and financing aspects of outsourcing. They add an important quantitative financial/economic dimension to the decision making process. Based on inferred outcomes for the five executive issues, we address the easily overlooked aspects of selecting partners, contracting, monitoring progress, and acceptance and delivery conditions for contracts.},
  doi           = {https://doi.org/10.1016/j.scico.2004.08.003},
  keywords      = {Outsourcing, Goalsourcing, Smartsourcing, Fastsourcing, Costsourcing, Offshore outsourcing, Eastsourcing, Tasksourcing, Backsourcing, Insourcing, Scalesourcing, Profitsourcing, Activity-based cost estimation, Total cost of ownership (TCO), Requirements creep risk, Time compression risk, Litigation risk, Failure risk, Overtime risk, Deglubitor risk, Payback period risk},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642304001406},
}

@InCollection{Ali2017,
  author        = {Shaukat Ali and Hong Lu and Shuai Wang and Tao Yue and Man Zhang},
  title         = {Chapter Two - Uncertainty-Wise Testing of Cyber-Physical Systems},
  publisher     = {Elsevier},
  year          = {2017},
  editor        = {Atif M. Memon},
  volume        = {107},
  series        = {Advances in Computers},
  pages         = {23 - 94},
  __markedentry = {[Juliana:]},
  abstract      = {As compared with classical software/system testing, uncertainty-wise testing explicitly addresses known uncertainty about the behavior of a System Under Test (SUT), its operating environment, and interactions between the SUT and its operational environment, across all testing phases, including test design, test generation, test optimization, and test execution, with the aim to mainly achieve the following two goals. First, uncertainty-wise testing aims to ensure that the SUT deals with known uncertainty adequately. Second, uncertainty-wise testing should be also capable of learning new (previously unknown) uncertainties such that the SUT's implementation can be improved to guard against newly learned uncertainties during its operation. The necessity to integrate uncertainty in testing is becoming imperative because of the emergence of new types of intelligent and communicating software-based systems such as Cyber-Physical Systems (CPSs). Intrinsically, such systems are exposed to uncertainty because of their interactions with highly indeterminate physical environments. In this chapter, we provide our understanding and experience of uncertainty-wise testing from the aspects of uncertainty-wise model-based testing, uncertainty-wise modeling and evolution of test ready models, and uncertainty-wise multiobjective test optimization, in the context of testing CPSs under uncertainty. Furthermore, we present our vision about this new testing paradigm and its plausible future research directions.},
  doi           = {https://doi.org/10.1016/bs.adcom.2017.06.001},
  issn          = {0065-2458},
  keywords      = {Uncertainty-wise testing, Cyber-Physical System, Belief Test Ready Model, Model evolution, Model-based testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245817300189},
}

@Article{Sangiovanni-Vincentelli2012,
  author        = {Alberto Sangiovanni-Vincentelli and Werner Damm and Roberto Passerone},
  title         = {Taming Dr. Frankenstein: Contract-Based Design for Cyber-Physical Systems*},
  journal       = {European Journal of Control},
  year          = {2012},
  volume        = {18},
  number        = {3},
  pages         = {217 - 238},
  issn          = {0947-3580},
  __markedentry = {[Juliana:]},
  abstract      = {Cyber-physical systems combine a cyber side (computing and networking) with a physical side (mechanical, electrical, and chemical processes). In many cases, the cyber component controls the physical side using sensors and actuators that observe the physical system and actuate the controls. Such systems present the biggest challenges as well as the biggest opportunities in several large industries, including electronics, energy, automotive, defense and aerospace, telecommunications, instrumentation, industrial automation. Engineers today do successfully design cyber-physical systems in a variety of industries. Unfortunately, the development of systems is costly, and development schedules are difficult to stick to. The complexity of cyber-physical systems, and particularly the increased performance that is offered from interconnecting what in the past have been separate systems, increases the design and verification challenges. As the complexity of these systems increases, our inability to rigorously model the interactions between the physical and the cyber sides creates serious vulnerabilities. Systems become unsafe, with disastrous inexplicable failures that could not have been predicted. Distributed control of multi-scale complex systems is largely an unsolved problem. A common view that is emerging in research programs in Europe and the US is “enabling contract-based design (CBD),” which formulates a broad and aggressive scope to address urgent needs in the systems industry. We present a design methodology and a few examples in controller design whereby contract-based design can be merged with platform-based design to formulate the design process as a meet-in-the-middle approach, where design requirements are implemented in a subsequent refinement process using as much as possible elements from a library of available components. Contracts are formalizations of the conditions for correctness of element integration (horizontal contracts), for lower level of abstraction to be consistent with the higher ones, and for abstractions of available components to be faithful representations of the actual parts (vertical contracts).},
  doi           = {https://doi.org/10.3166/ejc.18.217-238},
  keywords      = {Contract, cyber-physical, design methodologies, platform-based, correctness},
  url           = {http://www.sciencedirect.com/science/article/pii/S0947358012709433},
}

@InCollection{Hurtarte2007,
  author        = {Jeorge S. Hurtarte and Evert A. Wolsheimer and Lisa M. Tafoya},
  title         = {Chapter 7 - Intellectual Property},
  booktitle     = {Understanding Fabless IC Technology},
  publisher     = {Newnes},
  year          = {2007},
  editor        = {Jeorge S. Hurtarte and Evert A. Wolsheimer and Lisa M. Tafoya},
  pages         = {65 - 121},
  address       = {Burlington},
  isbn          = {978-0-7506-7944-2},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
This chapter explains the semiconductor intellectual property (SIP) overview, business environment, sourcing products, evaluating business models, product enablers, licensing products, and the provider and buyer perspectives. SIP has existed since the advent of the semiconductor industry. SIP business practices include elements similar to those found in the traditional semiconductor or application-specific integrated circuit (ASIC), electronic design automation (EDA), and design services markets. Outsourcing is unlikely if the SIP product is seen by the potential buyer as a core competency or a key differentiator in its product, or if its use requires third-party access to the buyer's patents or trade secrets. SIP License Agreements may require a significant effort depending upon the business objectives of the parties, the intended use of the SIP Product, the nature of the SIP product, and the risks associated with using the SIP product in the end application. The scope of license, warranty, indemnity, and limitation of liability provisions usually consume the majority of the effort in negotiating an SIP License Agreement. However, IC developers are finding more low-cost solutions at their fingertips, and the semiconductor supply chain has become increasingly stratified.},
  doi           = {https://doi.org/10.1016/B978-075067944-2/50008-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780750679442500084},
}

@Article{Tibermacine2010,
  author        = {Chouki Tibermacine and Régis Fleurquin and Salah Sadou},
  title         = {A family of languages for architecture constraint specification},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {5},
  pages         = {815 - 831},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {During software development, architecture decisions should be documented so that quality attributes guaranteed by these decisions and required in the software specification could be persisted. An important part of these architectural decisions is often formalized using constraint languages which differ from one stage to another in the development process. In this paper, we present a family of architectural constraint languages, called ACL. Each member of this family, called a profile, can be used to formalize architectural decisions at a given stage of the development process. An ACL profile is composed of a core constraint language, which is shared with the other profiles, and a MOF architecture metamodel. In addition to this family of languages, this paper introduces a transformation-based interpretation method of profiles and its associated tool.},
  doi           = {https://doi.org/10.1016/j.jss.2009.11.736},
  keywords      = {Architecture constraint, Constraint language, ADL, Software component, MOF, OCL, Constraint transformation},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120900315X},
}

@Article{Singh2017,
  author        = {Satwinder Singh and Sharanpreet Kaur},
  title         = {A systematic literature review: Refactoring for disclosing code smells in object oriented software},
  journal       = {Ain Shams Engineering Journal},
  year          = {2017},
  issn          = {2090-4479},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Reusing a design pattern is not always in the favor of developers. Thus, the code starts smelling. The presence of “Code Smells” leads to more difficulties for the developers. This racket of code smells is sometimes called Anti-Patterns.
Objective
The paper aimed at a systematic literature review of refactoring with respect to code smells. However the review of refactoring is done in general and the identification of code smells and anti-patterns is performed in depth.
Method
A systematic literature survey has been performed on 238 research items that includes articles from leading Conferences, Workshops and premier journals, theses of researchers and book chapters.
Results
Several data sets and tools for performing refactoring have been revealed under the specified research questions.
Conclusion
The work done in the paper is an addition to prior systematic literature surveys. With the study of paper the attentiveness of readers about code smells and anti-patterns will be enhanced.},
  doi           = {https://doi.org/10.1016/j.asej.2017.03.002},
  keywords      = {Code smells, Anti-patterns, Refactoring},
  url           = {http://www.sciencedirect.com/science/article/pii/S2090447917300412},
}

@Article{Le2010,
  author        = {T.T.Q. Le},
  title         = {P12-17 Clinical features and laboratory findings of 7 cases with Miller-Fisher syndrome in ChoRay hospital},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60721-3},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607213},
}

@InCollection{Breakfield2002,
  author        = {Charles V. Breakfield and Roxanne E. Burkey},
  title         = {4 - Development Cycle},
  booktitle     = {Managing Systems Migrations and Upgrades},
  publisher     = {Digital Press},
  year          = {2002},
  editor        = {Charles V. Breakfield and Roxanne E. Burkey},
  pages         = {93 - 144},
  address       = {Woburn},
  isbn          = {978-1-55558-256-2},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-155558256-2/50005-3},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781555582562500053},
}

@Article{Karam2008,
  author        = {Marcel Karam and Sergiu Dascalu and Haidar Safa and Rami Santina and Zeina Koteich},
  title         = {A product-line architecture for web service-based visual composition of web applications},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {855 - 867},
  issn          = {0164-1212},
  note          = {Agile Product Line Engineering},
  __markedentry = {[Juliana:]},
  abstract      = {A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.031},
  keywords      = {Product line engineering, Product line architecture, Agile methods, Web services, Visual languages},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120700252X},
}

@Article{Huuck2015,
  author        = {Ralf Huuck},
  title         = {Technology transfer: Formal analysis, engineering, and business value},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {103},
  pages         = {3 - 12},
  issn          = {0167-6423},
  note          = {Selected papers from the First International Workshop on Formal Techniques for Safety-Critical Systems (FTSCS 2012)},
  __markedentry = {[Juliana:]},
  abstract      = {In this work we report on our experiences on developing and commercializing Goanna, a source code analyzer for detecting software bugs and security vulnerabilities in C/C++ code. Goanna is based on formal software analysis techniques such as model checking, static analysis and SMT solving. The commercial version of Goanna is currently deployed in a wide range of organizations around the world. Moreover, the underlying technology is licensed to an independent software vendor with tens of thousands of customers, making it possibly one of the largest deployments of automated formal methods technology. This paper explains some of the challenges as well as the positive results that we encountered in the technology transfer process. In particular, we provide some background on the design decisions and techniques to deal with large industrial code bases, we highlight engineering challenges and efforts that are typically outside of a more academic setting, and we address core aspects of the bigger picture for transferring formal techniques into commercial products, namely, the adoption of such technology and the value for purchasing organizations. While we provide a particular focus on Goanna and our experience with that underlying technology, we believe that many of those aspects hold true for the wider field of formal analysis and verification technology and its adoption in industry.},
  doi           = {https://doi.org/10.1016/j.scico.2014.11.003},
  keywords      = {Static analysis, Model checking, SMT solving, Industrial application, Experience report},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314005358},
}

@Article{Sunye2014,
  author        = {Gerson Sunyé and Eduardo Cunha de Almeida and Yves Le Traon and Benoit Baudry and Jean-Marc Jézéquel},
  title         = {Model-based testing of global properties on large-scale distributed systems},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {7},
  pages         = {749 - 762},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Large-scale distributed systems are becoming commonplace with the large popularity of peer-to-peer and cloud computing. The increasing importance of these systems contrasts with the lack of integrated solutions to build trustworthy software. A key concern of any large-scale distributed system is the validation of global properties, which cannot be evaluated on a single node. Thus, it is necessary to gather data from distributed nodes and to aggregate these data into a global view. This turns out to be very challenging because of the system’s dynamism that imposes very frequent changes in local values that affect global properties. This implies that the global view has to be frequently updated to ensure an accurate validation of global properties.
Objective
In this paper, we present a model-based approach to define a dynamic oracle for checking global properties. Our objective is to abstract relevant aspects of such systems into models. These models are updated at runtime, by monitoring the corresponding distributed system.
Method
We conduce real-scale experimental validation to evaluate the ability of our approach to check global properties. In this validation, we apply our approach to test two open-source implementations of distributed hash tables. The experiments are deployed on two clusters of 32 nodes.
Results
The experiments reveal an important defect on one implementation and show clear performance differences between the two implementations. The defect would not be detected without a global view of the system.
Conclusion
Testing global properties on distributed software consists of gathering data from different nodes and building a global view of the system, where properties are validated. This process requires a distributed test architecture and tools for representing and validating global properties. Model-based techniques are an expressive mean for building oracles that validate global properties on distributed systems.},
  doi           = {https://doi.org/10.1016/j.infsof.2014.02.002},
  keywords      = {Software testing, Distributed software, Model-based testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914000366},
}

@Article{Easley1998,
  author        = {David Easley and Maureen O'Hara and Joseph Paperman},
  title         = {Financial analysts and information-based trade},
  journal       = {Journal of Financial Markets},
  year          = {1998},
  volume        = {1},
  number        = {2},
  pages         = {175 - 201},
  issn          = {1386-4181},
  __markedentry = {[Juliana:]},
  abstract      = {In this research, we investigate the informational role of financial analysts. Using a trade-based empirical technique, we estimate the probability of information-based trading for a sample of NYSE stocks that differ in analyst coverage. We determine how this probability differs across stocks followed by many analysts, and we investigate whether analysts increase or create the flow of information. We also determine the `normal' level of noise trading in each sample stock, thereby giving us the ability to assess the depth of the market for stocks with differing analysts followings. Our most important empirical result is that the number of financial analysts is not a good proxy for information-based trading.},
  doi           = {https://doi.org/10.1016/S1386-4181(98)00002-0},
  keywords      = {Microstructure, Financial analysts, Trade},
  url           = {http://www.sciencedirect.com/science/article/pii/S1386418198000020},
}

@Article{Mesbah2008,
  author        = {Ali Mesbah and Arie van Deursen},
  title         = {A component- and push-based architectural style for ajax applications},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {12},
  pages         = {2194 - 2209},
  issn          = {0164-1212},
  note          = {Best papers from the 2007 Australian Software Engineering Conference (ASWEC 2007), Melbourne, Australia, April 10-13, 2007},
  __markedentry = {[Juliana:]},
  abstract      = {A new breed of web application, dubbed ajax, is emerging in response to a limited degree of interactivity in large-grain stateless Web interactions. At the heart of this new approach lies a single page interaction model that facilitates rich interactivity. Also push-based solutions from the distributed systems are being adopted on the web for ajax applications. The field is, however, characterized by the lack of a coherent and precisely described set of architectural concepts. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. We have studied and experimented with several ajax frameworks trying to understand their architectural properties. In this paper, we summarize four of these frameworks and examine their properties and introduce the spiar architectural style which captures the essence of ajax applications. We describe the guiding software engineering principles and the constraints chosen to induce the desired properties. The style emphasizes user interface component development, intermediary delta-communication between client/server components, and push-based event notification of state changes through the components, to improve a number of properties such as user interactivity, user-perceived latency, data coherence, and ease of development. In addition, we use the concepts and principles to discuss various open issues in ajax frameworks and application development.},
  doi           = {https://doi.org/10.1016/j.jss.2008.04.005},
  keywords      = {, Web architectural style, Web engineering, Single page interface, Rich internet application},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121208000769},
}

@InCollection{Conrad2016,
  title         = {Index},
  booktitle     = {CISSP Study Guide (Third Edition)},
  publisher     = {Syngress},
  year          = {2016},
  editor        = {Eric Conrad and Seth Misenar and Joshua Feldman},
  pages         = {559 - 599},
  address       = {Boston},
  edition       = {Third Edition},
  isbn          = {978-0-12-802437-9},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-802437-9.00018-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128024379000187},
}

@Article{Straver2010,
  author        = {D.C.G. Straver and L.H. Van den Berg and H. Franssen},
  title         = {P12-18 Exercise-induced weakness in demyelinating neuropathies},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60722-5},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607225},
}

@Article{Boonstra2008,
  author        = {Rudy Boonstra and Jeffrey E. Lane and Stan Boutin and Adrian Bradley and Lanna Desantis and Amy E.M. Newman and Kiran K. Soma},
  title         = {Plasma DHEA levels in wild, territorial red squirrels: Seasonal variation and effect of ACTH},
  journal       = {General and Comparative Endocrinology},
  year          = {2008},
  volume        = {158},
  number        = {1},
  pages         = {61 - 67},
  issn          = {0016-6480},
  __markedentry = {[Juliana:]},
  abstract      = {In many species, territorial behavior is limited to the breeding season and is tightly coupled to circulating gonadal steroid levels. In contrast, both male and female red squirrels (Tamiasciurus hudsonicus) are highly aggressive in both the breeding and non-breeding seasons in defense of food stores on their individual territories throughout the boreal and northern forests of North America. Dehydroepiandrosterone (DHEA), an androgen precursor, is secreted from the adrenal cortex in some mammals, and DHEA has been linked to aggression in non-breeding songbirds. Here, we examined plasma DHEA levels in a natural population of red squirrels in the Yukon, Canada. Plasma DHEA levels in both males and females reached high concentrations (up to 16.952ng/ml in males and 14.602ng/ml in females), markedly exceeding plasma DHEA concentrations in laboratory rats and mice and similar to plasma DHEA concentrations in some primates. Circulating DHEA levels showed both seasonal and yearly variation. Seasonal variation in male plasma DHEA levels was negatively correlated with testes mass. Yearly variation in male DHEA levels was positively correlated with population density. In both males and females, circulating DHEA rapidly increased after ACTH treatment, implying an adrenal origin. This is the first examination of plasma DHEA concentrations in a wild rodent and the first field experiment on the regulation of plasma DHEA in any wild mammal. These data lay the foundation for future studies on the role of DHEA in non-breeding territoriality in this species and other mammals.},
  doi           = {https://doi.org/10.1016/j.ygcen.2008.05.004},
  keywords      = {ACTH, Adrenal cortex, Aggression, Boreal forest, Cortisol, DHEA, Food supply, Population density, Season, Sex difference, Stress, Territorial behavior, Testosterone, Winter},
  url           = {http://www.sciencedirect.com/science/article/pii/S0016648008001950},
}

@Article{Trainor2008,
  author        = {Brian C. Trainor and M. Sima Finy and Randy J. Nelson},
  title         = {Rapid effects of estradiol on male aggression depend on photoperiod in reproductively non-responsive mice},
  journal       = {Hormones and Behavior},
  year          = {2008},
  volume        = {53},
  number        = {1},
  pages         = {192 - 199},
  issn          = {0018-506X},
  __markedentry = {[Juliana:]},
  abstract      = {In three genuses and four species of rodents, housing in winter-like short days (8L:16D) increases male aggressive behavior. In all of these species, males undergo short-day induced regression of the reproductive system. Some studies, however, suggest that the effect of photoperiod on aggression may be independent of reproductive responses. We examined the effects of photoperiod on aggressive behavior in California mice (Peromyscus californicus), which do not display reproductive responsiveness to short days. As expected, short days had no effect on plasma testosterone. Estrogen receptor alpha and estrogen receptor beta immunostaining did not differ in the lateral septum, medial preoptic area, bed nucleus of the stria terminalis, or medial amygdala. However, males housed in short days were significantly more aggressive than males housed in long days. Similar to previous work in beach mice (Peromyscus polionotus), estradiol rapidly increased aggression when male California mice were housed in short days but not when housed in long days. These data suggest that the effects of photoperiod on aggression and estrogen signaling are independent of reproductive responses. The rapid action of estradiol on aggression in short-day mice also suggests that nongenomic mechanisms mediate the effects of estrogens in short days.},
  doi           = {https://doi.org/10.1016/j.yhbeh.2007.09.016},
  keywords      = {Aggressive behavior, , California mouse, c-fos, Nongenomic effects, Estrogen receptor alpha, Estrogen receptor beta},
  url           = {http://www.sciencedirect.com/science/article/pii/S0018506X07002334},
}

@InCollection{Reed1991,
  author        = {David P. Reed and Marvin A. Sirbu},
  title         = {AN ENGINEERING COST AND POLICY ANALYSIS OF INTRODUCING FIBER INTO THE RESIDENTIAL SUBSCRIBER LOOP},
  booktitle     = {Integrated Broadband Networks},
  publisher     = {North-Holland},
  year          = {1991},
  editor        = {MARTIN C.J. ELTON},
  pages         = {89 - 134},
  address       = {Amsterdam},
  isbn          = {978-0-444-89068-9},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
This chapter presents an analysis of the principal engineering and economic issues that have emerged as telephone companies consider rewiring the nation's residences with fiber optics. Assuming significant future reductions in component costs, running fiber optic to the home is likely to remain more expensive than copper, where current loop plant costs are roughly $920 per subscriber. To realize the introduction of a fiber Integrated Broadband Networks (IBN), it must be justified on the basis of additional revenue producing services, such as the delivery of entertainment video. Fiber optic network capable of providing both voice and video services to the home can be constructed for $1800 to $2500 per home passed. Future developments in microelectronics, lasers, photodetectors, and powering architectures will greatly influence the economics of network evolution.},
  doi           = {https://doi.org/10.1016/B978-0-444-89068-9.50013-5},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780444890689500135},
}

@Article{Deb2016,
  author        = {Novarun Deb and Nabendu Chaki and Aditya Ghose},
  title         = {Extracting finite state models from i* models},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {121},
  pages         = {265 - 280},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm.},
  doi           = {https://doi.org/10.1016/j.jss.2016.03.038},
  keywords      = {i model, Model transformation, Model checking},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300048},
}

@Article{Clariso2016,
  author        = {Robert Clarisó and Jordi Cabot and Esther Guerra and Juan de Lara},
  title         = {Backwards reasoning for model transformations: Method and applications},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {116},
  pages         = {113 - 132},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Model transformations are key elements of model driven engineering. Current challenges for transformation languages include improving usability (i.e., succinct means to express the transformation intent) and devising powerful analysis methods. In this paper, we show how backwards reasoning helps in both respects. The reasoning is based on a method that, given an OCL expression and a transformation rule, calculates a constraint that is satisfiable before the rule application if and only if the original OCL expression is satisfiable afterwards. With this method we can improve the usability of the rule execution process by automatically deriving suitable application conditions for a rule (or rule sequence) to guarantee that applying that rule does not break any integrity constraint (e.g. meta-model constraints). When combined with model finders, this method facilitates the validation, verification, testing and diagnosis of transformations, and we show several applications for both in-place and exogenous transformations.},
  doi           = {https://doi.org/10.1016/j.jss.2015.08.017},
  keywords      = {Model transformation, OCL, Weakest pre-condition},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215001788},
}

@Article{Landys2013,
  author        = {Mėta M. Landys and Wolfgang Goymann and Kiran K. Soma and Tore Slagsvold},
  title         = {Year-round territorial aggression is independent of plasma DHEA in the European nuthatch Sitta europaea},
  journal       = {Hormones and Behavior},
  year          = {2013},
  volume        = {63},
  number        = {1},
  pages         = {166 - 172},
  issn          = {0018-506X},
  __markedentry = {[Juliana:]},
  abstract      = {Plasma testosterone can play an important role in promoting aggressive behaviors relating to territory defense in breeding male birds. Some birds defend territories also during the non-breeding phase, when testosterone circulates at basal levels. In such species, plasma levels of the pro-hormone dehydroepiandrosterone (DHEA) may support non-breeding territoriality by acting as a local substrate for sex steroids. To test this possible role of plasma DHEA, we examined the seasonal DHEA profile of male (and female) European nuthatches Sitta europaea: a male and female nuthatch pair will defend an all-purpose territory throughout the year. We hypothesized that plasma DHEA would be detectable in wintering nuthatches with a territory. However, only ca. half of the territorial wintering males (and females) displayed detectable DHEA levels, suggesting that plasma DHEA is not a major sex steroid precursor during non-breeding. Further, among hatching-year birds, plasma DHEA was significantly lower in territorial birds than in “floaters”, i.e., subordinate birds without a territory. To experimentally examine the role of DHEA in non-breeding territoriality, we treated adult wintering males with DHEA and measured effects on aggressive responses to conspecific challenge. DHEA treatment elevated plasma levels of DHEA (and testosterone), but did not enhance territorial behaviors or their persistence. Taken together, our data suggest that DHEA (and, indeed, sex steroids per se) do not regulate non-breeding territoriality in the nuthatch. Given that territorial aggression in nuthatches is expressed year-round, a hormone for its activation may be redundant.},
  doi           = {https://doi.org/10.1016/j.yhbeh.2012.10.002},
  keywords      = {Nuthatch, Seasonal DHEA profile, Implant, Territorial aggression, Testosterone, Non-breeding, Wintering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0018506X12002449},
}

@Article{Scarabelli2004,
  author        = {Tiziano M Scarabelli and Evasio Pasini and Anastasis Stephanou and Carol Chen-Scarabelli and Louis Saravolatz and Richard A Knight and David S Latchman and Julius M Gardin},
  title         = {Nutritional supplementation with mixed essential amino acids enhances myocyte survival, preserving mitochondrial functional capacity during ischemia-reperfusion injury},
  journal       = {The American Journal of Cardiology},
  year          = {2004},
  volume        = {93},
  number        = {8, Supplement 1},
  pages         = {35 - 40},
  issn          = {0002-9149},
  __markedentry = {[Juliana:]},
  abstract      = {In patients undergoing coronary surgery, the uptake of amino acids, which has been shown to correlate with oxygen consumption, is a mechanism of cardiac adaptation to the iatrogenic ischemia-reperfusion injury associated with cardioplegic arrest. Based on these premises, we sought to determine whether oral supplementation with mixed amino acids may protect the rat heart exposed to ischemia-reperfusion and to address whether this hypothesized cardioprotection is achieved, at least in part, through preservation of the energy-producing properties of mitochondria. Sprague–Dawley rats were fed (by enteral route) a liquid diet, with or without mixed essential amino acids (daily dose of 1 g/kg) for 30 days. Hearts from anesthetized rats were perfused by the Langendorff method and randomized to 3 groups. The control group was perfused with buffer for 60 minutes; the ischemia-reperfusion control and the amino acid–treated groups were exposed to 35 minutes of ischemia, followed by 60 or 120 minutes of reperfusion. Amino acid supplements minimized infarct size (22 ± 1.8% vs 33 ± 2.5%; p <0.05) and occurrence of cardiomyocyte apoptosis, as assessed by co-localization of terminal deoxynucleotidyl transferase–mediated dUTP nick end labeling (TUNEL) and caspase-3–positive staining (p <0.01). Long-term treatment with amino acids also reduced the proportion of cardiomyocytes exhibiting immunostaining for cleaved caspase-9 (p <0.01) but was ineffective on processing of caspase-8. Similar results were obtained in the whole heart by caspase activity assays (p <0.01). The lessened activation of caspase-9 detected in amino acid-treated hearts paralleled a strong reduction in mitochondrial release of cytochrome c. Adenosine triphosphate (ATP) content and rate of ATP production in isolated mitochondria were reduced by >75% in control hearts after 2 hours of reperfusion (p <0.05 vs control hearts); these values returned toward those of the control group in hearts supplemented with amino acids (p <0.01). Finally, the oxygen consumption rate in myocardial skinned bundles was markedly reduced in ischemia-reperfusion control hearts and almost normalized in amino acid-treated hearts (approximately 20% and 93% of the value for normoxic hearts; p <0.01). These results suggest that oral amino acid supplementation attenuates the extent of ischemia-reperfusion injury in the rat heart, through preservation of the mitochondria-generated production of high-energy phosphates.},
  doi           = {https://doi.org/10.1016/j.amjcard.2003.11.008},
  url           = {http://www.sciencedirect.com/science/article/pii/S0002914903015145},
}

@Article{Katkalov2014,
  author        = {Kuzman Katkalov and Nina Moebius and Kurt Stenzel and Marian Borek and Wolfgang Reif},
  title         = {Modeling test cases for security protocols with SecureMDD},
  journal       = {Computer Networks},
  year          = {2014},
  volume        = {58},
  pages         = {99 - 111},
  issn          = {1389-1286},
  __markedentry = {[Juliana:]},
  abstract      = {Designing and executing test cases for security-critical protocols is a technically complicated and tedious process. SecureMDD is a model-driven approach that enables development of security-critical applications based on cryptographic protocols. In this paper we introduce a method which combines the model-driven approach used in SecureMDD with the design of functional and security tests. We construct and evaluate new modeling guidelines that allow the modeler to easily define such test cases during the modeling stage. We also implement model transformation routines to generate runnable tests for actual implementation of applications developed with SecureMDD.},
  doi           = {https://doi.org/10.1016/j.comnet.2013.08.024},
  keywords      = {Model-driven testing, Security protocols, Security tests, Model-driven software development, Unit tests},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128613002983},
}

@Article{Gran2014,
  author        = {Ernst Gunnar Gran and Thomas Dreibholz and Amund Kvalbein},
  title         = {NorNet Core – A multi-homed research testbed},
  journal       = {Computer Networks},
  year          = {2014},
  volume        = {61},
  pages         = {75 - 87},
  issn          = {1389-1286},
  note          = {Special issue on Future Internet Testbeds – Part I},
  __markedentry = {[Juliana:]},
  abstract      = {Over the last decade, the Internet has grown at a tremendous speed in both size and complexity. Nowadays, a large number of important services – for instance e-commerce, healthcare and many others – depend on the availability of the underlying network. Clearly, service interruptions due to network problems may have a severe impact. On the long way towards the Future Internet, the complexity will grow even further. Therefore, new ideas and concepts must be evaluated thoroughly, and particularly in realistic, real-world Internet scenarios, before they can be deployed for production networks. For this purpose, various testbeds – for instance PlanetLab, GpENI or G-Lab – have been established and are intensively used for research. However, all of these testbeds lack the support for so-called multi-homing. Multi-homing denotes the connection of a site to multiple Internet service providers, in order to achieve redundancy. Clearly, with the need for network availability, there is a steadily growing demand for multi-homing. The idea of the NorNet Core project is to establish a Future Internet research testbed with multi-homed sites, in order to allow researchers to perform experiments with multi-homed systems. Particular use cases for this testbed include realistic experiments in the areas of multi-path routing, load balancing, multi-path transport protocols, overlay networks and network resilience. In this paper, we introduce the NorNet Core testbed as well as its architecture.},
  doi           = {https://doi.org/10.1016/j.bjp.2013.12.035},
  keywords      = {NN C, Testbed, Multi-homing, Routing, Transport, Applications},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128613004489},
}

@InCollection{Young2015,
  author        = {Carl S. Young},
  title         = {Chapter 8 - Electronic Terrorism Threats, Risk, and Risk Mitigation},
  booktitle     = {The Science and Technology of Counterterrorism},
  publisher     = {Butterworth-Heinemann},
  year          = {2015},
  editor        = {Carl S. Young},
  pages         = {221 - 281},
  address       = {Boston},
  isbn          = {978-0-12-420056-2},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-420056-2.00008-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780124200562000087},
}

@Article{Panunzio2014,
  author        = {Marco Panunzio and Tullio Vardanega},
  title         = {An architectural approach with separation of concerns to address extra-functional requirements in the development of embedded real-time software systems},
  journal       = {Journal of Systems Architecture},
  year          = {2014},
  volume        = {60},
  number        = {9},
  pages         = {770 - 781},
  issn          = {1383-7621},
  __markedentry = {[Juliana:]},
  abstract      = {A large proportion of the requirements on embedded real-time systems stems from the extra-functional dimensions of time and space determinism, dependability, safety and security, and it is addressed at the software level. The adoption of a sound software architecture provides crucial aid in conveniently apportioning the relevant development concerns. This paper takes a software-centered interpretation of the ISO 42010 notion of architecture, enhancing it with a component model that attributes separate concerns to distinct design views. The component boundary becomes the border between functional and extra-functional concerns. The latter are treated as decorations placed on the outside of components, satisfied by implementation artifacts separate from and composable with the implementation of the component internals. The approach was evaluated by industrial users from several domains, with remarkably positive results.},
  doi           = {https://doi.org/10.1016/j.sysarc.2014.06.001},
  keywords      = {Embedded real-time systems, Extra-functional properties, Software architecture, Component-based software engineering, Separation of concerns},
  url           = {http://www.sciencedirect.com/science/article/pii/S1383762114000824},
}

@InCollection{Rodriguez2018,
  author        = {Pilar Rodríguez and Mika Mäntylä and Markku Oivo and Lucy Ellen Lwakatare and Pertti Seppänen and Pasi Kuvaja},
  title         = {Advances in Using Agile and Lean Processes for Software Development},
  publisher     = {Elsevier},
  year          = {2018},
  series        = {Advances in Computers},
  __markedentry = {[Juliana:]},
  abstract      = {Software development processes have evolved according to market needs. Fast changing conditions that characterize current software markets have favored methods advocating speed and flexibility. Agile and Lean software development are in the forefront of these methods. This chapter presents a unified view of Agile software development, Lean software development, and most recent advances toward rapid releases. First, we introduce the area and explain the reasons why the software development industry begun to move into this direction in the late 1990s. Section 2 characterizes the research trends on Agile software development. This section helps understand the relevance of Agile software development in the research literature. Section 3 provides a walk through the roots of Agile and Lean thinking, as they originally emerged in manufacturing. Section 4 develops into Agile and Lean for software development. Main characteristics and most popular methods and practices of Agile and Lean software development are developed in this section. Section 5 centers on rapid releases, continuous delivery, and continuous deployment, the latest advances in the area to get speed. The concepts of DevOps, as a means to take full (end-to-end) advantage of Agile and Lean, and Lean start-up, as an approach to foster innovation, are the focus of the two following 6 DevOps, 7 The Lean Startup Movement. Finally, Section 8 focuses on two important aspects of Agile and Lean software development: (1) metrics to guide decision making and (2) technical debt as a mechanism to gain business advantage. To wrap up the chapter, we peer into future directions in the area.},
  doi           = {https://doi.org/10.1016/bs.adcom.2018.03.014},
  issn          = {0065-2458},
  keywords      = {Software processes, Agile software development, Lean software development, Lean thinking, Leagility, Rapid releases, Continuous delivery, Continuous deployment, DevOps, Lean startup, Metrics, Technical debt},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245818300299},
}

@Article{Huang1984,
  author        = {Roger D. Huang},
  title         = {Some alternative tests of forward exchange rates as predictors of future spot rates},
  journal       = {Journal of International Money and Finance},
  year          = {1984},
  volume        = {3},
  number        = {2},
  pages         = {153 - 167},
  issn          = {0261-5606},
  __markedentry = {[Juliana:]},
  abstract      = {The paper provides empirical analysis on the issue of forward premiums as predictors of future exchange depreciations. The need to specify an alternative to the null hypothesis, other than its complement is emphasized. Two such alternatives are considered: the random walk model and the possibility of excessive or insufficient exchange rate volatility to accord with the efficiency of exchange markets.},
  doi           = {https://doi.org/10.1016/0261-5606(84)90003-2},
  url           = {http://www.sciencedirect.com/science/article/pii/0261560684900032},
}

@InCollection{Barbier2010,
  author        = {Gabriel Barbier and Hugo Bruneliere and Frédéric Jouault and Yves Lennon and Frédéric Madiot},
  title         = {Chapter 14 - MoDisco, a Model-Driven Platform to Support Real Legacy Modernization Use Cases},
  booktitle     = {Information Systems Transformation},
  publisher     = {Morgan Kaufmann},
  year          = {2010},
  editor        = {William M. Ulrich and Philip H. Newcomb},
  series        = {The MK/OMG Press},
  pages         = {365 - 400},
  address       = {Boston},
  isbn          = {978-0-12-374913-0},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
The case study presented in this chapter concentrates on a two-phase discussion of model-driven modernization. To deal with the myriad of technological combinations found in modernization roadmaps, model-driven approaches, and tools offer the requisite abstraction level to build up mature and flexible modernization solutions. This chapter presents the initial collaboration between AtlanMod and Sodifrance, which led to a model-driven legacy modernization approach. The use of model-driven technologies on major migration projects has proven the benefit of this approach. Based on metamodeling standards, the tools used to extract the knowledge from existing applications, to transform the knowledge into new paradigms and architecture, and to regenerate the application according to specific technical platforms and patterns are more flexible and reusable. The new MoDisco platform brings all these benefits into an open extensible framework containing model-driven, reverse-engineering tools and components. The current process and tools used by Sodifrance on its modernization projects are elaborated in the discussion. An illustration drawn from a real migration project carried out for Amadeus Hospitality is also included. A new Eclipse initiative capitalizing on this experience to deliver a model-driven platform for the development of legacy modernization tools is presented.},
  doi           = {https://doi.org/10.1016/B978-0-12-374913-0.00014-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123749130000147},
}

@InCollection{Wolf2007,
  title         = {Glossary},
  booktitle     = {High-Performance Embedded Computing},
  publisher     = {Morgan Kaufmann},
  year          = {2007},
  editor        = {Wayne Wolf},
  pages         = {433 - 466},
  address       = {San Francisco},
  isbn          = {978-0-12-369485-0},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-012369485-0/50009-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123694850500099},
}

@InCollection{Khalil2018,
  author        = {Amal Khalil and Juergen Dingel},
  title         = {Chapter Four - Optimizing the Symbolic Execution of Evolving Rhapsody Statecharts},
  publisher     = {Elsevier},
  year          = {2018},
  editor        = {Atif M. Memon},
  volume        = {108},
  series        = {Advances in Computers},
  pages         = {145 - 281},
  __markedentry = {[Juliana:]},
  abstract      = {Model-driven engineering (MDE) is an iterative and incremental software development process. Supporting the analysis and the verification of software systems developed following the MDE paradigm requires to adopt incrementality when carrying out these crucial tasks in a more optimized way. Communicating state machines are one of the various formalisms used in MDE tools to model and describe the behavior of distributed, concurrent, and real-time reactive systems (e.g., automotive and avionics systems). Modeling the overall behavior of such systems is carried out in a modular way and on different levels of abstraction (i.e., it starts with modeling the behavior of the individual objects in the system first then modeling the interaction between these objects). Similarly, analyzing and verifying the correctness of the developed models to ensure their quality and their integrity is performed on two main levels. The intralevel is used to analyze the correctness of the individual models in isolation of the others, while the interlevel is used to analyze the overall interoperability of those that are communicating with each other. One way to facilitate the analysis of the overall behavior of a system of communicating state machines is to build the global state space (also known as the global reachability tree) of the system. This process is very expensive and in some cases it may suffer from the state explosion problem. Symbolic execution is a technique that can be used to construct an abstract and a bounded version of the system global state space that is known as a symbolic execution tree (SET), yet the size of the generated trees can be very large especially with big and complex systems that are composed of multiple objects. As the system evolves, one way to avoid regenerating the entire SET and repeating any SET-based analyses that have been already conducted is to utilize the previous SET and its analysis results in optimizing the process of generating the SET of the system after the change. In this chapter, we propose two optimization techniques to direct the successive runs of the symbolic execution technique toward the impacted parts of an evolving state machine model using memoization (MSE) and dependency analysis (DSE), respectively. The evaluation results of both techniques showed significant reduction in some cases compared with the standard symbolic execution technique.},
  doi           = {https://doi.org/10.1016/bs.adcom.2017.09.003},
  issn          = {0065-2458},
  keywords      = {Model-driven engineering, Symbolic execution, Incremental verification, State-based behavioral models, State machines, Memoization, Dependency analysis, Model-based analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245817300487},
}

@Article{Bennett1991,
  author        = {KH Bennett},
  title         = {Automated support of software maintenance},
  journal       = {Information and Software Technology},
  year          = {1991},
  volume        = {33},
  number        = {1},
  pages         = {74 - 85},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Software maintenance is the general name given to the set of activities undertaken on a software system following its release for operational use. Surveys have shown that for many projects, software maintenance consumes the majority of the overall software life-cycle costs, and there are indications that the proportion is increasing. Inability to cope with software maintenance can also result in a backlog of application modifications. Despite the importance of software maintenance, it has acquired the reputation of being a second-class area in which to work. The paper defines in more detail the term software maintenance, and then addresses the issues of maintaining existing code, and producing maintainable systems, stressing the role of reengineering. Three projects that focus on software maintenance are then summarized. All three aim to provide automated assistance to the software maintainer, but in contrasting ways. The ReForm project is based on a formal method to extract specifications from code using transformations. MACS and REDO are both transnational European projects funded by the Esprit collaborative programme of research; the former uses expert system technology to assist the maintainer, while REDO aims to provide a set of integratable tools within a single environment, to support the reverse engineering process.},
  doi           = {https://doi.org/10.1016/0950-5849(91)90026-8},
  keywords      = {software maintenance, reengineering, reverse engineering, software process, tools},
  url           = {http://www.sciencedirect.com/science/article/pii/0950584991900268},
}

@Article{Straver2010a,
  author        = {D.C. Straver and Jan-Thies H. Van Asseldonk and N.C. Notermans and J.H. Wokke and L.H. Van den Berg and H. Franssen},
  title         = {P12-19 Cold paresis in multifocal motor neuropathy},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60723-7},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607237},
}

@Article{Navarro2011,
  author        = {Luis Daniel Benavides Navarro and Andrés Barrera and Kiyoshige Garcés and Hugo Arboleda},
  title         = {Detecting and Coordinating Complex Patterns of Distributed Events with KETAL},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2011},
  volume        = {281},
  pages         = {127 - 141},
  issn          = {1571-0661},
  note          = {Proceedings of the 2011 Latin American Conference in Informatics (CLEI)},
  __markedentry = {[Juliana:]},
  abstract      = {This paper presents an event-based kernel library designed to explicitly construct and coordinate complex interactions and communication patterns in distributed applications. The library integrates facilities for explicitly defining complex event patterns, detecting events in distributed systems, and validating sequences of events having into account causal ordering. Concretely we present the following contributions: i) An analysis of non trivial scenarios found in distributed applications in order to formulate a set of requirements and restrictions for a kernel event-based library, ii) the design and implementation of the library supporting the detection and coordination of complex event patterns and the support of causal manipulation of distributed events, iii) a qualitative evaluation of our approach showing how this library can be used to build a sophisticated distributed aspect oriented language.},
  doi           = {https://doi.org/10.1016/j.entcs.2011.11.030},
  keywords      = {Distributed event model, event patterns, causality, automata},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066111001794},
}

@Article{Garousi2015,
  author        = {Vahid Garousi and Ahmet Coşkunçay and Aysu Betin-Can and Onur Demirörs},
  title         = {A survey of software engineering practices in Turkey},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {108},
  pages         = {148 - 177},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Understanding the types of software engineering (SE) practices and techniques used in industry is important. There is a wide spectrum in terms of the types and maturity of SE practices conducted in industry. Turkey has a vibrant software industry and it is important to characterize and understand the state of its SE practices. Our objective is to characterize and grasp a high-level view on type of SE practices in the Turkish software industry. To achieve this objective, we systematically designed an online survey with 46 questions based on our past experience in the Canadian and Turkish contexts and using the Software Engineering Body of Knowledge (SWEBOK). Two hundred and two practicing software engineers from the Turkish software industry participated in the survey. The survey results reveal important and interesting findings about SE practices in Turkey and beyond. They also help track the profession of SE, and suggest areas for improved training, education and research. Among the findings are the followings: (1) The military and defense software sectors are quite prominent in Turkey, especially in the capital Ankara region, and many SE practitioners work for those companies. (2) 54% of the participants reported not using any software size measurement methods, while 33% mentioned that they have measured lines of code (LOC). (3) In terms of effort, after the development phase (on average, 31% of overall project effort), software testing, requirements, design and maintenance phases come next and have similar average values (14%, 12%, 12% and 11% respectively). (4) Respondents experience the most challenge in the requirements phase. (5) Waterfall, as a rather old but still widely used lifecycle model, is the model that more than half of the respondents (53%) use. The next most preferred lifecycle models are incremental and Agile/lean development models with usage rates of 38% and 34%, respectively. (6) The Waterfall and Agile methodologies have slight negative correlations, denoting that if one is used in a company, the other will less likely to be used. The results of our survey will be of interest to SE professionals both in Turkey and world-wide. It will also benefit researchers in observing the latest trends in SE industry identifying the areas of strength and weakness, which would then hopefully encourage further industry–academia collaborations in those areas.},
  doi           = {https://doi.org/10.1016/j.jss.2015.06.036},
  keywords      = {Software engineering, Industry practices, Turkey},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215001314},
}

@InCollection{Kalman2003,
  author        = {Deborah Bayles Kalman},
  title         = {Extranets},
  booktitle     = {Encyclopedia of Information Systems},
  publisher     = {Elsevier},
  year          = {2003},
  editor        = {Hossein Bidgoli},
  pages         = {301 - 312},
  address       = {New York},
  isbn          = {978-0-12-227240-0},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B0-12-227240-4/00069-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B0122272404000691},
}

@Article{Vidali2008,
  author        = {Matteo Vidali and Marie-Francoise Tripodi and Alesandra Ivaldi and Rosa Zampino and Giuseppa Occhino and Luciano Restivo and Salvatore Sutti and Aldo Marrone and Giuseppe Ruggiero and Emanuele Albano and Luigi E. Adinolfi},
  title         = {Interplay between oxidative stress and hepatic steatosis in the progression of chronic hepatitis C},
  journal       = {Journal of Hepatology},
  year          = {2008},
  volume        = {48},
  number        = {3},
  pages         = {399 - 406},
  issn          = {0168-8278},
  __markedentry = {[Juliana:]},
  abstract      = {Background/Aims
The contribution of oxidative stress to the pathogenesis of chronic hepatitis C (CHC) is still poorly elucidated. This study investigated the relationship between oxidative stress, insulin resistance, steatosis and fibrosis in CHC.
Methods
IgG against malondialdehyde-albumin adducts and HOMA-IR were measured as markers of oxidative stress and insulin resistance, respectively, in 107 consecutive CHC patients.
Results
Oxidative stress was present in 61% of the patients, irrespective of age, gender, viral load, BMI, aminotransferase level, histology activity index (HAI) and HCV genotype. Insulin resistance and steatosis were evident in 80% and 70% of the patients, respectively. In the patients infected by HCV genotype non-3, but not in those with genotype 3 infection HOMA-IR (p<0.03), steatosis (p=0.02) and fibrosis (p<0.05) were higher in the subjects with oxidative stress than in those without. Multiple regression analysis revealed that, HOMA-IR (p<0.01), fibrosis (p<0.01) and oxidative stress (p<0.05) were independently associated with steatosis, whereas steatosis was independently associated with oxidative stress (p<0.03) and HOMA-IR (p<0.02). Steatosis (p<0.02) and HAI (p=0.007) were also independent predictors of fibrosis.
Conclusions
In patients infected by HCV genotype non-3, oxidative stress and insulin resistance contribute to steatosis, which in turn exacerbates both insulin resistance and oxidative stress and accelerates the progression of fibrosis.},
  doi           = {https://doi.org/10.1016/j.jhep.2007.10.011},
  keywords      = {Oxidative stress, Steatosis, Lipid peroxidation, HCV infection, HOMA-IR, Liver fibrosis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0168827807006277},
}

@Article{Liaskos2012,
  author        = {Sotirios Liaskos and Shakil M. Khan and Marin Litoiu and Marina Daoud Jungblut and Vyacheslav Rogozhkin and John Mylopoulos},
  title         = {Behavioral adaptation of information systems through goal models},
  journal       = {Information Systems},
  year          = {2012},
  volume        = {37},
  number        = {8},
  pages         = {767 - 783},
  issn          = {0306-4379},
  note          = {Special Issue: Advanced Information Systems Engineering (CAiSE'11)},
  __markedentry = {[Juliana:]},
  abstract      = {Customizing software to perfectly fit individual needs is becoming increasingly important in information systems engineering. Users want to be able to customize software behavior through reference to terms familiar to their diverse needs and experience. We present a requirements-driven approach to behavioral customization of software systems. Goal models are constructed to represent alternative behaviors that users can exhibit to achieve their goals. Customization information is then added to restrict the space of possibilities to those that fit specific users, contexts, or situations. Meanwhile, elements of the goal models are mapped to units of source code. This way, customization preferences posed at the requirements level are directly translated into system customizations. Our approach, which we apply to an on-line shopping cart system and an automated teller machine simulator, does not assume adoption of a particular development methodology, platform, or variability implementation technique and keeps the reasoning computation overhead from interfering with the execution of the configured application.},
  doi           = {https://doi.org/10.1016/j.is.2012.05.006},
  keywords      = {Information systems engineering, Goal modeling, Software customization, Adaptive systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437912000737},
}

@InCollection{Walls2012,
  booktitle     = {Embedded Software (Second Edition)},
  publisher     = {Newnes},
  year          = {2012},
  editor        = {Colin Walls},
  pages         = {385 - 395},
  address       = {Oxford},
  edition       = {Second Edition},
  isbn          = {978-0-12-415822-1},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-415822-1.00021-0},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780124158221000210},
}

@Article{Barenghi2015,
  author        = {Alessandro Barenghi and Stefano Crespi Reghizzi and Dino Mandrioli and Federica Panella and Matteo Pradella},
  title         = {Parallel parsing made practical},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {112},
  pages         = {195 - 226},
  issn          = {0167-6423},
  __markedentry = {[Juliana:]},
  abstract      = {The property of local parsability allows to parse inputs through inspecting only a bounded-length string around the current token. This in turn enables the construction of a scalable, data-parallel parsing algorithm, which is presented in this work. Such an algorithm is easily amenable to be automatically generated via a parser generator tool, which was realized, and is also presented in the following. Furthermore, to complete the framework of a parallel input analysis, a parallel scanner can also combined with the parser. To prove the practicality of a parallel lexing and parsing approach, we report the results of the adaptation of JSON and Lua to a form fit for parallel parsing (i.e. an operator-precedence grammar) through simple grammar changes and scanning transformations. The approach is validated with performance figures from both high performance and embedded multicore platforms, obtained analyzing real-world inputs as a test-bench. The results show that our approach matches or dominates the performances of production-grade LR parsers in sequential execution, and achieves significant speedups and good scaling on multi-core machines. The work is concluded by a broad and critical survey of the past work on parallel parsing and future directions on the integration with semantic analysis and incremental parsing.},
  doi           = {https://doi.org/10.1016/j.scico.2015.09.002},
  keywords      = {Parallel parsing algorithms, Syntax analysis, Parallel parser, Operator precedence grammar},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642315002610},
}

@InCollection{Meillour2003,
  author        = {Patricia Nagnan-Le Meillour and Emmanuelle Jacquin-Joly},
  title         = {17 - Biochemistry and diversity of insect odorant-binding proteins},
  booktitle     = {Insect Pheromone Biochemistry and Molecular Biology},
  publisher     = {Academic Press},
  year          = {2003},
  editor        = {Gary Blomquist and Richard Vogt},
  pages         = {509 - 537},
  address       = {San Diego},
  isbn          = {978-0-12-107151-6},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
Odorant-binding proteins (OBPs) are abundant in the sensillar lymph of insect antennae and participate in olfactory perireceptor events, such as transport of the hydrophobic odorant through the aqueous medium, presentation of the odor to olfactory receptors, and deactivation of the signal. This chapter describes the biochemistry and molecular biology of pheromone detection in the noctuid moth, Mamestra brassicae. The OBPs are diverse ranging from numerous species of several insect orders including Lepidoptera, Diptera, Coleoptera, and Hymenoptera, Hemiptera, and Phasmatodea. Classification defines OBP-Type 1 and OBP-Type 2 based on phylogeny, tissue localization, and structural features. The advantage of the functional approach to characterize the diversity of OBPs inside species is characterized. The construction of EST antennal libraries with a high EST number to be representative of the sequence diversity has identified not only new OBPs, but also SAPs, pheromone-degrading enzymes, sensory-neuron-membrane-protein-like sequences, and other elements involved in the pheromone transduction process.},
  doi           = {https://doi.org/10.1016/B978-012107151-6/50019-0},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780121071516500190},
}

@Article{Peterson2018,
  author        = {John W. Peterson and Alexander D. Lindsay and Fande Kong},
  title         = {Overview of the incompressible Navier–Stokes simulation capabilities in the MOOSE framework},
  journal       = {Advances in Engineering Software},
  year          = {2018},
  volume        = {119},
  pages         = {68 - 92},
  issn          = {0965-9978},
  __markedentry = {[Juliana:]},
  abstract      = {The Multiphysics Object Oriented Simulation Environment (MOOSE) framework is a high-performance, open source, C++ finite element toolkit developed at Idaho National Laboratory. MOOSE was created with the aim of assisting domain scientists and engineers in creating customizable, high-quality tools for multiphysics simulations. While the core MOOSE framework itself does not contain code for simulating any particular physical application, it is distributed with a number of physics “modules” which are tailored to solving e.g. heat conduction, phase field, and solid/fluid mechanics problems. In this report, we describe the basic equations, finite element formulations, software implementation, and regression/verification tests currently available in MOOSE’s navier_stokes module for solving the Incompressible Navier-Stokes (INS) equations.},
  doi           = {https://doi.org/10.1016/j.advengsoft.2018.02.004},
  url           = {http://www.sciencedirect.com/science/article/pii/S0965997817310591},
}

@InCollection{Aarno2015,
  author        = {Daniel Aarno and Jakob Engblom},
  title         = {Chapter 6 - Building virtual platforms},
  booktitle     = {Full-System Simulation with Simics},
  publisher     = {Morgan Kaufmann},
  year          = {2015},
  editor        = {Daniel Aarno and Jakob Engblom},
  pages         = {161 - 210},
  address       = {Boston},
  isbn          = {978-0-12-800725-9},
  __markedentry = {[Juliana:]},
  abstract      = {Chapter 6 introduces the reader to how to best perform transaction-level modeling of individual devices and how such models are built in Simics. It covers the Device Modeling Language (DML), as well as device modeling in C, C++, Python, and SystemC. Chapter 6 provides detailed step-by-step instructions for how to create a simple device model in Simics.},
  doi           = {https://doi.org/10.1016/B978-0-12-800725-9.00006-8},
  keywords      = {DML, device model, TLM, SystemC, modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128007259000068},
}

@Article{Penton2012,
  author        = {Andrea L. Penton and Laura D. Leonard and Nancy B. Spinner},
  title         = {Notch signaling in human development and disease},
  journal       = {Seminars in Cell \& Developmental Biology},
  year          = {2012},
  volume        = {23},
  number        = {4},
  pages         = {450 - 457},
  issn          = {1084-9521},
  note          = {Cancer Cell Metabolism \& Notch Signaling},
  __markedentry = {[Juliana:]},
  abstract      = {Mutations in Notch signaling pathway members cause developmental phenotypes that affect the liver, skeleton, heart, eye, face, kidney, and vasculature. Notch associated disorders include the autosomal dominant, multi-system, Alagille syndrome caused by mutations in both a ligand (Jagged1 (JAG1)) and receptor (NOTCH2) and autosomal recessive spondylocostal dysostosis, caused by mutations in a ligand (Delta-like-3 (DLL3)), as well as several other members of the Notch signaling pathway. Mutations in NOTCH2 have also recently been connected to Hajdu-Cheney syndrome, a dominant disorder causing focal bone destruction, osteoporosis, craniofacial morphology and renal cysts. Mutations in the NOTCH1 receptor are associated with several types of cardiac disease and mutations in NOTCH3 cause the dominant adult onset disorder CADASIL (cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy), a vascular disorder with onset in the 4th or 5th decades. Studies of these human disorders and their inheritance patterns and types of mutations reveal insights into the mechanisms of Notch signaling.},
  doi           = {https://doi.org/10.1016/j.semcdb.2012.01.010},
  keywords      = {Alagille syndrome, Spondylocostal dysostosis, Hajdu Cheney, Cardiac disease, Notch signaling},
  url           = {http://www.sciencedirect.com/science/article/pii/S1084952112000146},
}

@Article{Ochoa2018,
  author        = {Lina Ochoa and Oscar González-Rojas and Alves Pereira Juliana and Harold Castro and Gunter Saake},
  title         = {A systematic literature review on the semi-automatic configuration of extended product lines},
  journal       = {Journal of Systems and Software},
  year          = {2018},
  volume        = {144},
  pages         = {511 - 532},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Product line engineering has become essential in mass customisation given its ability to reduce production costs and time to market, and to improve product quality and customer satisfaction. In product line literature, mass customisation is known as product configuration. Currently, there are multiple heterogeneous contributions in the product line configuration domain. However, a secondary study that shows an overview of the progress, trends, and gaps faced by researchers in this domain is still missing. In this context, we provide a comprehensive systematic literature review to discover which approaches exist to support the configuration process of extended product lines and how these approaches perform in practice. Extend product lines consider non-functional properties in the product line modelling. We compare and classify a total of 66 primary studies from 2000 to 2016. Mainly, we give an in-depth view of techniques used by each work, how these techniques are evaluated and their main shortcomings. As main results, our review identified (i) the need to improve the quality of the evaluation of existing approaches, (ii) a lack of hybrid solutions to support multiple configuration constraints, and (iii) a need to improve scalability and performance conditions.},
  doi           = {https://doi.org/10.1016/j.jss.2018.07.054},
  keywords      = {Extended product line, Product configuration, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121218301511},
}

@Article{Kraetzig2007,
  author        = {Markus Krätzig},
  title         = {A software framework for data analysis},
  journal       = {Computational Statistics \& Data Analysis},
  year          = {2007},
  volume        = {52},
  number        = {2},
  pages         = {618 - 634},
  issn          = {0167-9473},
  __markedentry = {[Juliana:]},
  abstract      = {The open-source Java software framework JStatCom is presented which supports the development of rich desktop clients for data analysis in a rather general way. The concept is to solve all recurring tasks with the help of reusable components and to enable rapid application development by adopting a standards based approach which is readily supported by existing programming tools. Furthermore, JStatCom allows to call external procedures from within Java that are written in other languages, for example Gauss, Ox or Matlab. This way it is possible to reuse an already existing code base for numerical routines written in domain-specific programming languages and to link them with the Java world. A reference application for JStatCom is the econometric software package JMulTi, which will shortly be introduced.},
  doi           = {https://doi.org/10.1016/j.csda.2006.08.007},
  keywords      = {Java, Object-oriented programming, Econometrics, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167947306002672},
}

@Article{Goncalves2009,
  author        = {Raquel M. Gonçalves and M. Cristina L. Martins and Graça Almeida-Porada and Mário A. Barbosa},
  title         = {Induction of notch signaling by immobilization of jagged-1 on self-assembled monolayers},
  journal       = {Biomaterials},
  year          = {2009},
  volume        = {30},
  number        = {36},
  pages         = {6879 - 6887},
  issn          = {0142-9612},
  __markedentry = {[Juliana:]},
  abstract      = {Notch signaling is a key mechanism during mammal development and stem cell regulation. This study aims to target and control Notch signaling by ligands immobilization using self-assembled monolayers (SAMs) as model surfaces. Non-fouling substrates were prepared by immersion of gold substrates in (1-Mercapto-11-undecyl)tetra(ethylene glycol) thiol solutions. These surfaces were activated with N,N′-carbonyldiimidazole (CDI) at different concentrations (0, 0.03, 0.3, 3 and 30mg/ml) and an anti-human IgG, Fc specific fragment antibody (Ab) was covalently bound to EG4-SAMs to guarantee the correct exposure of the Notch ligand Jagged-1/Fc chimera (Jag-1). The presence of Ab and Jag-1 was confirmed by radiolabeling, X-ray photoelectron spectroscopy (XPS), ellipsometry and ELISA. The biological activity of Jag-1-Ab-SAMs was assessed by real-time PCR for Hes-1 family gene expression, a Notch pathway target gene, in HL-60 cell line. Results have shown an increase of the amount of immobilized Ab with increasing surface activator concentrations. Jag-1 concentration also increases with Ab concentration. Interestingly, a higher Jagged-1 exposure and fold increase in Hes-1 expression were obtained for surfaces activated with the lowest concentration of CDI (0.03mg/ml). These results illustrate the great importance of ligands orientation and exposure, when compared with density. This investigation brings new insights into Notch signaling mechanisms. In particular, Jag-1-Ab-SAMs have shown to be adequate model surfaces to study Notch pathway activation and may provide a basis to develop new interfaces in biomaterials to control Notch mechanism in different cell systems.},
  doi           = {https://doi.org/10.1016/j.biomaterials.2009.09.010},
  keywords      = {Nanostructured surfaces, Self-assembled monolayers, Protein immobilization, Protein adsorption, Notch signaling},
  url           = {http://www.sciencedirect.com/science/article/pii/S0142961209009260},
}

@Article{Meyrand2013,
  author        = {M. Meyrand and D.C. Dallas and H. Caillat and F. Bouvier and P. Martin and D. Barile},
  title         = {Comparison of milk oligosaccharides between goats with and without the genetic ability to synthesize αs1-casein},
  journal       = {Small Ruminant Research},
  year          = {2013},
  volume        = {113},
  number        = {2},
  pages         = {411 - 420},
  issn          = {0921-4488},
  __markedentry = {[Juliana:]},
  abstract      = {Milk oligosaccharides (OS)—free complex carbohydrates—confer unique health benefits to the nursing neonate. Though human digestive enzymes cannot degrade these sugars, they provide nourishment to specific commensal microbes and act as decoys to prevent the adhesion of pathogenic micro-organisms to gastrointestinal cells. At present, the limited quantities of human milk oligosaccharides (HMO) impede research on these molecules and their potential applications in functional food formulations. Considerable progress has been made in the study of OS structures; however, the synthetic pathways leading to their synthesis in the mammary gland are poorly understood. Recent studies show that complex OS with fucose and N-acetyl neuraminic acid (key structural elements of HMO bioactivity) exist in goat milk. Polymorphisms in the CSN1S1 locus, which is responsible for synthesis of αs1-casein, affect lipid and casein micelle structure in goat milk. The present study sought to determine whether CSN1S1 polymorphisms also influence goat milk oligosaccharide (GMO) production and secretion. The GMO compositions of thirty-two goat milk samples, half of which were from genotype A/A (αs1-casein producers) and half from genotype O/O (αs1-casein non-producers), were determined with nanoflow liquid chromatography high-accuracy mass spectrometry. This study represents the most exhaustive characterization of GMO to date. A systematic and comprehensive GMO library was created, consolidating information available in the literature with the new findings. Nearly 30 GMO, 11 of which were novel, were confirmed via tandem mass spectrometric analyses. Six fucosylated OS were identified; 4 of these matched HMO compositions and three were identified for the first time in goat milk. Importantly, multivariate statistical analysis demonstrated that the OS profiles of the A/A and O/O genotype milks could be discriminated by the fucosylated OS. Quantitative analysis revealed that the goat milk samples contained 1.17g/L of OS; however, their concentration in milks from A/A and O/O genotypes was not different. This study provides evidence of a genetic influence on specific OS biosynthesis but not total OS production. The presence of fucosylated GMO suggests that goat milk represents a potential source of bioactive milk OS suitable as a functional food ingredient.},
  doi           = {https://doi.org/10.1016/j.smallrumres.2013.03.014},
  keywords      = {, Fucose, Genetic polymorphisms, Goat milk, Mass spectrometry, Oligosaccharides},
  url           = {http://www.sciencedirect.com/science/article/pii/S0921448813000990},
}

@Article{Schermann2018,
  author        = {Gerald Schermann and Jürgen Cito and Philipp Leitner and Uwe Zdun and Harald C. Gall},
  title         = {We’re doing it live: A multi-method empirical study on continuous experimentation},
  journal       = {Information and Software Technology},
  year          = {2018},
  volume        = {99},
  pages         = {41 - 57},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Continuous experimentation guides development activities based on data collected on a subset of online users on a new experimental version of the software. It includes practices such as canary releases, gradual rollouts, dark launches, or A/B testing.
Objective
Unfortunately, our knowledge of continuous experimentation is currently primarily based on well-known and outspoken industrial leaders. To assess the actual state of practice in continuous experimentation, we conducted a mixed-method empirical study.
Method
In our empirical study consisting of four steps, we interviewed 31 developers or release engineers, and performed a survey that attracted 187 complete responses. We analyzed the resulting data using statistical analysis and open coding.
Results
Our results lead to several conclusions: (1) from a software architecture perspective, continuous experimentation is especially enabled by architectures that foster independently deployable services, such as microservices-based architectures; (2) from a developer perspective, experiments require extensive monitoring and analytics to discover runtime problems, consequently leading to developer on call policies and influencing the role and skill sets required by developers; and (3) from a process perspective, many organizations conduct experiments based on intuition rather than clear guidelines and robust statistics.
Conclusion
Our findings show that more principled and structured approaches for release decision making are needed, striving for highly automated, systematic, and data- and hypothesis-driven deployment and experimentation.},
  doi           = {https://doi.org/10.1016/j.infsof.2018.02.010},
  keywords      = {Release engineering, Continuous deployment, Continuous experimentation, Empirical study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917302136},
}

@Article{Dijk2010,
  author        = {J.P. van Dijk and C. Verhamme and I.N. van Schaik and H.J. Schelhaas and E. Mans and L.J. Bour and D.F. Stegeman and M.J. Zwarts},
  title         = {P12-20 Age-related changes in motor unit number estimates in adult patients with Charcot-Marie-Tooth type 1A},
  journal       = {Clinical Neurophysiology},
  year          = {2010},
  volume        = {121},
  pages         = {S176},
  issn          = {1388-2457},
  note          = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1388-2457(10)60724-9},
  url           = {http://www.sciencedirect.com/science/article/pii/S1388245710607249},
}

@Article{Melvin1989,
  author        = {Michael Melvin and Su Zhou},
  title         = {Do centrally planned exchange rates behave differently from capitalist rates?},
  journal       = {Journal of Comparative Economics},
  year          = {1989},
  volume        = {13},
  number        = {2},
  pages         = {325 - 334},
  issn          = {0147-5967},
  __markedentry = {[Juliana:]},
  abstract      = {We conduct a statistical analysis of the time series of the dollar value of the pound, mark, yen, yuan, dinar, and forint exchange rates. The evidence indicates that the centrally planned exchange rates are well represented by random walks, as are the capitalist rates. This might be expected if the planned rates are pegged to the capitalist rates. However, a lack of cointegration between the planned rates and the capitalist rates suggests that this is not the reason for the nonstationarity of planned exchange rates.},
  doi           = {https://doi.org/10.1016/0147-5967(89)90008-5},
  url           = {http://www.sciencedirect.com/science/article/pii/0147596789900085},
}

@InCollection{Dyro2004,
  booktitle     = {Clinical Engineering Handbook},
  publisher     = {Academic Press},
  year          = {2004},
  editor        = {Joseph F Dyro},
  series        = {Biomedical Engineering},
  pages         = {665 - 674},
  address       = {Burlington},
  isbn          = {978-0-12-226570-9},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-012226570-9/50159-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780122265709501599},
}

@InCollection{Zuellighoven2005,
  title         = {INDEX},
  booktitle     = {Object-Oriented Construction Handbook},
  publisher     = {Morgan Kaufmann},
  year          = {2005},
  editor        = {Heinz Züllighoven},
  pages         = {501 - 520},
  address       = {San Francisco},
  isbn          = {978-1-55860-687-6},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-155860687-6/50014-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781558606876500141},
}

@InCollection{Kenyon2002,
  title         = {Index},
  booktitle     = {Data Networks},
  publisher     = {Digital Press},
  year          = {2002},
  editor        = {Tony Kenyon},
  pages         = {775 - 807},
  address       = {Burlington},
  isbn          = {978-1-55558-271-5},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-155558271-5/50038-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781555582715500389},
}

@Article{Adams2009,
  author        = {Bram Adams and Kris De Schutter and Andy Zaidman and Serge Demeyer and Herman Tromp and Wolfgang De Meuter},
  title         = {Using aspect orientation in legacy environments for reverse engineering using dynamic analysis—An industrial experience report},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {4},
  pages         = {668 - 684},
  issn          = {0164-1212},
  note          = {Special Issue: Selected papers from the 2008 IEEE Conference on Software Engineering Education and Training (CSEET08)},
  __markedentry = {[Juliana:]},
  abstract      = {This paper reports on the challenges of using aspect-oriented programming (AOP) to aid in re-engineering a legacy C application. More specifically, we describe how AOP helps in the important reverse engineering step which typically precedes a re-engineering effort. We first present a comparison of the available AOP tools for legacy C code bases, and then argue on our choice of Aspicere, our own AOP implementation for C. Then, we report on Aspicere’s application in reverse engineering a legacy industrial software system and we show how we apply a dynamic analysis to regain insight into the system. AOP is used for instrumenting the system and for gathering the data. This approach works and is conceptually very clean, but comes with a major quid pro quo: integration of AOP tools with the build system proves an important issue. This leads to the question of how to reconcile the notion of modular reasoning within traditional build systems with a programming paradigm which breaks this notion.},
  doi           = {https://doi.org/10.1016/j.jss.2008.09.031},
  keywords      = {Dynamic analysis, Aspect-oriented programming, Industrial case study, Program comprehension C},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121208002173},
}

@Article{Fecko2005,
  author        = {Mariusz A. Fecko and Christopher M. Lott},
  title         = {XML-based requirements engineering for an electronic clearinghouse},
  journal       = {Information and Software Technology},
  year          = {2005},
  volume        = {47},
  number        = {13},
  pages         = {841 - 858},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {We present methods and tools to support XML-based requirements engineering for an electronic clearinghouse that connects trading partners in the telecommunications area. The original semi-structured requirements, locally known as business rules, were written as message specifications in a non-standardized and error-prone format using MS Word. To remedy the resulting software failures and faults, we first formalized the requirements by designing an W3C XML Schema for the precise definition of the requirements structure. The schema allows a highly structured representation of the essential information in eXtensible Markup Language (XML). Second, to offer the requirements engineers the ability to edit the XML documents in a friendly way while preserving their information structure, we developed a custom editor called XLEdit. Third, by developing a converter from MS Word to the target XML format, we helped the requirements engineers to migrate the existing business rules. Fourth, we developed translators from the structured requirements to schema languages, which enabled automated generation of message-validation code. The increase in customer satisfaction and clearinghouse-service efficiency are primary gains from the investment in the technology for structured requirements editing and validation.},
  doi           = {https://doi.org/10.1016/j.infsof.2005.01.005},
  keywords      = {GUI, Message-processing, XML, Business rules, Requirements engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584905000145},
}

@InCollection{Sittig2017a,
  title         = {Index},
  booktitle     = {Clinical Informatics Literacy},
  publisher     = {Academic Press},
  year          = {2017},
  editor        = {Dean F. Sittig},
  pages         = {171 - 231},
  isbn          = {978-0-12-803206-0},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-803206-0.18001-0},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128032060180010},
}

@Article{Nishida2014,
  author        = {Tetsushi Nishida and James B. Pick and Avijit Sarkar},
  title         = {Japan׳s prefectural digital divide: A multivariate and spatial analysis},
  journal       = {Telecommunications Policy},
  year          = {2014},
  volume        = {38},
  number        = {11},
  pages         = {992 - 1010},
  issn          = {0308-5961},
  __markedentry = {[Juliana:]},
  abstract      = {This study of the digital divide within Japan utilizes data from the country׳s 47 prefectures for multivariate and spatial analysis of distributions of information and communication technology (ICT) variables. The paper constructs an exploratory conceptual model of technology utilization and expenditures in Japan, induced from prior literature. Ten dependent ICT utilization and expenditure factors are posited to be related to 12 independent demographic, economic, infrastructure, education, innovation and openness factors. The relationship of the independent to dependent factors is moderated by analysis of spatial patterns of technology utilization to examine proximities and reduce spatial bias. Based on the model, a multivariate analysis identifies correlates of the nation׳s digital divide, including patents registered by Japanese citizens, newspaper circulation, students and pupils per capita, household expenditures on education, rural/urban status, and Japan׳s aged population structure which has wide generational gaps. Spatial clusters and outliers of ICTs in prefectures are analyzed, with attention to their policy impacts. Findings suggest modifications to the conceptual model. Implications of findings for the country׳s official national technology planning policies are considered and recommendations made to expand them.},
  doi           = {https://doi.org/10.1016/j.telpol.2014.05.004},
  keywords      = {Japan, Digital divide, Information and communication technologies, ICT use factors, Theoretical model, Regression, Spatial autocorrelation, Cluster analysis, ICT policy implications},
  url           = {http://www.sciencedirect.com/science/article/pii/S0308596114000937},
}

@Article{Vogel-Heuser2017,
  author        = {Birgit Vogel-Heuser and Juliane Fischer and Stefan Feldmann and Sebastian Ulewicz and Susanne Rösch},
  title         = {Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {35 - 62},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies’ solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry.},
  doi           = {https://doi.org/10.1016/j.jss.2017.05.051},
  keywords      = {Factory automation, Automated production systems, Maturity, Modularity, Control software, Programmable logic controller},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300985},
}

@InCollection{Valli1993,
  author        = {V.E.O. Valli and B.W. Parry},
  title         = {CHAPTER 2 - The Hematopoietic System},
  booktitle     = {Pathology of Domestic Animals (Fourth Edition)},
  publisher     = {Academic Press},
  year          = {1993},
  editor        = {K.V.F. JUBB and PETER C. KENNEDY and NIGEL PALMER},
  pages         = {101 - 265},
  address       = {San Diego},
  edition       = {Fourth Edition},
  isbn          = {978-0-12-391607-5},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-391607-5.50010-8},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123916075500108},
}

@InCollection{Noergaard2005,
  title         = {Appendix D - Glossary},
  booktitle     = {Embedded Systems Architecture},
  publisher     = {Newnes},
  year          = {2005},
  editor        = {Tammy Noergaard},
  series        = {Embedded Technology},
  pages         = {610 - 626},
  address       = {Burlington},
  isbn          = {978-0-7506-7792-9},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-075067792-9/50022-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780750677929500224},
}

@Article{Myers2005,
  author        = {Toshia R. Myers and Iva Greenwald},
  title         = {lin-35 Rb Acts in the Major Hypodermis to Oppose Ras-Mediated Vulval Induction in C. elegans},
  journal       = {Developmental Cell},
  year          = {2005},
  volume        = {8},
  number        = {1},
  pages         = {117 - 123},
  issn          = {1534-5807},
  __markedentry = {[Juliana:]},
  abstract      = {Specification of vulval precursor cell (VPC) fates in C. elegans has served as an important signal transduction paradigm. Genetic studies have indicated that a large group of synthetic multivulva (SynMuv) genes, including the Rb ortholog lin-35, antagonizes the activity of the EGF receptor-Ras-MAP kinase pathway during VPC specification. A prevalent view has been that Rb-mediated transcriptional regulation and chromatin remodeling activities act in the VPCs to antagonize Ras activation through effects on promoters of target genes of the EGF receptor-Ras-MAP kinase pathway that promote vulval fates. Here, we have investigated the cellular focus of lin-35 using conventional genetic mosaic analysis and tissue-specific expression. Our results indicate that lin-35 activity is required in the major hypodermal syncytium and not in the VPCs to inhibit vulval fates. LIN-35 Rb may inhibit vulval fates by regulating a signal from hyp7 to the VPCs or the physiological state of hyp7.},
  doi           = {https://doi.org/10.1016/j.devcel.2004.11.015},
  url           = {http://www.sciencedirect.com/science/article/pii/S1534580704004228},
}

@Article{Tsai2016,
  author        = {Wei-Tek Tsai and Guanqiu Qi},
  title         = {Integrated fault detection and test algebra for combinatorial testing in TaaS (Testing-as-a-Service)},
  journal       = {Simulation Modelling Practice and Theory},
  year          = {2016},
  volume        = {68},
  pages         = {108 - 124},
  issn          = {1569-190X},
  __markedentry = {[Juliana:]},
  abstract      = {Testing-as-a-Service (TaaS) is a software testing service in a cloud that can leverage the computation power provided by the cloud. Specifically, a TaaS can be scaled to large and dynamic workloads, executed in a distributed environment with hundreds of thousands of processors, and these processors may support concurrent and distributed test execution and analysis. This paper proposes a TaaS system based on Adaptive Reasoning (AR) and Test Algebra (TA) for Combinatorial Testing (CT). AR performs testing and identifies faulty interactions, and TA eliminates related configurations from testing and there can be carried out concurrently. By combining these two, it is possible to perform large CT that were not possible before. Specifically, we performed experiments with 250 components with 2.83*1087 6-way interactions with about 21.1×1015 configurations, and this may be the largest CT experimentation as 2014. 98.6% of configurations have been eliminated out of total number of configurations.},
  doi           = {https://doi.org/10.1016/j.simpat.2016.08.003},
  keywords      = {Combinatorial testing, TaaS, Concurrent testing, Test algebra, Adaptive reasoning},
  url           = {http://www.sciencedirect.com/science/article/pii/S1569190X16302210},
}

@Article{Congote2005,
  author        = {Luis F. Congote},
  title         = {Monitoring insulin-like growth factors in HIV infection and AIDS},
  journal       = {Clinica Chimica Acta},
  year          = {2005},
  volume        = {361},
  number        = {1},
  pages         = {30 - 53},
  issn          = {0009-8981},
  __markedentry = {[Juliana:]},
  abstract      = {There is a close association between the growth hormone (GH)–insulin-like growth factor I (IGF-I) axis, infection and immunity. Infection with the human immunodeficiency virus (HIV) is often associated with a decrease of the concentrations of IGF-I, IGF-II, IGF-binding protein 3 (IGFBP-3) and an increase of IGFBP-1 and -2. Many investigators have studied the relationship between the GH-IGF-I system and some of the most common characteristics of disease progression, such as decreased CD4 cell counts, weight loss and fat redistribution. Although conditions for restoration of thymic function and lymphopoiesis with GH or IGF-I are still not well defined, many studies led to the development of clinical trials on the therapeutic use of GH, IGF-I and GHRH for the treatment of weight loss or fat redistribution, two problems which persist despite the introduction of highly active antiretroviral therapy. Monitoring IGF-I concentrations during treatment with GH and GHRH is likely to become an essential component of their therapeutic use. IGF-I levels are the first indicator of treatment efficacy and can be used to monitor compliance. High levels of IGF-I are a warning sign for the increased risk of potential adverse effects, such as acromegalic-like symptoms or malignancy. This could lead to a reduction of the therapeutic dose or the temporary interruption of treatment until IGF levels reach a safe range. IGF-I levels are also likely to increase with other hormones used in HIV patients, such as erythropoietin for the treatment of anemia or anabolic androgens in HIV-infected women.},
  doi           = {https://doi.org/10.1016/j.cccn.2005.05.001},
  keywords      = {IGF, Growth hormone, HAART, HIV, Wasting, Lipodystrophy},
  url           = {http://www.sciencedirect.com/science/article/pii/S0009898105002901},
}

@Article{Hueber2016,
  author        = {Thomas Hueber and Gérard Bailly},
  title         = {Statistical conversion of silent articulation into audible speech using full-covariance HMM},
  journal       = {Computer Speech \& Language},
  year          = {2016},
  volume        = {36},
  pages         = {274 - 293},
  issn          = {0885-2308},
  __markedentry = {[Juliana:]},
  abstract      = {This article investigates the use of statistical mapping techniques for the conversion of articulatory movements into audible speech with no restriction on the vocabulary, in the context of a silent speech interface driven by ultrasound and video imaging. As a baseline, we first evaluated the GMM-based mapping considering dynamic features, proposed by Toda et al. (2007) for voice conversion. Then, we proposed a ‘phonetically-informed’ version of this technique, based on full-covariance HMM. This approach aims (1) at modeling explicitly the articulatory timing for each phonetic class, and (2) at exploiting linguistic knowledge to regularize the problem of silent speech conversion. Both techniques were compared on continuous speech, for two French speakers (one male, one female). For modal speech, the HMM-based technique showed a lower spectral distortion (objective evaluation). However, perceptual tests (transcription and XAB discrimination tests) showed a better intelligibility of the GMM-based technique, probably related to its less fluctuant quality. For silent speech, a perceptual identification test revealed a better segmental intelligibility for the HMM-based technique on consonants.},
  doi           = {https://doi.org/10.1016/j.csl.2015.03.005},
  keywords      = {Silent speech interface, GMM, HMM, Ultrasound, Articulatory–acoustic mapping},
  url           = {http://www.sciencedirect.com/science/article/pii/S0885230815000340},
}

@Article{Crawford1985,
  author        = {S.G. Crawford and A.A. McIntosh and D. Pregibon},
  title         = {An analysis of static metrics and faults in C software},
  journal       = {Journal of Systems and Software},
  year          = {1985},
  volume        = {5},
  number        = {1},
  pages         = {37 - 48},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {In this empirical study, we evaluate the extent to which a set of software measures are correlated with the number of faults and the total estimated repair effort for a large software system. The measures we use are basic counts reflecting program size and structure and metrics proposed by McCabe and Halstead. The effect of program size has a major influence on these metrics, and we present a suitable method of adjusting the metrics for size. In modeling faults or repair effort as a function of one variable, a number of measures individually explain approximately one-quarter of the variation observed in the fault data. No one measure does significantly better than size in explaining the variation in faults found across software units, and thus multiple variable models are necessary to find metrics of importance in addition to program size. The “best” multivariate model explains approximately one-half the variation in the fault data. The metrics included in this model (in addition to size) are: the ratio of block comments to total lines of code, the number of decisions per function, and the relative vocabulary of program variables and operators. These metrics have potential for future use in the quality control of software.},
  doi           = {https://doi.org/10.1016/0164-1212(85)90005-6},
  url           = {http://www.sciencedirect.com/science/article/pii/0164121285900056},
}

@InCollection{Lackner2017,
  author        = {Hartmut Lackner and Bernd-Holger Schlingloff},
  title         = {Chapter Four - Advances in Testing Software Product Lines},
  publisher     = {Elsevier},
  year          = {2017},
  editor        = {Atif M. Memon},
  volume        = {107},
  series        = {Advances in Computers},
  pages         = {157 - 217},
  __markedentry = {[Juliana:]},
  abstract      = {In this chapter, we describe some recent techniques and results in model-based testing of software product lines. Presently, more and more software-based products and services are available in many different variants to choose from. However, this brings about challenges for the software quality assurance processes. Since only few of all possible variants can be tested at the developer's site, several questions arise. How shall the variability be described in order to make sure that all features are being tested? Is it better to test selected variants on a concrete level, or shall the whole software product line be tested abstractly? What is the quality of a test suite for a product line, anyway? If it is impossible to test all possible variants, which products should be selected for testing? Given a certain product, which test cases are appropriate for it, and given a test case, which products can be tested with it? We address these questions from an empirical software engineering point of view. We sketch modeling formalisms for software product lines. Then, we compare domain-centered and application-centered approaches to software product line testing. We define mutation operators for assessing software product line test suites. Subsequently, we analyze methods for selecting product variants on the basis of a given test suite. Finally, we show how model checking can be used to determine whether a certain test case is applicable for a certain product variant. For all our methods we describe supporting tools and algorithms. Currently, we are integrating these in an integrated tool suite supporting several aspects of model-based testing for software product lines.},
  doi           = {https://doi.org/10.1016/bs.adcom.2017.07.001},
  issn          = {0065-2458},
  keywords      = {Software product lines, Cyber physical systems, Model-based testing, Test generation, Variant management, Feature modeling, Domain analysis, Fault injection, Product sampling, Test case assignment},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245817300311},
}

@Article{Jia2016,
  author        = {Changjiang Jia and Yan Cai and Yuen Tak Yu and T.H. Tse},
  title         = {5W+1H pattern: A perspective of systematic mapping studies and a case study on cloud software testing},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {116},
  pages         = {206 - 219},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {A common type of study used by researchers to map out the landscape of a research topic is known as mapping study. Such a study typically begins with an exploratory search on the possible ideas of the research topic, which is often done in an unsystematic manner. Hence, the activity of formulating research questions in mapping studies is ill-defined, rendering it difficult for researchers who are new to the topic. There is a need to guide them kicking off a mapping study of an unfamiliar domain. This paper proposes a 5W+1H pattern to help investigators systematically examine a generic set of dimensions in a mapping study toward the formulation of research questions before identifying, reading, and analyzing sufficient articles of the topic. We have validated the feasibility of our proposal by conducting a case study of a mapping study on cloud software testing, that is, software testing for and on cloud computing platforms. The case study reveals that the 5W+1H pattern can lead investigators to define a set of systematic, generic, and complementary research questions, enabling them to kick off and expedite the mapping study process in a well-defined manner. We also share our experiences and lessons learned from our case study on the use of the 5W+1H pattern in mapping studies.},
  doi           = {https://doi.org/10.1016/j.jss.2015.01.058},
  keywords      = {5W+1H pattern, Cloud software testing, Systematic mapping study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215000370},
}

@Article{Santiago2012,
  author        = {Iván Santiago and Álvaro Jiménez and Juan Manuel Vara and Valeria De Castro and Verónica A. Bollati and Esperanza Marcos},
  title         = {Model-Driven Engineering as a new landscape for traceability management: A systematic literature review},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {12},
  pages         = {1340 - 1356},
  issn          = {0950-5849},
  note          = {Special Section on Software Reliability and Security},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Model-Driven Engineering provides a new landscape for dealing with traceability in software development.
Objective
Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering.
Method
We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments.
Results
Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals.
Conclusion
The evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information.},
  doi           = {https://doi.org/10.1016/j.infsof.2012.07.008},
  keywords      = {Traceability, Model-Driven Engineering, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001346},
}

@Article{1981,
  title         = {Preface},
  journal       = {IFAC Proceedings Volumes},
  year          = {1981},
  volume        = {14},
  number        = {3},
  pages         = {vi},
  issn          = {1474-6670},
  note          = {IFAC/IFIP Workshop on Real Time Programming, Kyoto, Japan, 31 August-2 September 1981},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/S1474-6670(17)63427-X},
  url           = {http://www.sciencedirect.com/science/article/pii/S147466701763427X},
}

@InCollection{Alunni2013,
  author        = {A. Alunni and M. Coolen and I. Foucher and L. Bally-Cuif},
  title         = {Chapter 32 - Neurogenesis in Zebrafish},
  booktitle     = {Patterning and Cell Type Specification in the Developing CNS and PNS},
  publisher     = {Academic Press},
  year          = {2013},
  editor        = {John L.R. Rubenstein and Pasko Rakic},
  pages         = {645 - 677},
  address       = {Oxford},
  isbn          = {978-0-12-397265-1},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-397265-1.00069-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123972651000691},
}

@Article{Karapantazis2009,
  author        = {Stylianos Karapantazis and Fotini-Niovi Pavlidou},
  title         = {VoIP: A comprehensive survey on a promising technology},
  journal       = {Computer Networks},
  year          = {2009},
  volume        = {53},
  number        = {12},
  pages         = {2050 - 2090},
  issn          = {1389-1286},
  __markedentry = {[Juliana:]},
  abstract      = {The Internet has burgeoned into a worldwide information superhighway during the past few years, giving rise to a host of new applications and services. Among them, Voice over IP (VoIP) is the most prominent one. Beginning more as a frolic among computer enthusiasts, VoIP has set off a feeding frenzy in both the industrial and scientific communities and has the potential to radically change telephone communications. In this article, we survey all these aspects that have the greatest impact on the quality of voice communications over IP networks. The survey begins with the merits and demerits of VoIP, followed by the Quality of Service (QoS) requirements that voice imposes and a description of test methods for the assessment of speech quality. We then proceed with a delineation of the issues related to the conversion of analog voice to packets, namely we spell out the details of the most well-known voice codecs, while light is also thrown on voice activity detection and voice packetization. Header compression schemes receive intense scrutiny as well. We also provide an overview of the signaling protocols that are tailored to the needs of VoIP, and we continue with the comparison of the call admission schemes that are geared towards the QoS constraints of VoIP. The pivotal issue of security is then discussed, pointing out potential threats as well as approaches for tackling them. Finally, the survey concludes with a discussion on the feasibility of providing VoIP over challenging satellite links.},
  doi           = {https://doi.org/10.1016/j.comnet.2009.03.010},
  keywords      = {VoIP, IP Telephony, Voice quality, Voice codecs, Signaling protocols, Call admission control, Security},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128609001200},
}

@InCollection{Kuhn2015,
  author        = {D. Richard Kuhn and Renee Bryce and Feng Duan and Laleh Sh. Ghandehari and Yu Lei and Raghu N. Kacker},
  title         = {Chapter One - Combinatorial Testing: Theory and Practice},
  publisher     = {Elsevier},
  year          = {2015},
  editor        = {Atif Memon},
  volume        = {99},
  series        = {Advances in Computers},
  pages         = {1 - 66},
  __markedentry = {[Juliana:]},
  abstract      = {Combinatorial testing has rapidly gained favor among software testers in the past decade as improved algorithms have become available and practical success has been demonstrated. This chapter reviews the theory and application of this method, focusing particularly on research since 2010, with a brief background providing the rationale and development of combinatorial methods for software testing. Significant advances have occurred in algorithm performance, and the critical area of constraint representation and processing. In addition to these foundational topics, we take a look at advances in specialized areas including test suite prioritization, sequence testing, fault localization, the relationship between combinatorial testing and structural coverage, and approaches to very large testing problems.},
  doi           = {https://doi.org/10.1016/bs.adcom.2015.05.003},
  issn          = {0065-2458},
  keywords      = {Algorithms, Combinatorial testing, Constraints, Covering array, Fault localization, Interaction testing, Sequence testing, Software faults, Software testing, Test suite prioritization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245815000352},
}

@Article{Mahmood2005,
  author        = {Sajjad Mahmood and Richard Lai and Yong Soo Kim and Ji Hong Kim and Seok Cheon Park and Hae Suk Oh},
  title         = {A survey of component based system quality assurance and assessment},
  journal       = {Information and Software Technology},
  year          = {2005},
  volume        = {47},
  number        = {10},
  pages         = {693 - 707},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Component Based Software Development (CBSD) is focused on assembling existing components to build a software system, with a potential benefit of delivering quality systems by using quality components. It departs from the conventional software development process in that it is integration centric as opposed to development centric. The quality of a component based system using high quality components does not therefore necessarily guarantee a system of high quality, but depends on the quality of its components, and a framework and integration process used. Hence, techniques and methods for quality assurance and assessment of a component based system would be different from those of the traditional software engineering methodology. It is essential to quantify factors that contribute to the overall quality, for instances, the trade off between cost and quality of a component, analytical techniques and formal methods, and quality attribute definitions and measurements. This paper presents a literature survey of component based system quality assurance and assessment; the areas surveyed include formalism, cost estimation, and assessment and measurement techniques for the following quality attributes: performance, reliability, maintainability and testability. The aim of this survey is to help provide a better understanding of CBSD in these aspects in order to facilitate the realisation of its potential benefits of delivering quality systems.},
  doi           = {https://doi.org/10.1016/j.infsof.2005.03.007},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584905000601},
}

@InCollection{Watson2013,
  author        = {David Watson and Andrew Jones},
  title         = {Chapter 7 - IT Infrastructure},
  booktitle     = {Digital Forensics Processing and Procedures},
  publisher     = {Syngress},
  year          = {2013},
  editor        = {David Watson and Andrew Jones},
  pages         = {233 - 312},
  address       = {Boston},
  isbn          = {978-1-59749-742-8},
  __markedentry = {[Juliana:]},
  abstract      = {This chapter looks at the policies and issues related to the IT infrastructure within the laboratory. It looks at the hardware, the software, and the infrastructure in some detail. It then looks at process management, addressing issues including incident and problem management; change control; Release Management; and configuration, capacity, and service management.},
  doi           = {https://doi.org/10.1016/B978-1-59749-742-8.00007-8},
  keywords      = {process, incident, change, release, hardware, software, capacity, service, management},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781597497428000078},
}

@Article{Chen2011,
  author        = {Lianping Chen and Muhammad Ali Babar},
  title         = {A systematic review of evaluation of variability management approaches in software product lines},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {4},
  pages         = {344 - 362},
  issn          = {0950-5849},
  note          = {Special section: Software Engineering track of the 24th Annual Symposium on Applied Computing},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated.
Objective
The objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches.
Method
We carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007.
Results
We selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects.
Conclusion
The findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
  doi           = {https://doi.org/10.1016/j.infsof.2010.12.006},
  keywords      = {Software product line, Variability management, Systematic literature reviews, Empirical studies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584910002223},
}

@Article{Carette2011,
  author        = {Jacques Carette and Oleg Kiselyov},
  title         = {Multi-stage programming with functors and monads: Eliminating abstraction overhead from generic code},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {5},
  pages         = {349 - 375},
  issn          = {0167-6423},
  note          = {Special Issue on Generative Programming and Component Engineering (Selected Papers from GPCE 2004/2005)},
  __markedentry = {[Juliana:]},
  abstract      = {We use multi-stage programming, monads and Ocaml’s advanced module system to demonstrate how to eliminate all abstraction overhead from generic programs, while avoiding any inspection of the resulting code. We demonstrate this clearly with Gaussian Elimination as a representative family of symbolic and numeric algorithms. We parameterize our code to a great extent–over domain, input and permutation matrix representations, determinant and rank tracking, pivoting policies, result types, etc.–at no run-time cost. Because the resulting code is generated just right and not changed afterward, MetaOCaml guarantees that the generated code is well-typed. We further demonstrate that various abstraction parameters (aspects) can be made orthogonal and compositional, even in the presence of name-generation for temporaries, and “interleaving” of aspects. We also show how to encode some domain-specific knowledge so that “clearly wrong” compositions can be rejected at or before generation time, rather than during the compilation or running of the generated code.},
  doi           = {https://doi.org/10.1016/j.scico.2008.09.008},
  keywords      = {MetaOCaml, Linear algebra, Genericity, Generative, Staging, Functor, Symbolic},
  url           = {http://www.sciencedirect.com/science/article/pii/S016764230800110X},
}

@Article{Martinez2017,
  author        = {Salvador Martínez and Valerio Cosentino and Jordi Cabot},
  title         = {Model-based analysis of Java EE web security misconfigurations},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2017},
  volume        = {49},
  pages         = {36 - 61},
  issn          = {1477-8424},
  __markedentry = {[Juliana:]},
  abstract      = {The Java EE framework, a popular technology of choice for the development of web applications, provides developers with the means to define access-control policies to protect application resources from unauthorized disclosures and manipulations. Unfortunately, the definition and manipulation of such security policies remains a complex and error prone task, requiring expert-level knowledge on the syntax and semantics of the Java EE access-control mechanisms. Thus, misconfigurations that may lead to unintentional security and/or availability problems can be easily introduced. In response to this problem, we present a (model-based) reverse engineering approach that automatically evaluates a set of security properties on reverse engineered Java EE security configurations, helping to detect the presence of anomalies. We evaluate the efficacy and pertinence of our approach by applying our prototype tool on a sample of real Java EE applications extracted from GitHub.},
  doi           = {https://doi.org/10.1016/j.cl.2017.02.001},
  keywords      = {Model-driven engineering, Security, Reverse-engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842416301348},
}

@Article{Lopez-Herrejon2015,
  author        = {Roberto E. Lopez-Herrejon and Lukas Linsbauer and Alexander Egyed},
  title         = {A systematic mapping study of search-based software engineering for software product lines},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {61},
  pages         = {33 - 51},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Search-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces.
Objective
The main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published.
Method
A systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014.
Results
The most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation.
Conclusions
Our study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
  doi           = {https://doi.org/10.1016/j.infsof.2015.01.008},
  keywords      = {Software product line, Systematic mapping study, Search based software engineering, Evolutionary algorithm, Metaheuristics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915000166},
}

@Article{Hoey2009,
  author        = {Timothy Hoey and Wan-Ching Yen and Fumiko Axelrod and Jesspreet Basi and Lucas Donigian and Scott Dylla and Maureen Fitch-Bruhns and Sasha Lazetic and In-Kyung Park and Aaron Sato and Sanjeev Satyal and Xinhao Wang and Michael F. Clarke and John Lewicki and Austin Gurney},
  title         = {DLL4 Blockade Inhibits Tumor Growth and Reduces Tumor-Initiating Cell Frequency},
  journal       = {Cell Stem Cell},
  year          = {2009},
  volume        = {5},
  number        = {2},
  pages         = {168 - 177},
  issn          = {1934-5909},
  __markedentry = {[Juliana:]},
  abstract      = {Summary
Previous studies have shown that blocking DLL4 signaling reduced tumor growth by disrupting productive angiogenesis. We developed selective anti-human and anti-mouse DLL4 antibodies to dissect the mechanisms involved by analyzing the contributions of selectively targeting DLL4 in the tumor or in the host vasculature and stroma in xenograft models derived from primary human tumors. We found that each antibody inhibited tumor growth and that the combination of the two antibodies was more effective than either alone. Treatment with anti-human DLL4 inhibited the expression of Notch target genes and reduced proliferation of tumor cells. Furthermore, we found that specifically inhibiting human DLL4 in the tumor, either alone or in combination with the chemotherapeutic agent irinotecan, reduced cancer stem cell frequency, as shown by flow cytometric and in vivo tumorigenicity studies.},
  doi           = {https://doi.org/10.1016/j.stem.2009.05.019},
  keywords      = {STEMCELL, CELLCYCLE},
  url           = {http://www.sciencedirect.com/science/article/pii/S1934590909002288},
}

@Article{Badros2000,
  author        = {Greg J Badros},
  title         = {JavaML: a markup language for Java source code},
  journal       = {Computer Networks},
  year          = {2000},
  volume        = {33},
  number        = {1},
  pages         = {159 - 177},
  issn          = {1389-1286},
  __markedentry = {[Juliana:]},
  abstract      = {The classical plain-text representation of source code is convenient for programmers but requires parsing to uncover the deep structure of the program. While sophisticated software tools parse source code to gain access to the program's structure, many lightweight programming aids such as grep rely instead on only the lexical structure of source code. I describe a new XML application that provides an alternative representation of Java source code. This XML-based representation, called JavaML, is more natural for tools and permits easy specification of numerous software-engineering analyses by leveraging the abundance of XML tools and techniques. A robust converter built with the Jikes Java compiler framework translates from the classical Java source code representation to JavaML, and an XSLT stylesheet converts from JavaML back into the classical textual form.},
  doi           = {https://doi.org/10.1016/S1389-1286(00)00037-2},
  keywords      = {Java, XML, Abstract syntax tree representation, Software-engineering analysis, Jikes compiler},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128600000372},
}

@Article{Thuem2014,
  author        = {Thomas Thüm and Christian Kästner and Fabian Benduhn and Jens Meinicke and Gunter Saake and Thomas Leich},
  title         = {FeatureIDE: An extensible framework for feature-oriented software development},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {79},
  pages         = {70 - 85},
  issn          = {0167-6423},
  note          = {Experimental Software and Toolkits (EST 4): A special issue of the Workshop on Academic Software Development Tools and Techniques (WASDeTT-3 2010)},
  __markedentry = {[Juliana:]},
  abstract      = {FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.002},
  keywords      = {Feature-oriented software development, Software product lines, Feature modeling, Feature-oriented programming, Aspect-oriented programming, Delta-oriented programming, Preprocessors, Tool support},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001128},
}

@Article{Ebeid2018,
  author        = {Emad Ebeid and Martin Skriver and Kristian Husum Terkildsen and Kjeld Jensen and Ulrik Pagh Schultz},
  title         = {A survey of Open-Source UAV flight controllers and flight simulators},
  journal       = {Microprocessors and Microsystems},
  year          = {2018},
  volume        = {61},
  pages         = {11 - 20},
  issn          = {0141-9331},
  __markedentry = {[Juliana:]},
  abstract      = {The current disruptive innovation in civilian drone (UAV) applications has led to an increased need for research and development in UAV technology. The key challenges currently being addressed are related to UAV platform properties such as functionality, reliability, fault tolerance, and endurance, which are all tightly linked to the UAV flight controller hardware and software. The lack of standardization of flight controller architectures and the use of proprietary closed-source flight controllers on many UAV platforms, however, complicates this work: solutions developed for one flight controller may be difficult to port to another without substantial extra development and testing. Using open-source flight controllers mitigates some of these challenges and enables other researchers to validate and build upon existing research. This paper presents a survey of the publicly available open-source drone platform elements that can be used for research and development. The survey covers open-source hardware, software, and simulation drone platforms and compares their main features.},
  doi           = {https://doi.org/10.1016/j.micpro.2018.05.002},
  keywords      = {Unmanned Aerial Vehicle (UAV), Drones, Flight controllers, Drone simulators, Open platforms, Survey},
  url           = {http://www.sciencedirect.com/science/article/pii/S0141933118300930},
}

@InCollection{Morrow2003,
  author        = {Monique Morrow and Kateel Vijayananda},
  title         = {Chapter 8 - Case Studies},
  booktitle     = {Developing IP-Based Services},
  publisher     = {Morgan Kaufmann},
  year          = {2003},
  editor        = {Monique Morrow and Kateel Vijayananda},
  series        = {The Morgan Kaufmann Series in Networking},
  pages         = {221 - 277},
  address       = {San Francisco},
  __markedentry = {[Juliana:]},
  abstract      = {Publisher Summary
This chapter outlines two conceptual case studies as well as a real world case study, which together demonstrate how Internet Protocol (IP)-based services can be developed and deployed by service providers to generate revenue. The case studies emphasize the role of operations support system (OSS), architecture in implementing IP-based services. The two conceptual case studies are composites of real experiences of various service providers. The combination of these two conceptual studies with a real-world example demonstrates the full impact of IP in both business and technical aspects. The conceptual case studies present scenarios for both Greenfield and incumbent service providers. The conceptual case studies and the Meta Telecom experience have drawn a picture of the business and technical aspects associated with IP-based service creation. There are common success factors in all three examples that include upper-management support and vision to use IP technology to transform a company, the redefinition of business–engineering processes for company-wide IP-based service development and deployment, an emphasis on OSS architecture as the base for service creation, and, finally, the positive interaction between customers, service providers, and vendors.},
  doi           = {https://doi.org/10.1016/B978-155860779-8/50010-5},
  issn          = {18759351},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781558607798500105},
}

@Article{Barrea2017,
  author        = {Luigi Barrea and Carolina Di Somma and Paolo Emidio Macchia and Andrea Falco and Maria Cristina Savanelli and Francesco Orio and Annamaria Colao and Silvia Savastano},
  title         = {Influence of nutrition on somatotropic axis: Milk consumption in adult individuals with moderate-severe obesity},
  journal       = {Clinical Nutrition},
  year          = {2017},
  volume        = {36},
  number        = {1},
  pages         = {293 - 301},
  issn          = {0261-5614},
  __markedentry = {[Juliana:]},
  abstract      = {Summary
Background & aims
Nutrition is the major environmental factor that influences the risk of developing pathologies, such as obesity. Although a number of recent reviews pinpoint a protective effects of milk on body weight and obesity related co-morbidities, an inaccurate estimate of milk might contribute to hamper its beneficial effects on health outcomes. Seven-day food records provide prospective food intake data, reducing recall bias and providing extra details about specific food items. Milk intake stimulates the somatotropic axis at multiple levels by increasing both growth hormone (GH) and insulin-like growth factor-1 (IGF-1) secretion. On the other hand, obesity is associated with reduced spontaneous and stimulated GH secretion and basal IGF-1 levels. Aim of this study was to evaluate the milk consumption by using the 7-days food record in obese individuals and to investigate the association between milk intake and GH secretory status in these subjects.
Methods
Cross-sectional observational study carried out on 281 adult individuals (200 women and 81 men, aged 18–74 years) with moderate-severe obesity (BMI 35.2–69.4 kg/m2). Baseline milk intake data were collected using a 7 day food record. Anthropometric measurements and biochemical profile were determined. The GH/IGF-1 axis was evaluated by peak GH response after GHRH + ARGININE and IGF-1 standard deviation score (SDS).
Results
The majority of individuals (72.2%) reported consuming milk; 250 mL low-fat milk was the most frequently serving of milk consumed, while no subjects reported to consume whole milk. Milk consumers vs no milk consumers presented the better anthropometric measurements and metabolic profile. At the bivariate proportional odds ratio model, after adjusting for BMI, age and gender, milk consumption was associated the better GH status (OR = 0.60; p < 0.001). Among milk consumers, subjects consuming 250 mL reduced-fat milk vs 250 mL low-fat milk presented the better anthropometric measurements and metabolic profile. At the bivariate proportional odds ratio model, after adjusting for BMI, age and gender, the consume of 250 mL reduced-fat milk was associated better GH status (OR = 0.54; p = 0.003).
Conclusions
A novel positive association between milk consumption, GH status, and metabolic profile in obese individuals was evidenced. Regardless of the pathogenetic mechanisms, this novel association might be relevant in a context where commonly obese individuals skip breakfast, and suggests the need of a growing cooperation between Nutritionists and Endocrinologists in the management of the obese patients.},
  doi           = {https://doi.org/10.1016/j.clnu.2015.12.007},
  keywords      = {Environmental factors, Milk consumption, Nutrition, Somatotropic axis, Obesity},
  url           = {http://www.sciencedirect.com/science/article/pii/S0261561415003453},
}

@Article{Hu2016,
  author        = {Fei Hu and Yu Lu and Athanasios V. Vasilakos and Qi Hao and Rui Ma and Yogendra Patil and Ting Zhang and Jiang Lu and Xin Li and Neal N. Xiong},
  title         = {Robust Cyber–Physical Systems: Concept, models, and implementation},
  journal       = {Future Generation Computer Systems},
  year          = {2016},
  volume        = {56},
  pages         = {449 - 475},
  issn          = {0167-739X},
  __markedentry = {[Juliana:]},
  abstract      = {In this paper we comprehensively survey the concept and strategies for building a resilient and integrated cyber–physical system (CPS). Here resilience refers to a 3S-oriented design, that is, stability, security, and systematicness: Stability means the CPS can achieve a stable sensing-actuation close-loop control even though the inputs (sensing data) have noise or attacks; Security means that the system can overcome the cyber–physical interaction attacks; and Systematicness means that the system has a seamless integration of sensors and actuators. We will also explain the CPS modeling issues since they serve as the basics of 3S design. We will use two detailed examples from our achieved projects to explain how to achieve arobust, systematic CPS design: Case study 1 is on the design of a rehabilitation system with cyber (sensors) and physical (robots) integration. Case Study 2 is on the implantable medical device design. It illustrates the nature of CPS security principle. The dominant feature of this survey is that it has both principle discussions and practical cyber–physical coupling design.},
  doi           = {https://doi.org/10.1016/j.future.2015.06.006},
  keywords      = {Cyber–Physical Systems, Stability, Security, Sensors and actuators, Survey},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X15002071},
}

@InCollection{Ahmad2002,
  title         = {Index},
  booktitle     = {Hack Proofing Your Network (Second Edition)},
  publisher     = {Syngress},
  year          = {2002},
  editor        = {David R. Mirza Ahmad and Ido Dubrawsky and Hal Flynn and Joseph “Kingpin” Grand and Robert Graham and Norris L. Johnson and Dan “Effugas” Kaminsky and F. William Lynch and Steve W. Manzuik and Ryan Permeh and Ken Pfeil and Rain Forest Puppy},
  pages         = {767 - 789},
  address       = {Burlington},
  edition       = {Second Edition},
  isbn          = {978-1-928994-70-1},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-192899470-1/50022-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781928994701500227},
}

@InCollection{Betz2011,
  title         = {Appendix A - Extended Definitions for the IT Architectural Catalogs},
  booktitle     = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
  publisher     = {Morgan Kaufmann},
  year          = {2011},
  editor        = {Charles T. Betz},
  pages         = {315 - 408},
  address       = {Boston},
  edition       = {Second Edition},
  isbn          = {978-0-12-385017-1},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-12-385017-1.00005-5},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123850171000055},
}

@InCollection{Jackler2005,
  title         = {Index},
  booktitle     = {Neurotology (Second Edition)},
  publisher     = {Mosby},
  year          = {2005},
  editor        = {Robert K. Jackler and Derald E. Brackmann},
  pages         = {1363 - 1411},
  address       = {Philadelphia},
  edition       = {Second Edition},
  isbn          = {978-0-323-01830-2},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-323-01830-2.50092-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780323018302500924},
}

@InCollection{Bidgoli2004,
  title         = {Subject Index},
  booktitle     = {Encyclopedia of Information Systems},
  publisher     = {Elsevier},
  year          = {2004},
  editor        = {Hossein Bidgoli},
  pages         = {703 - 807},
  address       = {New York},
  isbn          = {978-0-12-227240-0},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B0-12-227240-4/00202-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B0122272404002021},
}

@InCollection{Noergaard2013,
  booktitle     = {Embedded Systems Architecture (Second Edition)},
  publisher     = {Newnes},
  year          = {2013},
  editor        = {Tammy Noergaard},
  pages         = {623 - 638},
  edition       = {Second Edition},
  isbn          = {978-0-12-382196-6},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-12-382196-6.00026-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123821966000261},
}

@InCollection{Kitchin2009,
  title         = {Subject Index},
  booktitle     = {International Encyclopedia of Human Geography},
  publisher     = {Elsevier},
  year          = {2009},
  editor        = {Rob Kitchin and Nigel Thrift},
  pages         = {289 - 586},
  address       = {Oxford},
  isbn          = {978-0-08-044910-4},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-008044910-4.09007-6},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780080449104090076},
}

@InCollection{Betz2011a,
  author        = {Charles T. Betz},
  title         = {Chapter 2 - Architecture Approach},
  booktitle     = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
  publisher     = {Morgan Kaufmann},
  year          = {2011},
  editor        = {Charles T. Betz},
  pages         = {33 - 149},
  address       = {Boston},
  edition       = {Second Edition},
  isbn          = {978-0-12-385017-1},
  __markedentry = {[Juliana:6]},
  abstract      = {Publisher Summary
This chapter establishes a series of related principles that both support the architectural analysis and serve as high-level scoping and requirements for the value system. The chapter attempts to build a new kind of process framework for large scale IT management in the enterprise. The concept of an overall IT value chain and the four major, long-lived value streams of large-scale enterprise information technology (application services, infrastructure services, IT assets, and IT technology products) are discussed in the chapter. The shorter IT processes are also elaborated. The process architecture is represented as a set of larger-grained lifecycles that are acted upon by a series of well- understood IT processes with clear beginnings and endings. Value streams and processes are the key means by which value is delivered in IT, as in any business. A relentless attention to reducing unnecessary activity along these activity sequences is essential to increasing value. Synchronization points, dependencies, and critical paths would all be of interest, and constitute the foundation of IT value stream analysis. Much in IT is related to nonprocess concepts: quality concepts such as capacity and availability, or the actual functional areas that own the various lifecycles and processes. An overall systems architecture is presented for the business of IT, discussing fundamentals of enterprise application architecture and the major classes of systems encountered in IT management. Summary matrices are introduced in this discussion as a means of showing how the various architectural elements may interact.},
  doi           = {https://doi.org/10.1016/B978-0-12-385017-1.00002-X},
  url           = {http://www.sciencedirect.com/science/article/pii/B978012385017100002X},
}

@InCollection{Sharpe1984,
  title         = {SECTION 13 - Directory of UK Companies, Societies, Institutes, and Organizations},
  booktitle     = {Quality Technology Handbook (Fourth Edition)},
  publisher     = {Butterworth-Heinemann},
  year          = {1984},
  editor        = {R S Sharpe and J West and D S Dean and D A Tyler and H A Cole},
  pages         = {233 - 379},
  edition       = {Fourth Edition},
  isbn          = {978-0-408-01331-4},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-408-01331-4.50015-5},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780408013314500155},
}

@InCollection{Fisher2005,
  author        = {Joseph A. Fisher and Paolo Faraboschi and Cliff Young},
  title         = {Chapter 10 - Application Design and Customization},
  booktitle     = {Embedded Computing},
  publisher     = {Morgan Kaufmann},
  year          = {2005},
  editor        = {Joseph A. Fisher and Paolo Faraboschi and Cliff Young},
  pages         = {443 - 492},
  address       = {San Francisco},
  isbn          = {978-1-55860-766-8},
  __markedentry = {[Juliana:6]},
  abstract      = {Publisher Summary
One of the most important differentiators between embedded and general-purpose computing is that embedded systems typically run one single application or set of applications exclusively. This affects many aspects of computing, including code development. Although in most respects, embedded application development resembles general code development, special languages can play a role when the application under development will be embedded in a product because of different lifetime, user community, and underlying hardware considerations. Similarly, the process of making code perform can well change. Techniques that might be unacceptable in the general-purpose world can be almost essential in the embedded world, in which a product may be of no use if it cannot meet a speed or power requirement. Embedded computing is unique in that the hardware too is built to run a single application, and thus methodologies for customization become relevant. This topic is part of the larger area of hardware/software codesign. Embedded designers face issues at the hardware/software boundary that are closed to designers of general-purpose systems. Customized hardware presents an opportunity to vastly speed up individual applications. This opportunity has led to a lively research area and the foundation of more than a few startup companies.},
  doi           = {https://doi.org/10.1016/B978-155860766-8/50014-X},
  url           = {http://www.sciencedirect.com/science/article/pii/B978155860766850014X},
}

@Article{2005,
  title         = {Posters displayed on Monday 9 May 2005},
  journal       = {Clinica Chimica Acta},
  year          = {2005},
  volume        = {355},
  pages         = {S81 - S201},
  issn          = {0009-8981},
  note          = {Focus on the Patient: 16th IFCC - FESCC European Congress of Clinical Biochemistry and Laboratory Medicine},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/j.cccn.2005.03.006},
  url           = {http://www.sciencedirect.com/science/article/pii/S0009898105001233},
}

@InCollection{Conrad2012,
  booktitle     = {CISSP Study Guide (Second Edition)},
  publisher     = {Syngress},
  year          = {2012},
  editor        = {Eric Conrad and Seth Misenar and Joshua Feldman},
  pages         = {513 - 547},
  address       = {Boston},
  edition       = {Second Edition},
  isbn          = {978-1-59749-961-3},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-1-59749-961-3.09985-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781597499613099854},
}

@InCollection{Meyers2003,
  title         = {Subject Index},
  booktitle     = {Encyclopedia of Physical Science and Technology (Third Edition)},
  publisher     = {Academic Press},
  year          = {2003},
  editor        = {Robert A. Meyers},
  pages         = {1 - 344},
  address       = {New York},
  edition       = {Third Edition},
  isbn          = {978-0-12-227410-7},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B0-12-227410-5/09009-8},
  url           = {http://www.sciencedirect.com/science/article/pii/B0122274105090098},
}

@Article{2002,
  title         = {Subject index},
  journal       = {Journal of the American College of Cardiology},
  year          = {2002},
  volume        = {39},
  pages         = {501 - 575},
  issn          = {0735-1097},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0735-1097(02)82076-9},
  url           = {http://www.sciencedirect.com/science/article/pii/S0735109702820769},
}

@InCollection{Conrad2010,
  booktitle     = {CISSP Study Guide},
  publisher     = {Syngress},
  year          = {2010},
  editor        = {Eric Conrad and Seth Misenar and Joshua Feldman},
  pages         = {489 - 523},
  address       = {Boston},
  isbn          = {978-1-59749-563-9},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-1-59749-563-9.00020-2},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781597495639000202},
}

@Article{1992,
  title         = {Genomes and evolution},
  journal       = {Current Opinion in Genetics \& Development},
  year          = {1992},
  volume        = {2},
  number        = {6},
  pages         = {947 - 986},
  issn          = {0959-437X},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0959-437X(05)80122-4},
  url           = {http://www.sciencedirect.com/science/article/pii/S0959437X05801224},
}

@Article{2007,
  title         = {Society for Surgery of the Alimentary Tract (SSAT) Abstracts},
  journal       = {Gastroenterology},
  year          = {2007},
  volume        = {132},
  number        = {4, Supplement 2},
  pages         = {A-832 - A-894},
  issn          = {0016-5085},
  note          = {Annual Abstract Supplement},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0016-5085(07)60011-0},
  url           = {http://www.sciencedirect.com/science/article/pii/S0016508507600110},
}

@InCollection{Zelkowitz1999a,
  title         = {Subject Index},
  booktitle     = {Index Part I Subject Index Volumes 1-49},
  publisher     = {Elsevier},
  year          = {1999},
  editor        = {Marvin V. Zelkowitz},
  volume        = {50},
  series        = {Advances in Computers},
  pages         = {1 - 420},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0065-2458(08)60717-2},
  issn          = {0065-2458},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245808607172},
}

@InCollection{Conrad2016a,
  title         = {Glossary},
  booktitle     = {CISSP Study Guide (Third Edition)},
  publisher     = {Syngress},
  year          = {2016},
  editor        = {Eric Conrad and Seth Misenar and Joshua Feldman},
  pages         = {521 - 557},
  address       = {Boston},
  edition       = {Third Edition},
  isbn          = {978-0-12-802437-9},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-12-802437-9.00011-4},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128024379000114},
}

@Article{2002a,
  title         = {Joint Meeting of the European Pancreatic Club (EPC) and the International Association of Pancreatology (IAP)},
  journal       = {Pancreatology},
  year          = {2002},
  volume        = {2},
  number        = {3},
  pages         = {217 - 361},
  issn          = {1424-3903},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1159/000058036},
  url           = {http://www.sciencedirect.com/science/article/pii/S1424390302800047},
}

@Article{Fabry2016,
  author        = {Johan Fabry and Coen De Roover and Carlos Noguera and Steffen Zschaler and Awais Rashid and Viviane Jonckers},
  title         = {AspectJ code analysis and verification with GASR},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {528 - 544},
  issn          = {0164-1212},
  __markedentry = {[Juliana:6]},
  abstract      = {Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked.},
  doi           = {https://doi.org/10.1016/j.jss.2016.04.014},
  keywords      = {Aspect oriented programming, Source code analysis, Logic program querying},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300279},
}

@Article{2005a,
  title         = {Posters displayed on Tuesday 10 May 2005},
  journal       = {Clinica Chimica Acta},
  year          = {2005},
  volume        = {355},
  pages         = {S203 - S318},
  issn          = {0009-8981},
  note          = {Focus on the Patient: 16th IFCC - FESCC European Congress of Clinical Biochemistry and Laboratory Medicine},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/j.cccn.2005.03.005},
  url           = {http://www.sciencedirect.com/science/article/pii/S0009898105001221},
}

@Article{2016,
  title         = {Poster Abstracts},
  journal       = {Neuromuscular Disorders},
  year          = {2016},
  volume        = {26},
  pages         = {S4 - S42},
  issn          = {0960-8966},
  note          = {Abstracts of the UK Neuromuscular Translational Research Conference 2016},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/S0960-8966(16)30108-0},
  url           = {http://www.sciencedirect.com/science/article/pii/S0960896616301080},
}

@Article{2005b,
  title         = {Society for Development Biology 64th Annual Meeting},
  journal       = {Developmental Biology},
  year          = {2005},
  volume        = {283},
  number        = {2},
  pages         = {575 - 707},
  issn          = {0012-1606},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/j.ydbio.2005.04.040},
  url           = {http://www.sciencedirect.com/science/article/pii/S0012160605002940},
}

@InCollection{MISRA1992,
  title         = {4 - Reliability Prediction from Stress-Strength Models},
  booktitle     = {Reliability Analysis and Prediction},
  publisher     = {Elsevier},
  year          = {1992},
  editor        = {Krishna B. MISRA},
  volume        = {15},
  series        = {Fundamental Studies in Engineering},
  pages         = {247 - 316},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/B978-0-444-89606-3.50011-7},
  issn          = {1572-4433},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780444896063500117},
}

@Article{2013,
  title         = {Abstracts from the XXI World Congress of the International Society for Heart ResearchJune 30-July 4, 20132013, San Diego, California, USA},
  journal       = {Journal of Molecular and Cellular Cardiology},
  year          = {2013},
  volume        = {65},
  pages         = {S1 - S162},
  issn          = {0022-2828},
  note          = {Abstracts from the XXI World Congress of the International Society for Heart Research, June 30-July 4, 2013, 2013, San Diego, California, USA},
  __markedentry = {[Juliana:6]},
  doi           = {https://doi.org/10.1016/j.yjmcc.2013.10.011},
  url           = {http://www.sciencedirect.com/science/article/pii/S0022282813003106},
}

@InCollection{Halstead1979,
  author        = {M.H. Halstead},
  title         = {Advances in Software Science},
  publisher     = {Elsevier},
  year          = {1979},
  editor        = {Marshall C. Yovits},
  volume        = {18},
  series        = {Advances in Computers},
  pages         = {119 - 172},
  __markedentry = {[Juliana:6]},
  abstract      = {Publisher Summary
The chapter presents the overview of the present status of software science. Software science is an intellectually exciting discipline currently undergoing rapid development. Software science can be treated as a proper basis or foundation for the field of software engineering, but not as synonymous with it. This is not unlike other branches, in that the engineering usually preceded and indeed stimulated the development of the underlying science. It is interesting to note, however, that it is only after the development of thermodynamics for power engineering, electrodynamics for electrical engineering, or statics, dynamics, and strength-of-materials for mechanical engineering that those branches could be considered quasi complete, highly competent, and dependable engineering disciplines. Such a goal for software engineering clearly motivates much of the work in software science. Despite the fact that there are no theorems, and perhaps never can be any, in the field of software science, one basic attribute shared by the equations in this field has become quite noticeable. This is the total and complete lack of arbitrary constants or unknown coefficients among the basic equations. It cannot yet be predicted what impact this discipline may be expected to have on the field of software engineering, or of computer programming in general in the future, but perhaps its greatest impact may result from one conclusion that seems inescapable. This conclusion is that natural laws govern language and the mental activity of using it far more strictly than previously recognized.},
  doi           = {https://doi.org/10.1016/S0065-2458(08)60583-5},
  issn          = {0065-2458},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245808605835},
}

@Comment{jabref-meta: databaseType:bibtex;}
