@article{OTADUY2017212,
title = {User acceptance testing for Agile-developed web-based applications: Empowering customers through wikis and mind maps},
journal = {Journal of Systems and Software},
volume = {133},
pages = {212-229},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S016412121730002X},
author = {I. Otaduy and O. Diaz},
keywords = {Agile development, User acceptance testing, Test automation},
abstract = {User Acceptance Testing (UAT) involves validating software in a real setting by the intended audience. The aim is not so much to check the defined requirements but to ensure that the software satisfies the customer’s needs. Agile methodologies put stringent demands on UAT, if only for the frequency at which it needs to be conducted due to the iterative development of small product releases. In this setting, traditional in-person meetings might not scale up well. Complementary ways are needed to reduce the costs of developer-customer collaboration during UAT. This work introduces a wiki-based approach where customers and developers asynchronously collaborate: developers set the UAT scaffolding that will later shepherd customers when testing. To facilitate understanding, mind maps are used to represent UAT sessions. To facilitate engagement, a popular mind map editor, FreeMind, is turned into an editor for FitNesse, the wiki engine in which these ideas are borne out. The approach is evaluated through a case study involving three real customers. First evaluations are promising. Though at different levels of completeness, the three customers were able to complete a UAT. Customers valued asynchronicity, mind map structuredness, and the transparent generation of documentation out of the UAT session.}
}
@article{TICHY2017159,
title = {Rapid Continuous Software Engineering},
journal = {Journal of Systems and Software},
volume = {133},
pages = {159},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217301887},
author = {Matthias Tichy and Michael Goedicke and Jan Bosch and Brian Fitzgerald}
}
@article{BRIENZA2014124,
title = {3T diffusion tensor imaging and electroneurography of peripheral nerve: A morphofunctional analysis in carpal tunnel syndrome},
journal = {Journal of Neuroradiology},
volume = {41},
number = {2},
pages = {124-130},
year = {2014},
issn = {0150-9861},
doi = {https://doi.org/10.1016/j.neurad.2013.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0150986113000540},
author = {Marianna Brienza and Francesco Pujia and M. Chiara Colaiacomo and M. Grazia Anastasio and Francesco Pierelli and Claudio {Di Biasi} and Chiara Andreoli and Gianfranco Gualdi and Gabriele O.R. Valente},
keywords = {Diffusion tensor imaging, Median nerve, Fractional anisotropy, Apparent diffusion coefficient, Electroneurography},
abstract = {Summary
Objective
The aim of the study was to assess the diagnostic potential of diffusion tensor imaging (DTI) for pathologies of the peripheral nervous system (PNS) through clinical, electrophysiological and morphological evaluation of the median nerve.
Methods
The present work was a multilevel prospective study involving 30 subjects, 15 of whom had carpal tunnel syndrome (CTS) and 15 healthy controls. All subjects underwent clinical evaluation through administration of the Boston Carpal Tunnel Questionnaire (BCTQ), electroneurography (ENG), 3-Tesla magnetic resonance imaging with DTI, and calculation of fractional anisotropy (FA) and the apparent diffusion coefficient (ADC) at the flexor retinaculum. Tractography was also performed for three-dimensional reconstruction of the route of the median nerve through the carpal tunnel. The degree of functional impairment was compared with the anatomical damage to the median nerve according to ENG and DTI.
Results
FA and ADC were significantly correlated with ENG parameters of CTS and BCTQ data. Mean FA and ADC values in the CTS patients were 0.359±0.06 and 1.866±0.050×10−3mm2/s, respectively, vs 0.59±0.014 and 1.395±0.035×10−3mm2/s, respectively, in the controls. FA was decreased and ADC increased in patients with CTS compared with healthy controls (P<0.05).
Conclusion
DTI parameters were clearly confirmed by both clinical and ENG data and, therefore, may be used for the diagnosis of CTS.}
}
@incollection{BIALY201739,
title = {3 - Software Engineering for Model-Based Development by Domain Experts},
editor = {Edward Griffor},
booktitle = {Handbook of System Safety and Security},
publisher = {Syngress},
address = {Boston},
pages = {39-64},
year = {2017},
isbn = {978-0-12-803773-7},
doi = {https://doi.org/10.1016/B978-0-12-803773-7.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037737000036},
author = {M. Bialy and V. Pantelic and J. Jaskolka and A. Schaap and L. Patcas and M. Lawford and A. Wassyng},
keywords = {Software engineering, domain experts, functional safety, embedded software, model-based development, Simulink, automotive},
abstract = {Model-Based Development (MBD) has been gaining traction in the development of embedded software in many industries, especially in safety-critical domains. The models are typically described using domain-specific languages and tools that are readily accessible to the domain experts. Consequently, domain experts, despite not having formal software engineering training, find themselves creating models (designs) from which code is generated, thus effectively contributing to the design and coding activities of software development. This new role for domain experts as software developers can have a direct impact on the system safety if the domain experts do not follow software engineering best practices. In this chapter, we describe our experiences as software engineers in multiyear collaborations with domain experts from the automotive industry, who are developing embedded software with the MBD approach. We provide guidelines that strengthen the collaboration between domain experts and software engineers and improve the quality, and hence safety, of embedded software systems developed using MBD. We clarify the role of some of the most commonly used software engineering principles and artefacts, while also addressing issues and misconceptions encountered in adopting software engineering practices in MBD. Although this chapter focuses on the MBD of automotive embedded software in Matlab Simulink, the guidelines we provide are applicable to the MBD of software in general.}
}
@article{MATALONGA2022106937,
title = {Alternatives for testing of context-aware software systems in non-academic settings: results from a Rapid Review},
journal = {Information and Software Technology},
volume = {149},
pages = {106937},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106937},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000878},
author = {Santiago Matalonga and Domenico Amalfitano and Andrea Doreste and Anna Rita Fasolino and Guilherme Horta Travassos},
keywords = {Context-aware software systems, Software testing, Rapid review, Contemporary software systems},
abstract = {Context
Context-awareness challenges the engineering of contemporary software systems and jeopardizes their testing. The variation of context represents a relevant behavior that deepens the limitations of available software testing practices and technologies. However, such software systems are mainstream. Therefore, researchers in non-academic settings also face challenges when developing and testing contemporary software systems.
Objective
To understand how researchers deal with the variation of context when testing context-aware software systems developed in non-academic settings.
Method
To undertake a secondary study (Rapid Review) to uncover the necessary evidence from primary sources describing the testing of context-aware software systems outside academia.
Results
The current testing initiatives in non-academic settings aim to generate or improve test suites that can deal with the context variation and the sheer volume of test input possibilities. They mostly rely on modeling the systems' dynamic behavior and increasing computing resources to generate test inputs to achieve this. We found no evidence of test results aiming at managing context variation through the testing lifecycle process.
Conclusions
So far, the identified testing initiatives and strategies are not ready for mainstream adoption. They are all domain-specific, and while the ideas and approaches can be reproduced in distinct settings, the technologies are to be re-engineered and tailored to the context-awareness of contemporary software systems in different problem domains. Further and joint investigations in academia and experiences in non-academic settings can evolve the body of knowledge regarding the testing of contemporary software systems in the field.}
}
@incollection{UTTING201653,
title = {Chapter Two - Recent Advances in Model-Based Testing},
editor = {Atif Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {101},
pages = {53-120},
year = {2016},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2015.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0065245815000650},
author = {Mark Utting and Bruno Legeard and Fabrice Bouquet and Elizabeta Fourneret and Fabien Peureux and Alexandre Vernotte},
keywords = {Model-based testing, Modeling approaches, Test generation Technology, Security testing},
abstract = {This chapter gives an overview of the field of model-based testing (MBT), particularly the recent advances in the last decade. It gives a summary of the MBT process, the modeling languages that are currently used by the various communities who practice MBT, the technologies used to generate tests from models, and discusses best practices, such as traceability between models and tests. It also briefly describes several findings from a recent survey of MBT users in industry, outlines the increasingly popular use of MBT for security testing, and discusses future challenges for MBT.}
}
@article{GRAUPP2011175,
title = {Association of genetic variants in the two isoforms of 5α-reductase, SRD5A1 and SRD5A2, in lean patients with polycystic ovary syndrome},
journal = {European Journal of Obstetrics & Gynecology and Reproductive Biology},
volume = {157},
number = {2},
pages = {175-179},
year = {2011},
issn = {0301-2115},
doi = {https://doi.org/10.1016/j.ejogrb.2011.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S0301211511001849},
author = {M. Graupp and E. Wehr and N. Schweighofer and T.R. Pieber and B. Obermayer-Pietsch},
keywords = {Polycystic ovary syndrome, 5α-Reductase, Polymorphisms, Hyperandrogenemia},
abstract = {Objective
Given its role in converting testosterone to dihydrotestosterone and cortisol to dihydrocortisol, 5α-reductase may be important in the pathophysiology of the polycystic ovary syndrome (PCOS). Increased activity of this enzyme has already been demonstrated in ovaries of affected women, and might be caused by genetic alterations. The aim of this study was to analyze representative genetic variants of both isoforms of 5α-reductase with regard to PCOS parameters in lean and obese women.
Study design
We analyzed one single nucleotide polymorphism (SNP) (rs523349) of the isoform 2 (SRD5A2) and one haplotype of the isoform 1 (SRD5A1), consisting of the two SNPs rs39848 and rs3797179, in 249 women with PCOS and 226 healthy women using a 5′-exonuclease-assay. The genotypes were associated with anthropometric, metabolic and hormonal as well as functional tests in these women.
Results
In the investigated haplotype of SRD5A1, the TA variant was associated with an increased frequency of PCOS (P=0.022) and an increased Ferriman–Gallwey Score (hirsutism) (P=0.016) in women with normal weight. The G allele at the examined position of the SRD5A2 showed a decreased frequency of PCOS (P=0.03) in women with normal weight.
Conclusion
One of the keys in the development of the PCOS is hyperandrogenism, which might be caused by an increased 5α-reductase activity, as it is often seen in obesity. This mechanism might therefore be of importance in lean PCOS patients and contribute to the clinical findings.}
}
@article{YAMIN2022102635,
title = {Modeling and executing cyber security exercise scenarios in cyber ranges},
journal = {Computers & Security},
volume = {116},
pages = {102635},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102635},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822000347},
author = {Muhammad Mudassar Yamin and Basel Katt},
keywords = {Cyber range, Security, Exercises, Scenario, Modeling},
abstract = {The skill shortage in global cybersecurity is a well-known problem; to overcome this issue, cyber ranges have been developed. These ranges provide a platform for conducting cybersecurity exercises; however, conducting such exercises is a complex process because they involve people with different skill sets for the scenario modeling, infrastructure preparation, dry run, execution, and evaluation. This process is very complex and inefficient in terms of time and resources. Moreover, the exercise infrastructure created in current cyber ranges does not reflect the dynamic environment of real-world systems and does not provide adaptability for changing requirements. To tackle these issues, we developed a system that can automate many tasks of the cybersecurity exercise life cycle. We used model-driven approaches to (1) model the roles of the different teams present in the cybersecurity exercises and (2) generate automation artifacts to execute their functions efficiently in an autonomous manner. By executing different team roles such as attackers and defenders, we can add friction in the environment, making it dynamic and realistic. We conducted case studies in the form of operational cybersecurity exercises involving national-level cybersecurity competitions and a university class setting in Norway to evaluate our developed system for its efficiency, adaptability, autonomy, and skill improvement of the exercise participants. In the right conditions, our proposed system could create a complex cybersecurity exercise infrastructure involving 400 nodes with customized vulnerabilities, emulated attackers, defenders, and traffic generators under 40 minutes. It provided a realistic environment for cybersecurity exercises and positively affected the exercise participants’ skill sets.}
}
@incollection{FELDERER20161,
title = {Chapter One - Security Testing: A Survey},
editor = {Atif Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {101},
pages = {1-51},
year = {2016},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2015.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065245815000649},
author = {Michael Felderer and Matthias Büchler and Martin Johns and Achim D. Brucker and Ruth Breu and Alexander Pretschner},
keywords = {Security testing, Security testing techniques, Model-based security testing, White-box security testing, Black-box security testing, Penetration testing, Security regression testing, Security engineering, Software testing, Survey},
abstract = {Identifying vulnerabilities and ensuring security functionality by security testing is a widely applied measure to evaluate and improve the security of software. Due to the openness of modern software-based systems, applying appropriate security testing techniques is of growing importance and essential to perform effective and efficient security testing. Therefore, an overview of actual security testing techniques is of high value both for researchers to evaluate and refine the techniques and for practitioners to apply and disseminate them. This chapter fulfills this need and provides an overview of recent security testing techniques. For this purpose, it first summarize the required background of testing and security engineering. Then, basics and recent developments of security testing techniques applied during the secure software development life cycle, ie, model-based security testing, code-based testing and static analysis, penetration testing and dynamic analysis, as well as security regression testing are discussed. Finally, the security testing techniques are illustrated by adopting them for an example three-tiered web-based business application.}
}
@article{AKDUR2021101063,
title = {Modeling knowledge and practices in the software industry: An exploratory study of Turkey-educated practitioners},
journal = {Journal of Computer Languages},
volume = {66},
pages = {101063},
year = {2021},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2021.101063},
url = {https://www.sciencedirect.com/science/article/pii/S2590118421000423},
author = {Deniz Akdur},
keywords = {Model driven, Modeling education, UML, DSL, Computing discipline, Practitioner survey},
abstract = {Modeling helps software professionals to achieve various purposes via different approaches. Modeling stakeholders, who have different academic backgrounds (e.g., not only based on ”computing disciplines”), might be lacking knowledge in key modeling topics, which might be learned or improved at the workplace. To better align the modeling skills learnt at the academia with the expectations of the industry, it is important to analyze the relation between the educational skill-set and the modeling practices of the practitioners based on various modeling approaches. Moreover, understanding the-state-of-the-practices for modeling characteristics (e.g., purposes, languages, software development lifecycle phases, where modeling is used) is also necessary to provide a view on various modeling approaches used in the software industry. To achieve these goals, we designed and conducted a practitioner survey. 628 software practitioners, whose undergraduate degree was completed in Turkey, working in 13 countries responded the survey. This paper sheds light on the latest modeling practices with various cross-factor analyses by analyzing the relation of educational background with those modeling practices. Investigating how the way(s) of learning about software modeling affect related practices helps bridge the knowledge gaps. We believe that our findings would provide practical benefits to all modeling professionals (from developers to systems engineers and testers) and educators by influencing both the academia (mostly Turkish universities) and the industry.}
}
@article{NARDONE2020110478,
title = {An OSLC-based environment for system-level functional testing of ERTMS/ETCS controllers},
journal = {Journal of Systems and Software},
volume = {161},
pages = {110478},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.110478},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219302523},
author = {Roberto Nardone and Stefano Marrone and Ugo Gentile and Aniello Amato and Gregorio Barberio and Massimo Benerecetti and Renato {De Guglielmo} and Beniamino {Di Martino} and Nicola Mazzocca and Adriano Peron and Gaetano Pisani and Luigi Velardi and Valeria Vittorini},
keywords = {Critical systems, Life-cycle collaboration, Model based testing, OSLC, Testing automation},
abstract = {Product and application life-cycle management (PLM/ALM) are the processes that govern a product and a software system, respectively, encompassing the creation, deployment and operation of a system from the beginning to the end of its life. As both PLM and ALM require cross-discipline collaboration and cooperation, tools integration and inter-operation are necessary to enable the efficient and effective usage of tool suites supporting the management of the entire system life-cycle and overcome the limitations of all-in-one solutions from one tool vendor. In this context, the Open Services for Life-cycle Collaboration (OSLC) initiative proposes a set of specifications to allow a seamless integration based on linked data. This paper describes the work performed within the ARTEMIS JU project CRYSTAL to develop an environment for the functional system-level testing of railway controllers, relying on OSLC to enable inter-operation with existing PLM/ALM tools. A concrete realization of the proposed architecture is described also discussing some design and implementation choices. A real industrial case study is used to exemplify the features and the usage of the environment in testing one of the functionalities of the Radio Block Centre, the vital core of the European Rail Traffic Management System/European Train Control System (ERTMS/ETCS) Control System.}
}
@incollection{ALHADDAD2022,
title = {FSMApp: Testing mobile apps},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2022},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245822000821},
author = {Ahmed Alhaddad and Anneliese Andrews and Zeinab Abdalla},
keywords = {Mobile applications, Model-based testing, Black-box, FSMApp, Android},
abstract = {A mobile application is a software program that runs on mobile devices. In 2017, 178.1 billion mobile apps were downloaded, and the number is expected to grow to 258.2 billion app downloads in 2022. The number of apps poses a challenge for mobile application testers to find the right approach to test apps. This paper presents a black-box, model-based testing approach to test mobile apps (FSMApp). It is an extension of an existing approach to test web applications. We present the FSMApp approach and compare the approach with another black-box MBT approach. A number of case studies explore applicability, scalability, effectiveness, and efficiency of FSMApp with this approach. (number of words is 16,349).}
}
@article{SHIN2021110813,
title = {Uncertainty-aware specification and analysis for hardware-in-the-loop testing of cyber-physical systems},
journal = {Journal of Systems and Software},
volume = {171},
pages = {110813},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110813},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302132},
author = {Seung Yeob Shin and Karim Chaouch and Shiva Nejati and Mehrdad Sabetzadeh and Lionel C. Briand and Frank Zimmer},
keywords = {Test case specification and analysis, Cyber-physical systems, UML profile, Simulation, Model checking, Machine learning},
abstract = {Hardware-in-the-loop (HiL) testing is important for developing cyber-physical systems (CPS). HiL test cases manipulate hardware, are time-consuming and their behaviors are impacted by the uncertainties in the CPS environment. To mitigate the risks associated with HiL testing, engineers have to ensure that (1) test cases are well-behaved, e.g., they do not damage hardware, and (2) test cases can execute within a time budget. Leveraging the UML profile mechanism, we develop a domain-specific language, HITECS, for HiL test case specification. Using HITECS, we provide uncertainty-aware analysis methods to check the well-behavedness of HiL test cases. In addition, we provide a method to estimate the execution times of HiL test cases before the actual HiL testing. We apply HITECS to an industrial case study from the satellite domain. Our results show that: (1) HITECS helps engineers define more effective assertions to check HiL test cases, compared to the assertions defined without any systematic guidance; (2) HITECS verifies in practical time that HiL test cases are well-behaved; (3) HITECS is able to resolve uncertain parameters of HiL test cases by synthesizing conditions under which test cases are guaranteed to be well-behaved; and (4) HITECS accurately estimates HiL test case execution times.}
}
@article{WANG2011181,
title = {A Modeling Language Based on UML for Modeling Simulation Testing System of Avionic Software},
journal = {Chinese Journal of Aeronautics},
volume = {24},
number = {2},
pages = {181-194},
year = {2011},
issn = {1000-9361},
doi = {https://doi.org/10.1016/S1000-9361(11)60022-8},
url = {https://www.sciencedirect.com/science/article/pii/S1000936111600228},
author = {Lize WANG and Bin LIU and Minyan LU},
keywords = {avionics, hardware-in-the-loop, test facilities, meta-model, UML profile, domain-specific modeling language, abstract state machine},
abstract = {With direct expression of individual application domain patterns and ideas, domain-specific modeling language (DSML) is more and more frequently used to build models instead of using a combination of one or more general constructs. Based on the profile mechanism of unified modeling language (UML) 2.2, a kind of DSML is presented to model simulation testing systems of avionic software (STSAS). To define the syntax, semantics and notions of the DSML, the domain model of the STSAS from which we generalize the domain concepts and relationships among these concepts is given, and then, the domain model is mapped into a UML meta-model, named UML-STSAS profile. Assuming a flight control system (FCS) as system under test (SUT), we design the relevant STSAS. The results indicate that extending UML to the simulation testing domain can effectively and precisely model STSAS.}
}
@article{SYRIANI201843,
title = {Systematic mapping study of template-based code generation},
journal = {Computer Languages, Systems & Structures},
volume = {52},
pages = {43-62},
year = {2018},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1477842417301239},
author = {Eugene Syriani and Lechanceux Luhunu and Houari Sahraoui},
keywords = {Code generation, Systematic mapping study, Model-driven engineering},
abstract = {Context: Template-based code generation (TBCG) is a synthesis technique that produces code from high-level specifications, called templates. TBCG is a popular technique in model-driven engineering (MDE) given that they both emphasize abstraction and automation. Given the diversity of tools and approaches, it is necessary to classify existing TBCG techniques to better guide developers in their choices. Objective: The goal of this article is to better understand the characteristics of TBCG techniques and associated tools, identify research trends, and assess the importance of the role of MDE in this code synthesis approach. Method: We survey the literature to paint an interesting picture about the trends and uses of TBCG in research. To this end, we follow a systematic mapping study process. Results: Our study shows, among other observations, that the research community has been diversely using TBCG over the past 16 years. An important observation is that TBCG has greatly benefited from MDE. It has favored a template style that is output-based and high-level modeling languages as input. TBCG is mainly used to generate source code and has been applied to many domains. Conclusion: TBCG is now a mature technique and much research work is still conducted in this area. However, some issues remain to be addressed, such as support for template definition and assessment of the correctness and quality of the generated code.}
}
@article{AFANASYEV2021100707,
title = {GridTools: A framework for portable weather and climate applications},
journal = {SoftwareX},
volume = {15},
pages = {100707},
year = {2021},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2021.100707},
url = {https://www.sciencedirect.com/science/article/pii/S2352711021000522},
author = {Anton Afanasyev and Mauro Bianco and Lukas Mosimann and Carlos Osuna and Felix Thaler and Hannes Vogt and Oliver Fuhrer and Joost VandeVondele and Thomas C. Schulthess},
keywords = {C++, Weather, Climate, Library, Embedded DSL},
abstract = {Weather forecasts and climate projections are of tremendous importance for economical and societal reasons. Software implementing weather and climate models is complex to develop and hard to maintain, and requires a large range of different competencies, ranging from environmental sciences, numerical methods, to low level programming. In order to manage this complexity we developed GridTools, a set of software libraries targeted at weather and climate model developers. By separating the model description (front-end) from its efficient implementation on the target platform (back-end), GridTools allows the implementation of performance-portable simulations on a variety of platforms, such as multicore and GPU-accelerated systems. We discuss the application of GridTools to the regional weather and climate model COSMO and show performance results on simple benchmarks as well as on COSMO.}
}
@article{ODUNAYO2021761,
title = {A systematic mapping study of cloud policy languages and programming models},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {7},
pages = {761-768},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819301533},
author = {Isaac Odun-Ayo and Rowland Goddy-Worlu and Jamaiah Yahaya and Victor Geteloma},
keywords = {Cloud computing, Cloud computing policy languages, Programming models, Systematic mapping},
abstract = {Cloud computing can be considered as a disruptive technology that is making life easier for Cloud users. Determining a focus of research in a specific subject area is sometimes challenging. A systematic map enables a synthesis of a scheme for categorizing data in a field of interest. The goal of this research paper is to carry out a systematic mapping study of policy language and programming models on the cloud. The mapping involved contribution category such as method, research category such as evaluation and major topics extracted from the abstracts of primary studies. The result indicated there are more publications on evaluation research in term of security with 8.9%. There were more papers published on validation research, solution proposal and experience research on the topic of paradigms with 7.53%, 6.85% and 4.11% respectively. Also, there were more publications on philosophical research in terms of privacy with 4.11%. In addition, there were more articles published on opinion research in terms of the survey with 4.11%. On the other hand, there were no articles on metric in terms of framework, paradigms and accountability, and reliability to the best of the researchers’ knowledge. The outcome of this systematic study will be of benefit to cloud users, researchers, practitioners and providers.}
}
@article{STURM20141390,
title = {Evaluating the productivity of a reference-based programming approach: A controlled experiment},
journal = {Information and Software Technology},
volume = {56},
number = {10},
pages = {1390-1402},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001062},
author = {Arnon Sturm and Oded Kramer},
keywords = {Productivity, Programming, Software reusability, Software quality, Domain engineering},
abstract = {Context
Domain engineering aims at facilitating software development in an efficient and economical way. One way to measure that is through productivity indicators, which refer to the ability of creating a quality software product in a limited period and with limited resources. Many approaches have been devised to increase productivity; however, these approaches seem to suffer from a tension between expressiveness on the one hand, and applicability (or the lack of it) in providing guidance for developers.
Objective
This paper evaluates the applicability and efficiency of adopting a domain engineering approach, called Application-based DOmain Modeling (ADOM), in the context of the programming task with Java, and thus termed ADOM-Java, for improving productivity in terms of code quality and development time.
Method
To achieve that objective we have qualitatively evaluate the approach using questionnaires and following a text analysis procedure. We also set a controlled experiment in which 50 undergraduate students performed a Java-based programming task using either ADOM-Java or Java alone.
Results
The qualitative evaluation reveal that the approach is easy to uses and provides valuable guidance. Nevertheless, it requires training. The outcomes of the experiment indicate that the approach is applicable and that the students that used ADOM-Java achieved better code quality, as well as better functionality and within less time than the students who used only Java.
Conclusion
The results of the experiments imply that by providing a code base equipped with reuse guidelines for programmers can increase programming productivity in terms of quality and development time. These guidelines may also enforce coding standards and architectural design.}
}
@article{BASSO2016612,
title = {Automated design of multi-layered web information systems},
journal = {Journal of Systems and Software},
volume = {117},
pages = {612-637},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.04.060},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216300358},
author = {Fábio Paulo Basso and Raquel Mainardi Pillat and Toacy Cavalcante Oliveira and Fabricia Roos-Frantz and Rafael Z. Frantz},
keywords = {Model-driven web engineering, Rapid application prototype, Domain-specific language, Prototyping, Automated design, Mockup, Experience report},
abstract = {In the development of web information systems, design tasks are commonly used in approaches for Model-Driven Web Engineering (MDWE) to represent models. To generate fully implemented prototypes, these models require a rich representation of the semantics for actions (e.g., database persistence operations). In the development of some use case scenarios for the multi-layered development of web information systems, these design tasks may consume weeks of work even for experienced designers. The literature pointed out that the impossibility for executing a software project with short iterations hampers the adoption of some approaches for design in some contexts, such as start-up companies. A possible solution to introduce design tasks in short iterations is the use of automated design techniques, which assist the production of models by means of transformation tasks and refinements. This paper details our methodology for MDWE, which is supported by automated design techniques strictly associated with use case patterns of type CRUD. The novelty relies on iterations that are possible for execution with short time-scales. This is a benefit from automated design techniques not observed in MDWE approaches based on manual design tasks. We also report on previous experiences and address open questions relevant for the theory and practice of MDWE.}
}
@article{YUSSUPOV2021110906,
title = {FaaSten your decisions: A classification framework and technology review of function-as-a-Service platforms},
journal = {Journal of Systems and Software},
volume = {175},
pages = {110906},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110906},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000030},
author = {Vladimir Yussupov and Jacopo Soldani and Uwe Breitenbücher and Antonio Brogi and Frank Leymann},
keywords = {Serverless, Function-as-a-Service, FaaS, Platform, Classification framework, Technology review},
abstract = {Function-as-a-Service (FaaS) is a cloud service model enabling developers to offload event-driven executable snippets of code. The execution and management of such functions becomes a FaaS provider’s responsibility, therein included their on-demand provisioning and automatic scaling. Key enablers for this cloud service model are FaaS platforms, e.g., AWS Lambda, Microsoft Azure Functions, or OpenFaaS. At the same time, the choice of the most appropriate FaaS platform for deploying and running a serverless application is not trivial, as various organizational and technical aspects have to be taken into account. In this work, we present (i) a FaaS platform classification framework derived using a multivocal review and (ii) a technology review of the ten most prominent FaaS platforms, based on the proposed classification framework. We also present a FaaS platform selection support system, called FaaStener, which can help researchers and practitioners to choose the FaaS platform most suited for their requirements.}
}
@article{GAROUSI20102251,
title = {A replicated survey of software testing practices in the Canadian province of Alberta: What has changed from 2004 to 2009?},
journal = {Journal of Systems and Software},
volume = {83},
number = {11},
pages = {2251-2262},
year = {2010},
note = {Interplay between Usability Evaluation and Software Development},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2010.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0164121210001834},
author = {Vahid Garousi and Tan Varma},
keywords = {Survey, Software testing practices, Canada},
abstract = {Software organizations have typically de-emphasized the importance of software testing. In an earlier study in 2004, our colleagues reported the results of an Alberta-wide regional survey of software testing techniques in practice. Five years after that first study, the authors felt it is time to replicate the survey and analyze what has changed and what not from 2004 to 2009. This study was conducted during the summer of 2009 by surveying software organizations in the Canadian province of Alberta. The survey results reveal important and interesting findings about software testing practices in Alberta, and point out what has changed from 2004 to 2009 and what not. Note that although our study is conducted in the province of Alberta, we have compared the results to few international similar studies, such as the ones conducted in the US, Turkey, Hong Kong and Australia, The study should thus be of interest to all testing professionals world-wide. Among the findings are the followings: (1) almost all companies perform unit and system testing with a slight increase since 2004, (2) automation of unit, integration and systems tests has increased sharply since 2004, (3) more organization are using observations and expert opinion to conduct usability testing, (4) the choices of test-case generation mechanisms have not changed much from 2004, (5) JUnit and IBM Rational tools are the most widely used test tools, (6) Alberta companies still face approximately the same defect-related economic issues as do companies in other jurisdictions, (7) Alberta software firms have improved their test automation capability since 2004, but there is still some room for improvement, and (8) compared to 2004, more companies are spending more effort on pre-release testing.}
}
@article{MARIJAN2022100492,
title = {Blockchain verification and validation: Techniques, challenges, and research directions},
journal = {Computer Science Review},
volume = {45},
pages = {100492},
year = {2022},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100492},
url = {https://www.sciencedirect.com/science/article/pii/S1574013722000314},
author = {Dusica Marijan and Chhagan Lal},
keywords = {Blockchain, Smart contracts, P2P, Consensus, Ledger, Testing, Verification, Validation, Simulation, Benchmarking, Software testing, Security testing, Performance testing, System under test, Formal verification, Platform testing},
abstract = {As blockchain technology is gaining popularity in industry and society, solutions for Verification and Validation (V&V) of blockchain-based software applications (BC-Apps) have started gaining equal attention. To ensure that BC-Apps are properly developed before deployment, it is paramount to apply systematic V&V to verify their functional and non-functional requirements. While existing research aims at addressing the challenges of engineering BC-Apps by providing testing techniques and tools, blockchain-based software development is still an emerging research discipline, and therefore, best practices and tools for the V&V of BC-Apps are not yet sufficiently developed. In this paper, we provide a comprehensive survey on V&V solutions for BC-Apps. Specifically, using a layered approach, we synthesize V&V tools and techniques addressing different components at various layers of the BC-App stack, as well as across the whole stack. Next, we provide a discussion on the challenges associated with BC-App V&V, and summarize a set of future research directions based on the challenges and gaps identified in existing research work. Our study aims to highlight the importance of BC-App V&V and pave the way for a disciplined, testable, and verifiable BC development.}
}
@article{DAMOTASILVEIRANETO2011407,
title = {A systematic mapping study of software product lines testing},
journal = {Information and Software Technology},
volume = {53},
number = {5},
pages = {407-423},
year = {2011},
note = {Special Section on Best Papers from XP2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002193},
author = {Paulo Anselmo {da Mota Silveira Neto} and Ivan do {Carmo Machado} and John D. McGregor and Eduardo Santana {de Almeida} and Silvio Romero {de Lemos Meira}},
keywords = {Software product lines, Software testing, Mapping study},
abstract = {Context
In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development.
Objective
This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature.
Method
A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated.
Results
Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed.
Conclusion
The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet.}
}
@article{CAPILLA201962,
title = {Software variability in dynamic environments},
journal = {Journal of Systems and Software},
volume = {156},
pages = {62-64},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301323},
author = {Rafael Capilla and Lidia Fuentes and Malte Lochau}
}
@incollection{EELES20141,
title = {Chapter 1 - Relating System Quality and Software Architecture: Foundations and Approaches},
editor = {Ivan Mistrik and Rami Bahsoon and Peter Eeles and Roshanak Roshandel and Michael Stal},
booktitle = {Relating System Quality and Software Architecture},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-20},
year = {2014},
isbn = {978-0-12-417009-4},
doi = {https://doi.org/10.1016/B978-0-12-417009-4.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124170094000016},
author = {Peter Eeles and Rami Bahsoon and Ivan Mistrik and Roshanak Roshandel and Michael Stal},
keywords = {Quality Attributes, System quality, Architecture, Assessment, Documentation, Design, Development Process, Lifecycle Approach, Scenario},
abstract = {The field of software architecture has gone through significant evolution over the past two decades. Early research in software architecture focused on technological contributions such as the modeling of structural and behavioral properties of software systems. Automated analysis of these models resulted in the development of tools and approaches aimed at ensuring a system’s functional and nonfunctional properties such as performance, interoperability, and schedulability. More recently, however, software architecture research has shifted in fundamental ways. The emphasis on capturing design decisions and their relationship to both a software system’s requirements and its implementation is predominant. The synergy between the design decisions captured in the software architecture and system quality is the primary motivation behind this book.}
}
@article{IQBAL20226324,
title = {Test case prioritization for model transformations},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part B},
pages = {6324-6338},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821002147},
author = {Saqib Iqbal and Issam Al-Azzoni},
keywords = {Model transformations, Model-driven engineering, Regression testing, Test case prioritization},
abstract = {The application of model transformations is a critical component in Model-Driven Engineering (MDE). To ensure the correctness of the generated models, these model transformations need to be extensively tested. However, during the regression testing of these model transformations, it becomes too costly to frequently run a large number of test cases. Test case prioritization techniques are needed to rank the test cases and help the tester during the regression testing to be more efficient. The objective is to rank the fault revealing test cases higher so that a tester can only execute the top ranked test cases and still be able to detect as many faults as possible in the case of limited budget and resources. The aim of this paper is to present a test prioritization approach for the regression testing of model transformations. The approach is based on exploiting the rule coverage information of the test cases. The paper presents an empirical study which compares several techniques introduced by our approach for prioritizing test cases. The approach is complemented with a tool that implements the proposed techniques and can automatically generate test case orderings.}
}
@article{MUSIC2022100251,
title = {AVA: A component-oriented abstraction layer for virtual plug&produce automation systems engineering},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100251},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100251},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000509},
author = {Goran Musić and Bernhard Heinzl and Wolfgang Kastner},
keywords = {Automation, Domain language, Architectural pattern, Framework},
abstract = {The prevailing system and software model in automation systems engineering is defined by the IEC 61131 norm. It is to date the best way we know how to express low-level logic and manipulate electrical hardware signals. However, the exponential technological growth is continuing to raise the expectations on what automation systems are supposed to be capable of doing. Fulfilling rising requirements and managing the exploding complexity requires a systematic support for high-level descriptions, structuring, and communication, which the original approach was not built to provide. This work proposes the introduction of an abstraction layer, a component-container infrastructure, defined on top of standard system and software models in automation and mirroring the world of cyber–physical systems, where independent components are interconnected to realize the systems’ purpose by using each other’s functionalities. The concept is implemented in the form of a domain-specific modeling language, applying a classical two-level Model-driven Software Engineering (MDSE) approach. By engineering distinct industrial use cases in accordance with the proposed approach, it is shown that the defined abstractions and mechanisms are capable of expressing the nuances of software design in different domains and can enable the streamlining of the automation systems engineering workflow into a virtual plug&produce process.}
}
@article{KOZIOLEK2020110575,
title = {A classification framework for automated control code generation in industrial automation},
journal = {Journal of Systems and Software},
volume = {166},
pages = {110575},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110575},
url = {https://www.sciencedirect.com/science/article/pii/S016412122030056X},
author = {Heiko Koziolek and Andreas Burger and Marie Platenius-Mohr and Raoul Jetley},
keywords = {Software design and implementation, Industrial automation, Control engineering, Model-driven development, Code generation, UML / SysML},
abstract = {Software development for the automation of industrial facilities (e.g., oil platforms, chemical plants, power plants, etc.) involves implementing control logic, often in IEC 61131-3 programming languages. Developing safe and efficient program code is expensive and today still requires substantial manual effort. Researchers have thus proposed numerous approaches for automatic control logic generation in the last two decades, but a systematic, in-depth analysis of their capabilities and assumptions is missing. This paper proposes a novel classification framework for control logic generation approaches defining criteria derived from industry best practices. The framework is applied to compare and analyze 13 different control logic generation approaches. Prominent findings include different categories of control logic generation approaches, the challenge of dealing with iterative engineering processes, and the need for more experimental validations in larger case studies.}
}
@incollection{2023519,
title = {Index},
editor = {Marilyn Wolf},
booktitle = {Computers as Components (Fifth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fifth Edition},
pages = {519-530},
year = {2023},
series = {The Morgan Kaufmann Series in Computer Architecture and Design},
issn = {15459888},
doi = {https://doi.org/10.1016/B978-0-323-85128-2.18001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851282180016}
}
@article{MA2019724,
title = {Graph-based and scenario-driven microservice analysis, retrieval, and testing},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {724-735},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.048},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302614},
author = {Shang-Pin Ma and Chen-Yuan Fan and Yen Chuang and I-Hsiu Liu and Ci-Wei Lan},
keywords = {Microservice retrieval, Microservice testing, Microservice analysis, Service dependency graph, Behavior-driven development},
abstract = {The microservice architecture (MSA) differs fundamentally from the monolithic, layered architecture. The use of microservices provides a high degree of autonomy, composability, scalability, and fault-tolerance. MSA is regarded by many as a promising architecture for smart-city applications; however, a number of issues remain, including (1) the management of complex call relationships among microservices; (2) ensuring the quality of the overall software system even as new microservices are added and existing ones are modified, and (3) locating existing microservices that satisfy new requirements. In this paper, we propose a novel approach to the development of microservice-based systems, referred to as GSMART (Graph-based and Scenario-driven Microservice Analysis, Retrieval and Testing). GSMART enables the automatic generation of a “Service Dependency Graph (SDG)” by which to visualize and analyze dependency relationships between microservices as well as between services and scenarios. It also enables the automatic retrieval of test cases required for system changes to reduce the time and costs associated with regression testing. A microservice retrieval method using VSM and word2vec accelerates the development of new microservices tailored specifically to the needs of users based on user-provided scenarios. Experiment results demonstrate the feasibility, effectiveness, and efficiency of all of the main features of GSMART.}
}
@incollection{SAMPATH2016155,
title = {Chapter Four - Advances in Web Application Testing, 2010–2014},
editor = {Atif Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {101},
pages = {155-191},
year = {2016},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2015.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245815000674},
author = {Sreedevi Sampath and Sara Sprenkle},
keywords = {web applications, Software testing, Web testing, Test case generation, Oracles, Test effectiveness, Regression testing},
abstract = {As web applications increase in popularity, complexity, and size, approaches and tools to automate testing the correctness of web applications must continually evolve. In this chapter, we provide a broad background on web applications and the challenges in testing these distributed, dynamic applications made up of heterogeneous components. We then focus on the recent advances in web application testing that were published between 2010 and 2014, including work on test-case generation, oracles, testing evaluation, and regression testing. Through this targeted survey, we identify trends in web application testing and open problems that still need to be addressed.}
}
@article{VANDENBRAND201575,
title = {Software engineering: Redundancy is key},
journal = {Science of Computer Programming},
volume = {97},
pages = {75-81},
year = {2015},
note = {Special Issue on New Ideas and Emerging Results in Understanding Software},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2013.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167642313003043},
author = {Mark {van den Brand} and Jan Friso Groote},
keywords = {Software engineering, Software quality, Redundancy},
abstract = {Software engineers are humans and so they make lots of mistakes. Typically 1 out of 10 to 100 tasks go wrong. The only way to avoid these mistakes is to introduce redundancy in the software engineering process. This article is a plea to consciously introduce several levels of redundancy for each programming task. Depending on the required level of correctness, expressed in a residual error probability (typically 10−3 to 10−10), each programming task must be carried out redundantly 4 to 8 times. This number is hardly influenced by the size of a programming endeavour. Training software engineers do have some effect as non-trained software engineers require a double amount of redundant tasks to deliver software of a desired quality. More compact programming, for instance by using domain specific languages, only reduces the number of redundant tasks by a small constant.}
}
@article{TOKMAKOFF2016537,
title = {AusPlots Rangelands field data collection and publication: Infrastructure for ecological monitoring},
journal = {Future Generation Computer Systems},
volume = {56},
pages = {537-549},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15002782},
author = {Andrew Tokmakoff and Ben Sparrow and David Turner and Andrew Lowe},
keywords = {Ecological data, Mobile, Data collection, Data publishing},
abstract = {The TERN AusPlots Rangelands field data collection system has been developed to facilitate simple and efficient data collection by ecologists operating in the Australian outback. The infrastructure provides tooling for ‘clean’ data collection on mobile (tablet) devices, associated data storage in a cloud-based server infrastructure, facilities for data curation and management and interfaces with the Australian Ecological Knowledge and Observation System (ÆKOS) data repository for long-term data management and semantic enrichment. In this paper, we introduce the AusPlots Rangelands field data collection solution, providing a systems-level view and motivating its development through the discussion of key functional requirements. We provide an outline of the ÆKOS data repository and demonstrate that the combined system provides a unique end-to-end data collection, curation, archiving and publishing mechanism for ecological data.}
}
@article{SCARABELLI200435,
title = {Nutritional supplementation with mixed essential amino acids enhances myocyte survival, preserving mitochondrial functional capacity during ischemia-reperfusion injury},
journal = {The American Journal of Cardiology},
volume = {93},
number = {8, Supplement 1},
pages = {35-40},
year = {2004},
issn = {0002-9149},
doi = {https://doi.org/10.1016/j.amjcard.2003.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0002914903015145},
author = {Tiziano M Scarabelli and Evasio Pasini and Anastasis Stephanou and Carol Chen-Scarabelli and Louis Saravolatz and Richard A Knight and David S Latchman and Julius M Gardin},
abstract = {In patients undergoing coronary surgery, the uptake of amino acids, which has been shown to correlate with oxygen consumption, is a mechanism of cardiac adaptation to the iatrogenic ischemia-reperfusion injury associated with cardioplegic arrest. Based on these premises, we sought to determine whether oral supplementation with mixed amino acids may protect the rat heart exposed to ischemia-reperfusion and to address whether this hypothesized cardioprotection is achieved, at least in part, through preservation of the energy-producing properties of mitochondria. Sprague–Dawley rats were fed (by enteral route) a liquid diet, with or without mixed essential amino acids (daily dose of 1 g/kg) for 30 days. Hearts from anesthetized rats were perfused by the Langendorff method and randomized to 3 groups. The control group was perfused with buffer for 60 minutes; the ischemia-reperfusion control and the amino acid–treated groups were exposed to 35 minutes of ischemia, followed by 60 or 120 minutes of reperfusion. Amino acid supplements minimized infarct size (22 ± 1.8% vs 33 ± 2.5%; p <0.05) and occurrence of cardiomyocyte apoptosis, as assessed by co-localization of terminal deoxynucleotidyl transferase–mediated dUTP nick end labeling (TUNEL) and caspase-3–positive staining (p <0.01). Long-term treatment with amino acids also reduced the proportion of cardiomyocytes exhibiting immunostaining for cleaved caspase-9 (p <0.01) but was ineffective on processing of caspase-8. Similar results were obtained in the whole heart by caspase activity assays (p <0.01). The lessened activation of caspase-9 detected in amino acid-treated hearts paralleled a strong reduction in mitochondrial release of cytochrome c. Adenosine triphosphate (ATP) content and rate of ATP production in isolated mitochondria were reduced by >75% in control hearts after 2 hours of reperfusion (p <0.05 vs control hearts); these values returned toward those of the control group in hearts supplemented with amino acids (p <0.01). Finally, the oxygen consumption rate in myocardial skinned bundles was markedly reduced in ischemia-reperfusion control hearts and almost normalized in amino acid-treated hearts (approximately 20% and 93% of the value for normoxic hearts; p <0.01). These results suggest that oral amino acid supplementation attenuates the extent of ischemia-reperfusion injury in the rat heart, through preservation of the mitochondria-generated production of high-energy phosphates.}
}
@article{KORAKAKIS2018235,
title = {Blood Flow Restriction induces hypoalgesia in recreationally active adult male anterior knee pain patients allowing therapeutic exercise loading},
journal = {Physical Therapy in Sport},
volume = {32},
pages = {235-243},
year = {2018},
issn = {1466-853X},
doi = {https://doi.org/10.1016/j.ptsp.2018.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S1466853X17305035},
author = {Vasileios Korakakis and Rodney Whiteley and Konstantinos Epameinontidis},
keywords = {Blood flow restriction, Occlusion, Resistance training, Ischaemia, Rehabilitation},
abstract = {Objective
To evaluate if a single blood flow restriction (BFR)-exercise bout would induce hypoalgaesia in patients with anterior knee pain (AKP) and allow painless application of therapeutic exercise.
Design
Cross-sectional repeated measures design.
Setting
Institutional out-patients physiotherapy clinic.
Patients
Convenience sample of 30 AKP patients.
Intervention
BFR was applied at 80% of complete vascular occlusion. Four sets of low-load open kinetic chain knee extensions were implemented using a pain monitoring model.
Main outcome measurements
Pain (0–10) was assessed immediately after BFR application and after a physiotherapy session (45 min) during shallow and deep single-leg squat (SSLS, DSLS), and step-down test (SDT). To estimate the patient rating of clinical effectiveness, previously described thresholds for pain change (≥40%) were used, with appropriate adjustments for baseline pain levels.
Results
Significant effects were found with greater pain relief immediate after BFR in SSLS (d = 0.61, p < 0.001), DSLS (d = 0.61, p < 0.001), and SDT (d = 0.60, p < 0.001). Time analysis revealed that pain reduction was sustained after the physiotherapy session for all tests (d(SSLS) = 0.60, d(DSLS) = 0.60, d(SDT) = 0.58, all p < 0.001). The reduction in pain effect size was found to be clinically significant in both post-BFR assessments.
Conclusion
A single BFR-exercise bout immediately reduced AKP with the effect sustained for at least 45 min.}
}
@article{RUTLE2015545,
title = {Model-driven Software Engineering in Practice: A Content Analysis Software for Health Reform Agreements},
journal = {Procedia Computer Science},
volume = {63},
pages = {545-552},
year = {2015},
note = {The 6th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2015)/ The 5th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2015)/ Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.383},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915025181},
author = {Adrian Rutle and Kent Inge Fagerland Simonsen and Hans Georg Schaathun and Ralf Kirchhoff},
keywords = {Model-driven Software Engineering, Coordination Reforms in healthcare, Metamodelling, MVCore},
abstract = {The Coordination Reform of 2012 requires Norwegian municipalities and regional health authorities to enter into legally binding service agreements. Although several research projects have been undertaken to analyse the implications of this reform, there is no central database where researches can be given access and analyse the service agreements. In this paper we present how we use model-driven software engineering and user-centric design in an initial development of an information system designed to allow researches to access and analyse service agreements. For this project, it was crucial to discuss the requirements of the system with domain-experts at a high level of abstraction in order to elicit feedback so that the development could proceed at a fast pace and in the right direction. Furthermore, given time and resource constraints, we elected to use a model driven approach using automatic code generation coupled with high-productivity frameworks. In this way we were able to create prototypes so that the developers could get fast feedback from the domain-experts and improvements could be implemented with minimal effort.}
}
@article{COLE2019202,
title = {Model-based systems engineering: application and lessons from a technology maturation project},
journal = {Procedia Computer Science},
volume = {153},
pages = {202-209},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307306},
author = {Bjorn Cole and Vikram Mittal and Stephen Gillespie and Nguyen La and Richard Wise and Alex MacCalman},
keywords = {Model-Based Systems Engineering, test framework, integration},
abstract = {The Tactical Assault Light Operator Suit (TALOS) project is a Special Operations Command (SOCOM) initiative to enhance operator performance, situational awareness, survivability, and lethality. The project adopted a Model-Based Systems Engineering (MBSE) approach to manage the structural system configuration and support the test and integration plan. This approach relies on a unified model in the Systems Modeling Language (SysML) to capture the logical and physical aspects of the system design. The MBSE approach was used to develop a test and evaluation framework that allows for traceability of test plans back to performance requirements. Additionally, the model supported the integration of hardware and software as well as the design of wire harnesses; the MBSE approach provided benefits over more traditional integration techniques. This paper provides lessons learned including the need to balance requirements analysis with functional characterization and the products that were generated from the model. Overall the adoption of the MBSE approach provided lessons on managing a system’s configuration among a distributed team.}
}
@article{SCHAR2022100987,
title = {Child maltreatment and hypothalamic-pituitary-adrenal axis functioning: A systematic review and meta-analysis},
journal = {Frontiers in Neuroendocrinology},
volume = {66},
pages = {100987},
year = {2022},
issn = {0091-3022},
doi = {https://doi.org/10.1016/j.yfrne.2022.100987},
url = {https://www.sciencedirect.com/science/article/pii/S0091302222000103},
author = {Selina Schär and Ines Mürner-Lavanchy and Stefanie J. Schmidt and Julian Koenig and Michael Kaess},
keywords = {Child maltreatment, Hypothalamic–pituitary–adrenal axis, Diurnal cortisol, Cortisol awakening response, Stress reactivity, Dexamethasone suppression test, Combined dexamethasone-corticotropin releasing hormone test, Corticotropin-releasing hormone test, Hair cortisol, Urinary free cortisol, Meta-analysis, Systematic review},
abstract = {Alterations in hypothalamic–pituitary–adrenal (HPA) axis and its effector hormone cortisol have been proposed as one possible mechanism linking child maltreatment experiences to health disparities. In this series of meta-analyses, we aimed to quantify the existing evidence on the effect of child maltreatment on various measures of HPA axis activity. The systematic literature search yielded 1,858 records, of which 87 studies (k = 132) were included. Using random-effects models, we found evidence for blunted cortisol stress reactivity in individuals exposed to child maltreatment. In contrast, no overall differences were found in any of the other HPA axis activity measures (including measures of daily activity, cortisol assessed in the context of pharmacological challenges and cumulative measures of cortisol secretion). The impact of several moderators (e.g., sex, psychopathology, study quality), the role of methodological shortcomings of existing studies, as well as potential directions for future research are discussed.}
}
@article{RYSSEL201283,
title = {Automatic library migration for the generation of hardware-in-the-loop models},
journal = {Science of Computer Programming},
volume = {77},
number = {2},
pages = {83-95},
year = {2012},
note = {Special Issue on Automatic Program Generation for Embedded Systems},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2010.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167642310001115},
author = {Uwe Ryssel and Joern Ploennigs and Klaus Kabitzsch},
keywords = {Generative programming, Function-block-based design, Library migration, Structural comparison},
abstract = {Embedded systems are widely used in several applications nowadays. As they integrate hard- and software elements, their functionality and reliability are often tested by hardware-in-the-loop methods, in which the system under test runs in a simulated environment. Due to the rising complexity of the embedded functions, performance limitations and practicability reasons, the simulations are often specialized to test specific aspects of the embedded system and develop a high diversity by themselves. This diversity is difficult to manage for a user and results in erroneously selected test components and compatibility problems in the test configuration. This paper presents a generative programming approach that handles the diversity of test libraries. Compatibility issues are explicitly evaluated by a new interface concept. Furthermore, a novel model analyzer facilitates the efficient application in practice by migrating existing libraries. The approach is evaluated for an example from the automotive domain using MATLAB/Simulink.}
}
@incollection{2017207,
title = {Index},
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {Eleventh Hour CISSP® (Third Edition)},
publisher = {Syngress},
edition = {Third Edition},
pages = {207-221},
year = {2017},
isbn = {978-0-12-811248-9},
doi = {https://doi.org/10.1016/B978-0-12-811248-9.09992-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128112489099927}
}
@article{JIMENEZ20153,
title = {MeTAGeM-Trace: Improving trace generation in model transformation by leveraging the role of transformation models},
journal = {Science of Computer Programming},
volume = {98},
pages = {3-27},
year = {2015},
note = {Fifth issue of Experimental Software and Toolkits (EST): A special issue on Academics Modelling with Eclipse (ACME2012)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2014.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642314003700},
author = {Álvaro Jiménez and Juan M. Vara and Verónica A. Bollati and Esperanza Marcos},
keywords = {Model-driven engineering, Traceability, Model transformation},
abstract = {In the context of Model-Driven Engineering (MDE), generation of traces can be automated using the implicit traceability relationships contained in any model transformation. Besides, if transformations are developed adopting a Model-Driven Engineering (MDE) approach, i.e. promoting the role of models and the level of automation, model transformation will benefit from the promised advantages of MDE in terms of less costly software development while reducing the inherent complexity of coding model transformations. To put these ideas into practice, this work introduces MeTAGeM-Trace, the first prototype of an EMF-based toolkit for the MDD of model-to-model transformations which supports trace generation, i.e. it allows developing model transformations that produce not only the corresponding target models, but also a trace model between the elements of the source and target models involved in the transformation.}
}
@article{WOMELDORFF2017555,
title = {Taking Lessons Learned from a Proxy Application to a Full Application for SNAP and PARTISN},
journal = {Procedia Computer Science},
volume = {108},
pages = {555-565},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.243},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917308700},
author = {Geoff Womeldorff and Joshua Payne and Ben Bergen},
abstract = {SNAP is a proxy application which simulates the computational motion of a neutral particle transport code, PARTISN. In this work, we have adapted parts of SNAP separately; we have re-implemented the iterative shell of SNAP in the task-model runtime Legion, showing an improvement to the original schedule, and we have created multiple Kokkos implementations of the computational kernel of SNAP, displaying similar performance to the native Fortran. We then translate our Kokkos experiments in SNAP to PARTISN, necessitating engineering development, regression testing, and further thought.}
}
@article{HASER201652,
title = {Is business domain language support beneficial for creating test case specifications: A controlled experiment},
journal = {Information and Software Technology},
volume = {79},
pages = {52-62},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S095058491630115X},
author = {Florian Häser and Michael Felderer and Ruth Breu},
keywords = {Domain Specific Languages (DSL), Behavior driven development, Controlled experiment, Software testing, Student experiment},
abstract = {Context: Behavior Driven Development (BDD), widely used in modern software development, enables easy creation of acceptance test case specifications and serves as a communication basis between business- and technical-oriented stakeholders. BDD is largely facilitated through simple domain specific languages (DSL) and usually restricted to technical test domain concepts. Integrating business domain concepts to implement a ubiquitous language for all members of the development team is an appealing test language improvement issue. But the integration of business domain concepts into BDD toolkits has so far not been investigated. Objective: The objective of the study presented in this paper is to examine whether supporting the ubiquitous language features inside a DSL, by extending a DSL with business domain concepts, is beneficial over using a DSL without those concepts. In the context of the study, benefit is measured in terms of perceived quality, creation time and length of the created test case specifications. In addition, we analyze if participants feel supported when using predefined business domain concepts. Method: We investigate the creation of test case specifications, similar to BDD, in a controlled student experiment performed with graduate students based on a novel platform for DSL experimentation. The experiment was carried out by two groups, each solving a similar comparable test case, one with the simple DSL, the other one with the DSL that includes business domain concepts. A crossover design was chosen for evaluating the perceived quality of the resulting specifications. Results: Our experiment indicates that a business domain aware language allows significant faster creation of documents without lowering the perceived quality. Subjects felt better supported by the DSL with business concepts. Conclusion: Based on our findings we propose that existing BDD toolkits could be further improved by integrating business domain concepts.}
}
@article{ZHU2007265,
title = {MDABench: Customized benchmark generation using MDA},
journal = {Journal of Systems and Software},
volume = {80},
number = {2},
pages = {265-282},
year = {2007},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2006.10.052},
url = {https://www.sciencedirect.com/science/article/pii/S0164121206003384},
author = {Liming Zhu and Ngoc Bao Bui and Yan Liu and Ian Gorton},
keywords = {MDA, Model-driven development, Performance, Testing, Code generation},
abstract = {This paper describes an approach for generating customized benchmark suites from a software architecture description following a Model Driven Architecture (MDA) approach. The benchmark generation and performance data capture tool implementation (MDABench) is based on widely used open source MDA frameworks. The benchmark application is modeled in UML and generated by taking advantage of the existing community-maintained code generation “cartridges” so that current component technology can be exploited. We have also tailored the UML 2.0 Testing Profile so architects can model the performance testing and data collection architecture in a standards compatible way. We then extended the MDA framework to generate a load testing suite and automatic performance measurement infrastructure. This greatly reduces the effort and expertise needed for benchmarking with complex component and Web service technologies while being fully MDA standard compatible. The approach complements current model-based performance prediction and analysis methods by generating the benchmark application from the same application architecture that the performance models are derived from. We illustrate the approach using two case studies based on Enterprise JavaBean component technology and Web services.}
}
@article{ZAMPETTI2020106311,
title = {Demystifying the adoption of behavior-driven development in open source projects},
journal = {Information and Software Technology},
volume = {123},
pages = {106311},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106311},
url = {https://www.sciencedirect.com/science/article/pii/S095058492030063X},
author = {Fiorella Zampetti and Andrea {Di Sorbo} and Corrado Aaron Visaggio and Gerardo Canfora and Massimiliano {Di Penta}},
keywords = {Behavior-driven development, Acceptance testing, Empirical study, Co-evolution},
abstract = {Context:Behavior-Driven Development (BDD) features the capability, through appropriate domain-specific languages, of specifying acceptance test cases and making them executable. The availability of frameworks such as Cucumber or RSpec makes the application of BDD possible in practice. However, it is unclear to what extent developers use such frameworks, and whether they use them for actually performing BDD, or, instead, for other purposes such as unit testing. Objective:In this paper, we conduct an empirical investigation about the use of BDD tools in open source, and how, when a BDD tool is in place, BDD specifications co-evolve with source code. Method:Our investigation includes three different phases: (i) a large-scale analysis to understand the extent to which BDD frameworks are used in 50,000 popular open-source projects written in five programming languages; (ii) a study on the co-evolution of scenarios, fixtures and production code in a sample of 20 Ruby projects, through the Granger’s causality test, and (iii) a survey with 31 developers to understand how they use BDD frameworks. Results:Results of the study indicate that  ≃  27% of the sampled projects use BDD frameworks, with a prevalence in Ruby projects (68%). In about 37% of the cases, we found a co-evolution between scenarios/fixtures and production code. Specifically, changes to scenarios and fixtures often happen together or after changes to source code. Moreover, survey respondents indicate that, while they understand the intended purpose of BDD frameworks, most of them write tests while/after coding rather than strictly applying BDD. Conclusions:Even if the BDD frameworks usage is widespread among open source projects, in many cases they are used for different purposes such as unit testing activities. This mainly happens because developers felt BDD remains quite effort-prone, and its application goes beyond the simple adoption of a BDD framework.}
}
@article{ZITKO20112259,
title = {SNEG – Mathematica package for symbolic calculations with second-quantization-operator expressions},
journal = {Computer Physics Communications},
volume = {182},
number = {10},
pages = {2259-2264},
year = {2011},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2011.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010465511001792},
author = {Rok Žitko},
keywords = {Symbolic manipulation, Second-quantization operators, Wickʼs theorem, Occupation-number representation, Bra–ket notation},
abstract = {In many-particle problems involving interacting fermions or bosons, the most natural language for expressing the Hamiltonian, the observables, and the basis states is the language of the second-quantization operators. It thus appears advantageous to write numerical computer codes which allow the user to define the problem and the quantities of interest directly in terms of operator strings, rather than in some low-level programming language. Here I describe a Mathematica package which provides a flexible framework for performing the required translations between several different representations of operator expressions: condensed notation using pure ASCII character strings, traditional notation (“pretty printing”), internal Mathematica representation using nested lists (used for automatic symbolic manipulations), and various higher-level (“macro”) expressions. The package consists of a collection of transformation rules that define the algebra of operators and a comprehensive library of utility functions. While the emphasis is given on the problems from solid-state and atomic physics, the package can be easily adapted to any given problem involving non-commuting operators. It can be used for educational and demonstration purposes, but also for direct calculations of problems of moderate size.
Program summary
Program title: SNEG Catalogue identifier: AEJL_vl_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEJL_vl_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: GNU General Public License No. of lines in distributed program, including test data, etc.: 319 808 No. of bytes in distributed program, including test data, etc.: 1 081 247 Distribution format: tar.gz Programming language: Mathematica Computer: Any computer which runs Mathematica Operating system: Any OS which runs Mathematica RAM: Problem dependent Classification: 2.9, 5, 6.2 Nature of problem: Manipulation of expressions involving second-quantization operators and other non-commuting objects. Calculation of commutators, anticommutators, expectation values. Generation of matrix representations of the Hamiltonians expressed in the second-quantization language. Solution method: Automatic reordering of operator strings in some well specified canonical order; (anti)commutation rules are used where needed. States may be represented in occupation-number representation. Dirac bra–ket notation may be intermixed with non-commuting operator expressions. Restrictions: For very long operator strings, the brute-force automatic reordering becomes slow, but it can be turned off. In such cases, the expectation values may still be evaluated using Wickʼs theorem. Unusual features: SNEG provides the natural notation of second-quantization operators (dagger for creation operators, etc.) when used interactively using the Mathematica notebook interface. Running time: Problem dependent}
}
@incollection{AKESSON2020245,
title = {Chapter 10 - Reducing design time and promoting evolvability using Domain-Specific Languages in an industrial context},
editor = {Bedir Tekinerdogan and Önder Babur and Loek Cleophas and Mark {van den Brand} and Mehmet Akşit},
booktitle = {Model Management and Analytics for Large Scale Systems},
publisher = {Academic Press},
pages = {245-272},
year = {2020},
isbn = {978-0-12-816649-9},
doi = {https://doi.org/10.1016/B978-0-12-816649-9.00020-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012816649900020X},
author = {Benny Akesson and Jozef Hooman and Jack Sleuters and Adrian Yankov},
keywords = {Model-Based Engineering, Domain-Specific Languages, evolvability, simulation, validation, co-evolution, modularity},
abstract = {The complexity of contemporary systems is increasing, driven by integration of more functionality and trends towards mass-customization. This has resulted in complex systems with many variants that require much time to develop and are difficult to adapt to changing requirements and introduction of new technology. New methodologies are hence required to reduce development time, simplify customization for a particular customer, and improve evolvability both during development and after deployment. This chapter explains how these challenges are addressed by an approach to Model-Based Engineering (MBE) based on Domain-Specific Languages (DSLs). However, applying the approach in industry has resulted in five technical research questions, namely, how to (RQ1) achieve modularity and reuse in a DSL ecosystem, (RQ2) achieve consistency between model and realizations, (RQ3) manage an evolving DSL ecosystem, (RQ4) ensure model quality, and (RQ5) ensure quality of generated code. The five research questions are explored in the context of the published state of the art, as well as practically investigated through a case study from the defense domain.}
}
@article{LUCENA2013890,
title = {Contributions to the emergence and consolidation of Agent-oriented Software Engineering},
journal = {Journal of Systems and Software},
volume = {86},
number = {4},
pages = {890-904},
year = {2013},
note = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212002567},
author = {Carlos Lucena and Ingrid Nunes},
keywords = {Multiagent systems, Agent-oriented Software Engineering, LES, PUC-Rio, SBES 25 years},
abstract = {Many of the issues addressed with multi-agent approaches, such as distributed coordination and self-organization, are now becoming part of industrial and business systems. However, Multiagent Systems (MASs) are still not widely adopted in industry owing to the lack of a connection between MAS and software engineering. Since 2000, there is an effort to bridge this gap and to produce software engineering techniques for agent-based systems that guide the processes of design, development and maintenance. In Brazil, Agent-oriented Software Engineering (AOSE) was first investigated by the research group in the Software Engineering Laboratory (LES) at PUC-Rio, which after one decade of study in this area has built an AOSE community. This paper presents the history of AOSE at LES by discussing the sub-areas of MAS Software Engineering research and development that have been focus of the LES research group. We give examples of relevant results and present a subset of the extensive literature the group has produced during the last decade. We also report how we faced the challenges that emerged from our research by organizing and developing a research community at the intersection of software engineering, programming and MASs with a concern for scalability of solutions.}
}
@article{RABISER201986,
title = {A domain analysis of resource and requirements monitoring: Towards a comprehensive model of the software monitoring domain},
journal = {Information and Software Technology},
volume = {111},
pages = {86-109},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300606},
author = {Rick Rabiser and Klaus Schmid and Holger Eichelberger and Michael Vierhauser and Sam Guinea and Paul Grünbacher},
keywords = {Software monitoring, Requirements monitoring, Resource monitoring, Domain model, Reference architecture},
abstract = {[Context] Complex and heterogeneous software systems need to be monitored as their full behavior often only emerges at runtime, e.g., when interacting with other systems or the environment. Software monitoring approaches observe and check properties or quality attributes of software systems during operation. Such approaches have been developed in diverse communities for various kinds of systems and purposes. For instance, requirements monitoring aims to check at runtime whether a software system adheres to its requirements, while resource or performance monitoring collects information about the consumption of computing resources by the monitored system. Many venues publish research on software monitoring, often using diverse terminology, and focusing on different monitoring aspects and phases. The lack of a comprehensive overview of existing research often leads to re-inventing the wheel. [Objective] We provide a domain model to structure and systematize the field of software monitoring, starting with requirements and resource monitoring. [Method] We developed an initial domain model based on (i) our extensive experiences with requirements and resource monitoring, (ii) earlier efforts to develop a comparison framework for monitoring approaches, and (iii) an earlier systematic literature review on requirements monitoring frameworks. We then systematically analyzed 47 existing requirements and resource monitoring approaches to iteratively refine the domain model and to develop a reference architecture for software monitoring approaches. [Results] Our domain model covers the key elements of monitoring approaches and allows analyzing their commonalities and differences. Together with the reference architecture, our domain model supports the development of integrated monitoring solutions. We provide details on 47 approaches we analyzed with the model to assess its coverage. We also evaluate the reference architecture by instantiating it for five different monitoring solutions. [Conclusions] We conclude that requirements and resource monitoring have more commonalities than differences, which is promising for the future integration of existing monitoring solutions.}
}
@incollection{2007371,
title = {Chapter 11 - Putting it into practice},
editor = {Mark Utting and Bruno Legeard},
booktitle = {Practical Model-Based Testing},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {371-389},
year = {2007},
isbn = {978-0-12-372501-1},
doi = {https://doi.org/10.1016/B978-012372501-1/50012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123725011500120},
abstract = {Publisher Summary
This chapter discusses several practical issues and techniques for adopting model-based testing. It starts with the prerequisites of model-based testing and then goes through taxonomy of the possible approaches with the goal of helping one to choose a good approach for one's needs. It deals with people and training issues and discusses how model based testing can fit into agile development processes and the Unified Modeling Language (UML) unified process. One benefit of model-based testing is that it identifies faults in the analysis model and in the requirements earlier in the design process than is usually the case. This can prevent those faults from flowing into the design model and the SUT implementation. This happens because the modeling and model-validation stages of model-based testing raise questions about the requirements and can expose faults well before any tests are executed.}
}
@incollection{2016521,
title = {Glossary},
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP Study Guide (Third Edition)},
publisher = {Syngress},
edition = {Third Edition},
address = {Boston},
pages = {521-557},
year = {2016},
isbn = {978-0-12-802437-9},
doi = {https://doi.org/10.1016/B978-0-12-802437-9.00011-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128024379000114}
}
@article{TYUGASHEV20181457,
title = {Verification and online updating of decision making control logic for onboard real-time control systems},
journal = {Procedia Computer Science},
volume = {126},
pages = {1457-1466},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918313966},
author = {Andrey Tyugashev and Dmitrii Zheleznov},
keywords = {Control logic, real-time control algorithm, complex technical system, flight control software, intelligent control},
abstract = {The errors during the decision making process in the control system of a modern complex technical system such as a ship, a plane, a spaceship or a power station can lead to unacceptable consequences. Meanwhile, decision making is based on so-named “control logic” described in dedicated specification documents and then implemented by the hardware and software in the real time mode. There are some problems in this process, caused by contradictions and incompletenesses in the specification documents written in natural language, and misunderstanding between specialists in onboard systems, operational engineers, and programmers. In this paper, two practical examples of verification and online updating of spacecraft control logic are described. The approaches we used allow avoiding the mentioned problems. The theoretical basis for verification is Real-Time Control Algorithms Logic RTCAL. Using this, we have developed and successfully applied software tools and domain-specific languages used at the design and operational stages of spacecraft control. The ongoing work includes introducing SMT solvers into our approach, and automatic generation of valid control logic.}
}
@article{LOCHAU201463,
title = {Delta-oriented model-based integration testing of large-scale systems},
journal = {Journal of Systems and Software},
volume = {91},
pages = {63-84},
year = {2014},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2013.11.1096},
url = {https://www.sciencedirect.com/science/article/pii/S0164121213002781},
author = {Malte Lochau and Sascha Lity and Remo Lachmann and Ina Schaefer and Ursula Goltz},
keywords = {Large-scale systems, Model-based testing, Regression testing, Variable software architectures},
abstract = {Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain.}
}
@article{HUMMER20131884,
title = {Enforcement of entailment constraints in distributed service-based business processes},
journal = {Information and Software Technology},
volume = {55},
number = {11},
pages = {1884-1903},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913001006},
author = {Waldemar Hummer and Patrick Gaubatz and Mark Strembeck and Uwe Zdun and Schahram Dustdar},
keywords = {Identity and access management, Business process management, Entailment constraints, Service-Oriented Architecture (SOA), WS-BPEL},
abstract = {Context
A distributed business process is executed in a distributed computing environment. The service-oriented architecture (SOA) paradigm is a popular option for the integration of software services and execution of distributed business processes. Entailment constraints, such as mutual exclusion and binding constraints, are important means to control process execution. Mutually exclusive tasks result from the division of powerful rights and responsibilities to prevent fraud and abuse. In contrast, binding constraints define that a subject who performed one task must also perform the corresponding bound task(s).
Objective
We aim to provide a model-driven approach for the specification and enforcement of task-based entailment constraints in distributed service-based business processes.
Method
Based on a generic metamodel, we define a domain-specific language (DSL) that maps the different modeling-level artifacts to the implementation-level. The DSL integrates elements from role-based access control (RBAC) with the tasks that are performed in a business process. Process definitions are annotated using the DSL, and our software platform uses automated model transformations to produce executable WS-BPEL specifications which enforce the entailment constraints. We evaluate the impact of constraint enforcement on runtime performance for five selected service-based processes from existing literature.
Results
Our evaluation demonstrates that the approach correctly enforces task-based entailment constraints at runtime. The performance experiments illustrate that the runtime enforcement operates with an overhead that scales well up to the order of several ten thousand logged invocations. Using our DSL annotations, the user-defined process definition remains declarative and clean of security enforcement code.
Conclusion
Our approach decouples the concerns of (non-technical) domain experts from technical details of entailment constraint enforcement. The developed framework integrates seamlessly with WS-BPEL and the Web services technology stack. Our prototype implementation shows the feasibility of the approach, and the evaluation points to future work and further performance optimizations.}
}
@article{SLAWIK2018846,
title = {Establishing User-centric Cloud Service Registries},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {846-867},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304813},
author = {Mathias Slawik and Begüm İlke Zilci and Axel Küpper},
keywords = {Cloud service registry, Cloud computing, Service matchmaking, Cloud brokering},
abstract = {Many potential cloud consumers are overburdened by the challenges persisting when discovering, assessing, and selecting contemporary Cloud Service offerings: the cloud market is vast and fast-moving, the selection criteria are ambiguous, service knowledge is scattered through the Internet, and features as well as prices are complex and incomparable. Much research has been carried out to create cloud service registries to help users select cloud services for eventual consumption, especially within the field of semantic web services. Through analyzing real-world requirements of six use cases we identified a gap in research for user-centric technologies. We fill this gap by creating a business vocabulary reflecting common service selection criteria, defining a textual domain specific language to let any user describe services easily, and implementing a novel brokering and matchmaking component to support users in their selection process. As a combination of those technologies, we create the Open Service Compendium (OSC), a crowd-sourced cloud service registry. Our evaluation activities highlight how these developments solve real-world challenges in diverse near-production settings. All of this implies that a substantial benefit for service registry users can be created by following a simple architecture that is focused on their concrete needs — instead of aiming for highest sophistication and broadest applicability as observed in many of the related works.}
}
@article{FOURATI20222638,
title = {Validating Event-B models using PDDL},
journal = {Procedia Computer Science},
volume = {207},
pages = {2638-2647},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.322},
url = {https://www.sciencedirect.com/science/article/pii/S187705092201211X},
author = {Farah FOURATI and Mohamed Tahar BHIRI and Riadh ROBBANA},
keywords = {Event-B, PDDL, Transformation, MDE plugin, Planification, Plan solution},
abstract = {The formal Event-B method makes it possible to obtain systems or software correct by construction. The modeling activity aims to establish formal models in Event-B. As for the proof activity, it aims at correcting these models. A proven Event-B model is not necessarily valid. We propose to open the Event-B formal method on PDDL in order to further validate Event-B models. We aim to validate the dynamic properties related to the sequence of events authorized on Event-B models. PDDL is a de facto standard language for planning problems. It is equipped with planners to automatically generate solution-plans. To achieve this, we successfully used the MDE approach, Xtext to create an integrated development environment specific to the Event-B language and the Xtend tool to implement our transformation EventB2PDDL and produce PDDL descriptions using a top-down approach.}
}
@article{CASADEI2022101248,
title = {ScaFi: A Scala DSL and Toolkit for Aggregate Programming},
journal = {SoftwareX},
volume = {20},
pages = {101248},
year = {2022},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2022.101248},
url = {https://www.sciencedirect.com/science/article/pii/S2352711022001662},
author = {Roberto Casadei and Mirko Viroli and Gianluca Aguzzi and Danilo Pianini},
keywords = {Aggregate programming, Computational fields, Macro-level programming, Distributed computing, Scala toolkit},
abstract = {Supported by current socio-scientific trends, programming the global behaviour of whole computational collectives makes for great opportunities, but also significant challenges. Recently, aggregate computing has emerged as a prominent paradigm for so-called collective adaptive systems programming. To shorten the gap between such research endeavours and mainstream software development and engineering, we present ScaFi, a Scala toolkit providing an internal domain-specific language, libraries, a simulation environment, and runtime support for practical aggregate computing systems development.}
}
@article{DIAZ2012737,
title = {Wiki Scaffolding: Aligning wikis with the corporate strategy},
journal = {Information Systems},
volume = {37},
number = {8},
pages = {737-752},
year = {2012},
note = {Special Issue: Advanced Information Systems Engineering (CAiSE'11)},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2012.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437912000695},
author = {Oscar Díaz and Gorka Puente},
keywords = {Wiki, Wiki management, DSL, Mind map, FreeMind},
abstract = {Wikis are main exponents of collaborative development by user communities. This community may be created around the wiki itself (e.g., community of contributors in Wikipedia) or already exist (e.g., company employees in corporate wikis). In the latter case, the wiki is not created in a vacuum but as part of the information ecosystem of the hosting organization. As any other Information System resource, wiki success highly depends on the interplay of technology, work practice and the organization. Thus, wiki contributions should be framed along the concerns already in use in the hosting organization in terms of glossaries, schedules, policies, organigrams and the like. The question is then, how can corporate strategies permeate wiki construction while preserving wiki openness and accessibility? We advocate for the use of “Wiki Scaffoldings”, i.e., a wiki installation that is provided at the onset to mimic these corporate concerns: categories, users, templates, articles initialized with boilerplate text, are all introduced in the wiki before any contribution is made. To retain wikis' friendliness and engage layman participation, we propose scaffoldings to be described as mind maps. Mind maps are next “exported” as wiki installations. We show the feasibility of the approach introducing a Wiki Scaffolding Language (WSL). WSL is realized as a plugin for FreeMind, a popular tool for mind mapping. Finally, we validate the expressiveness of WSL in four case studies. WSL is available for download.}
}
@incollection{LAM2022203,
title = {Chapter 7 - End-to-end fiber access system & network scaling},
editor = {Cedric F. Lam and Shuang Yin and Tao Zhang},
booktitle = {Advanced Fiber Access Networks},
publisher = {Academic Press},
pages = {203-251},
year = {2022},
isbn = {978-0-323-85499-3},
doi = {https://doi.org/10.1016/B978-0-323-85499-3.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323854993000011},
author = {Cedric F. Lam and Shuang Yin and Tao Zhang},
keywords = {Network deployment, Network scaling, Network hierarchy, PON redundancy, Broadband network gateway (BNG), Edge cache, Content distribution network (CDN), Network traffic characteristics, Statistical multiplexing, Equivalent circuit rate model, Muxtender, Network economics, Disaggregated access network, Central Office Re-Architected as Datacenter (CORD), Software defined network (SDN), SDN enabled broadband access (SEBA), Virtual OLT, Virtual OLT hardware abstraction (VOLTHA)},
abstract = {A real fiber access network is much more than just the last-mile PON network. In this chapter, we review the end-to-end system that makes up a full fiber access network. We will realize in this chapter that network upgrade and optimization involves much more than the PON section from the OLT to the ONT. In fact, the PON access network may be the last segment that affects the overall network cost and performance. We will look at the various system bottlenecks in scaling an end-to-end FTTH access network. PON redundancy is another subject we will discuss here. Traditionally, residential access networks (whether they are FTTH, HFC, or DSL networks) were rarely built with redundancy. We will explain how to achieve significant advantages in cost and availability to construct FTTH networks with PON access layer redundancy. Lastly, we will explain how Software Defined Network (SDN), a concept widely employed in datacenter networks, can benefit telecom operators.}
}
@incollection{2017267,
title = {Index},
editor = {Edward Griffor},
booktitle = {Handbook of System Safety and Security},
publisher = {Syngress},
address = {Boston},
pages = {267-273},
year = {2017},
isbn = {978-0-12-803773-7},
doi = {https://doi.org/10.1016/B978-0-12-803773-7.00022-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012803773700022X}
}
@article{BJARNASON201661,
title = {A multi-case study of agile requirements engineering and the use of test cases as requirements},
journal = {Information and Software Technology},
volume = {77},
pages = {61-79},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300544},
author = {Elizabeth Bjarnason and Michael Unterkalmsteiner and Markus Borg and Emelie Engström},
keywords = {Agile development, Requirements, Testing, Test-first development, Test-driven development, Behaviour-driven development, Acceptance test, Case study, Empirical software engineering},
abstract = {Context
It is an enigma that agile projects can succeed ‘without requirements’ when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases.
Objective
We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies.
Method
We performed an iterative case study at three companies and collected data through 14 interviews and two focus groups.
Results
The use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements.
Conclusions
The findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change.}
}
@article{VANBEEK2020301021,
title = {Digital forensics as a service: Stepping up the game},
journal = {Forensic Science International: Digital Investigation},
volume = {35},
pages = {301021},
year = {2020},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2020.301021},
url = {https://www.sciencedirect.com/science/article/pii/S2666281720300706},
author = {H.M.A. {van Beek} and J. {van den Bos} and A. Boztas and E.J. {van Eijk} and R. Schramp and M. Ugen},
keywords = {Digital forensics, Digital forensics as a service, DFaaS, Hansken},
abstract = {After providing Digital Forensics as a Service (DFaaS) implementations to law enforcement agencies for close to a decade, we present our view from an inside-out perspective. We share the lessons learned from an organizational, operational and development perspective in a forensic and legal context. We conclude with our vision on how to bring the DFaaS concept to the next level for both investigative and innovative purposes.}
}
@article{CHAVARRIAGA2017133,
title = {An approach to build XML-based domain specific languages solutions for client-side web applications},
journal = {Computer Languages, Systems & Structures},
volume = {49},
pages = {133-151},
year = {2017},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1477842416301634},
author = {Enrique Chavarriaga and Francisco Jurado and Fernando Díez},
keywords = {Domain-Specific Languages, XML interpreter, JavaScript, Web Application, XML programing language},
abstract = {Summary
Domain-Specific Languages (DSLs) allow for the building of applications that ease the labour of both software engineers and domain experts thanks to the level of abstraction they provide. In cases where the domain is restricted to Client-Side Web Applications (CSWA), XML-based languages, frameworks and widgets are commonly combined in order to provide fast, robust and flexible solutions. This article presents an approach designed to create XML-based DSL solutions for CSWA that includes an evaluation engine, a programming model and a lightweight development environment. The approach is able to evaluate multiple XML-based DSL programs simultaneously to provide solutions to those Domain Specific Problems for CSWAs. To better demonstrate the capabilities and potential of this novel approach, we will employ a couple of case studies, namely Anisha and FeedPsi.}
}
@incollection{BUCHGEHER2014161,
title = {Chapter 7 - Continuous Software Architecture Analysis},
editor = {Muhammad {Ali Babar} and Alan W. Brown and Ivan Mistrik},
booktitle = {Agile Software Architecture},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {161-188},
year = {2014},
isbn = {978-0-12-407772-0},
doi = {https://doi.org/10.1016/B978-0-12-407772-0.00006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012407772000006X},
author = {Georg Buchgeher and Rainer Weinreich},
keywords = {Agile development, Continuous quality control, Continuous software architecture analysis, Software architecture, Software architecture analysis},
abstract = {This chapter discusses software architecture analysis in the context of agile processes. Agile processes are characterized by incremental and interleaved activities and by a focus on continuous improvement and delivery. Most approaches to software architecture analysis, however, have been developed to be performed at dedicated points in the development process or as external evaluation activities and not as continuous activities throughout the development process. This chapter discusses continuous software architecture analysis (CSAA). It reviews important requirements for CSAA and outlines how CSAA is supported by current software architecture analysis approaches. It further presents experiences with an approach for continuous structural and conformance analysis and identifies future research challenges.}
}
@article{LAZAR201091,
title = {Behaviour-Driven Development of Foundational UML Components},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {264},
number = {1},
pages = {91-105},
year = {2010},
note = {Proceedings of the 7th International Workshop on Formal Engineering approaches to Software Components and Architectures (FESCA 2010)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2010.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066110000666},
author = {Ioan Lazăr and Simona Motogna and Bazil Pârv},
keywords = {behaviour-driven development, executable UML, user story, executable specification, acceptance criteria},
abstract = {Behaviour-Driven Development (BDD) focuses all development activities on the delivery of behaviours – what a system should do, described such that developers and domain experts speak the same language. BDD frameworks allow users to represent the required system behaviour as executable user stories and the acceptance criteria as executable scenarios attached to user stories. In this paper we define a UML profile that allows users to create executable Foundational UML (fUML) stories and scenarios. In order to easily construct scenarios we introduce a BDD model library which contains fUML activities for testing equalities and inclusions. We also present an Eclipse-based development tool that supports a BDD approach for developing fUML components. The tool provides developers a concrete syntax for defining executable scenarios, and automatically updates the project status based on verified delivered behaviorus.}
}
@article{KOS201674,
title = {Test automation of a measurement system using a domain-specific modelling language},
journal = {Journal of Systems and Software},
volume = {111},
pages = {74-88},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215002058},
author = {Tomaž Kos and Marjan Mernik and Tomaž Kosar},
keywords = {Test automation, Domain-specific modelling languages, Usage experience},
abstract = {The construction of domain-specific modelling languages (DSMLs) is only the first step within the needed toolchain. Models need to be maintained, modified or functional errors searched for. Therefore, tool support is vital for the DSML end-user’s efficiency. This paper presents SeTT, a simple but very useful tool for DSML end-users, a testing framework integrated within a DSML Sequencer. This Sequencer, part of the DEWESoft data acquisition system, supports the development of model-based tests using a high-level abstraction. The tests are used during the whole data acquisition process and able to test different systems’ parts. This paper shows how high-level specifications can be extended to describe a testing infrastructure for a specific DSML. In this manner, the Sequencer and SeTT were combined at the metamodel level. The contribution of the paper is to show that one can leverage on the DSML to build a testing framework with relatively little effort, by implementing assertions to it.}
}
@article{CALDERON2018238,
title = {MEdit4CEP-Gam: A model-driven approach for user-friendly gamification design, monitoring and code generation in CEP-based systems},
journal = {Information and Software Technology},
volume = {95},
pages = {238-264},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917303464},
author = {Alejandro Calderón and Juan Boubeta-Puig and Mercedes Ruiz},
keywords = {Gamification, Model-Driven Engineering, Complex event processing, Strategy design, Monitoring, Graphical modelling editor},
abstract = {Context
Gamification has been proven to increase engagement and motivation in multiple and different non-game contexts such as healthcare, education, workplace, and marketing, among others. However, many of these applications fail to achieve the desired benefits of gamification, mainly because of a poor design.
Objective
This paper explores the conceptualization, implementation and monitoring phases of meaningful gamification strategies and proposes a solution for strategy experts that hides the implementation details and helps them focus only on what is crucial for the success of the strategy. The solution makes use of Model-Driven Engineering (MDE) and Complex Event Processing (CEP) technology.
Method
An easy-to-use graphical editor is used to provide the high-level models that represent the design of the gamification strategy and its deployment and monitoring. These models contain the event pattern definitions to be automatically transformed into code. This code is then deployed both in a CEP engine to detect the conditions expressed in such patterns and in an enterprise service bus to execute the corresponding pattern actions.
Results
The paper reports on the use of both a graphical modeling editor for gamification domain definition and a graphical modeling editor for gamification strategy design, monitoring and code generation in event-based systems. It also shows how the proposal can be used to design and automate the implementation and monitoring of a gamification strategy in an educational domain supported by a well-known Learning Management System (LMS) such as Moodle.
Conclusion
It can be concluded that this unprecedented model-driven approach leveraging gamification and CEP technology provides strategy experts with the ability to graphically define gamification strategies, which can be directly transformed into code executable by event-based systems. Therefore, this is a novel solution for bringing CEP closer to any strategy expert, positively influencing the gamification strategy design, implementation and real-time monitoring processes.}
}
@article{TYUGASHEV2016120,
title = {Language and Toolset for Visual Construction of Programs for Intelligent Autonomous Spacecraft Control},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {5},
pages = {120-125},
year = {2016},
note = {4th IFAC Conference on Intelligent Control and Automation SciencesICONS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.100},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316302968},
author = {Andrey A. Tyugashev},
keywords = {Autonomous Control, Satellite Control, Diagnostic Programs, Software Tools, Software Engineering, Domain Specific Language, Visual Software Construction},
abstract = {The paper describes approach to Autonomous Fault Tolerant Intelligent Control of Spacecraft based on usage of Onboard Real-Time Interpreter of Integrated Control Programs, including special Diagnostic Routine. Rules of the Autonomous Control Program could be added or refined from Earth in operative manner by radio channel. Specially designed Visual Domain Specific Language allowing Control Logic Designers check, analyze and construct Rules in user friendly graphical environment excluding necessity to involve Software Developers. Proposed approach allows reducing of costs and labor consuming of Space Mission because of reducing of efforts needed for common-style Flight Control Software coding, multi-stage testing and support. Special Software Engineering Toolset that including Visualizer and Graphical Constructor of these autonomous control programs presented as well as the principles of its design and development. The Prototype of the Toolset has been successfully introduced at JSC Information Satellite Systems, Krasnoyarsk Region, Russia.}
}
@article{BRYCE2006960,
title = {Prioritized interaction testing for pair-wise coverage with seeding and constraints},
journal = {Information and Software Technology},
volume = {48},
number = {10},
pages = {960-970},
year = {2006},
note = {Advances in Model-based Testing},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2006.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584906000401},
author = {Renée C. Bryce and Charles J. Colbourn},
keywords = {Biased covering arrays, Covering arrays, Greedy algorithm, Mixed-level covering arrays, Pair-wise interaction coverage, Software interaction testing, Test prioritization},
abstract = {Interaction testing is widely used in screening for faults. In software testing, it provides a natural mechanism for testing systems to be deployed on a variety of hardware and software configurations. In many applications where interaction testing is needed, the entire test suite is not run as a result of time or budget constraints. In these situations, it is essential to prioritize the tests. Here, we adapt a “one-test-at-a-time” greedy method to take importance of pairs into account. The method can be used to generate a set of tests in order, so that when run to completion all pair-wise interactions are tested, but when terminated after any intermediate number of tests, those deemed most important are tested. In addition, practical concerns of seeding and avoids are addressed. Computational results are reported.}
}
@article{DOGAN2014174,
title = {Web application testing: A systematic literature review},
journal = {Journal of Systems and Software},
volume = {91},
pages = {174-201},
year = {2014},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0164121214000223},
author = {Serdar Doğan and Aysu Betin-Can and Vahid Garousi},
keywords = {Systematic literature review, Web application, Testing},
abstract = {Context
The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013.
Objective
As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field.
Methods
We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area.
Results
Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance.
Conclusion
We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community.}
}
@article{PONCELET2016143,
title = {Model-based testing for building reliable realtime interactive music systems},
journal = {Science of Computer Programming},
volume = {132},
pages = {143-172},
year = {2016},
note = {Special Issue on Software Verification and Testing (SAC-SVT'15)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167642316301022},
author = {Clement Poncelet and Florent Jacquemard},
keywords = {Model based testing, Interactive music systems, Timed automata},
abstract = {The role of an Interactive Music System (IMS) is to accompany musicians during live performances, acting like a real musician. It must react in realtime to audio signals from musicians, according to a timed high-level requirement called mixed score, written in a domain specific language. Such goals imply strong requirements of temporal reliability and robustness to unforeseen errors in input, yet not much addressed by the computer music community. We present the application of Model-Based Testing techniques and tools to a state-of-the-art IMS, including in particular: offline and on-the-fly approaches for the generation of relevant input data for testing (including timing values), with coverage criteria, the computation of the corresponding expected output, according to the semantics of a given mixed score, the black-box execution of the test data on the System Under Test and the production of a verdict. Our method is based on formal models in a dedicated intermediate representation, compiled directly from mixed scores (high-level requirements), and either passed, to the model-checker Uppaal (after conversion to Timed Automata) in the offline approach, or executed by a virtual machine in the online approach. Our fully automatic framework has been applied to real mixed scores used in concerts and the results obtained have permitted to identify bugs in the target IMS.}
}
@incollection{2023485,
title = {Glossary},
editor = {Marilyn Wolf},
booktitle = {Computers as Components (Fifth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fifth Edition},
pages = {485-504},
year = {2023},
series = {The Morgan Kaufmann Series in Computer Architecture and Design},
issn = {15459888},
doi = {https://doi.org/10.1016/B978-0-323-85128-2.17001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032385128217001X}
}
@incollection{2010489,
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP Study Guide},
publisher = {Syngress},
address = {Boston},
pages = {489-523},
year = {2010},
isbn = {978-1-59749-563-9},
doi = {https://doi.org/10.1016/B978-1-59749-563-9.00020-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781597495639000202}
}
@article{KHREIS2022107596,
title = {Impact of long-term air pollution exposure on incidence of neurodegenerative diseases: A protocol for a systematic review and exposure-response meta-analysis},
journal = {Environment International},
volume = {170},
pages = {107596},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2022.107596},
url = {https://www.sciencedirect.com/science/article/pii/S0160412022005232},
author = {Haneen Khreis and Christiaan Bredell and Kwan {Wai Fung} and Lucy Hong and Magdalena Szybka and Veronica Phillips and Ali Abbas and Youn-Hee Lim and Zorana {Jovanovic Andersen} and James Woodcock and Carol Brayne},
keywords = {Neurodegenerative, Dementia, Alzheimer’s disease, Parkinson’s disease, Multiple sclerosis, Motor neuron disease, Air pollution, Protocol},
abstract = {Background
Ambient air pollution is a pervasive and ubiquitous hazard, which has been linked to premature morbidity and a growing number of morbidity endpoints. Air pollution may be linked to neurodegeneration, and via this or other pathways, to neurodegenerative diseases. Emerging evidence suggests that air pollution may contribute to neurodegenerative diseases such as dementia, Parkinson’s Disease (PD), Multiple Sclerosis (MS) and Motor Neuron Diseases (MND), although this evidence remains inconsistent and very limited for MS and MND. In addition, this evidence base is rapidly emerging and would benefit from a wide and critical synthesis, including a better understanding of heterogeneity.
Objectives
In this paper, we present a protocol for a systematic review and meta-analysis and specify our methods a priori. The main aim of the planned systematic review is to answer the question of whether long-term exposure (>1 year) to ambient (outdoor) air pollution (exposure, compared to lower exposure) increases the risk of adult (population) incidence of neurodegenerative diseases (outcomes) in epidemiological observational studies (study design). Another aim is to meta-analyze the associations between long-term exposure to ambient air pollutants and the risk of the selected outcomes and assess the shape of exposure–response functions. To set the stage for the proposed work, we also overview the existing epidemiological evidence in this protocol, but do not critically evaluate it, as these results will be fully presented in the planned systematic review.
Search and study eligibility
We will search the electronic databases Medline (via Ovid), Embase (via Ovid), Cochrane Library, Cinahl (via Ebscohost), Global Health (via Ebscohost), PsycINFO (via Ebscohost), Scopus, Web of Science (Core Collection), from inception to October 2022. Eligible studies must contain primary research investigating the link between 1-year + exposure to any outdoor air pollutant, from any source, and dementia, PD, MS, and MND, or dementia subtypes: Alzheimer’s Disease, vascular dementia, and mixed dementia. The search strategy and eligibility criteria are pre-determined and described in full in this protocol.
Study appraisal and synthesis methods
Articles will be stored and screened using Rayyan QCRI. Title and abstract screening, full text review, data extraction, risk of bias assessment and data preparation for statistical analysis will be conducted independently by two reviewers using pre-defined forms and criteria, described in this protocol. All these steps will also be piloted and the forms and/or methods adapted if issues arise. Meta-analysis and assessment of the shape of the exposure–response functions will be conducted if four independent exposure-outcomes pairs are available, and the remainder of results will be synthesized in the forms of tables and via a narrative summary. Certainty in the body of evidence will be assessed using the OHAT approach. This protocol describes the planned analysis and synthesis a priori and serves to increase transparency and impact of this systematic review and meta-analysis.}
}
@article{ALMENDROSJIMENEZ2016332,
title = {PTL: A model transformation language based on logic programming},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {85},
number = {2},
pages = {332-366},
year = {2016},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2015.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352220815000565},
author = {Jesús M. Almendros-Jiménez and Luis Iribarne and Jesús López-Fernández and Ángel Mora-Segura},
keywords = {Logic programming, Model transformation, Software engineering, Model driven engineering, Domain specific languages},
abstract = {In this paper we present a model transformation language based on logic programming. The language, called PTL (Prolog based Transformation Language), can be considered as a hybrid language in which ATL (Atlas Transformation Language)-style rules are combined with logic rules for defining transformations. ATL-style rules are used to define mappings from source models to target models while logic rules are used as helpers. The implementation of PTL is based on the encoding of the ATL-style rules by Prolog rules. Thus, PTL makes use of Prolog as a transformation engine. We have provided a declarative semantics to PTL and proved the semantics equivalent to the encoded program. We have studied an encoding of OCL (Object Constraint Language) with Prolog goals in order to map ATL to PTL. Thus a subset of PTL can be considered equivalent to a subset of ATL. The proposed language can be also used for model validation, that is, for checking constraints on models and transformations. We have equipped our language with debugging and tracing capabilities which help developers to detect programming errors in PTL rules. Additionally, we have developed an Eclipse plugin for editing PTL programs, as well as for debugging, tracing and validation. Finally, we have evaluated the language with several transformation examples as well as tested the performance with large models.}
}
@article{AFZAL201886,
title = {The MegaM@Rt2 ECSEL project: MegaModelling at Runtime – Scalable model-based framework for continuous development and runtime validation of complex systems},
journal = {Microprocessors and Microsystems},
volume = {61},
pages = {86-95},
year = {2018},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S014193311830022X},
author = {Wasif Afzal and Hugo Bruneliere and Davide {Di Ruscio} and Andrey Sadovykh and Silvia Mazzini and Eric Cariou and Dragos Truscan and Jordi Cabot and Abel Gómez and Jesús Gorroñogoitia and Luigi Pomante and Pavel Smrz},
keywords = {Model-driven engineering, Design time, Runtime, Megamodelling},
abstract = {A major challenge for the European electronic industry is to enhance productivity by ensuring quality of development, integration and maintenance while reducing the associated costs. Model-Driven Engineering (MDE) principles and techniques have already shown promising capabilities, but they still need to scale up to support real-world scenarios implied by the full deployment and use of complex electronic components and systems. Moreover, maintaining efficient traceability, integration, and communication between two fundamental system life cycle phases (design time and runtime) is another challenge requiring the scalability of MDE. This paper presents an overview of the ECSEL 11http://www.ecsel-ju.eu/web/index.php. project entitled “MegaModelling at runtime – Scalable model-based framework for continuous development and runtime validation of complex systems” (MegaM@Rt2), whose aim is to address the above mentioned challenges facing MDE. Driven by both large and small industrial enterprises, with the support of research partners and technology providers, MegaM@Rt2 aims to deliver a framework of tools and methods for: 1) system engineering/design and continuous development, 2) related runtime analysis and 3) global models and traceability management. Diverse industrial use cases (covering strategic domains such as aeronautics, railway, construction and telecommunications) will integrate and demonstrate the validity of the MegaM@Rt2 solution. This paper provides an overview of the MegaM@Rt2 project with respect to its approach, mission, objectives as well as to its implementation details. It further introduces the consortium as well as describes the work packages and few already produced deliverables.}
}
@article{HUTCHESSON2013525,
title = {Trusted Product Lines},
journal = {Information and Software Technology},
volume = {55},
number = {3},
pages = {525-540},
year = {2013},
note = {Special Issue on Software Reuse and Product Lines},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001085},
author = {Stuart Hutchesson and John McDermid},
keywords = {Software Product Lines, High-integrity software, DO-178B/ED-12B, SPARK, Model transformation, GSN},
abstract = {Context
The paper addresses the use of a Software Product Line approach in the context of developing software for a high-integrity, regulated domain such as civil aerospace. The success of a Software Product Line approach must be judged on whether useful products can be developed more effectively (lower cost, reduced schedule) than with traditional single-system approaches. When developing products for regulated domains, the usefulness of the product is critically dependent on the ability of the development process to provide approval evidence for scrutiny by the regulating authority.
Objective
The objective of the work described is to propose a framework for arguing that a product instantiated using a Software Product Line approach can be approved and used within a regulated domain, such that the development cost of that product would be less than if it had been developed in isolation.
Method
The paper identifies and surveys the issues relating the adoption of Software Product Lines as currently understood (including related technologies such as feature modelling, component-based development and model transformation) when applied to high-integrity software development. We develop an argument framework using Goal Structuring Notation to structure the claims made and the evidence required to support the approval of an instantiated product in such domains. Any unsubstantiated claims or missing/sub-standard evidence is identified, and we propose potential approaches or pose research questions to help address this.
Results
The paper provides an argument framework supporting the use of a Software Product Line approach within a high-integrity regulated domain. It shows how lifecycle evidence can be collected, managed and used to credibly support a regulatory approval process, and provides a detailed example showing how claims regarding model transformation may be supported. Any attempt to use a Software Product Line approach in a regulated domain will need to provide evidence to support their approach in accordance with the argument outlined in the paper.
Conclusion
Product Line practices may complicate the generation of convincing evidence for approval of instantiated products, but it is possible to define a credible Trusted Product Line approach.}
}
@article{ISLAM2023111543,
title = {Configuring mission-specific behavior in a product line of collaborating Small Unmanned Aerial Systems},
journal = {Journal of Systems and Software},
volume = {197},
pages = {111543},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111543},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002199},
author = {Md Nafee Al Islam and Muhammed Tawfiq Chowdhury and Ankit Agrawal and Michael Murphy and Raj Mehta and Daria Kudriavtseva and Jane Cleland-Huang and Michael Vierhauser and Marsha Chechik},
keywords = {Dynamic configuration, Small unmanned aerial system, sUAS, Emergency response, Product line},
abstract = {In emergency response scenarios, autonomous small Unmanned Aerial Systems (sUAS) must be configured and deployed quickly and safely to perform mission-specific tasks. In this paper, we present Drone Response, a Software Product Line for rapidly configuring and deploying a multi-role, multi-sUAS mission whilst guaranteeing a set of safety properties related to the sequencing of tasks within the mission. Individual sUAS behavior is governed by an onboard state machine, combined with coordination handlers which are configured dynamically within seconds of launch and ultimately determine the sUAS’ behaviors, transition decisions, and interactions with other sUAS, as well as human operators. The just-in-time manner in which missions are configured precludes robust upfront testing of all conceivable combinations of features — both within individual sUAS and across cohorts of collaborating ones. To ensure the absence of common types of configuration failures and to promote safe deployments, we check vital properties of the dynamically generated sUAS specifications and coordination handlers before sUAS are assigned their missions. We evaluate our approach in two ways. First, we perform validation tests to show that the end-to-end configuration process results in correctly executed missions, and second, we apply fault-based mutation testing to show that our safety checks successfully detect incorrect task sequences.}
}
@incollection{MARIANI2015157,
title = {Chapter Four - Recent Advances in Automatic Black-Box Testing},
editor = {Atif Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {99},
pages = {157-193},
year = {2015},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2015.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245815000315},
author = {Leonardo Mariani and Mauro Pezzè and Daniele Zuddas},
keywords = {Black-box testing, Model-based testing, Random testing, Testing with complex inputs, Combinatorial interaction testing, Test automation},
abstract = {Research in black-box testing has produced impressive results in the past 40 years, addressing many aspects of the problem that span from integration with the development process, to test case generation and execution. In the past few years, the research in this area has focused mostly on the automation of black-box approaches to improve applicability and scalability. This chapter surveys the recent advances in automatic black-box testing, covering contributions from 2010 to 2014, presenting the main research results and discussing the research trends.}
}
@article{EDWARDS2019277,
title = {Robust configuration of the JET Real-Time Protection Sequencer},
journal = {Fusion Engineering and Design},
volume = {146},
pages = {277-280},
year = {2019},
note = {SI:SOFT-30},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2018.12.045},
url = {https://www.sciencedirect.com/science/article/pii/S092037961830810X},
author = {James S. Edwards and I.S. Carvalho and R. Felton and C. Hogben and D. Karkinsky and P.J. Lomas and P.A. McCullen and F.G. Rimini and A.V. Stephen},
keywords = {Configuration, Machine protection, Real-time framework, Domain-specific language, Software validation, Operations},
abstract = {The JET Real-Time Protection Sequencer (RTPS) co-ordinates responses for magnetic and kinetic actuators to protect the ITER-Like Wall from possible melting events and other undesirable scenarios. It allows programmable stop responses per pulse, based on alarms raised by other systems. The architecture combines a modular run-time application developed using MARTe (Multithreaded Application Real-Time executor) with the top-level JET supervisory and configuration software, Level-1. Operational experience since 2011 drove a requirement to refactor the system in 2017, moving the maximum degree of functionality from compiled code to configuration data, providing more flexibility, maintainability and verifiability of action(s) to be taken during a pulse. This paper discusses the features of the architecture that made this clean separation of rule-based logic and real-time signal processing possible and practical, including how functions and interfaces between MARTe and Level-1 are organised. It also explains management of configuration data to address development, testing, commissioning and operations, each with individual ownership, responsibility and lifecycles. The core technology enabling this is the Level-1 domain specific language, able to manipulate, validate and load into plant configuration parameter sets. The language also enables implementation of advanced user interfaces, providing operators with the tools to focus on essentials tasks for their area of responsibility. It exemplifies this with recent verification and validation of the refactored protection system: unit/low-level integration tests defined by core developers and integration/behavioural tests defined by JET’s Plasma Operations Group, respectively, ensuring robust and consistent behaviour. We show how the wide scope and power of this language has enabled evolution of JET operations efficiently and correctly over decades of operational experience.}
}
@article{HABERMAIER201544,
title = {Executable Specifications of Safety-Critical Systems with S#},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {7},
pages = {44-49},
year = {2015},
note = {5th IFAC International Workshop on Dependable Control of Discrete Systems},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.06.471},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315007107},
author = {Axel Habermaier and Johannes Leupolz and Wolfgang Reif},
keywords = {safety analysis, executable specification, design tools and techniques, formal methods, model checking, deductive cause consequence analysis, software engineering},
abstract = {Abstract:
Model-based safety analysis techniques use formal methods to rigorously assess the risks associated with safety-critical systems. The adequacy of the results obtained from those formal techniques, however, is greatly influenced by the quality and comprehensibility of the underlying formal models. We introduce our S# modeling framework (pronounced "safety sharp"), an executable, systematic, high-level specification language and tool suite specifically designed for the convenient modeling and formal analysis of safety-critical systems. This paper shows how S# facilitates and improves model simulation, debugging, and testing during all stages of the development of such systems.}
}
@article{PRASETYA2018223,
title = {Temporal algebraic query of test sequences},
journal = {Journal of Systems and Software},
volume = {136},
pages = {223-236},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S016412121730153X},
author = {I.S.W.B. Prasetya},
keywords = {Query based testing, Property based testing, LTL Test oracles, Algebraic test oracles, Dynamic analysis},
abstract = {Nowadays tools can generate test suites consisting of large number of test sequences. The used algorithms are typically random-based. Although more advanced variations may incorporate an advanced search algorithm to cover difficult scenarios, many decisions still have to be made randomly simply because no information is available to calculate the best decision. Because of this, many of the generated sequences may be redundant, while some others may be rare and hard to get. This paper presents a rich formalism that is based on a mix of algebraic relations and Linear Temporal Logic (LTL) to query test suites, and an efficient algorithm to execute such queries. Queries can be used as correctness specifications (oracles) to validate a test suite. They are however more general as they can be used to filter out test sequences with interesting properties, e.g. to archive them for future use. The proposed formalism is quite expressive: it can express algebraic equations with logical variables, Hoare triples, class invariants, as well as their temporal modalities. An evaluation of the query algorithm’s performance is included in this paper. The whole query framework has been implemented in a testing tool for Java called T3i.}
}
@incollection{ALEXANDER2007109,
title = {Chapter 5 - Protocol testing},
editor = {Tom Alexander},
booktitle = {Optimizing and Testing WLANs},
publisher = {Newnes},
address = {Burlington},
pages = {109-135},
year = {2007},
isbn = {978-0-7506-7986-2},
doi = {https://doi.org/10.1016/B978-075067986-2/50006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780750679862500069},
author = {Tom Alexander},
abstract = {Publisher Summary
This chapter describes the metrics and measurements pertinent to the wireless LAN (WLAN) Medium Access Control (MAC), as well as the Transmission Control Protocol (TCP)/Internet Protocol (IP) stack. A functional test is concerned with verifying that a device or system functions properly. A performance test is much more broad-based and generic, and is aimed at characterizing a device or system according to some well-defined metric. Indeed, a single performance metric may be applicable to a wide variety of devices. WLAN protocol tests, almost without exception, inject packet traffic into the device under test (DUT) as test stimuli, and measure the DUT response in terms of the number or rate of specific types of packets that it generates in turn. As tests are being performed on the MAC layer, the RF characteristics of the packet traffic are rarely of much interest, beyond ensuring that these characteristics do not skew the test results by causing unexpected issues at the physical (PHY) layer. Conformance tests are almost exclusively performed in a highly controlled and isolated environment, as external interference at the wrong moment can invalidate an entire test. It is found that IEEE 802.11 protocol conformance tests are performed on the different aspects of the WLAN-MAC and PHY protocol specifications.}
}
@article{KUMARA2021106593,
title = {The do’s and don’ts of infrastructure code: A systematic gray literature review},
journal = {Information and Software Technology},
volume = {137},
pages = {106593},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106593},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000720},
author = {Indika Kumara and Martín Garriga and Angel Urbano Romeu and Dario {Di Nucci} and Fabio Palomba and Damian Andrew Tamburri and Willem-Jan {van den Heuvel}},
keywords = {Infrastructure-as-code, DevOps, Gray literature review},
abstract = {Context:
Infrastructure-as-code (IaC) is the DevOps tactic of managing and provisioning software infrastructures through machine-readable definition files, rather than manual hardware configuration or interactive configuration tools.
Objective:
From a maintenance and evolution perspective, the topic has picked the interest of practitioners and academics alike, given the relative scarcity of supporting patterns and practices in the academic literature. At the same time, a considerable amount of gray literature exists on IaC. Thus we aim to characterize IaC and compile a catalog of best and bad practices for widely used IaC languages, all using gray literature materials.
Method:
In this paper, we systematically analyze the industrial gray literature on IaC, such as blog posts, tutorials, white papers using qualitative analysis techniques.
Results:
We proposed a definition for IaC and distilled a broad catalog summarized in a taxonomy consisting of 10 and 4 primary categories for best practices and bad practices, respectively, both language-agnostic and language-specific ones, for three IaC languages, namely Ansible, Puppet, and Chef. The practices reflect implementation issues, design issues, and the violation of/adherence to the essential principles of IaC.
Conclusion:
Our findings reveal critical insights concerning the top languages as well as the best practices adopted by practitioners to address (some of) those challenges. We evidence that the field of development and maintenance IaC is in its infancy and deserves further attention.}
}
@incollection{FRIEDRICH2014139,
title = {Chapter 11 - Knowledge Engineering for Configuration Systems},
editor = {Alexander Felfernig and Lothar Hotz and Claire Bagley and Juha Tiihonen},
booktitle = {Knowledge-Based Configuration},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {139-155},
year = {2014},
isbn = {978-0-12-415817-7},
doi = {https://doi.org/10.1016/B978-0-12-415817-7.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124158177000116},
author = {Gerhard Friedrich and Dietmar Jannach and Markus Stumptner and Markus Zanker},
keywords = {Knowledge-based Configuration, Development Life Cycle of Configurators, Debugging of Configuration Knowledge Bases, Organizational Challenges},
abstract = {Developing a product configuration system is a nontrivial and challenging task for various reasons. First, the domain knowledge that has to be encoded into the system is often spread over several departments or functions within a company. Besides that, in many cases data from existing information systems have to be integrated into the configurator. Finally, the business rules or technical constraints that define the space of possible configurations can be relatively complex and also subject to frequent changes. This makes acquiring and encoding domain knowledge as well as testing and debugging particularly demanding tasks. In this chapter, we give an overview of the challenges when developing a knowledge-based configuration system. We will particularly focus on questions related to the knowledge acquisition process and will additionally show how model-based debugging techniques can be applied to support the knowledge engineer in the testing and debugging process.}
}
@article{CADAVID2022106984,
title = {System and software architecting harmonization practices in ultra-large-scale systems of systems: A confirmatory case study},
journal = {Information and Software Technology},
volume = {150},
pages = {106984},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106984},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001197},
author = {Héctor Cadavid and Vasilios Andrikopoulos and Paris Avgeriou and P. Chris Broekema},
keywords = {Systems of systems, SoS architecting, Confirmatory case study, Empirical software engineering, Scientific instruments, Qualitative research},
abstract = {Context:
The challenges posed by the architecting of System of Systems (SoS) has motivated a significant number of research efforts in the area. However, literature is lacking when it comes to the interplay between the disciplines involved in the architecting process, a key factor in addressing these challenges.
Objective:
This paper aims to contribute to this line of research by confirming and extending previously characterized architecting harmonization practices from Systems and Software Engineering, adopted in an ultra-large-scale SoS.
Methods:
We conducted a confirmatory case study on the Square-Kilometre Array (SKA) project to evaluate and extend the findings of our exploratory case on the LOFAR/LOFAR2.0 radio-telescope projects. In doing so, a pre-study was conducted to map the findings of the previous study with respect to the SKA context. A survey was then designed, through which the views of 46 SKA engineers were collected and analyzed.
Results:
The study confirmed in various degrees the four practices identified in the exploratory case, and provided further insights about them: (1) the friction between disciplines caused by long-term system requirements, and how they can be ameliorated through intermediate, short-term requirements; (2) the way design choices with a cross-cutting impact on multiple agile teams have an indirect impact on the system architecture; (3) how these design choices are often caused by the criteria that guided early system decomposition; (4) the seemingly recurrent issue with the lack of details about the dynamic elements of the interfaces; and (5) the use of machine-readable interface specifications for aligning hardware/software development processes.
Conclusions:
The findings of this study and its predecessor support the importance of a cross-disciplinary view in the Software Engineering research agenda in SoS as a whole, not to mention their value as a convergence point for research on SoS architecting from the Systems and Software Engineering standpoints.}
}
@article{KAMEYAMA2015120,
title = {Combinators for impure yet hygienic code generation},
journal = {Science of Computer Programming},
volume = {112},
pages = {120-144},
year = {2015},
note = {Selected and extended papers from Partial Evaluation and Program Manipulation 2014},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S016764231500249X},
author = {Yukiyoshi Kameyama and Oleg Kiselyov and Chung-chieh Shan},
keywords = {Multi-stage programming, Mutable state and control effects, CPS, Higher-order abstract syntax, Lexical scope},
abstract = {Code generation is the leading approach to making high-performance software reusable. Effects are indispensable in code generators, whether to report failures or to insert let-statements and if-guards. Extensive painful experience shows that unrestricted effects interact with generated binders in undesirable ways to produce unexpectedly unbound variables, or worse, unexpectedly bound ones. These subtleties hinder domain experts in using and extending the generator. A pressing problem is thus to express the desired effects while regulating them so that the generated code is correct, or at least correctly scoped, by construction. We present a code-combinator framework that lets us express arbitrary monadic effects, including mutable references and delimited control, that move open code across generated binders. The static types of our generator expressions not only ensure that a well-typed generator produces well-typed and well-scoped code. They also express the lexical scopes of generated binders and prevent mixing up variables with different scopes. For the first time ever we demonstrate statically safe and well-scoped loop interchange and constant factoring from arbitrarily nested loops. Our framework is implemented as a Haskell library that embeds an extensible typed higher-order domain-specific language. It may be regarded as ‘staged Haskell.’ To become practical, the library relies on higher-order abstract syntax and polymorphism over generated type environments, and is written in a mature language.}
}
@article{RODRIGUESDASILVA2015139,
title = {Model-driven engineering: A survey supported by the unified conceptual model},
journal = {Computer Languages, Systems & Structures},
volume = {43},
pages = {139-155},
year = {2015},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2015.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1477842415000408},
author = {Alberto {Rodrigues da Silva}},
keywords = {Model, Metamodel, Modeling language, Software system, Model-driven engineering, Model-driven approaches},
abstract = {During the last decade a new trend of approaches has emerged, which considers models not just documentation artefacts, but also central artefacts in the software engineering field, allowing the creation or automatic execution of software systems starting from those models. These proposals have been classified generically as Model-Driven Engineering (MDE) and share common concepts and terms that need to be abstracted, discussed and understood. This paper presents a survey on MDE based on a unified conceptual model that clearly identifies and relates these essential concepts, namely the concepts of system, model, metamodel, modeling language, transformations, software platform, and software product. In addition, this paper discusses the terminologies relating MDE, MDD, MDA and others. This survey is based on earlier work, however, contrary to those, it intends to give a simple, broader and integrated view of the essential concepts and respective terminology commonly involved in the MDE, answering to key questions such as: What is a model? What is the relation between a model and a metamodel? What are the key facets of a modeling language? How can I use models in the context of a software development process? What are the relations between models and source code artefacts and software platforms? and What are the relations between MDE, MDD, MDA and other MD approaches?}
}
@incollection{2012513,
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP Study Guide (Second Edition)},
publisher = {Syngress},
edition = {Second Edition},
address = {Boston},
pages = {513-547},
year = {2012},
isbn = {978-1-59749-961-3},
doi = {https://doi.org/10.1016/B978-1-59749-961-3.09985-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781597499613099854}
}
@article{LIU2019231,
title = {A scala based framework for developing acceleration systems with FPGAs},
journal = {Journal of Systems Architecture},
volume = {98},
pages = {231-242},
year = {2019},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2019.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1383762119302097},
author = {Yanqiang Liu and Yao Li and Zhengwei Qi and Haibing Guan},
keywords = {FPGA, DSL, EDA, Scala},
abstract = {Field-Programmable Gate Arrays (FPGAs) in heterogeneous computing have been attracting more and more attention. Development of acceleration systems based on FPGAs involves the cooperation of both hardware and software developers. However, although most hardware acceleration systems are motivated by software developers, software developers are difficult to participant in the system building because of the steep learning curve of the hardware concept and design tools. Moreover, due to the complexity of hardware-software integration in traditional ways, end-to-end performance is hard to evaluate before trivial engineering effort. To address these concerns, we propose an open-source Domain-Specific Language (DSL) based framework called VeriScala11https://www.github.com/VeriScala/VeriScala. to support hardware defining in a high-level language, programmatical testing, and rapid acceleration system deploying. By adopting DSL embedded in Scala language, we introduce modern software development concepts into hardware design and provide a familiar environment to software developers. And by building a stack of middle layers from hardware to software, we provide reduced hardware abstraction and communication interface to facilitate both software and hardware development and system deployment. Through the evaluation of some basic components and real-world demos, we show that VeriScala provides a practical approach to rapid prototyping of hardware acceleration systems.}
}
@article{GRANGEL2019116,
title = {Agile Model-Driven Methodology to Implement Corporate Social Responsibility},
journal = {Computers & Industrial Engineering},
volume = {127},
pages = {116-128},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218305928},
author = {R. Grangel and C. Campos},
keywords = {Agile Methods, Model-Driven Engineering, Domain-Specific Modelling Language, Methodology, Corporate Social Responsibility, Case Study},
abstract = {Corporate Social Responsibility (CSR) has become a key factor in the success of enterprises. There are well-defined case studies in the literature as well as standards that include recommendations about CSR, yet there are no specific methodologies that guide enterprises in integrating CSR within their information system. This paper describes an Agile Model-Driven Methodology especially focused on implementing CSR Computer Systems. This Methodology is included within a larger conceptual framework developed to define, model and manage CSR requirements in enterprises. It provides a set of Model-Driven processes that guide the implementation of a CSR Computer System following agile principles.}
}
@article{COTRONEO2022103334,
title = {ThorFI: a Novel Approach for Network Fault Injection as a Service},
journal = {Journal of Network and Computer Applications},
volume = {201},
pages = {103334},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103334},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000030},
author = {Domenico Cotroneo and Luigi {De Simone} and Roberto Natella},
keywords = {Network fault injection, Virtualization, Chaos engineering, OpenStack},
abstract = {In this work, we present a novel fault injection solution (ThorFI) for virtual networks in cloud computing infrastructures. ThorFI is designed to provide non-intrusive fault injection capabilities for a cloud tenant, and to isolate injections from interfering with other tenants on the infrastructure. We present the solution in the context of the OpenStack cloud management platform, and release this implementation as open-source software. Finally, we present two relevant case studies of ThorFI, respectively in an NFV IMS and of a high-availability cloud application. The case studies show that ThorFI can enhance functional tests with fault injection, as in 4%–34% of the test cases the IMS is unable to handle faults; and that despite redundancy in virtual networks, faults in one virtual network segment can propagate to other segments, and can affect the throughput and response time of the cloud application as a whole, by about 3 times in the worst case.}
}
@article{ZHANG2006209,
title = {Using source transformation to test and model check implicit-invocation systems},
journal = {Science of Computer Programming},
volume = {62},
number = {3},
pages = {209-227},
year = {2006},
note = {Special issue on Source code analysis and manipulation (SCAM 2005)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2006.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167642306000955},
author = {Hongyu Zhang and Jeremy S. Bradbury and James R. Cordy and Juergen Dingel},
keywords = {Source transformation, Domain-specific language, Verification, Testing, Model checking, Implicit invocation},
abstract = {In this paper we present a source transformation-based framework to support uniform testing and model checking of implicit-invocation software systems. The framework includes a new domain-specific programming language, the Implicit-Invocation Language (IIL), explicitly designed for directly expressing implicit-invocation software systems, and a set of formal rule-based source transformation tools that allow automatic generation of both executable and formal verification artifacts. We provide details of these transformation tools, evaluate the framework in practice, and discuss the benefits of formal automatic transformation in this context. Our approach is designed not only to advance the state-of-the-art in validating implicit-invocation systems, but also to further explore the use of automated source transformation as a uniform vehicle to assist in the implementation, validation and verification of programming languages and software systems in general.}
}
@article{WASEEM2021111061,
title = {Design, monitoring, and testing of microservices systems: The practitioners’ perspective},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111061},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111061},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001588},
author = {Muhammad Waseem and Peng Liang and Mojtaba Shahin and Amleto {Di Salle} and Gastón Márquez},
keywords = {Microservices architecture, Design, Monitoring, Testing, Industrial survey},
abstract = {Context:
Microservices Architecture (MSA) has received significant attention in the software industry. However, little empirical evidence exists on design, monitoring, and testing of microservices systems.
Objective:
This research aims to gain a deep understanding of how microservices systems are designed, monitored, and tested in the industry.
Methods:
A mixed-methods study was conducted with 106 survey responses and 6 interviews from microservices practitioners.
Results:
The main findings are: (1) a combination of domain-driven design and business capability is the most used strategy to decompose an application into microservices, (2) over half of the participants used architecture evaluation and architecture implementation when designing microservices systems, (3) API gateway and Backend for frontend patterns are the most used MSA patterns, (4) resource usage and load balancing as monitoring metrics, log management and exception tracking as monitoring practices are widely used, (5) unit and end-to-end testing are the most used testing strategies, and (6) the complexity of microservices systems poses challenges for their design, monitoring, and testing, for which there are no dedicated solutions.
Conclusions:
Our findings reveal that more research is needed to (1) deal with microservices complexity at the design level, (2) handle security in microservices systems, and (3) address the monitoring and testing challenges through dedicated solutions.}
}
@article{BELETE201749,
title = {An overview of the model integration process: From pre-integration assessment to testing},
journal = {Environmental Modelling & Software},
volume = {87},
pages = {49-63},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2016.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216308805},
author = {Getachew F. Belete and Alexey Voinov and Gerard F. Laniak},
keywords = {Integrated modeling, Interoperability, Interfaces, Wrapping, Components, Web services},
abstract = {Integration of models requires linking models which can be developed using different tools, methodologies, and assumptions. We performed a literature review with the aim of improving our understanding of model integration process, and also presenting better strategies for building integrated modeling systems. We identified five different phases to characterize integration process: pre-integration assessment, preparation of models for integration, orchestration of models during simulation, data interoperability, and testing. Commonly, there is little reuse of existing frameworks beyond the development teams and not much sharing of science components across frameworks. We believe this must change to enable researchers and assessors to form complex workflows that leverage the current environmental science available. In this paper, we characterize the model integration process and compare integration practices of different groups. We highlight key strategies, features, standards, and practices that can be employed by developers to increase reuse and interoperability of science software components and systems.}
}
@article{FERNANDEZCANDEL201938,
title = {Developing a model-driven reengineering approach for migrating PL/SQL triggers to Java: A practical experience},
journal = {Journal of Systems and Software},
volume = {151},
pages = {38-64},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.068},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300214},
author = {Carlos Javier {Fernández Candel} and Jesús {García Molina} and Francisco Javier {Bermúdez Ruiz} and Jose Ramón {Hoyos Barceló} and Diego {Sevilla Ruiz} and Benito José {Cuesta Viera}},
keywords = {Software modernization, Reengineering, KDM, Oracle forms, Model-driven software modernization, Model-driven development},
abstract = {Model-driven software engineering (MDE) techniques are not only useful in forward engineering scenarios, but can also be successfully applied to evolve existing systems. RAD (Rapid Application Development) platforms emerged in the nineties, but the success of modern software technologies motivated that a large number of enterprises tackled the migration of their RAD applications, such as Oracle Forms. Our research group has collaborated with a software company in developing a solution to migrate PL/SQL monolithic code on Forms triggers and program units to Java code separated in several tiers. Our research focused on the model-driven reengineering process applied to develop the migration tool for the conversion of PL/SQL code to Java. Legacy code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In this paper, we propose a software process to implement a model-driven re-engineering. This process integrates a TDD-like approach to incrementally develop model transformations with three kinds of validations for the generated code. The implementation and validation of the re-engineering approach are explained in detail, as well as the evaluation of some issues related with the application of MDE.}
}
@article{PENG201633,
title = {Reusing simulation experiment specifications to support developing models by successive extension},
journal = {Simulation Modelling Practice and Theory},
volume = {68},
pages = {33-53},
year = {2016},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2016.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X16302118},
author = {Danhua Peng and Tom Warnke and Fiete Haack and Adelinde M. Uhrmacher},
keywords = {Model extension, Stochastic modeling, Simulation experiments, Experiment specification, Experiment generation and execution},
abstract = {Model development is a successive process of validating, revising, and extending models, and requires iterative execution of simulation experiments. While developing a model by extension, executing similar simulation experiments to those performed with the original model reveals important behavioral insights into the extended model. An automatic generation and execution of these simulation experiments can provide valuable support in the process of developing models. A prerequisite is an explicit specification of simulation experiments. Therefore, we annotate models with simulation experiments that are specified in a declarative domain specific language SESSL (Simulation Experiment Specification via a Scala Layer). Based on experiment specifications of the original model, we introduce a mechanism to automatically generate and execute simulation experiments for the extended model with necessary adaptations. Furthermore, as we experiment with stochastic models, we exploit statistical model checking and specify the expected model behavioral properties, against which the simulation results are checked. Thereby, when a model is extended, the original experiment specifications are reused, adapted, and applied to the extended model. Accordingly, the generated simulation trajectories are probed to check whether the expected properties hold with a certain probability or not. Thus, more fast and frequent feedback during model development can be provided to the modeler. Based on a model of membrane related dynamics, we show how the developed approach can be used in successively extending models.}
}
@article{LOPEZFERNANDEZ2016104,
title = {Combining unit and specification-based testing for meta-model validation and verification},
journal = {Information Systems},
volume = {62},
pages = {104-135},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2016.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916301934},
author = {Jesús J. López-Fernández and Esther Guerra and Juan {de Lara}},
keywords = {Model-driven engineering, Meta-modelling, Domain-specific modelling languages, Validation & verification, Meta-model quality},
abstract = {Meta-models play a cornerstone role in Model-Driven Engineering as they are used to define the abstract syntax of modelling languages, and so models and all sorts of model transformations depend on them. However, there are scarce tools and methods supporting their Validation and Verification (V&V), which are essential activities for the proper engineering of meta-models. In order to fill this gap, we propose two complementary meta-model V&V languages. The first one has similar philosophy to the xUnit framework, as it enables the definition of meta-model unit test suites comprising model fragments and assertions on their (in-)correctness. The second one is directed to express and verify expected properties of a meta-model, including domain and design properties, quality criteria and platform-specific requirements. As a proof of concept, we have developed tooling for both languages in the Eclipse platform, and illustrate its use within an example-driven approach for meta-model construction. The expressiveness of our languages is demonstrated by their application to build a library of meta-model quality issues, which has been evaluated over the ATL zoo of meta-models and some OMG specifications. The results show that integrated support for meta-model V&V (as the one we propose here) is urgently needed in meta-modelling environments.}
}
@article{SNOOK2021101833,
title = {Domain-specific scenarios for refinement-based methods},
journal = {Journal of Systems Architecture},
volume = {112},
pages = {101833},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101833},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120301259},
author = {Colin Snook and Thai Son Hoang and Dana Dghaym and Asieh Salehi Fathabadi and Michael Butler},
keywords = {Event-B, Cucumber, Validation, Domain specific language},
abstract = {Formal methods use abstraction and rigorously verified refinement to manage the design of complex systems, ensuring that they satisfy important invariant properties. However, formal verification is not sufficient: models must also be tested to ensure that they behave according to the informal requirements and validated by domain experts who may not be expert in formal modelling. This can be satisfied by scenarios that complement the requirements specification. The model can be animated to check whether the scenario is feasible in the model and that the model reaches the states expected in the scenario. However, there are two problems with this approach. 1) The natural language used to describe the scenarios is often verbose, ambiguous and therefore difficult to understand; especially if the modeller is not a domain expert. 2) Provided scenarios are typically at the most concrete level corresponding to the full requirements and cannot be used until all the refinements have been completed in the model. We show by example how a precise and concise domain specific language can be used for writing these abstract scenarios in a style that can be easily understood by the domain expert (for validation purposes) as well as the modeller (for behavioural verification) and can be used as the persistence for automated tool support. We propose two alternative approaches to using scenarios during formal modelling: A method of refining scenarios before the model is refined so that the scenarios guide the modelling, and a method of abstracting scenarios from provided concrete ones so that they can be used to test early refinements of the model. We illustrate the two approaches on the ‘Tokeneer’ secure enclave example and the ERTMS/ETCS Hybrid Level 3 specification for railway controls. We base our approach on the Cucumber framework for scenarios and the Event-B modelling language and tool set. We have developed a new ‘Scenario Checker’ plugin to manage the animation of scenarios.11The example model and scenario scripts supporting this paper are openly available at https://doi.org/10.5258/SOTON/D1026.}
}
@article{VIANA20133123,
title = {Domain-Specific Modeling Languages to improve framework instantiation},
journal = {Journal of Systems and Software},
volume = {86},
number = {12},
pages = {3123-3139},
year = {2013},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2013.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0164121213001775},
author = {Matheus C. Viana and Rosângela A.D. Penteado and Antônio F. {do Prado}},
keywords = {Framework, Domain-Specific Modeling Language, Reuse},
abstract = {Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The DSML of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the DSML into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach.}
}
@article{AHELEROFF2020101043,
title = {IoT-enabled smart appliances under industry 4.0: A case study},
journal = {Advanced Engineering Informatics},
volume = {43},
pages = {101043},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101043},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300124},
author = {Shohin Aheleroff and Xun Xu and Yuqian Lu and Mauricio Aristizabal and Juan {Pablo Velásquez} and Benjamin Joa and Yesid Valencia},
keywords = {Industry 4.0, Internet of Things, Big data analytics, Smart products},
abstract = {Manufacturers expect the extra value of Industry 4.0 as the world is experiencing digital transformation. Studies have proved the potential of the Internet of Things (IoT) for reducing cost, improving efficiency, quality, and achieving data-oriented predictive maintenance services. Collecting a wide range of real-time data from products and the environment requires smart sensors, reliable communications, and seamless integration. IoT, as a critical Industry 4.0 enabler emerges smart home appliances for higher customer satisfaction, energy efficiency, personalisation, and advanced Big data analytics. However, established factories with limited resources are facing challenges to change the longstanding production lines and meet customer’s requirements. This study aims to fulfil the gaps by transforming conventional home appliances to IoT-enabled smart systems with the ability to integrate into a smart home system. An industry-led case study demonstrates how to turn conventional appliances to smart products and systems (SPS) by utilising the state-of-the-art Industry 4.0 technologies.}
}
@article{ZHU2015447,
title = {Serum DHEAS levels are associated with the development of depression},
journal = {Psychiatry Research},
volume = {229},
number = {1},
pages = {447-453},
year = {2015},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2015.05.093},
url = {https://www.sciencedirect.com/science/article/pii/S0165178115004084},
author = {Guang Zhu and You Yin and Chun-Lan Xiao and Rong-Jie Mao and Bo-Hai Shi and Yong Jie and Zuo-Wei Wang},
keywords = {Dehydroepiandrosterone sulfate, Depression, Serum levels, Case–control study, Meta-analysis},
abstract = {The aim of study was to evaluate the association between serum DHEAS levels and depression with a case–control study together with a meta-analysis. Radioimmunoassay (RIA) was performed to measure the serum DHEAS levels of all participants before and after treatment. Depression Patients were divided into mild depression and severe depression based on Hamilton depression scale (HAMD24) and received 5-hydroxytryptamine (5-HT) and citalopram (20mg/d) for 8 weeks. Case–control studies related to our study theme were enrolled for meta-analysis and Comprehensive Meta-analysis 2.0 (CMA 2.0) was used for statistical analysis. After treatment, DHEAS levels in depression patients were significantly increased, while before and after treatment, DHEAS levels were all lower in depression patients than in controls (all P<0.001); further analysis on age revealed that DHEAS levels were decreased with the rising of age. Meta-analysis results suggested that serum DHEAS levels (ng/mL) were significantly higher in healthy controls compared to depression patients (SMD=0.777, 95%CI=0.156–1.399, P=0.014). In conclusion, our study suggests that serum DHEAS levels are associated with the development of depression and it decreased with the rising of age.}
}
@article{JOZWIAK1992159,
title = {On the use of OR-BDDs for test generation},
journal = {Microprocessing and Microprogramming},
volume = {35},
number = {1},
pages = {159-166},
year = {1992},
note = {Software and Hardware: Specification and Design},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(92)90310-4},
url = {https://www.sciencedirect.com/science/article/pii/0165607492903104},
author = {Lech Jóźwiak and Hein Mijland},
abstract = {The binary decision diagrams (BDDs) have recently been recognized as efficient means for modelling the Boolean functions for the purpose of testability analysis and test pattern generation; however, they have too low modelling power in order to model the logical implementation structure accurately and, therefore, there is no direct correspondence between the BDD's fault models and the more realistic structural fault models. In this paper, we propose an extension to BDDs, referred to as OR-BDDs, that enables the accurate modelling of the circuit structure in a compact manner. We show how to construct the minimal OR-BDDs. We introduce the fault model for OR-BDDs and show it's one-to-one correspondence with the structural stuck-at-value model. Finally, we present an algorithm for test pattern generation that uses OR-BDDs and their fault model. This algorithm discovers efficiently circuit redundancy and enables 100% fault coverage for all detectable faults by a compact set of test vectors.}
}
@article{JUNG2022105323,
title = {Thematic domain analysis for ocean modeling},
journal = {Environmental Modelling & Software},
volume = {150},
pages = {105323},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105323},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222000299},
author = {Reiner Jung and Sven Gundlach and Wilhelm Hasselbring},
keywords = {Ocean modeling, Thematic domain analysis, Research software engineering},
abstract = {Ocean science is a discipline that employs ocean models as an essential research asset. Such scientific modeling provides mathematical abstractions of real-world systems, e.g., the oceans. These models are then coded as implementations of the mathematical abstractions. The developed software systems are called models of the real-world system. To advance the state in engineering such ocean models, we intend to better understand how ocean models are developed and maintained in ocean science. In this paper, we present the results of semi-structured interviews and the Thematic Analysis (TA) of the interview results to analyze the domain of ocean modeling. Thereby, we identified developer requirements and impediments to model development and evolution, and related themes. This analysis can help to understand where methods from software engineering should be introduced and which challenges need to be addressed. We suggest that other researchers extend and repeat our TA with model developers and research software engineers working in related domains to further advance our knowledge and skills in scientific modeling.}
}
@article{1977197,
title = {Useful list of abbreviations commonly used in reliability},
journal = {Microelectronics Reliability},
volume = {16},
number = {3},
pages = {197-206},
year = {1977},
issn = {0026-2714},
doi = {https://doi.org/10.1016/0026-2714(77)90757-0},
url = {https://www.sciencedirect.com/science/article/pii/0026271477907570}
}
@incollection{2010367,
title = {Appendix B - Embedded Systems Glossary},
editor = {Tammy Noergaard},
booktitle = {Demystifying Embedded Systems Middleware},
publisher = {Newnes},
address = {Burlington},
pages = {367-387},
year = {2010},
isbn = {978-0-7506-8455-2},
doi = {https://doi.org/10.1016/B978-0-7506-8455-2.00011-X},
url = {https://www.sciencedirect.com/science/article/pii/B978075068455200011X}
}
@article{MOHIYIDDEEN2012677,
title = {Follicle-stimulating hormone receptor gene polymorphisms are not associated with ovarian reserve markers},
journal = {Fertility and Sterility},
volume = {97},
number = {3},
pages = {677-681},
year = {2012},
issn = {0015-0282},
doi = {https://doi.org/10.1016/j.fertnstert.2011.12.040},
url = {https://www.sciencedirect.com/science/article/pii/S0015028211029244},
author = {Lamiya Mohiyiddeen and William G. Newman and Helen McBurney and Betselot Mulugeta and Stephen A. Roberts and Luciano G. Nardo},
keywords = {Single nucleotide polymorphism, Ser680Asn, Thr307Ala FSH receptor, antimüllerian hormone, antral follicle count, follicle-stimulating hormone},
abstract = {Objective
To evaluate the association between variants in the FSHR receptor (FSHR) gene and current markers of ovarian reserve (antimüllerian hormone, antral follicle count, FSH).
Design
Prospective observational study.
Setting
Tertiary referral center for reproductive medicine.
Patient(s)
Women (n = 421) undergoing their first cycle of controlled ovarian stimulation for IVF.
Intervention(s)
Baseline pelvic ultrasound and blood tests were taken on day 2–3 of the cycle for assessment of baseline hormones and for DNA extraction. Genotypes for FSHR p.Asn680Ser and p.Thr307Ala variants were determined using TaqMan allelic discrimination assays.
Main Outcome Measure(s)
Association of FSHR single nucleotide polymorphisms with markers of ovarian reserve.
Result(s)
There was no evidence of any difference in basal FSH, antimüllerian hormone, or antral follicle count between the patients with different genotypes, with or without an adjustment for age or body mass index.
Conclusion(s)
No associations of FSHR genotypes with markers of ovarian reserve were detected in our cohort.}
}
@article{GRATACOS2010S175,
title = {P12-16 Neurophysiological characterization of Miller Fisher syndrome patients: Report of 10 patients},
journal = {Clinical Neurophysiology},
volume = {121},
pages = {S175-S176},
year = {2010},
note = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
issn = {1388-2457},
doi = {https://doi.org/10.1016/S1388-2457(10)60720-1},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710607201},
author = {M. Gratacos and N. Raguer and J. Gamez and J.L. Seoane and M. Benito}
}
@incollection{CHO1991427,
title = {VHDL Semantics for Behavioral Test Generation},
editor = {DOMINIQUE BORRIONE and RONALD WAXMAN},
booktitle = {Computer Hardware Description Languages and their Applications},
publisher = {North-Holland},
address = {Amsterdam},
pages = {427-444},
year = {1991},
isbn = {978-0-444-89208-9},
doi = {https://doi.org/10.1016/B978-0-444-89208-9.50030-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444892089500302},
author = {Chang H. Cho and James R. Armstrong},
abstract = {In this paper, we discuss how the VHDL semantics which represent the concepts of event-driven simulation and bus resolution function affect the test generation algorithm, and present methods of generating realistic tests without being affected by the VHDL semantics. A formal representation of the VHDL process statement is described and the concept of event-driven simulation and its impact on test generation are discussed using the formal representation. The new test generation method generates realistic tests by ignoring the sensitivity list of a process statement and identifying the type of the behavior described by the statements inside the process statement (two types of behavior - synchronous and asynchronous, are defined.). A systematic way of converting a VHDL model to one suitable for checking the validity of the generated tests is presented. A method of further compacting the generated tests is also presented. Finally, an approach to generating tests in the presence of different types of bus resolution functions is discussed.}
}
@article{LOPEZSANCHEZ2011384,
title = {Surfactant protein A (SP-A)-tacrolimus complexes have a greater anti-inflammatory effect than either SP-A or tacrolimus alone on human macrophage-like U937 cells},
journal = {European Journal of Pharmaceutics and Biopharmaceutics},
volume = {77},
number = {3},
pages = {384-391},
year = {2011},
note = {Biological Barriers},
issn = {0939-6411},
doi = {https://doi.org/10.1016/j.ejpb.2010.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0939641110003310},
author = {Almudena López-Sánchez and Alejandra Sáenz and Cristina Casals},
keywords = {Lung, Tacrolimus, Surfactant protein A, Inflammation, Macrophages, P-glycoprotein},
abstract = {Intratracheal administration of immunosuppressive agents to the lung is a novel treatment after lung transplantation. Nanoparticles of tacrolimus (FK506) might interact with human SP-A, which is the most abundant lipoprotein in the alveolar fluid. This study was undertaken to determine whether the formation of FK506/SP-A complexes interferes with FK506 immunosuppressive actions on stimulated human macrophage-like U937 cells. We found that SP-A was avidly bound to FK506 (Kd=35±4nM), as determined by solid phase–binding assays and dynamic light scattering. Free FK506, at concentrations ⩽1μM, had no effect on the inflammatory response of LPS-stimulated U937 macrophages. However, coincubation of FK506 and SP-A, at concentrations where each component alone did not affect LPS-stimulated macrophage response, significantly inhibited LPS-induced NF-κB activation and TNF-alpha secretion. Free FK506, but not FK506/SP-A, functioned as substrate for the efflux transporter P-glycoprotein. FK506 bound to SP-A was delivered to macrophages by endocytosis, since several endocytosis inhibitors blocked FK506/SP-A anti-inflammatory effects. This process depended partly on SP-A binding to its receptor, SP-R210. These results indicate that FK506/SP-A complexes have a greater anti-inflammatory effect than either FK506 or SP-A alone and suggest that SP-A strengthened FK506 anti-inflammatory activity by facilitating FK506 entrance into the cell, overcoming P-glycoprotein.}
}
@article{LEE2020106272,
title = {Test coverage criteria for software product line testing: Systematic literature review},
journal = {Information and Software Technology},
volume = {122},
pages = {106272},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106272},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300227},
author = {Jihyun Lee and Sungwon Kang and Pilsu Jung},
keywords = {Software product line, Software product line testing, Test coverage, Test coverage criteria},
abstract = {Context
In software product line testing (SPLT), test coverage criterion is an important concept, as it provides a means of measuring the extent to which domain testing has been performed and redundant application testing can be avoided based on the test coverage level achieved in domain testing. However, no previous literature reviews on SPLT have addressed test coverage criterion in SPLT.
Objective
The objectives of this paper are as follows: (1) to clarify the notions of test basis and test coverage criterion for SPLT; (2) to identify the test coverage criteria currently used for SPLT; (3) to investigate how various SPLT aspects, such as the SPLT method, variability implementation mechanism, and variability management approach, affect the choice of test coverage criterion for SPLT; and (4) to analyze the limitations of test coverage criteria currently used for SPLT.
Method
This paper conducts a systematic review of test coverage criteria in SPLT with 78 selected studies.
Results
We have several findings that can guide the future research on SPLT. One important finding is that choice of test coverage criterion in SPLT is independent from variability implementation mechanism, variability management, SPL approach, and binding time but is dependent on the variability representation used in development artifacts. Another that is easily overlooked is that SPL test coverage criteria with the same test coverage criterion names of single system testing neither adequately convey what should be covered by the test methods applying them, nor can they be more generally regarded as extensions or generalizations for SPLT of their corresponding test coverage criteria of single system testing.
Conclusion
This study showed that SPL test coverage criteria should be defined or redefined so that they can clearly deliver the target properties to be satisfied by SPLT.}
}
@article{LACERDA2020110610,
title = {Code smells and refactoring: A tertiary systematic review of challenges and observations},
journal = {Journal of Systems and Software},
volume = {167},
pages = {110610},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110610},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300881},
author = {Guilherme Lacerda and Fabio Petrillo and Marcelo Pimenta and Yann Gaël Guéhéneuc},
keywords = {Code smells, Refactoring, Tertiary systematic review},
abstract = {Refactoring and smells have been well researched by the software-engineering research community these past decades. Several secondary studies have been published on code smells, discussing their implications on software quality, their impact on maintenance and evolution, and existing tools for their detection. Other secondary studies addressed refactoring, discussing refactoring techniques, opportunities for refactoring, impact on quality, and tools support. In this paper, we present a tertiary systematic literature review of previous surveys, secondary systematic literature reviews, and systematic mappings. We identify the main observations (what we know) and challenges (what we do not know) on code smells and refactoring. We perform this tertiary review using eight scientific databases, based on a set of five research questions, identifying 40 secondary studies between 1992 and 2018. We organize the main observations and challenges about code smell and their refactoring into: smells definitions, most common code-smell detection approaches, code-smell detection tools, most common refactoring, and refactoring tools. We show that code smells and refactoring have a strong relationship with quality attributes, i.e., with understandability, maintainability, testability, complexity, functionality, and reusability. We argue that code smells and refactoring could be considered as the two faces of a same coin. Besides, we identify how refactoring affects quality attributes, more than code smells. We also discuss the implications of this work for practitioners, researchers, and instructors. We identify 13 open issues that could guide future research work. Thus, we want to highlight the gap between code smells and refactoring in the current state of software-engineering research. We wish that this work could help the software-engineering research community in collaborating on future work on code smells and refactoring.}
}
@article{VERHOEF2005275,
title = {Quantitative aspects of outsourcing deals},
journal = {Science of Computer Programming},
volume = {56},
number = {3},
pages = {275-313},
year = {2005},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2004.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642304001406},
author = {C. Verhoef},
keywords = {Outsourcing, Goalsourcing, Smartsourcing, Fastsourcing, Costsourcing, Offshore outsourcing, Eastsourcing, Tasksourcing, Backsourcing, Insourcing, Scalesourcing, Profitsourcing, Activity-based cost estimation, Total cost of ownership (TCO), Requirements creep risk, Time compression risk, Litigation risk, Failure risk, Overtime risk, Deglubitor risk, Payback period risk},
abstract = {There are many goals for outsourcing information technology: for instance, cost reduction, speed to market, quality improvement, or new business opportunities. Based on our real-world experience in advising organizations with goal-driven outsourcing deals, we identified the most prominent quantitative input needed to close such deals. These comprise what we named the five executive issues enabling rational decision making. They concern cost, duration, risk, return, and financing aspects of outsourcing. They add an important quantitative financial/economic dimension to the decision making process. Based on inferred outcomes for the five executive issues, we address the easily overlooked aspects of selecting partners, contracting, monitoring progress, and acceptance and delivery conditions for contracts.}
}
@article{MITCHELL2022100012,
title = {A case for ovarian transdifferentiation in six adult captive masculinized lionesses (Panthera leo) in South Africa: Pathological evidence},
journal = {Theriogenology Wild},
volume = {1},
pages = {100012},
year = {2022},
issn = {2773-093X},
doi = {https://doi.org/10.1016/j.therwi.2022.100012},
url = {https://www.sciencedirect.com/science/article/pii/S2773093X22000125},
author = {Emily P. Mitchell and Adrian Tordiffe and Peter Caldwell},
keywords = {Lion, Ovarian transdifferentiation, Mane, Sertoli cells, Interstitial cells, Clitoromegaly},
abstract = {abstract
Until recently, mammalian sex differentiation was thought to be finalized in the embryo. Development of the tubular genital tract, external genitalia, secondary sexual characteristics and sexual behavior are determined largely by the developing gonad. In the last decade, however, it has been shown that continuous sex maintenance is required throughout life. We document proposed ovary to testis transdifferentiation in six captive South African lionesses. These lions were phenotypically female until, as older adults, they began to develop manes, clitoromegaly and increased levels of restlessness and aggression. Similar phenotypic and behavioral changes have been reported very rarely in free-ranging lionesses. Five of the lionesses had been treated repeatedly with deslorelin until they stopped showing estrus. Serum levels of testosterone were similar to those recorded in domestic tom cats in the three lionesses tested. Histological examination revealed an absence of ovarian follicles and small to large scattered islands of interstitial-like cells associated, in larger lesions, with Sertoli-like cells arranged in tubular structures. Oviduct and uterus were present in the four animals in which these were sampled. To date, most studies on this phenomenon have been conducted in laboratory animals. This is the first report of this condition in felids.}
}
@article{ZHAO20191056,
title = {Diagnostic Value of Anti-Müllerian Hormone as a Biomarker for Polycystic Ovary Syndrome: A Meta-Analysis Update},
journal = {Endocrine Practice},
volume = {25},
number = {10},
pages = {1056-1066},
year = {2019},
issn = {1530-891X},
doi = {https://doi.org/10.4158/EP-2019-0098},
url = {https://www.sciencedirect.com/science/article/pii/S1530891X2035196X},
author = {Yang Zhao and Yinlong Zhao and Chunpeng Wang and Zhenzhen Liang and Xin Liu},
abstract = {ABSTRACT
Objective: A previous meta-analysis carried out on the predictive ability of anti-Müllerian hormone (AMH) for polycystic ovary syndrome (PCOS) showed that independent AMH may be a useful initial diagnostic test for PCOS. The aims of this study were to update the meta-analysis and to evaluate the diagnostic efficacy of AMH when it replaces polycystic ovary morphology (PCOM) in the Rotterdam criteria. Methods: Two independent reviewers searched PubMed, Cochrane Library, and the Web of Science databases systematically to identify relevant articles by using the key words “anti-Müllerian hormone” and “polycystic ovary syndrome.” The deadline for manuscript inclusion was July 31, 2018. A random effects model was used and subgroup analysis and meta regression were performed to identify possible sources of heterogeneity. The methodologic quality of each study was assessed by QUADAS-2 and funnel plot asymmetry test. Results: According to the inclusion criteria, 29 studies were included in this meta-analysis. The pooled sensitivity, specificity, and diagnostic odds ratio (DOR) for AMH alone detecting PCOS were 0.76 (95% confidence interval &lsqb;CI] 0.71 to 0.81), 0.86 (95% CI 0.82 to 0.90) and 20 (95% CI 12 to 33), respectively. When AMH replaces polycystic ovary morphology (PCOM) for the diagnosis of PCOS, the pooled sensitivity, specificity, and DOR rose to 0.93 (95% CI 0.89 to 0.96), 0.99 (95% CI 0.95 to 1.00), and 1,634 (95% CI 217 to 12,324), respectively. The area under the summary receiver-operating characteristic curve for AMH alone and for AMH replacing PCOM detecting PCOS were 0.88 (95% CI 0.85 to 0.91) and 0.97 (95% CI 0.95 to 0.98), respectively, which was found to be significantly different (Z = 4.89, P<.01). Conclusion: When AMH replaces PCOM in the Rotterdam criteria, the diagnostic efficacy for polycystic ovary syndrome is better. Abbreviations: AMH = anti-Müllerian hormone; AUC = area under the summary receiver operating characteristic curve; BMI = body mass index; CI = confidence interval; DOR = diagnostic odds ratio; HA = hyperandrogenism; IBC = Immunotech-Beckman Coulter; NLR = negative likelihood ratio; OA = oligo-anovulation; PCOM = polycystic ovary morphology; PCOS = polycystic ovary syndrome; PLR = positive likelihood ratio; QUADAS = the Quality Assessment of Diagnostic Accuracy Studies; SENS = sensitivity; SPEC = specificity}
}
@article{CORONA2010574,
title = {Liquid chromatography tandem mass spectrometry assay for fast and sensitive quantification of estrone-sulfate},
journal = {Clinica Chimica Acta},
volume = {411},
number = {7},
pages = {574-580},
year = {2010},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cca.2010.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S000989811000032X},
author = {Giuseppe Corona and Caterina Elia and Bruno Casetta and Alessandro {Da Ponte} and Lino {Del Pup} and Enzo Ottavian and Giuseppe Toffoli},
keywords = {Estrone-sulfate, LC–MS/MS, Estrogen, Menopausal, Estrogen suppression, Aromatase inhibitors},
abstract = {Background
The circulating pool of estrone-sulfate is considered as a “reservoir” of slowly-metabolized estrogen that can be exploited for assessing overall individual estrogenicity. The aim of this study was to develop a rapid and sensitive liquid chromatography–tandem mass spectrometry assay for the determination of estrone-sulfate, suitable for routine clinical investigations.
Methods
The proposed assay is based on a simple protein precipitation procedure and on a fast measurement with a triple–quadrupole mass spectrometer operating in negative ion mode and in multiple reaction monitoring. The method was assessed for intra- and inter-day precision, accuracy, recovery, and clinical suitability. A comparison with available radioimmunoassay was also performed.
Results
The LC–MS/MS method is able to detect estrone-sulfate concentrations ≤1pg/mL and has a low limit of quantification of 7.8pg/mL. Intra- and inter-day precision and accuracy were less than 10.5% and 5.0% respectively. The recovery was in the range of 93%–110%. When compared with radioimmunoassay the method resulted more accurate and therefore more suitable for quantifying the estrone-sulfate in different clinical settings, including patients treated with aromatase inhibitors.
Conclusions
The proposed LC–MS/MS method represents a convincing alternative to the immunoassay for a fast, cost-effective and reliable measurement of estrone-sulfate in routine clinical investigations and in large epidemiological studies. It may contribute in shedding a new light on the diagnostic value of estrone-sulfate in normal and pathological conditions.}
}
@article{ELALLAOUI2016221,
title = {Automated Model Driven Testing Using AndroMDA and UML2 Testing Profile in Scrum Process},
journal = {Procedia Computer Science},
volume = {83},
pages = {221-228},
year = {2016},
note = {The 7th International Conference on Ambient Systems, Networks and Technologies (ANT 2016) / The 6th International Conference on Sustainable Energy Information Technology (SEIT-2016) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.04.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916301429},
author = {Meryem Elallaoui and Khalid Nafil and Raja Touahni and Rochdi Messoussi},
keywords = {Testing, Model-Driven Testing, UML2, UML2 Testing Profile (U2TP), MDA},
abstract = {Software testing is an important step in the life cycle of agile development; it represents an efficient way to ensure the good functioning of the product. But as the complexity of a system increases, the effort and expertise to test it also increases. To significantly reduce these efforts, and reduce the cost and time; several studies have been carried out and various tools and test automation techniques have been proposed. In this paper, we present an approach to automatic generation of test cases from UML 2 Models at the Scrum agile process. This approach automates two important steps: the transformation of design models into test models and generating test cases, based on an open source MDA framework.}
}
@article{LE2010S176,
title = {P12-17 Clinical features and laboratory findings of 7 cases with Miller-Fisher syndrome in ChoRay hospital},
journal = {Clinical Neurophysiology},
volume = {121},
pages = {S176},
year = {2010},
note = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
issn = {1388-2457},
doi = {https://doi.org/10.1016/S1388-2457(10)60721-3},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710607213},
author = {T.T.Q. Le}
}
@incollection{WILLIAMS2014201,
title = {Chapter 10 - Change Management},
editor = {Timothy J. Shimeall and Jonathan M. Spring},
booktitle = {Introduction to Information Security},
publisher = {Syngress},
address = {Boston},
pages = {201-231},
year = {2014},
isbn = {978-1-59749-969-9},
doi = {https://doi.org/10.1016/B978-1-59749-969-9.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781597499699000109},
author = {James G. Williams},
keywords = {change management, configuration management, configuration management database (CMDB), change automation, configuration automation, information assurance, Alan Turing, SNMP, Chef, CFEngine, automation systems, change process, patch management, system maintenance, certifications, change types, critical change, major change, minor change, insignificant change},
abstract = {This chapter covers the related but distinct fields of change and configuration management. The chapter introduces the two topics, why they are useful and important, and how they fit in the project management life cycle. The two fields are distinguished, and then each is described in detail. Change management topics discussed are phases of the process, differential processes for different severities of changes (e.g., critical, minor, or insignificant), and how change management directly bears on security. Automation and documentation of change management processes are also discussed. Configuration management begins with an item that straddles change and configuration management: software patch management. Configuration management is then discussed in the context of management systems, configuring software, configuring network devices, configuration information-assurance critical items, configuration and system maintenance, and finally configuration management databases for tracking the various configurations. The chapter concludes with the relevant change and configuration management certification bodies.}
}
@article{HAVEMAN2013293,
title = {Requirements for High Level Models Supporting Design Space Exploration in Model-based Systems Engineering},
journal = {Procedia Computer Science},
volume = {16},
pages = {293-302},
year = {2013},
note = {2013 Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S187705091300032X},
author = {Steven P. Haveman and G. Maarten Bonnema},
keywords = {Model-based systems engineering, systems engineering challenges, high level models, communication, design space exploration, design trade-offs},
abstract = {Most formal models are used in detailed design and focus on a single domain. Few effective approaches exist that can effectively tie these lower level models to a high level system model during design space exploration. This complicates the validation of high level system requirements during detailed design. In this paper, we define requirements for a high level model that is firstly driven by key systems engineering challenges present in industry and secondly connects to several formal and domain specific models used in model-based design. We analysed part of the systems engineering process at a company developing complex systems, by observing the design process and by analysing design documentation and development databases. By generalizing these observations, we identified several high level systems engineering challenges. They are compared to literature, focusing on reported systems engineering challenges and on existing approaches that incorporate high level models in model-based systems engineering. Finally, we argue that high level system models supporting design space exploration should be able to communicate information regarding design trade-offs (e.g. safety versus ease of use) effectively in a multidisciplinary setting. In our outlook, we propose how to continue our research, by recommending further research and defining a research question.}
}
@article{ARRUDA2020102377,
title = {Automation and consistency analysis of test cases written in natural language: An industrial context},
journal = {Science of Computer Programming},
volume = {189},
pages = {102377},
year = {2020},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2019.102377},
url = {https://www.sciencedirect.com/science/article/pii/S0167642319301698},
author = {Filipe Arruda and Flávia Barros and Augusto Sampaio},
keywords = {Test automation, Controlled natural language, Alloy, Test case consistency},
abstract = {We present here a novel test automation strategy that receives as input a freestyle natural language (NL) test case (consisting of a sequence of test steps) and produces executable test scripts. This strategy relies on a database of previously automated seed test steps, available for reuse. New steps are automated via a capturing process by a tester, without requiring any programming knowledge. Automated tests can be executed by a replay facility. We discuss the reuse improvement, implementation effort, and user feedback regarding the industrial applicability and usability of our capture & replay tool. We then show that restricting the input textual description to obey a proposed Controlled NL (CNL) brings significant advantages: (1) reuse improvement; (2) the possibility of integration with a test generation framework; and (3) definition of consistency notions for test actions and test action sequences, that ensure, respectively, well-formedness of each action and a proper configuration to safely execute a sequence of actions. We formalize these consistency notions in Alloy and use the Alloy Analyzer to carry out the consistency check; the scalability of the analysis is assessed via an evaluation considering a repository with real test cases; the practical context of our work is mobile device testing, involving a partnership with Motorola Mobility, a Lenovo company.}
}
@article{SUNYE2014749,
title = {Model-based testing of global properties on large-scale distributed systems},
journal = {Information and Software Technology},
volume = {56},
number = {7},
pages = {749-762},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000366},
author = {Gerson Sunyé and Eduardo Cunha {de Almeida} and Yves {Le Traon} and Benoit Baudry and Jean-Marc Jézéquel},
keywords = {Software testing, Distributed software, Model-based testing},
abstract = {Context
Large-scale distributed systems are becoming commonplace with the large popularity of peer-to-peer and cloud computing. The increasing importance of these systems contrasts with the lack of integrated solutions to build trustworthy software. A key concern of any large-scale distributed system is the validation of global properties, which cannot be evaluated on a single node. Thus, it is necessary to gather data from distributed nodes and to aggregate these data into a global view. This turns out to be very challenging because of the system’s dynamism that imposes very frequent changes in local values that affect global properties. This implies that the global view has to be frequently updated to ensure an accurate validation of global properties.
Objective
In this paper, we present a model-based approach to define a dynamic oracle for checking global properties. Our objective is to abstract relevant aspects of such systems into models. These models are updated at runtime, by monitoring the corresponding distributed system.
Method
We conduce real-scale experimental validation to evaluate the ability of our approach to check global properties. In this validation, we apply our approach to test two open-source implementations of distributed hash tables. The experiments are deployed on two clusters of 32 nodes.
Results
The experiments reveal an important defect on one implementation and show clear performance differences between the two implementations. The defect would not be detected without a global view of the system.
Conclusion
Testing global properties on distributed software consists of gathering data from different nodes and building a global view of the system, where properties are validated. This process requires a distributed test architecture and tools for representing and validating global properties. Model-based techniques are an expressive mean for building oracles that validate global properties on distributed systems.}
}
@article{STRAVER2010S176,
title = {P12-18 Exercise-induced weakness in demyelinating neuropathies},
journal = {Clinical Neurophysiology},
volume = {121},
pages = {S176},
year = {2010},
note = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
issn = {1388-2457},
doi = {https://doi.org/10.1016/S1388-2457(10)60722-5},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710607225},
author = {D.C.G. Straver and L.H. {Van den Berg} and H. Franssen}
}
@incollection{2016559,
title = {Index},
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP Study Guide (Third Edition)},
publisher = {Syngress},
edition = {Third Edition},
address = {Boston},
pages = {559-599},
year = {2016},
isbn = {978-0-12-802437-9},
doi = {https://doi.org/10.1016/B978-0-12-802437-9.00018-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128024379000187}
}
@article{BOONSTRA200861,
title = {Plasma DHEA levels in wild, territorial red squirrels: Seasonal variation and effect of ACTH},
journal = {General and Comparative Endocrinology},
volume = {158},
number = {1},
pages = {61-67},
year = {2008},
issn = {0016-6480},
doi = {https://doi.org/10.1016/j.ygcen.2008.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016648008001950},
author = {Rudy Boonstra and Jeffrey E. Lane and Stan Boutin and Adrian Bradley and Lanna Desantis and Amy E.M. Newman and Kiran K. Soma},
keywords = {ACTH, Adrenal cortex, Aggression, Boreal forest, Cortisol, DHEA, Food supply, Population density, Season, Sex difference, Stress, Territorial behavior, Testosterone, Winter},
abstract = {In many species, territorial behavior is limited to the breeding season and is tightly coupled to circulating gonadal steroid levels. In contrast, both male and female red squirrels (Tamiasciurus hudsonicus) are highly aggressive in both the breeding and non-breeding seasons in defense of food stores on their individual territories throughout the boreal and northern forests of North America. Dehydroepiandrosterone (DHEA), an androgen precursor, is secreted from the adrenal cortex in some mammals, and DHEA has been linked to aggression in non-breeding songbirds. Here, we examined plasma DHEA levels in a natural population of red squirrels in the Yukon, Canada. Plasma DHEA levels in both males and females reached high concentrations (up to 16.952ng/ml in males and 14.602ng/ml in females), markedly exceeding plasma DHEA concentrations in laboratory rats and mice and similar to plasma DHEA concentrations in some primates. Circulating DHEA levels showed both seasonal and yearly variation. Seasonal variation in male plasma DHEA levels was negatively correlated with testes mass. Yearly variation in male DHEA levels was positively correlated with population density. In both males and females, circulating DHEA rapidly increased after ACTH treatment, implying an adrenal origin. This is the first examination of plasma DHEA concentrations in a wild rodent and the first field experiment on the regulation of plasma DHEA in any wild mammal. These data lay the foundation for future studies on the role of DHEA in non-breeding territoriality in this species and other mammals.}
}
@article{BELT2023102789,
title = {Model-driven development for the seL4 microkernel using the HAMR framework},
journal = {Journal of Systems Architecture},
volume = {134},
pages = {102789},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102789},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122002740},
author = {Jason Belt and John Hatcliff and  Robby and John Shackleton and Jim Carciofini and Todd Carpenter and Eric Mercer and Isaac Amundson and Junaid Babar and Darren Cofer and David Hardin and Karl Hoech and Konrad Slind and Ihor Kuz and Kent Mcleod},
keywords = {Software architecture, Model-driven development, Microkernel, Code generation, High assurance},
abstract = {Verified microkernels such as seL4 provide trustworthy foundations for safety- and security-critical systems. However, their full potential remains unrealized due, in part, to the lack of application development environments that help engineers integrate the microkernel’s configuration and hosting of application code with modeling, analysis, and verification tools that address broader aspects of the development lifecycle. This paper presents a model-driven tool chain for the seL4 microkernel based on the open source High Assurance Modeling and Rapid engineering (HAMR) code generation framework for the Architecture and Analysis Definition Language (AADL). We describe how the semantics of AADL communication and threading can be realized in terms of the access primitives and strong spatial and temporal partitioning mechanisms provided by seL4. For AADL users, seL4 provides a high-assurance platform with formally verified enforcement of component boundaries and communication pathways. For seL4 users, AADL provides high-level abstractions for developing seL4 applications, along with an ecosystem of system engineering and analysis tools. We illustrate the framework by applying a model-based development environment for increasing resiliency against cyber attacks to an unmanned aircraft flight control system.}
}
@article{TRAINOR2008192,
title = {Rapid effects of estradiol on male aggression depend on photoperiod in reproductively non-responsive mice},
journal = {Hormones and Behavior},
volume = {53},
number = {1},
pages = {192-199},
year = {2008},
issn = {0018-506X},
doi = {https://doi.org/10.1016/j.yhbeh.2007.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0018506X07002334},
author = {Brian C. Trainor and M. {Sima Finy} and Randy J. Nelson},
keywords = {Aggressive behavior, , California mouse, c-fos, Nongenomic effects, Estrogen receptor alpha, Estrogen receptor beta},
abstract = {In three genuses and four species of rodents, housing in winter-like short days (8L:16D) increases male aggressive behavior. In all of these species, males undergo short-day induced regression of the reproductive system. Some studies, however, suggest that the effect of photoperiod on aggression may be independent of reproductive responses. We examined the effects of photoperiod on aggressive behavior in California mice (Peromyscus californicus), which do not display reproductive responsiveness to short days. As expected, short days had no effect on plasma testosterone. Estrogen receptor alpha and estrogen receptor beta immunostaining did not differ in the lateral septum, medial preoptic area, bed nucleus of the stria terminalis, or medial amygdala. However, males housed in short days were significantly more aggressive than males housed in long days. Similar to previous work in beach mice (Peromyscus polionotus), estradiol rapidly increased aggression when male California mice were housed in short days but not when housed in long days. These data suggest that the effects of photoperiod on aggression and estrogen signaling are independent of reproductive responses. The rapid action of estradiol on aggression in short-day mice also suggests that nongenomic mechanisms mediate the effects of estrogens in short days.}
}
@article{BOEHANSEN201993,
title = {An update on boar semen assessments by flow cytometry and CASA},
journal = {Theriogenology},
volume = {137},
pages = {93-103},
year = {2019},
note = {Proceedings of the IX International Conference of Boar Semen Preservation},
issn = {0093-691X},
doi = {https://doi.org/10.1016/j.theriogenology.2019.05.043},
url = {https://www.sciencedirect.com/science/article/pii/S0093691X19301827},
author = {Gry Brandt Boe-Hansen and Nana Satake},
keywords = {Boar, Spermatozoa, Motility, Flow cytometry, CASA},
abstract = {In the quest for predicting fertility of an individual, enhancing semen handling, dilution and storage protocols, and understanding the impact of environment and, andrologists have changed their approaches to semen analysis. The technologies used today are fast developing and readily implemented in research. Semen is one of a few naturally occurring monocellular suspensions, so sperm function analysis by flow cytometry (FC) and utilization of fluorochromes is an ideal technique for high throughput, objective and accurate analysis. The complementary use of microscopical assessments by Computer-Assisted Semen Analysis (CASA), where sperm cell parameters can be objectively assessed is equally important. The objectivity and repeatability of these techniques have driven research on the function, identification of heterogeneity and fertility of the ejaculate. The wealth of knowledge obtained from the application of these powerful methods has changed our view of the spermatozoon. Although there is some application of these methods in the industry producing boar semen for artificial insemination (AI) and to eliminate sires of sub-standard semen quality, uptake of advanced methods is still slow. Instruments are becoming cheaper and technically more user friendly. Standardization of methodology and optimization of instrument settings is important for full implementation of these systems, including comparison between labs. This review provides an update on two technologies: flow cytometry and CASA for objective analysis of boar semen quality.}
}
@article{CLARISO2016113,
title = {Backwards reasoning for model transformations: Method and applications},
journal = {Journal of Systems and Software},
volume = {116},
pages = {113-132},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215001788},
author = {Robert Clarisó and Jordi Cabot and Esther Guerra and Juan {de Lara}},
keywords = {Model transformation, OCL, Weakest pre-condition},
abstract = {Model transformations are key elements of model driven engineering. Current challenges for transformation languages include improving usability (i.e., succinct means to express the transformation intent) and devising powerful analysis methods. In this paper, we show how backwards reasoning helps in both respects. The reasoning is based on a method that, given an OCL expression and a transformation rule, calculates a constraint that is satisfiable before the rule application if and only if the original OCL expression is satisfiable afterwards. With this method we can improve the usability of the rule execution process by automatically deriving suitable application conditions for a rule (or rule sequence) to guarantee that applying that rule does not break any integrity constraint (e.g. meta-model constraints). When combined with model finders, this method facilitates the validation, verification, testing and diagnosis of transformations, and we show several applications for both in-place and exogenous transformations.}
}
@article{LANDYS2013166,
title = {Year-round territorial aggression is independent of plasma DHEA in the European nuthatch Sitta europaea},
journal = {Hormones and Behavior},
volume = {63},
number = {1},
pages = {166-172},
year = {2013},
issn = {0018-506X},
doi = {https://doi.org/10.1016/j.yhbeh.2012.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0018506X12002449},
author = {Mėta M. Landys and Wolfgang Goymann and Kiran K. Soma and Tore Slagsvold},
keywords = {Nuthatch, Seasonal DHEA profile, Implant, Territorial aggression, Testosterone, Non-breeding, Wintering},
abstract = {Plasma testosterone can play an important role in promoting aggressive behaviors relating to territory defense in breeding male birds. Some birds defend territories also during the non-breeding phase, when testosterone circulates at basal levels. In such species, plasma levels of the pro-hormone dehydroepiandrosterone (DHEA) may support non-breeding territoriality by acting as a local substrate for sex steroids. To test this possible role of plasma DHEA, we examined the seasonal DHEA profile of male (and female) European nuthatches Sitta europaea: a male and female nuthatch pair will defend an all-purpose territory throughout the year. We hypothesized that plasma DHEA would be detectable in wintering nuthatches with a territory. However, only ca. half of the territorial wintering males (and females) displayed detectable DHEA levels, suggesting that plasma DHEA is not a major sex steroid precursor during non-breeding. Further, among hatching-year birds, plasma DHEA was significantly lower in territorial birds than in “floaters”, i.e., subordinate birds without a territory. To experimentally examine the role of DHEA in non-breeding territoriality, we treated adult wintering males with DHEA and measured effects on aggressive responses to conspecific challenge. DHEA treatment elevated plasma levels of DHEA (and testosterone), but did not enhance territorial behaviors or their persistence. Taken together, our data suggest that DHEA (and, indeed, sex steroids per se) do not regulate non-breeding territoriality in the nuthatch. Given that territorial aggression in nuthatches is expressed year-round, a hormone for its activation may be redundant.}
}
@article{DEOLIVEIRANETO2019246,
title = {Evolution of statistical analysis in empirical software engineering research: Current state and steps forward},
journal = {Journal of Systems and Software},
volume = {156},
pages = {246-267},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301451},
author = {Francisco Gomes {de Oliveira Neto} and Richard Torkar and Robert Feldt and Lucas Gren and Carlo A. Furia and Ziwei Huang},
keywords = {Empirical software engineering, Statistical methods, Practical significance, Semi-automated literature review},
abstract = {Software engineering research is evolving and papers are increasingly based on empirical data from a multitude of sources, using statistical tests to determine if and to what degree empirical evidence supports their hypotheses. To investigate the practices and trends of statistical analysis in empirical software engineering (ESE), this paper presents a review of a large pool of papers from top-ranked software engineering journals. First, we manually reviewed 161 papers and in the second phase of our method, we conducted a more extensive semi-automatic classification of papers spanning the years 2001–2015 and 5196 papers. Results from both review steps was used to: i) identify and analyse the predominant practices in ESE (e.g., using t-test or ANOVA), as well as relevant trends in usage of specific statistical methods (e.g., nonparametric tests and effect size measures) and, ii) develop a conceptual model for a statistical analysis workflow with suggestions on how to apply different statistical methods as well as guidelines to avoid pitfalls. Lastly, we confirm existing claims that current ESE practices lack a standard to report practical significance of results. We illustrate how practical significance can be discussed in terms of both the statistical analysis and in the practitioner’s context.}
}
@incollection{2007185,
title = {Index},
editor = {Michael Guttman and John Parodi},
booktitle = {Real-Life MDA},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {185-200},
year = {2007},
series = {The MK/OMG Press},
isbn = {978-0-12-370592-1},
doi = {https://doi.org/10.1016/B978-012370592-1/50014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123705921500143}
}
@article{KATKALOV201499,
title = {Modeling test cases for security protocols with SecureMDD},
journal = {Computer Networks},
volume = {58},
pages = {99-111},
year = {2014},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2013.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S1389128613002983},
author = {Kuzman Katkalov and Nina Moebius and Kurt Stenzel and Marian Borek and Wolfgang Reif},
keywords = {Model-driven testing, Security protocols, Security tests, Model-driven software development, Unit tests},
abstract = {Designing and executing test cases for security-critical protocols is a technically complicated and tedious process. SecureMDD is a model-driven approach that enables development of security-critical applications based on cryptographic protocols. In this paper we introduce a method which combines the model-driven approach used in SecureMDD with the design of functional and security tests. We construct and evaluate new modeling guidelines that allow the modeler to easily define such test cases during the modeling stage. We also implement model transformation routines to generate runnable tests for actual implementation of applications developed with SecureMDD.}
}
@incollection{YOUNG2015221,
title = {Chapter 8 - Electronic Terrorism Threats, Risk, and Risk Mitigation},
editor = {Carl S. Young},
booktitle = {The Science and Technology of Counterterrorism},
publisher = {Butterworth-Heinemann},
address = {Boston},
pages = {221-281},
year = {2015},
isbn = {978-0-12-420056-2},
doi = {https://doi.org/10.1016/B978-0-12-420056-2.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124200562000087},
author = {Carl S. Young}
}
@article{HAIDER201765,
title = {Notch signalling in placental development and gestational diseases},
journal = {Placenta},
volume = {56},
pages = {65-72},
year = {2017},
note = {Exploring common mechanisms between placental and tumour growth},
issn = {0143-4004},
doi = {https://doi.org/10.1016/j.placenta.2017.01.117},
url = {https://www.sciencedirect.com/science/article/pii/S0143400417301194},
author = {S. Haider and J. Pollheimer and M. Knöfler},
keywords = {Placenta, Trophoblast, Notch signalling, Gestational diseases},
abstract = {Activation of Notch signalling upon cell-cell contact of neighbouring cells controls a plethora of cellular processes such as stem cell maintenance, cell lineage determination, cell proliferation, and survival. Accumulating evidence suggests that the pathway also critically regulates these events during placental development and differentiation. Herein, we summarize our present knowledge about Notch signalling in murine and human placentation and discuss its potential role in the pathophysiology of gestational disorders. Studies in mice suggest that Notch controls trophectoderm formation, decidualization, placental branching morphogenesis and endovascular trophoblast invasion. In humans, the particular signalling cascade promotes formation of the extravillous trophoblast lineage and regulates trophoblast proliferation, survival and differentiation. Expression patterns as well as functional analyses indicate distinct roles of Notch receptors in different trophoblast subtypes. Altered effects of Notch signalling have been detected in choriocarcinoma cells, consistent with its role in cancer development and progression. Moreover, deregulation of Notch signalling components were observed in pregnancy disorders such as preeclampsia and fetal growth restriction. In summary, Notch plays fundamental roles in different developmental processes of the placenta. Abnormal signalling through this pathway could contribute to the pathogenesis of gestational diseases with aberrant placentation and trophoblast function.}
}
@article{SNEED2019162,
title = {Re-implementing a legacy system},
journal = {Journal of Systems and Software},
volume = {155},
pages = {162-184},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301050},
author = {Harry Sneed and Chris Verhoef},
keywords = {Software migration, Reverse engineering, Software re-implementation, Code refactoring, Code rewriting, Data renaming},
abstract = {Re-implementation is one of the alternatives to migrate a legacy software system next to conversion, wrapping and redevelopment. It is a compromise solution between automated conversion and complete redevelopment. The technical architecture can be revised and the code replaced, but the functional architecture – the use cases remains as it was. The challenge of this approach is to preserve the functionality while changing the technical implementation. This approach is taken when conversion is not feasible and redevelopment is too expensive or too great a risk. It entails more than a 1:1 transformation but less than a total rewrite. The same components remain with different contents. In this paper the case for reimplementation is presented and the process described. The tools required to support the process are identified and their use illustrated. Finally, two industrial case studies are presented, one with a VisualAge/ PL/I-DB2 system and one with a COBOL-IMS application.}
}
@incollection{2010525,
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP Study Guide},
publisher = {Syngress},
address = {Boston},
pages = {525-567},
year = {2010},
isbn = {978-1-59749-563-9},
doi = {https://doi.org/10.1016/B978-1-59749-563-9.00022-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781597495639000226}
}
@article{SANCHEZGORDON2017162,
title = {A standard-based framework to integrate software work in small settings},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {162-175},
year = {2017},
note = {Standards in Software Process Improvement and Capability Determination},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916301891},
author = {Mary-Luz Sanchez-Gordon and Antonio {de Amescua} and Rory V. O’Connor and Xabier Larrucea},
keywords = {Small companies, VSE, Small settings, Software process improvement, Human factors, Socio-technical system},
abstract = {Small software companies have to work hard in order to survive. They usually find it challenging to spend time and effort on improving their operations and processes. Therefore, it is important to address such needs by the introduction of a proposed framework that specifies ways of getting things done while consciously encourage them to enhance their ability to improve. Although there are many software process improvement approaches, none of them address the human factors of small companies in a comprehensive and holistic way. Samay is a proposed framework to integrate human factors in the daily work as a way to deal with that challenge. This study suggests managing human factors but pointing out the software process life cycle. The purpose is to converge toward a continuous improvement by means of alternative mechanisms that impact on people. This framework was developed based upon reviews of relevant standards (such as ISO/IEC 29110, ISO 10018, OMG Essence and ISO/IEC 33014) and previously published studies in this field. Moreover, an expert review and validation findings supported the view that Samay could support practitioners when small software companies want to start improving their ways of work.}
}
@article{PORUBSKY2020109,
title = {Best Practices for Making Reproducible Biochemical Models},
journal = {Cell Systems},
volume = {11},
number = {2},
pages = {109-120},
year = {2020},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2020.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S2405471220302416},
author = {Veronica L. Porubsky and Arthur P. Goldberg and Anand K. Rampadarath and David P. Nickerson and Jonathan R. Karr and Herbert M. Sauro},
keywords = {reproducibility, systems biology, modeling, biochemical models, standards, FAIR principles, COmputational Modeling in BIology NEtwork},
abstract = {Summary
Like many scientific disciplines, dynamical biochemical modeling is hindered by irreproducible results. This limits the utility of biochemical models by making them difficult to understand, trust, or reuse. We comprehensively list the best practices that biochemical modelers should follow to build reproducible biochemical model artifacts—all data, model descriptions, and custom software used by the model—that can be understood and reused. The best practices provide advice for all steps of a typical biochemical modeling workflow in which a modeler collects data; constructs, trains, simulates, and validates the model; uses the predictions of a model to advance knowledge; and publicly shares the model artifacts. The best practices emphasize the benefits obtained by using standard tools and formats and provides guidance to modelers who do not or cannot use standards in some stages of their modeling workflow. Adoption of these best practices will enhance the ability of researchers to reproduce, understand, and reuse biochemical models.}
}
@article{STRAVER2010S176,
title = {P12-19 Cold paresis in multifocal motor neuropathy},
journal = {Clinical Neurophysiology},
volume = {121},
pages = {S176},
year = {2010},
note = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
issn = {1388-2457},
doi = {https://doi.org/10.1016/S1388-2457(10)60723-7},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710607237},
author = {D.C. Straver and Jan-Thies H. {Van Asseldonk} and N.C. Notermans and J.H. Wokke and L.H. {Van den Berg} and H. Franssen}
}
@article{VIDALI2008399,
title = {Interplay between oxidative stress and hepatic steatosis in the progression of chronic hepatitis C},
journal = {Journal of Hepatology},
volume = {48},
number = {3},
pages = {399-406},
year = {2008},
issn = {0168-8278},
doi = {https://doi.org/10.1016/j.jhep.2007.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0168827807006277},
author = {Matteo Vidali and Marie-Francoise Tripodi and Alesandra Ivaldi and Rosa Zampino and Giuseppa Occhino and Luciano Restivo and Salvatore Sutti and Aldo Marrone and Giuseppe Ruggiero and Emanuele Albano and Luigi E. Adinolfi},
keywords = {Oxidative stress, Steatosis, Lipid peroxidation, HCV infection, HOMA-IR, Liver fibrosis},
abstract = {Background/Aims
The contribution of oxidative stress to the pathogenesis of chronic hepatitis C (CHC) is still poorly elucidated. This study investigated the relationship between oxidative stress, insulin resistance, steatosis and fibrosis in CHC.
Methods
IgG against malondialdehyde-albumin adducts and HOMA-IR were measured as markers of oxidative stress and insulin resistance, respectively, in 107 consecutive CHC patients.
Results
Oxidative stress was present in 61% of the patients, irrespective of age, gender, viral load, BMI, aminotransferase level, histology activity index (HAI) and HCV genotype. Insulin resistance and steatosis were evident in 80% and 70% of the patients, respectively. In the patients infected by HCV genotype non-3, but not in those with genotype 3 infection HOMA-IR (p<0.03), steatosis (p=0.02) and fibrosis (p<0.05) were higher in the subjects with oxidative stress than in those without. Multiple regression analysis revealed that, HOMA-IR (p<0.01), fibrosis (p<0.01) and oxidative stress (p<0.05) were independently associated with steatosis, whereas steatosis was independently associated with oxidative stress (p<0.03) and HOMA-IR (p<0.02). Steatosis (p<0.02) and HAI (p=0.007) were also independent predictors of fibrosis.
Conclusions
In patients infected by HCV genotype non-3, oxidative stress and insulin resistance contribute to steatosis, which in turn exacerbates both insulin resistance and oxidative stress and accelerates the progression of fibrosis.}
}
@incollection{2012385,
editor = {Colin Walls},
booktitle = {Embedded Software (Second Edition)},
publisher = {Newnes},
edition = {Second Edition},
address = {Oxford},
pages = {385-395},
year = {2012},
isbn = {978-0-12-415822-1},
doi = {https://doi.org/10.1016/B978-0-12-415822-1.00021-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124158221000210}
}
@article{KUMAR2015859,
title = {Model Based Distributed Testing of Object Oriented Programs},
journal = {Procedia Computer Science},
volume = {46},
pages = {859-866},
year = {2015},
note = {Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace & Island Resort, Kochi, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.02.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915002197},
author = {K.S. Vipin Kumar and Sheena Mathew},
keywords = {Distributed Testing, SDG, FCFS, Object Oriented Program Testing ;},
abstract = {In recent times the software systems have evolved in size and complexity. This has resulted in usage of object oriented programming in the development of such systems. Though object oriented programs are helpful in programming large systems, testing of such systems requires much more effort and time. For this the program is analyzed to create a model based on System Dependence Graph(SDG) which is then used to find locations within the program where the state of the program can be freezed and reused while executing other test cases.}
}
@incollection{OSIS201753,
title = {Chapter 2 - Software Designing With Unified Modeling Language Driven Approaches},
editor = {Janis Osis and Uldis Donins},
booktitle = {Topological UML Modeling},
publisher = {Elsevier},
address = {Boston},
pages = {53-82},
year = {2017},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-805476-5},
doi = {https://doi.org/10.1016/B978-0-12-805476-5.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054765000022},
author = {Janis Osis and Uldis Donins},
keywords = {Software modeling methods and approaches, UML application, benefits and limitations of UML modeling driven approaches},
abstract = {The Unified Modeling Language (UML) is a notation and as such its specification does not contain any guidelines for software development process. Despite that UML is independent of particular methods and approaches, most of the UML modeling driven methods uses use case driven approach thus raising incomplete analysis of the problem domain functioning. Since UML modeling driven approaches are elaborated by different authors, their prescriptions differ. There is also difference in the use of use case narratives across various methods due to the lack of guidance on narrative format in the UML specification. The UML specification only states that “use cases are typically specified in various idiosyncratic formats such as natural language, tables, trees, etc. Therefore, it is not easy to capture its structure accurately or generally by a formal model.” This chapter discusses the current state of the art of UML-based software development approaches. Most attention is paid on the artifacts created by using the UML.}
}
@incollection{AHMAD20181,
title = {Chapter One - Model-Based Testing for Internet of Things Systems},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {108},
pages = {1-58},
year = {2018},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300517},
author = {Abbas Ahmad and Fabrice Bouquet and Elizabeta Fourneret and Bruno Legeard},
keywords = {Model-based testing, Modeling approaches, Test generation technology, Security testing},
abstract = {The Internet of Things (IoT) is nowadays globally a mean of innovation and transformation for many companies. Applications extend to a large number of domains, such as smart cities, smart homes, and health care. The Gartner Group estimates an increase up to 21 billion connected things by 2020. The large span of “things” introduces problematic aspects, such as interoperability due to the heterogeneity of communication protocols and the lack of a globally accepted standard. The large span of usages introduces problems regarding secure deployments and scalability of the network over large-scale infrastructures. This chapter describes the challenges for the IoT testing, includes state-of-the-art testing of IoT systems using models, and presents a model-based testing as a service approach to respond to its challenges through demonstrations with real use cases involving two of the most accepted standards worldwide: FIWARE and oneM2M.}
}
@incollection{KAWAMOTO2014819,
title = {Chapter 29 - Integration of Knowledge Resources into Applications to Enable CDS: Architectural Considerations},
editor = {Robert A. Greenes},
booktitle = {Clinical Decision Support (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {819-849},
year = {2014},
isbn = {978-0-12-398476-0},
doi = {https://doi.org/10.1016/B978-0-12-398476-0.00029-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123984760000294},
author = {Kensaku Kawamoto and Emory Fry and Robert Greenes},
keywords = {Clinical decision support, scalable CDS, Health eDecisions, OpenCDS, Service-Oriented Architecture (SOA)},
abstract = {This chapter examines in detail the bridge between the creation and delivery of CDS content – in other words, how knowledge resources can be integrated with clinical information systems (CISs) to enable CDS. While such integration can be relatively straightforward for a single instance – that is, the integration of a specific knowledge resource in a specific clinical information system – the challenge lies in the fact that both knowledge resources and clinical information systems are quite diverse. Consequently, there is no single knowledge integration architecture that can address all circumstances. However, there are several architectural patterns for knowledge integration that can, taken together, enable the effective integration of knowledge resources into applications. The primary purpose of this chapter, then, is to outline the main CDS knowledge integration architectures that are available and to detail the pros and cons of each approach. The appropriateness of a given architecture for a particular organization depends on a variety of factors, including the existing clinical information system infrastructure and the type of CDS capability involved (e.g. real-time vs. non-real-time applications. These various approaches are outlined here, with special attention being placed on knowledge integration architectures aligned with broad trends in the IT landscape, such as service-oriented architectures, cloud-based computing, and app-based software ecosystems. The chapter also discusses how CDS architectures must align with larger changes in the health care industry as a whole, as shifting health care reimbursement models are requiring continuity of care across multiple organizations and health IT systems centered around patients and populations rather than care episodes at individual care settings.}
}
@article{KESSEL2022111442,
title = {Diversity-driven unit test generation},
journal = {Journal of Systems and Software},
volume = {193},
pages = {111442},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111442},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001406},
author = {Marcus Kessel and Colin Atkinson},
keywords = {Diversity, Test generation, Test amplification, Automation, Behavior, Experiment, Evaluation, Test quality},
abstract = {The goal of automated unit test generation tools is to create a set of test cases for the software under test that achieve the highest possible coverage for the selected test quality criteria. The most effective approaches for achieving this goal at the present time use meta-heuristic optimization algorithms to search for new test cases using fitness functions defined on existing sets of test cases and the system under test. Regardless of how their search algorithms are controlled, however, all existing approaches focus on the analysis of exactly one implementation, the software under test, to drive their search processes, which is a limitation on the information they have available. In this paper we investigate whether the practical effectiveness of white box unit test generation tools can be increased by giving them access to multiple, diverse implementations of the functionality under test harvested from widely available Open Source software repositories. After presenting a basic implementation of such an approach, DivGen (Diversity-driven Generation), on top of the leading test generation tool for Java (EvoSuite), we assess the performance of DivGen compared to EvoSuite when applied in its traditional, mono-implementation oriented mode (MonoGen). The results show that while DivGen outperforms MonoGen in 33% of the sampled classes for mutation coverage (+16% higher on average), MonoGen outperforms DivGen in 12.4% of the classes for branch coverage (+10% higher average).}
}
@article{PENTON2012450,
title = {Notch signaling in human development and disease},
journal = {Seminars in Cell & Developmental Biology},
volume = {23},
number = {4},
pages = {450-457},
year = {2012},
note = {Cancer Cell Metabolism & Notch Signaling},
issn = {1084-9521},
doi = {https://doi.org/10.1016/j.semcdb.2012.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S1084952112000146},
author = {Andrea L. Penton and Laura D. Leonard and Nancy B. Spinner},
keywords = {Alagille syndrome, Spondylocostal dysostosis, Hajdu Cheney, Cardiac disease, Notch signaling},
abstract = {Mutations in Notch signaling pathway members cause developmental phenotypes that affect the liver, skeleton, heart, eye, face, kidney, and vasculature. Notch associated disorders include the autosomal dominant, multi-system, Alagille syndrome caused by mutations in both a ligand (Jagged1 (JAG1)) and receptor (NOTCH2) and autosomal recessive spondylocostal dysostosis, caused by mutations in a ligand (Delta-like-3 (DLL3)), as well as several other members of the Notch signaling pathway. Mutations in NOTCH2 have also recently been connected to Hajdu-Cheney syndrome, a dominant disorder causing focal bone destruction, osteoporosis, craniofacial morphology and renal cysts. Mutations in the NOTCH1 receptor are associated with several types of cardiac disease and mutations in NOTCH3 cause the dominant adult onset disorder CADASIL (cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy), a vascular disorder with onset in the 4th or 5th decades. Studies of these human disorders and their inheritance patterns and types of mutations reveal insights into the mechanisms of Notch signaling.}
}
@article{KRATZIG2007618,
title = {A software framework for data analysis},
journal = {Computational Statistics & Data Analysis},
volume = {52},
number = {2},
pages = {618-634},
year = {2007},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2006.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167947306002672},
author = {Markus Krätzig},
keywords = {Java, Object-oriented programming, Econometrics, Software engineering},
abstract = {The open-source Java software framework JStatCom is presented which supports the development of rich desktop clients for data analysis in a rather general way. The concept is to solve all recurring tasks with the help of reusable components and to enable rapid application development by adopting a standards based approach which is readily supported by existing programming tools. Furthermore, JStatCom allows to call external procedures from within Java that are written in other languages, for example Gauss, Ox or Matlab. This way it is possible to reuse an already existing code base for numerical routines written in domain-specific programming languages and to link them with the Java world. A reference application for JStatCom is the econometric software package JMulTi, which will shortly be introduced.}
}
@article{GONCALVES20096879,
title = {Induction of notch signaling by immobilization of jagged-1 on self-assembled monolayers},
journal = {Biomaterials},
volume = {30},
number = {36},
pages = {6879-6887},
year = {2009},
issn = {0142-9612},
doi = {https://doi.org/10.1016/j.biomaterials.2009.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0142961209009260},
author = {Raquel M. Gonçalves and M. Cristina L. Martins and Graça Almeida-Porada and Mário A. Barbosa},
keywords = {Nanostructured surfaces, Self-assembled monolayers, Protein immobilization, Protein adsorption, Notch signaling},
abstract = {Notch signaling is a key mechanism during mammal development and stem cell regulation. This study aims to target and control Notch signaling by ligands immobilization using self-assembled monolayers (SAMs) as model surfaces. Non-fouling substrates were prepared by immersion of gold substrates in (1-Mercapto-11-undecyl)tetra(ethylene glycol) thiol solutions. These surfaces were activated with N,N′-carbonyldiimidazole (CDI) at different concentrations (0, 0.03, 0.3, 3 and 30mg/ml) and an anti-human IgG, Fc specific fragment antibody (Ab) was covalently bound to EG4-SAMs to guarantee the correct exposure of the Notch ligand Jagged-1/Fc chimera (Jag-1). The presence of Ab and Jag-1 was confirmed by radiolabeling, X-ray photoelectron spectroscopy (XPS), ellipsometry and ELISA. The biological activity of Jag-1-Ab-SAMs was assessed by real-time PCR for Hes-1 family gene expression, a Notch pathway target gene, in HL-60 cell line. Results have shown an increase of the amount of immobilized Ab with increasing surface activator concentrations. Jag-1 concentration also increases with Ab concentration. Interestingly, a higher Jagged-1 exposure and fold increase in Hes-1 expression were obtained for surfaces activated with the lowest concentration of CDI (0.03mg/ml). These results illustrate the great importance of ligands orientation and exposure, when compared with density. This investigation brings new insights into Notch signaling mechanisms. In particular, Jag-1-Ab-SAMs have shown to be adequate model surfaces to study Notch pathway activation and may provide a basis to develop new interfaces in biomaterials to control Notch mechanism in different cell systems.}
}
@article{SHALA2019100058,
title = {Novel trust consensus protocol and blockchain-based trust evaluation system for M2M application services},
journal = {Internet of Things},
volume = {7},
pages = {100058},
year = {2019},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2019.100058},
url = {https://www.sciencedirect.com/science/article/pii/S2542660519301234},
author = {Besfort Shala and Ulrich Trick and Armin Lehmann and Bogdan Ghita and Stavros Shiaeles},
keywords = {Distributed Ledger, Blockchain, Consensus Protocol, Trust, Security, M2M, Service and Application},
abstract = {The increasing number of intelligent Machine-to-Machine Communication (M2M) devices in the end-user domain provide good resources for creating and sharing M2M application services. Therefore, transferring the role of a traditional centralized service provider to decentralized peers (end-users) acting as service providers is very promising. However, the future of decentralized M2M application services which are independently provided or consumed by several end-users in the M2M community depends on trust. Untrustworthy peers trying to deploy malfunctioning services for others mitigate the benefits of decentralized systems. Nowadays, the concept of distributed ledger and blockchain has an increased popularity regarding trustless computing among communities operating without centralized authorities. This research publication provides a comprehensive analysis and approach merging the concepts of M2M application services, trust and distributed ledger technologies. Moreover, this publication presents an optimized Trust Evaluation System which is used to ensure trustworthiness among peers in a M2M community. To improve the Trust Evaluation System the integration of blockchain for data storage is introduced. Additionally, blockchain technology is used to extend the existing trust model of the Trust Evaluation System to enable tamper-proof data and detection of untrustworthy peers. This publication reviews several existing approaches in the academic and industry sector to highlight the limitations of the blockchain regarding the consensus and proposes a novel Trust Consensus Protocol. This research publication also provides a practical evaluation of the proposed protocol.}
}
@incollection{2020797,
title = {Index},
editor = {Christian Vargel},
booktitle = {Corrosion of Aluminium (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Amsterdam},
pages = {797-813},
year = {2020},
isbn = {978-0-08-099925-8},
doi = {https://doi.org/10.1016/B978-0-08-099925-8.20001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008099925820001X}
}
@incollection{2013623,
editor = {Tammy Noergaard},
booktitle = {Embedded Systems Architecture (Second Edition)},
publisher = {Newnes},
edition = {Second Edition},
pages = {623-638},
year = {2013},
isbn = {978-0-12-382196-6},
doi = {https://doi.org/10.1016/B978-0-12-382196-6.00026-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123821966000261}
}
@article{FUCCI2022111389,
title = {When traceability goes awry: An industrial experience report},
journal = {Journal of Systems and Software},
volume = {192},
pages = {111389},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111389},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001091},
author = {Davide Fucci and Emil Alégroth and Thomas Axelsson},
keywords = {Industry-academia collaboration, Traceability, Software quality},
abstract = {The concept of traceability between artifacts is considered an enabler for software project success. This concept has received plenty of attention from the research community and is by many perceived to always be available in an industrial setting. In this industry-academia collaborative project, a team of researchers, supported by testing practitioners from a large telecommunication company, sought to investigate the partner company’s issues related to software quality. However, it was soon identified that the fundamental traceability links between requirements and test cases were missing. This lack of traceability impeded the implementation of a solution to help the company deal with its quality issues. In this experience report, we discuss lessons learned about the practical value of creating and maintaining traceability links in complex industrial settings and provide a cautionary tale for researchers.}
}
@article{VANDIJK2010S176,
title = {P12-20 Age-related changes in motor unit number estimates in adult patients with Charcot-Marie-Tooth type 1A},
journal = {Clinical Neurophysiology},
volume = {121},
pages = {S176},
year = {2010},
note = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
issn = {1388-2457},
doi = {https://doi.org/10.1016/S1388-2457(10)60724-9},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710607249},
author = {J.P. {van Dijk} and C. Verhamme and I.N. {van Schaik} and H.J. Schelhaas and E. Mans and L.J. Bour and D.F. Stegeman and M.J. Zwarts}
}
@article{MELVIN1989325,
title = {Do centrally planned exchange rates behave differently from capitalist rates?},
journal = {Journal of Comparative Economics},
volume = {13},
number = {2},
pages = {325-334},
year = {1989},
issn = {0147-5967},
doi = {https://doi.org/10.1016/0147-5967(89)90008-5},
url = {https://www.sciencedirect.com/science/article/pii/0147596789900085},
author = {Michael Melvin and Su Zhou},
abstract = {We conduct a statistical analysis of the time series of the dollar value of the pound, mark, yen, yuan, dinar, and forint exchange rates. The evidence indicates that the centrally planned exchange rates are well represented by random walks, as are the capitalist rates. This might be expected if the planned rates are pegged to the capitalist rates. However, a lack of cointegration between the planned rates and the capitalist rates suggests that this is not the reason for the nonstationarity of planned exchange rates.}
}
@article{ALTURJMAN2019732,
title = {5G-enabled devices and smart-spaces in social-IoT: An overview},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {732-744},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311962},
author = {Fadi Al-Turjman},
keywords = {Internet of Things (IoT), Smart environments, Sensors, 5G, Smartphones},
abstract = {The abundance of smartphones, with their growing capabilities potentiates applications in numerous domains. A typical smartphone nowadays is equipped with an array of embedded sensors (e.g., GPS, accelerometers, gyroscopes, RFID readers, cameras, and microphones) along with different communication interfaces (e.g. Cellular, WiFi, Bluetooth, etc.). Thus, a smartphone is a significant provider for sensory data that awaits the utilization in many critical applications. Primers of this vision have demonstrated success, both in the literature and application’s market. In this literature review, we present the main motivations in carrying these smart devices, and the correlation between the user surrounding context and the application usage. We focus on context-awareness in smart systems and space discovery paradigms; online versus offline, the femtocell usage and energy aspects to be considered, and about the ongoing social IoT applications. Moreover, we highlight the most up-to-date open research issues in this area.}
}
@article{SMITH2016404,
title = {A Document-Driven Method for Certifying Scientific Computing Software for Use in Nuclear Safety Analysis},
journal = {Nuclear Engineering and Technology},
volume = {48},
number = {2},
pages = {404-418},
year = {2016},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2015.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1738573315002582},
author = {W. Spencer Smith and Nirmitha Koothoor},
keywords = {Literate Programming, Nuclear Safety Analysis, Numerical Simulation, Requirements Specification, Software Engineering, Software Quality},
abstract = {This paper presents a documentation and development method to facilitate the certification of scientific computing software used in the safety analysis of nuclear facilities. To study the problems faced during quality assurance and certification activities, a case study was performed on legacy software used for thermal analysis of a fuelpin in a nuclear reactor. Although no errors were uncovered in the code, 27 issues of incompleteness and inconsistency were found with the documentation. This work proposes that software documentation follow a rational process, which includes a software requirements specification following a template that is reusable, maintainable, and understandable. To develop the design and implementation, this paper suggests literate programming as an alternative to traditional structured programming. Literate programming allows for documenting of numerical algorithms and code together in what is termed the literate programmer's manual. This manual is developed with explicit traceability to the software requirements specification. The traceability between the theory, numerical algorithms, and implementation facilitates achieving completeness and consistency, as well as simplifies the process of verification and the associated certification.}
}
@article{BREIT20061151,
title = {Activating NOTCH1 mutations predict favorable early treatment response and long-term outcome in childhood precursor T-cell lymphoblastic leukemia},
journal = {Blood},
volume = {108},
number = {4},
pages = {1151-1157},
year = {2006},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood-2005-12-4956},
url = {https://www.sciencedirect.com/science/article/pii/S0006497120526761},
author = {Stephen Breit and Martin Stanulla and Thomas Flohr and Martin Schrappe and Wolf-Dieter Ludwig and Gabriele Tolle and Margit Happich and Martina U. Muckenthaler and Andreas E. Kulozik},
abstract = {Abstract
Activating mutations of the transmembrane receptor NOTCH1 are common in precursor T-cell lymphoblastic leukemia (T-ALL). We systematically analyzed the impact of activating NOTCH1 mutations on early treatment response and long-term outcome in 157 patients with T-ALL of the pediatric ALL–Berlin-Frankfurt-Munster (BFM) 2000 study. We confirm previous results that NOTCH1 mutations occur in more than 50% of T-ALL in children. In 82 patients (82/157; 52.2%), activating NOTCH1 mutations were identified either in the heterodimerization (55/82; 67.1%), in the PEST (13/82; 15.9%), or in both domains (14/82; 17.0%). The presence of NOTCH1 mutations was significantly correlated with a good prednisone response and favorable minimal residual disease (MRD) kinetics, which was independent from sex, age, white blood cell count, and T-cell immunophenotype at the time of diagnosis. Furthermore, activating NOTCH1 mutations specified a large subgroup of patients with an excellent prognosis. These findings indicate that in the context of the ALL-BFM 2000 treatment strategy, NOTCH1 mutations predict a more rapid early treatment response and a favorable long-term outcome in children with T-ALL.}
}
@article{REBAHI201839,
title = {Towards a next generation 112 testbed: The EMYNOS ESInet},
journal = {International Journal of Critical Infrastructure Protection},
volume = {22},
pages = {39-50},
year = {2018},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1874548217302081},
author = {Yacine Rebahi and Kin Tsun Chiu and Nikolay Tcholtchev and Simon Hohberg and Evangelos Pallis and Evangelos Markakis},
keywords = {Emergency services, 112, 911, EMYNOS, SIP, IMS, ESInet, Location, i3 architecture, RTT},
abstract = {This paper belongs to a series of research documents describing the progress in the specification and development of the EMYNOS framework offering an IP based platform for emergency services. EMYNOS is an international research project funded by the European Commission. Although migrating to Next Generation 112 and 911 is not new as a topic, no real testbed that can be used for evaluating the relevant standards exists so far. In this paper we discuss the EMYNOS approach and in particular some mechanisms that have been developed in this context. Special attention is paid to the EMYNOS testbed that was assessed during the ETSI NG112 plugtests in 2016 and 2017. Some experiments and test results are provided as well.}
}
@article{ADAMS2009668,
title = {Using aspect orientation in legacy environments for reverse engineering using dynamic analysis—An industrial experience report},
journal = {Journal of Systems and Software},
volume = {82},
number = {4},
pages = {668-684},
year = {2009},
note = {Special Issue: Selected papers from the 2008 IEEE Conference on Software Engineering Education and Training (CSEET08)},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2008.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S0164121208002173},
author = {Bram Adams and Kris {De Schutter} and Andy Zaidman and Serge Demeyer and Herman Tromp and Wolfgang {De Meuter}},
keywords = {Dynamic analysis, Aspect-oriented programming, Industrial case study, Program comprehension C},
abstract = {This paper reports on the challenges of using aspect-oriented programming (AOP) to aid in re-engineering a legacy C application. More specifically, we describe how AOP helps in the important reverse engineering step which typically precedes a re-engineering effort. We first present a comparison of the available AOP tools for legacy C code bases, and then argue on our choice of Aspicere, our own AOP implementation for C. Then, we report on Aspicere’s application in reverse engineering a legacy industrial software system and we show how we apply a dynamic analysis to regain insight into the system. AOP is used for instrumenting the system and for gathering the data. This approach works and is conceptually very clean, but comes with a major quid pro quo: integration of AOP tools with the build system proves an important issue. This leads to the question of how to reconcile the notion of modular reasoning within traditional build systems with a programming paradigm which breaks this notion.}
}
@article{WASEEM2020110798,
title = {A Systematic Mapping Study on Microservices Architecture in DevOps},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110798},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110798},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302053},
author = {Muhammad Waseem and Peng Liang and Mojtaba Shahin},
keywords = {Microservices Architecture, DevOps, Systematic Mapping Study},
abstract = {Context:
Applying Microservices Architecture (MSA) in DevOps has received significant attention in recent years. However, there exists no comprehensive review of the state of research on this topic.
Objective:
This work aims to systematically identify, analyze, and classify the literature on MSA in DevOps.
Methods:
A Systematic Mapping Study (SMS) has been conducted on the literature published between January 2009 and July 2018.
Results:
Forty-seven studies were finally selected and the key results are: (1) Three themes on the research on MSA in DevOps are “microservices development and operations in DevOps”, “approaches and tool support for MSA based systems in DevOps”, and “MSA migration experiences in DevOps”. (2) 24 problems with their solutions regarding implementing MSA in DevOps are identified. (3) MSA is mainly described by using boxes and lines. (4) Most of the quality attributes are positively affected when employing MSA in DevOps. (5) 50 tools that support building MSA based systems in DevOps are collected. (6) The combination of MSA and DevOps has been applied in a wide range of application domains.
Conclusion:
The results and findings will benefit researchers and practitioners to conduct further research and bring more dedicated solutions for the issues of MSA in DevOps.}
}
@incollection{KHAN2012141,
title = {Chapter 4 - Pragmatic Directions in Engineering Secure Dependable Systems},
editor = {Ali Hurson and Sahra Sedigh},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {84},
pages = {141-167},
year = {2012},
booktitle = {Dependable and Secure Systems Engineering},
issn = {0065-2458},
doi = {https://doi.org/10.1016/B978-0-12-396525-7.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123965257000058},
author = {M. Farrukh Khan and Raymond A. Paul},
keywords = {Engineering dependable systems, Engineering complex systems, Dependable software/hardware systems, Dependable cloud infrastructure, Secure and reliable systems},
abstract = {All large and complex computer and communications systems have an intrinsic requirement to be dependable since their failure can cause significant losses in terms of life or treasure. Such the systems are expected to have the attributes of reliability, availability, safety, confidentiality, survivability, integrity, and maintainability. Current software and hardware systems continue to evolve in complexity at rapid rates. Although the increase in the complexity of single artifact (such as number of logical decision points in a software package) can often be tracked with Moore's Law like approximations, systems constructed out of larger number of smaller subsystems defy such classifications. The reason for this added complexity is that interactions between the subsystems explode exponentially in the size of the parent system. Yet all component interactions must be addressed exhaustively to predict accurate behavior of the whole system. The challenge that we face is that it is seldom possible to model or test all such interactions in a given system. As a result, building dependable complex systems with realistic assessment of risks of failure is an extremely difficult endeavor. Attempts have been made to ameliorate the difficulty in the engineering of dependable complex systems using lessons from engineering methodologies in other domains. We discuss key attributes of dependable complex systems, with a special emphasis on security where information is involved. We review classical approaches to designing, building, and maintaining dependable complex systems. We present promising features and novel ideas applicable to the lifecycle of dependable complex systems. Most of our discussion is focused within the domain of hardware and software systems. Over time, practitioners in dependable engineering have learned lessons from previous experience and continue to present prescriptive approaches discovered through research and analysis. These lessons and approaches are often applicable to other engineering domains such as construction, transportation, and industrial control. We look at specific engineering challenges and proposed solutions pertaining to the following general domains, with occasional examples from any branch of engineering:•dependable hardware/software systems;•secure dependable systems;•dependable cloud infrastructure and applications. Finally, we conclude with the observation that several approaches are applicable across all these domains and identify accessible techniques that have good potential to increase the dependability of systems. These approaches can be considered as axiomatic in building any future complex systems with a high degree of dependability.}
}
@article{HE2018109,
title = {Testing bidirectional model transformation using metamorphic testing},
journal = {Information and Software Technology},
volume = {104},
pages = {109-129},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301538},
author = {Xiao He and Xing Chen and Sibo Cai and Ying Zhang and Gang Huang},
keywords = {Metamorphic testing, Bidirectional model transformation, Model driven engineering},
abstract = {Context
In model-based software development, bidirectional model transformation (BX) provides a fundamental solution to model synchronization that can retain the consistency among models. Similar to conventional programs, a BX program may also contain bugs. Accordingly, a BX program must be tested prior to being used in practice. A challenging problem of testing BX programs is to construct test oracles (e.g., assertions and expected output models), which are usually difficult and/or expensive to manually specify.
Objective
In we paper, we investigate how to alleviate the oracle problem in BX testing via reducing the costs of developing test oracles.
Method
We propose a metamorphic testing approach for BX. First, we identify three generic metamorphic relations for BX. Afterwards, we define a metamodel MT4MT to establish metamorphic test groups and test scripts. We also propose a testing framework to support metamorphic testing based on MT4MT.
Results
We conducted an experimental study of mutation analysis and a case study on three ATL-based ad-hoc BXs. The results of the experimental study and the case study showed that our approach killed 79.38% mutants and enabled us to test real-world ATL-based ad-hoc BXs. We also demonstrated that MT4MT can be used to test the semantics properties of BXs.
Conclusion
Our approach is an effective and practical approach with lower costs of developing test oracles.}
}
@article{AYNUTDINOV2009227,
title = {The prototype string for the km3-scale Baikal neutrino telescope},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {602},
number = {1},
pages = {227-234},
year = {2009},
note = {Proceedings of the 3rd International Workshop on a Very Large Volume Neutrino Telescope for the Mediterranean Sea},
issn = {0168-9002},
doi = {https://doi.org/10.1016/j.nima.2008.12.149},
url = {https://www.sciencedirect.com/science/article/pii/S0168900208018998},
author = {V. Aynutdinov and A. Avrorin and V. Balkanov and I. Belolaptikov and D. Bogorodsky and N. Budnev and I. Danilchenko and G. Domogatsky and A. Doroshenko and A. Dyachok and Zh.-A. Dzhilkibaev and S. Fialkovsky and O. Gaponenko and K. Golubkov and O. Gress and T. Gress and O. Grishin and A. Klabukov and A. Klimov and A. Kochanov and K. Konischev and A. Koshechkin and V. Kulepov and D. Kuleshov and L. Kuzmichev and E. Middell and S. Mikheyev and M. Milenin and R. Mirgazov and E. Osipova and G. Pan’kov and L. Pan’kov and A. Panfilov and D. Petukhov and E. Pliskovsky and P. Pokhil and V. Poleschuk and E. Popova and V. Prosin and M. Rozanov and V. Rubtzov and A. Sheifler and A. Shirokov and B. Shoibonov and Ch. Spiering and O. Suvorova and B. Tarashansky and R. Wischnewski and I. Yashin and V. Zhukov},
keywords = {Neutrino telescopes, BAIKAL},
abstract = {A prototype string for the future km3-scale Baikal neutrino telescope has been deployed in April, 2008, and is fully integrated into the NT200+ telescope. All basic string elements–optical modules (with 12″/13″ hemispherical photomultipliers), 200MHz FADC readout and calibration system–have been redesigned following experience with NT200+. First results of in-situ operation of this prototype string are presented.}
}
@article{TRAN2022103460,
title = {A framework for automating deployment and evaluation of blockchain networks},
journal = {Journal of Network and Computer Applications},
volume = {206},
pages = {103460},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103460},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001102},
author = {Nguyen Khoi Tran and M. Ali Babar and Andrew Walters},
keywords = {Blockchain, Distributed Ledger, Deployment, Evaluation, Automation, Framework},
abstract = {A blockchain network is a distributed system established by mutually distrusting participants to operate a blockchain, enabling them to manage critical information such as account balances or asset ownership without a centralised third party. Blockchain network deployment and evaluation have become prevalent due to the emerging blockchain use cases by enterprises, governments, and Internet of Things (IoT) applications, which demand private blockchains rather than participating in public ones. A blockchain network architecture drives deployment and evaluation activities. Nevertheless, practitioners must learn and perform error-prone activities to transform architecture into a blockchain network and evaluate it. Therefore, it is beneficial to automate these activities so that practitioners can focus on the architecture design, a valuable and hard-to-automate activity. The key challenges of such an automation framework are keeping up with the advances in blockchain technologies and the increasing complexity of blockchain network architecture. This paper proposes NVAL, a software framework that implements a novel architecture-driven, community-supported approach to automate blockchain network deployment and evaluation. NVAL accepts blockchain network architecture as input. It supports complex multi-channel blockchain networks, an increasingly prevalent architecture for private blockchain. The framework keeps up with blockchain technologies by leveraging platform-specific automation programmes developed by a practitioner community via runtime composition to handle new networks. We evaluated NVAL with a case study and showed that the framework requires only seven automation programmes to deploy 65 blockchain networks with 12 diverse architectures and generate 295 evaluation datasets. Furthermore, it consumes only 95.5 ms to plan and orchestrate the deployment and evaluation, which is minuscule compared to the total time required for deploying and benchmarking a blockchain network.}
}
@incollection{2005610,
title = {Appendix D - Glossary},
editor = {Tammy Noergaard},
booktitle = {Embedded Systems Architecture},
publisher = {Newnes},
address = {Burlington},
pages = {610-626},
year = {2005},
series = {Embedded Technology},
isbn = {978-0-7506-7792-9},
doi = {https://doi.org/10.1016/B978-075067792-9/50022-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780750677929500224}
}
@article{TSAI20091578,
title = {Experience on knowledge-based software engineering: A logic-based requirements language and its industrial applications},
journal = {Journal of Systems and Software},
volume = {82},
number = {10},
pages = {1578-1587},
year = {2009},
note = {SI: YAU},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2009.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0164121209000521},
author = {Jeffrey J.P. Tsai and Alan Liu},
keywords = {Formal specification language, Nonmonotonic logic, Formal verification, Automatic code generation, Knowledge-based software engineering},
abstract = {A formal requirements specification language plays an important role in software development. Not only can such language be used for stating requirements specification, but also can be used in many phases of software development life cycle. The FRORL project started from constructing a language with a solid logical foundation and further expanded to research in verification, validation, requirements analysis, debugging, and transformation. Research in this project aided in some industrial applications in which a code generation tool produced software for embedded systems. This article reports the experiences gained from this project and states the value of research in knowledge-based software engineering.}
}
@article{MYERS2005117,
title = {lin-35 Rb Acts in the Major Hypodermis to Oppose Ras-Mediated Vulval Induction in C. elegans},
journal = {Developmental Cell},
volume = {8},
number = {1},
pages = {117-123},
year = {2005},
issn = {1534-5807},
doi = {https://doi.org/10.1016/j.devcel.2004.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S1534580704004228},
author = {Toshia R. Myers and Iva Greenwald},
abstract = {Specification of vulval precursor cell (VPC) fates in C. elegans has served as an important signal transduction paradigm. Genetic studies have indicated that a large group of synthetic multivulva (SynMuv) genes, including the Rb ortholog lin-35, antagonizes the activity of the EGF receptor-Ras-MAP kinase pathway during VPC specification. A prevalent view has been that Rb-mediated transcriptional regulation and chromatin remodeling activities act in the VPCs to antagonize Ras activation through effects on promoters of target genes of the EGF receptor-Ras-MAP kinase pathway that promote vulval fates. Here, we have investigated the cellular focus of lin-35 using conventional genetic mosaic analysis and tissue-specific expression. Our results indicate that lin-35 activity is required in the major hypodermal syncytium and not in the VPCs to inhibit vulval fates. LIN-35 Rb may inhibit vulval fates by regulating a signal from hyp7 to the VPCs or the physiological state of hyp7.}
}
@article{HUGUES2022102376,
title = {A correct-by-construction AADL runtime for the Ravenscar profile using SPARK2014},
journal = {Journal of Systems Architecture},
volume = {123},
pages = {102376},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102376},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121002599},
author = {Jerome Hugues},
keywords = {AADL, Runtime, SPARK2014, Ravenscar},
abstract = {Middleware is an integral part of critical software, providing core services for data exchange and manipulation, job execution, and scheduling. Their correctness is central to the correct execution of the software. They must be carefully configured to meet all functional and non-functional requirements. From a set of valid configuration parameters, one then has to demonstrate the implementation is correct and can fulfill its mission. Model-based techniques provide the foundations for correct-by-construction engineering. Most notably, they can be used to model a system, assess its configuration is correct, and then generate the corresponding middleware instance. The SAE AADL language supports the modeling of safety-critical systems and covers its design, configuration, and analysis. In this paper, we present several contributions: the definition of a model of computation aligned with the Ada Ravenscar profile supported by an architectural model expressed using AADL; derivation rules from AADL constructs to middleware services using Ada 2012 and SPARK 2014, and the proof of correctness of the implementation. Our contribution illustrates how one can prove the absence of runtime errors in middleware configured from high-level descriptions. This effort illustrates the positive effect models, programming languages and associated toolsets have on developing high-assurance middleware.}
}
@article{CONGOTE200530,
title = {Monitoring insulin-like growth factors in HIV infection and AIDS},
journal = {Clinica Chimica Acta},
volume = {361},
number = {1},
pages = {30-53},
year = {2005},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cccn.2005.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0009898105002901},
author = {Luis F. Congote},
keywords = {IGF, Growth hormone, HAART, HIV, Wasting, Lipodystrophy},
abstract = {There is a close association between the growth hormone (GH)–insulin-like growth factor I (IGF-I) axis, infection and immunity. Infection with the human immunodeficiency virus (HIV) is often associated with a decrease of the concentrations of IGF-I, IGF-II, IGF-binding protein 3 (IGFBP-3) and an increase of IGFBP-1 and -2. Many investigators have studied the relationship between the GH-IGF-I system and some of the most common characteristics of disease progression, such as decreased CD4 cell counts, weight loss and fat redistribution. Although conditions for restoration of thymic function and lymphopoiesis with GH or IGF-I are still not well defined, many studies led to the development of clinical trials on the therapeutic use of GH, IGF-I and GHRH for the treatment of weight loss or fat redistribution, two problems which persist despite the introduction of highly active antiretroviral therapy. Monitoring IGF-I concentrations during treatment with GH and GHRH is likely to become an essential component of their therapeutic use. IGF-I levels are the first indicator of treatment efficacy and can be used to monitor compliance. High levels of IGF-I are a warning sign for the increased risk of potential adverse effects, such as acromegalic-like symptoms or malignancy. This could lead to a reduction of the therapeutic dose or the temporary interruption of treatment until IGF levels reach a safe range. IGF-I levels are also likely to increase with other hormones used in HIV patients, such as erythropoietin for the treatment of anemia or anabolic androgens in HIV-infected women.}
}
@article{STRUBER2017196,
title = {A text-based visual notation for the unit testing of model-driven tools},
journal = {Computer Languages, Systems & Structures},
volume = {49},
pages = {196-215},
year = {2017},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2016.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1477842416300276},
author = {Daniel Strüber and Felix Rieger and Gabriele Taentzer},
keywords = {Model-driven engineering, Tools, Model notation, Flexible modeling, Testing},
abstract = {During the unit testing of model-driven tools, a large number of models and test classes needs to be managed and maintained. Typically, some of these artifacts are specified manually, some are generated automatically. Existing approaches to test management rely on the available visual and textual modeling notations. As these notations are not tailored to unit testing, distinct maintainability trade-offs arise. In this paper, we propose a notation that aims to combine the benefits of visual and text-based approaches. The notation is at the same time visual and text-based, as it uses ASCII characters to emulate the familiar graphical notations. In our evaluation based on real models, we identify problematic model shapes challenging the scalability our notation, while finding that it is well-suited to capture typical test models.}
}
@article{CRAWFORD198537,
title = {An analysis of static metrics and faults in C software},
journal = {Journal of Systems and Software},
volume = {5},
number = {1},
pages = {37-48},
year = {1985},
issn = {0164-1212},
doi = {https://doi.org/10.1016/0164-1212(85)90005-6},
url = {https://www.sciencedirect.com/science/article/pii/0164121285900056},
author = {S.G. Crawford and A.A. McIntosh and D. Pregibon},
abstract = {In this empirical study, we evaluate the extent to which a set of software measures are correlated with the number of faults and the total estimated repair effort for a large software system. The measures we use are basic counts reflecting program size and structure and metrics proposed by McCabe and Halstead. The effect of program size has a major influence on these metrics, and we present a suitable method of adjusting the metrics for size. In modeling faults or repair effort as a function of one variable, a number of measures individually explain approximately one-quarter of the variation observed in the fault data. No one measure does significantly better than size in explaining the variation in faults found across software units, and thus multiple variable models are necessary to find metrics of importance in addition to program size. The “best” multivariate model explains approximately one-half the variation in the fault data. The metrics included in this model (in addition to size) are: the ratio of block comments to total lines of code, the number of decisions per function, and the relative vocabulary of program variables and operators. These metrics have potential for future use in the quality control of software.}
}
@article{LAUCIELLO2016310,
title = {A high yield optimized method for the production of acylated ACPs enabling the analysis of enzymes involved in P. falciparum fatty acid biosynthesis},
journal = {Biochemistry and Biophysics Reports},
volume = {8},
pages = {310-317},
year = {2016},
issn = {2405-5808},
doi = {https://doi.org/10.1016/j.bbrep.2016.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S2405580816302114},
author = {Leonardo Lauciello and Gabriela Lack and Leonardo Scapozza and Remo Perozzo},
keywords = {Acyl carrier protein, Acylation, Fatty acid biosynthesis, , Enzyme kinetics, Natural substrates},
abstract = {The natural substrates of the enzymes involved in type-II fatty acid biosynthesis (FAS-II) are acylated acyl carrier proteins (acyl-ACPs). The state of the art method to produce acyl-ACPs involves the transfer of a phosphopantetheine moiety from CoA to apo-ACP by E. coli holo-ACP synthase (EcACPS), yielding holo-ACP which subsequently becomes thioesterified with free fatty acids by the E. coli acyl-ACP synthase (EcAAS). Alternatively, acyl-ACPs can be synthesized by direct transfer of acylated phosphopantetheine moieties from acyl-CoA to apo-ACP by means of EcACPS. The need for native substrates to characterize the FAS-II enzymes of P. falciparum prompted us to investigate the potential and limit of the two methods to efficiently acylate P. falciparum ACP (PfACP) with respect to chain length and β-modification and in preparative amounts. The EcAAS activity is found to be independent from the oxidation state at the β-position and accepts fatty acids as substrates with chain lengths starting from C8 to C20, whereas EcACPS accepts very efficiently acyl-CoAs with chain lengths up to C16, and with decreasing activity also longer chains (C18 to C20). Methods were developed to synthesize and purify preparative amounts of high quality natural substrates that are fully functional for the enzymes of the P. falciparum FAS-II system.}
}
@incollection{JILANI2019135,
title = {Chapter Three - Advances in Applications of Object Constraint Language for Software Engineering},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {112},
pages = {135-184},
year = {2019},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300554},
author = {Atif A. Jilani and Muhammad Z. Iqbal and Muhammad U. Khan and Muhammad Usman},
keywords = {Object Constraint Language, Model-driven engineering, Secondary study, Software engineering},
abstract = {Object Constraint Language (OCL) is a standard language defined by Object Management Group for specifying constraints on models. Since its introduction as part of Unified Modeling Language, OCL has received significant attention by researchers with works in the literature ranging from temporal extensions of OCL to automated test generation by solving OCL constraints. In this chapter, we provide a survey of the various works discussed in literature related to OCL with the aim of highlighting the advances made in the field. We classify the literature into five broad categories and provide summaries for various works in the literature. The chapter also provides insights and highlights the potentials areas of further research in the field.}
}
@incollection{2011183,
editor = {Eric Conrad},
booktitle = {Eleventh Hour CISSP},
publisher = {Syngress},
address = {Boston},
pages = {183-196},
year = {2011},
isbn = {978-1-59749-566-0},
doi = {https://doi.org/10.1016/B978-1-59749-566-0.00016-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781597495660000163}
}
@article{RAMLER2018248,
title = {Adapting automated test generation to GUI testing of industry applications},
journal = {Information and Software Technology},
volume = {93},
pages = {248-263},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916303676},
author = {Rudolf Ramler and Georg Buchgeher and Claus Klammer},
abstract = {Context
Automated test generation promises to improve the effectiveness of software testing and to reduce the involved manual effort. While automated test generation has been successfully applied for code-level API testing, it has not found widespread adoption in practice for testing of graphical user interfaces. Tools for test generation do not support GUI testing out-of-the-box but require dedicated extensions.
Objective
This paper explores the applicability of automated test generation for testing GUIs of industry applications. We propose a test adapter approach to bridge the gap between automated test generation tools and industry applications.
Method
A multiple case study was conducted in which automated test generation with test adapters has been applied at the unit, integration, and system test level in three industry projects from two different companies.
Results
Automated test generation via test adapters could be applied at all test levels. It has led to an increase of coverage as well as the detection of new defects that were not found by preceding testing activities in the projects. While test adapters can easily be implemented at the unit test level, their complexity and the corresponding effort for providing adapter implementations rises at higher test levels.
Conclusion
Test adapters can be used for applying automated test generation for testing GUIs of industry applications. They bridge the gap between automated test generation tools and industry applications. The development of test adapters requires no tool-specific knowledge and can be performed by members of the development team.}
}
@article{SANTIAGO20121340,
title = {Model-Driven Engineering as a new landscape for traceability management: A systematic literature review},
journal = {Information and Software Technology},
volume = {54},
number = {12},
pages = {1340-1356},
year = {2012},
note = {Special Section on Software Reliability and Security},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001346},
author = {Iván Santiago and Álvaro Jiménez and Juan Manuel Vara and Valeria {De Castro} and Verónica A. Bollati and Esperanza Marcos},
keywords = {Traceability, Model-Driven Engineering, Systematic literature review},
abstract = {Context
Model-Driven Engineering provides a new landscape for dealing with traceability in software development.
Objective
Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering.
Method
We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments.
Results
Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals.
Conclusion
The evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information.}
}
@incollection{ALUNNI2013645,
title = {Chapter 32 - Neurogenesis in Zebrafish},
editor = {John L.R. Rubenstein and Pasko Rakic},
booktitle = {Patterning and Cell Type Specification in the Developing CNS and PNS},
publisher = {Academic Press},
address = {Oxford},
pages = {645-677},
year = {2013},
isbn = {978-0-12-397265-1},
doi = {https://doi.org/10.1016/B978-0-12-397265-1.00069-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123972651000691},
author = {A. Alunni and M. Coolen and I. Foucher and L. Bally-Cuif}
}
@incollection{ALUNNI2020643,
title = {Chapter 26 - Neurogenesis in zebrafish},
editor = {John Rubenstein and Pasko Rakic and Bin Chen and Kenneth Y. Kwan},
booktitle = {Patterning and Cell Type Specification in the Developing CNS and PNS (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {643-697},
year = {2020},
isbn = {978-0-12-814405-3},
doi = {https://doi.org/10.1016/B978-0-12-814405-3.00026-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128144053000266},
author = {Alessandro Alunni and Marion Coolen and Isabelle Foucher and Laure Bally-Cuif},
keywords = {Development, Neural progenitor, Neural stem cell, Neurogenesis, Notch, Zebrafish},
abstract = {This chapter provides an overview of the cellular and molecular mechanisms underlying neurogenesis, from the earliest embryonic stage until adult, focusing on the major contributions brought to the field by the zebrafish model. The mechanisms and specificities of primary neurogenesis, which occurs from the recruitment of progenitors from the earliest proneural clusters by lateral inhibition, are first described. Next, during secondary neurogenesis, neuronal and glial cell diversities are generated, in particular with the establishment of neuromodulatory circuits. Finally, and in contrast to mammals, neurogenesis in zebrafish does not stall with nervous system maturation but is constitutively maintained to account for the continuous growth and regenerating properties of the adult brain. Recent studies identified the localization, identities, and properties of adult progenitor cells. Interestingly, the same proneural and neurogenic pathways seem to be reiterated at these late stages.}
}
@article{MARTINEZ201846,
title = {Feature location benchmark for extractive software product line adoption research using realistic and synthetic Eclipse variants},
journal = {Information and Software Technology},
volume = {104},
pages = {46-59},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301472},
author = {Jabier Martinez and Tewfik Ziadi and Mike Papadakis and Tegawendé F. Bissyandé and Jacques Klein and Yves le Traon},
keywords = {Feature location, Software families, Eclipse, Benchmark, Software product lines, Static analysis, Information retrieval},
abstract = {Context: It is common belief that high impact research in software reuse requires assessment in non-trivial, comparable, and reproducible settings. However, software artefacts and common representations are usually unavailable. Also, establishing a representative ground truth is a challenging and debatable subject. Feature location in the context of software families, which is key for software product line adoption, is a research field that is becoming more mature with a high proliferation of techniques. Objective: We present EFLBench, a benchmark and a framework to provide a common ground for the evaluation of feature location techniques in families of systems. Method: EFLBench leverages the efforts made by the Eclipse Community which provides feature-based family artefacts and their plugin-based implementations. Eclipse is an active and non-trivial project and thus, it establishes an unbiased ground truth which is realistic and challenging. Results: EFLBench is publicly available and supports all tasks for feature location techniques integration, benchmark construction and benchmark usage. We demonstrate its usage, simplicity and reproducibility by comparing four techniques in Eclipse releases. As an extension of our previously published work, we consider a decade of Eclipse releases and we also contribute an approach to automatically generate synthetic Eclipse variants to benchmark feature location techniques in tailored settings. We present and discuss three strategies for this automatic generation and we present the results using different settings. Conclusion: EFLBench is a contribution to foster the research in feature location in families of systems providing a common framework and a set of baseline techniques and results.}
}
@incollection{MEHTA2015479,
title = {Chapter 16 - Asset management systems},
editor = {B.R. Mehta and Y.J. Reddy},
booktitle = {Industrial Process Automation Systems},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {479-506},
year = {2015},
isbn = {978-0-12-800939-0},
doi = {https://doi.org/10.1016/B978-0-12-800939-0.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128009390000164},
author = {B.R. Mehta and Y.J. Reddy},
keywords = {Asset, smart instrument, fault models, EDDL, diagnostics, predictive, preventive, FDI},
abstract = {Asset management systems are a class of software and hardware applications used in the process plants for the efficient and optimum utilization of the equipment. An asset management system helps the plant to drive the operational and maintenance excellence. There are various systems developed for various applications; they together as a suite help to monitor, analyze, predict, and report the asset performance. This chapter deals with plant asset management systems, instrument asset management systems (IAMS), and some of the key rendering technologies and standards. The key drivers for the deployment of such systems and role-based access to the systems are discussed at length. Some of the communication technologies mentioned in this chapter are dealt separately as individual chapters. At the end of the topic, the key features from the asset management systems from an instrumentation perspective are dealt in much more length with little overview on enterprise asset management systems. Some of the rendering technologies such as DD/EDDL and FDT are discussed in more detail for the instrumentation engineers to get more familiar with these technologies and hence help them to choose the same selectively, considering the needs of the plants. The plant asset management systems and the modeling and underlying benefits of such systems are also discussed to create awareness on the possibilities of asset management in a plant context extending the limits from instrumentation and control to assets such as pumps, electrical machinery, and rotating equipment.}
}
@article{MAHMOOD2005693,
title = {A survey of component based system quality assurance and assessment},
journal = {Information and Software Technology},
volume = {47},
number = {10},
pages = {693-707},
year = {2005},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2005.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584905000601},
author = {Sajjad Mahmood and Richard Lai and Yong {Soo Kim} and Ji {Hong Kim} and Seok {Cheon Park} and Hae {Suk Oh}},
abstract = {Component Based Software Development (CBSD) is focused on assembling existing components to build a software system, with a potential benefit of delivering quality systems by using quality components. It departs from the conventional software development process in that it is integration centric as opposed to development centric. The quality of a component based system using high quality components does not therefore necessarily guarantee a system of high quality, but depends on the quality of its components, and a framework and integration process used. Hence, techniques and methods for quality assurance and assessment of a component based system would be different from those of the traditional software engineering methodology. It is essential to quantify factors that contribute to the overall quality, for instances, the trade off between cost and quality of a component, analytical techniques and formal methods, and quality attribute definitions and measurements. This paper presents a literature survey of component based system quality assurance and assessment; the areas surveyed include formalism, cost estimation, and assessment and measurement techniques for the following quality attributes: performance, reliability, maintainability and testability. The aim of this survey is to help provide a better understanding of CBSD in these aspects in order to facilitate the realisation of its potential benefits of delivering quality systems.}
}
@article{GEORG2015109,
title = {Synergy between Activity Theory and goal/scenario modeling for requirements elicitation, analysis, and evolution},
journal = {Information and Software Technology},
volume = {59},
pages = {109-135},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914002419},
author = {Geri Georg and Gunter Mussbacher and Daniel Amyot and Dorina Petriu and Lucy Troup and Saul Lozano-Fuentes and Robert France},
keywords = {Requirements engineering, Activity Theory, User Requirements Notation, Goal modeling, Scenario modeling},
abstract = {Context
It is challenging to develop comprehensive, consistent, analyzable requirements models for evolving requirements. This is particularly critical for certain highly interactive types of socio-technical systems that involve a wide range of stakeholders with disparate backgrounds; system success is often dependent on how well local social constraints are addressed in system design.
Objective
This paper describes feasibility research, combining a holistic social system perspective provided by Activity Theory (AT), a psychological paradigm, with existing system development methodologies and tools, specifically goal and scenario modeling.
Method
AT is used to understand the relationships between a system, its stakeholders, and the system’s evolving context. The User Requirements Notation (URN) is used to produce rigorous, analyzable specifications combining goal and scenario models. First, an AT language was developed constraining the framework for automation, second consistency heuristics were developed for constructing and analyzing combined AT/URN models, third a combined AT/URN methodology was developed, and consequently applied to a proof-of-concept system.
Results
An AT language with limited tool support was developed, as was a combined AT/URN methodology. This methodology was applied to an evolving disease management system to demonstrate the feasibility of adapting AT for use in system development with existing methodologies and tools. Bi-directional transformations between the languages allow proposed changes in system design to be propagated to AT models for use in stakeholder discussions regarding system evolution.
Conclusions
The AT framework can be constrained for use in requirements elicitation and combined with URN tools to provide system designs that include social system perspectives. The developed AT/URN methodology can help engineers to track the impact on system design due to requirement changes triggered by changes in the system’s social context. The methodology also allows engineers to assess the impact of proposed system design changes on the social elements of the system context.}
}
@article{BEHUTIYE2020106225,
title = {Management of quality requirements in agile and rapid software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {123},
pages = {106225},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106225},
url = {https://www.sciencedirect.com/science/article/pii/S095058491930240X},
author = {Woubshet Behutiye and Pertti Karhapää and Lidia López and Xavier Burgués and Silverio Martínez-Fernández and Anna Maria Vollmer and Pilar Rodríguez and Xavier Franch and Markku Oivo},
keywords = {Quality requirements, Non-functional requirements, Agile software development, Rapid software development, Systematic mapping study, Systematic literature reviews},
abstract = {Context
Quality requirements (QRs) describe the desired quality of software, and they play an important role in the success of software projects. In agile software development (ASD), QRs are often ill-defined and not well addressed due to the focus on quickly delivering functionality. Rapid software development (RSD) approaches (e.g., continuous delivery and continuous deployment), which shorten delivery times, are more prone to neglect QRs. Despite the significance of QRs in both ASD and RSD, there is limited synthesized knowledge on their management in those approaches.
Objective
This study aims to synthesize state-of-the-art knowledge about QR management in ASD and RSD, focusing on three aspects: bibliometric, strategies, and challenges.
Research method
Using a systematic mapping study with a snowballing search strategy, we identified and structured the literature on QR management in ASD and RSD.
Results
We found 156 primary studies: 106 are empirical studies, 16 are experience reports, and 34 are theoretical studies. Security and performance were the most commonly reported QR types. We identified various QR management strategies: 74 practices, 43 methods, 13 models, 12 frameworks, 11 advices, 10 tools, and 7 guidelines. Additionally, we identified 18 categories and 4 non-recurring challenges of managing QRs. The limited ability of ASD to handle QRs, time constraints due to short iteration cycles, limitations regarding the testing of QRs and neglect of QRs were the top categories of challenges.
Conclusion
Management of QRs is significant in ASD and is becoming important in RSD. This study identified research gaps, such as the need for more tools and guidelines, lightweight QR management strategies that fit short iteration cycles, investigations of the link between QRs challenges and technical debt, and extension of empirical validation of existing strategies to a wider context. It also synthesizes QR management strategies and challenges, which may be useful for practitioners.}
}
@article{KESSENTINI201949,
title = {Automated metamodel/model co-evolution: A search-based approach},
journal = {Information and Software Technology},
volume = {106},
pages = {49-67},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301915},
author = {Wael Kessentini and Houari Sahraoui and Manuel Wimmer},
keywords = {Metamodel/model co-evolution, Model migration, Coupled evolution, Search based software engineering},
abstract = {Context: Metamodels evolve over time to accommodate new features, improve existing designs, and fix errors identified in previous releases. One of the obstacles that may limit the adaptation of new metamodels by developers is the extensive manual changes that have been applied to migrate existing models. Recent studies addressed the problem of automating the metamodel/model co-evolution based on manually defined migration rules. The definition of these rules requires the list of changes at the metamodel level which are difficult to fully identify. Furthermore, different possible alternatives may be available to translate a metamodel change to a model change. Thus, it is hard to generalize these co-evolution rules. Objective: We propose an alternative automated approach for the metamodel/model co-evolution. The proposed approach refines an initial model instantiated from the previous metamodel version to make it as conformant as possible to the new metamodel version by finding the best compromise between three objectives, namely minimizing (i) the non-conformities with new metamodel version, (ii) the changes to existing models, and (iii) the textual and structural dissimilarities between the initial and revised models. Method: We formulated the metamodel/model co-evolution as a multi-objective optimization problem to handle the different conflicting objectives using the Non-dominated Sorting Genetic Algorithm II (NSGA-II) and the Multi-Objective Particle Swarm Optimization (MOPSO). Results: We evaluated our approach on several evolution scenarios extracted from different widely used metamodels. The results confirm the effectiveness of our approach with average manual correctness, precision and recall respectively higher than 91%, 88% and 89% on the different co-evolution scenarios. Conclusion: A comparison with our previous work confirms the out-performance of our multi-objective formulation.}
}
@article{OHARA201473,
title = {Glycan receptors of the Polyomaviridae: structure, function, and pathogenesis},
journal = {Current Opinion in Virology},
volume = {7},
pages = {73-78},
year = {2014},
note = {Virus-glycan interactions and pathogenesis / Viruses and RNA interference},
issn = {1879-6257},
doi = {https://doi.org/10.1016/j.coviro.2014.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1879625714001242},
author = {Samantha D O’Hara and Thilo Stehle and Robert Garcea},
abstract = {Multiple glycans have been identified as potential cell surface binding motifs for polyomaviruses (PyVs) using both crystallographic structural determinations and in vitro binding assays. However, binding alone does not necessarily imply that a glycan is a functional receptor, and confirmation that specific glycans are important for infection has proved challenging. In vivo analysis of murine polyomavirus (MPyV) infection has shown that subtle alterations in PyV–glycan interactions alone can result in dramatic changes in pathogenicity, implying that similar effects will be found for other PyVs. Our discussion will review the assays used for determining virus–glycan binding, and how these relate to known PyV tropism and pathogenesis.}
}
@article{DABHOLKAR2009756,
title = {The role of perceived control and gender in consumer reactions to download delays},
journal = {Journal of Business Research},
volume = {62},
number = {7},
pages = {756-760},
year = {2009},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2008.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0148296308001380},
author = {Pratibha A. Dabholkar and Xiaojing Sheng},
keywords = {Perceived control, Gender differences, Download delays, Attitudes, Intentions, Online marketing},
abstract = {An empirical study finds that perceived control strongly mediates the effects of perceived speed of a Web site download on consumers' attitudes and intentions to use the Web site. Moreover, results show that men are more likely to react positively to the perceived speed of a Web site download, whereas women are more likely to base their reactions on perceptions of control in the context of download delays. In contrast to past online research, the gender differences are intrinsic in two ways—they are context independent, and they are not caused by length of Internet experience, extent of Internet usage, or type of Internet connections.}
}
@article{IRSHAD2021110944,
title = {Adapting Behavior Driven Development (BDD) for large-scale software systems},
journal = {Journal of Systems and Software},
volume = {177},
pages = {110944},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110944},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000418},
author = {Mohsin Irshad and Ricardo Britto and Kai Petersen},
keywords = {Behavior-driven, Large-scale, BDD, Software processes, System of systems},
abstract = {Context:
Large-scale software projects require interaction between many stakeholders. Behavior-driven development (BDD) facilitates collaboration between stakeholders, and an adapted BDD process can help improve cooperation in a large-scale project.
Objective:
The objective of this study is to propose and empirically evaluate a BDD based process adapted for large-scale projects.
Method:
A technology transfer model was used to propose a BDD based process for large-scale projects. We conducted six workshop sessions to understand the challenges and benefits of BDD. Later, an industrial evaluation was performed for the process with the help of practitioners.
Results:
From our investigations, understanding of a business aspect of requirements, their improved quality, a guide to system-level use-cases, reuse of artifacts, and help for test organization are found as benefits of BDD. Practitioners identified the following challenges: specification and ownership of behaviors, adoption of new tools, the software projects’ scale, and versioning of behaviors. We proposed a process to address these challenges and evaluated the process with the help of practitioners.
Conclusion:
The evaluation proved that BDD could be adapted and used to facilitate interaction in large-scale software projects in the software industry. The feedback from the practitioners helped in improving the proposed process.}
}
@article{GONZALEZALONSO2012889,
title = {Towards a new open communication standard between homes and service robots, the DHCompliant case},
journal = {Robotics and Autonomous Systems},
volume = {60},
number = {6},
pages = {889-900},
year = {2012},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2012.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0921889012000188},
author = {Ignacio {González Alonso} and Omar {Álvarez Fres} and Alberto {Alonso Fernández} and Pablo Gómez {del Torno} and J.M. Maestre and M.d.P. {Almudena García Fuente}},
keywords = {Home&Building automation, Interoperability, Service robot, UPnP, SysML, DHCompliant},
abstract = {The interoperability of service robots and digital home was a user demand from the past years. In response to that necessity, the researchers from the Infobotica Research Group, in cooperation with a group of companies and universities, have proposed a new open standard and architecture. It is composed of different virtual services, protocols as well as an open adapters’ architecture, on top of the UPnP protocol stack. The proposed application protocols and the general architecture provide a communication environment for positioning devices, rules compliance checks, the collaboration between devices and managing energy efficiently. The different tools, adapters, and protocols, developed within the DHCompliant architecture, have defined a new level of application protocol that has allowed increased integration of those modules into home automation, improving their interoperability, and allowing the addition of new services to the same standard and commercial hardware.}
}
@incollection{AERTS2017287,
title = {Chapter 19 - Model-Based Testing of Cyber-Physical Systems},
editor = {Houbing Song and Danda B. Rawat and Sabina Jeschke and Christian Brecher},
booktitle = {Cyber-Physical Systems},
publisher = {Academic Press},
address = {Boston},
pages = {287-304},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-803801-7},
doi = {https://doi.org/10.1016/B978-0-12-803801-7.00019-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038017000195},
author = {A. Aerts and M. Reniers and M.R. Mousavi},
keywords = {Cyber-physical systems, V-model, Model-based testing, Conformance, Test-case generation, Test coverage},
abstract = {Cyber-physical systems (CPSs) are the result of the integration of connected computer systems with the physical world. They feature complex interactions that go beyond traditional communication schemes and protocols in computer systems. One distinguished feature of such complex interactions is the tight coupling between discrete and continuous interactions, captured by hybrid system models. Due to the complexity of CPSs, providing rigorous and model-based analysis methods and tools for verifying correctness of such systems is of the utmost importance. Model-based testing (MBT) is one such verification technique that can be used for checking the conformance of an implementation of a system to its specification (model). In this chapter, we first review the main concepts and techniques in MBT. Subsequently, we review the most common modeling formalisms for CPSs, with focus on hybrid system models. Subsequently, we provide a brief overview of conformance relations and conformance testing techniques for CPSs.}
}
@incollection{SLATTEN2013119,
title = {Chapter 4 - Model-Driven Engineering of Reliable Fault-Tolerant Systems—A State-of-the-Art Survey},
editor = {Atif Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {91},
pages = {119-205},
year = {2013},
booktitle = {Advances in Computers},
issn = {0065-2458},
doi = {https://doi.org/10.1016/B978-0-12-408089-8.00004-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080898000045},
author = {Vidar Slåtten and Peter Herrmann and Frank Alexander Kraemer},
keywords = {Model-driven engineering, Fault tolerance, Reliability, Verification, Survey},
abstract = {To improve the reliability of a system, we can add fault-tolerance mechanisms. This, however, leads to a rise of complexity that increases the probability of software faults being introduced. Hence, unless the process is handled carefully, adding fault tolerance may even lead to a less reliable system. As a way to deal with the inherently high level of complexity of fault-tolerant systems, some research groups have turned to the paradigm of model-driven engineering. This results in a research field that crosscuts the established fields of software engineering, system verification, fault-tolerant systems and distributed systems. Many works are presented in the context of one of these traditional fields, making it difficult to get a good overview of what is presently offered. We survey 10 approaches for model-driven engineering of reliable fault-tolerant systems and present 13 characteristics classifying the approaches in a manner useful for both users and developers of such approaches. We further discuss the state of the field and what the future may bring.}
}
@article{THUM201470,
title = {FeatureIDE: An extensible framework for feature-oriented software development},
journal = {Science of Computer Programming},
volume = {79},
pages = {70-85},
year = {2014},
note = {Experimental Software and Toolkits (EST 4): A special issue of the Workshop on Academic Software Development Tools and Techniques (WASDeTT-3 2010)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167642312001128},
author = {Thomas Thüm and Christian Kästner and Fabian Benduhn and Jens Meinicke and Gunter Saake and Thomas Leich},
keywords = {Feature-oriented software development, Software product lines, Feature modeling, Feature-oriented programming, Aspect-oriented programming, Delta-oriented programming, Preprocessors, Tool support},
abstract = {FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.}
}
@incollection{MORROW2003221,
title = {Chapter 8 - Case Studies},
editor = {Monique Morrow and Kateel Vijayananda},
booktitle = {Developing IP-Based Services},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {221-277},
year = {2003},
series = {The Morgan Kaufmann Series in Networking},
issn = {18759351},
doi = {https://doi.org/10.1016/B978-155860779-8/50010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781558607798500105},
author = {Monique Morrow and Kateel Vijayananda},
abstract = {Publisher Summary
This chapter outlines two conceptual case studies as well as a real world case study, which together demonstrate how Internet Protocol (IP)-based services can be developed and deployed by service providers to generate revenue. The case studies emphasize the role of operations support system (OSS), architecture in implementing IP-based services. The two conceptual case studies are composites of real experiences of various service providers. The combination of these two conceptual studies with a real-world example demonstrates the full impact of IP in both business and technical aspects. The conceptual case studies present scenarios for both Greenfield and incumbent service providers. The conceptual case studies and the Meta Telecom experience have drawn a picture of the business and technical aspects associated with IP-based service creation. There are common success factors in all three examples that include upper-management support and vision to use IP technology to transform a company, the redefinition of business–engineering processes for company-wide IP-based service development and deployment, an emphasis on OSS architecture as the base for service creation, and, finally, the positive interaction between customers, service providers, and vendors.}
}
@article{DASILVA2015527,
title = {Using a multi-method approach to understand Agile software product lines},
journal = {Information and Software Technology},
volume = {57},
pages = {527-542},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001438},
author = {Ivonei Freitas {da Silva} and Paulo Anselmo {da Mota Silveira Neto} and Pádraig O’Leary and Eduardo Santana {de Almeida} and Silvio Romero {de Lemos Meira}},
keywords = {Agile, Software product lines, Multi-method approach, Case study, Mapping study, Expert opinion},
abstract = {Context
Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method.
Objective
This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach.
Method
Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence.
Results
This combination results in 23 findings that provide evidence on how Agile and SPL could be combined.
Conclusion
Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies.}
}
@article{INUPAKUTIKA2020103420,
title = {Facilitating the development of cross-platform mHealth applications for chronic supportive care and a case study},
journal = {Journal of Biomedical Informatics},
volume = {105},
pages = {103420},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103420},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420300484},
author = {Devasena Inupakutika and Sahak Kaghyan and David Akopian and Patricia Chalela and Amelie G. Ramirez},
keywords = {Smartphones, Mobile devices, Mobile computing, Cross-platform, Mobile health, Applications},
abstract = {Mobile health (mHealth) apps have received increasing attention, due to their abilities to support patients who suffer from various conditions. mHealth apps may be especially helpful for patients with chronic diseases, by providing pertinent information, tracking symptoms, and inspiring adherence to medication regimens. To achieve these objectives, researchers need to prototype mHealth apps with dedicated software architectures. In this paper, a cloud-based mHealth application development concept is presented for chronic patient supportive care apps. The concept integrates existing software platforms and services for simplified app development that can be reused for other target applications. This developmental method also facilitates app portability, through the use of common components found across multiple mobile platforms, and scalability, through the loose coupling of services. The results are demonstrated by the development of native Android and cross-platform web apps, in a case study that presents an mHealth solution for endocrine hormone therapy (EHT). A performance analysis methodology, an app usability evaluation, based on focus group responses, and alpha and pre-beta testing results are provided.}
}
@incollection{2012549,
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP Study Guide (Second Edition)},
publisher = {Syngress},
edition = {Second Edition},
address = {Boston},
pages = {549-577},
year = {2012},
isbn = {978-1-59749-961-3},
doi = {https://doi.org/10.1016/B978-1-59749-961-3.09984-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781597499613099842}
}
@incollection{2015481,
title = {Index},
editor = {Carl S. Young},
booktitle = {The Science and Technology of Counterterrorism},
publisher = {Butterworth-Heinemann},
address = {Boston},
pages = {481-492},
year = {2015},
isbn = {978-0-12-420056-2},
doi = {https://doi.org/10.1016/B978-0-12-420056-2.09981-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124200562099814}
}
@article{JIANG2006584,
title = {Modeling real-time communication systems: Practices and experiences in Motorola},
journal = {Journal of Visual Languages & Computing},
volume = {17},
number = {6},
pages = {584-605},
year = {2006},
note = {Visual Modeling for Software Intensive Systems},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2006.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X06000607},
author = {Michael Jiang and Michael Groble and Andrij Neczwid and Allan Willey},
keywords = {UML modeling, SDL modeling, MDE code generation, Model validation, Real-time communication systems, TTCN, Structured methods},
abstract = {Visual modeling languages and techniques have been increasingly adopted for software specification, design, development, and testing. With the major improvements of UML 2.0 and tools support, visual modeling technologies have significant potential for simplifying design, facilitating collaborations, and reducing development cost. In this paper, we describe our practices and experiences of applying visual modeling techniques to the design and development of real-time wireless communication systems within Motorola. A model-driven engineering approach of integrating visual modeling with development and validation is described. Results, issues, and our viewpoints are also discussed.}
}
@article{ALMAKHOUR2020101227,
title = {Verification of smart contracts: A survey},
journal = {Pervasive and Mobile Computing},
volume = {67},
pages = {101227},
year = {2020},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2020.101227},
url = {https://www.sciencedirect.com/science/article/pii/S1574119220300821},
author = {Mouhamad Almakhour and Layth Sliman and Abed Ellatif Samhat and Abdelhamid Mellouk},
keywords = {Smart contracts, Blockchain, Verification, Correctness, Security assurance},
abstract = {To achieve trust and continuity in the smart contracts-based business process execution, the verification of such smart contracts is mandatory. A blockchain-based smart contract should work as intended before using it. Due to the immutable nature of blockchain, any bugs or errors will become permanent once published and could lead to huge economic losses. To avoid such problems, verification is required to check the correctness and the security of the smart contract. In this survey, we consider the smart contracts and we investigate smart contacts formal verification methods. We also investigate the security assurance for smart contracts using vulnerabilities detection methods. In this context, we provide a detailed overview of the different approaches to verify the smart contracts and we present the used methods and tools. We show a description of each method as well as its advantages and limitations and we draw several conclusions.}
}
@incollection{STURM2017211,
title = {Chapter 16 - The Case for Standards},
editor = {Rick Sturm and Carol Pollard and Julie Craig},
booktitle = {Application Performance Management (APM) in the Digital Enterprise},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {211-235},
year = {2017},
isbn = {978-0-12-804018-8},
doi = {https://doi.org/10.1016/B978-0-12-804018-8.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128040188000164},
author = {Rick Sturm and Carol Pollard and Julie Craig},
keywords = {Application description files (ADFs), Application response measurement (ARM), Cloud application management for platforms (CAMP), Cloud auditing data federation (CADF), Common information model (CIM), Component description files (CDFs), Desktop and mobile architecture for system hardware (DASH), Global description file (GDF), IEEE 1220, ISO/IEC 16350, ISO/IEC 17023:2011, ISO/IEC 17963:2013, Object identifier (OID), Organization for advancing open standards for the information society (OASIS), POSIX 1387.2, System application MIB (sysApplMIB), System management architecture for server management (SMASH), Tivoli application management specification (AMS), Web services management (WS-MAN)},
abstract = {In response to customer demands for increased efficiency and effectiveness in application manageability, portability, and interoperability, several organizations have stepped up to develop standards to guide deployment and management of software and hardware components. These organizations recognize the importance of creating standards to enable the development of standardized technologies to instrument applications. They include the Internet Engineering Task Force, Desktop Management Task Force, Institute of Electronic Engineers, Tivoli, the ASL-BiSL Foundation, and the International Organization of Standards. In this chapter, these various standards organizations are introduced and the standards developed by these influential organizations are described vis-à-vis the different aspects of the application management lifecycle. Finally, the pros and cons of using standards to facilitate the management of applications are presented. The primary utility of this chapter is that it shows how application management standards have evolved over the past 25-plus years and provides an overview of the various standards in one concise resource.}
}
@article{CATALA20131930,
title = {A meta-model for dataflow-based rules in smart environments: Evaluating user comprehension and performance},
journal = {Science of Computer Programming},
volume = {78},
number = {10},
pages = {1930-1950},
year = {2013},
note = {Special section on Language Descriptions Tools and Applications (LDTA’08 & ’09) & Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2012.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167642312001232},
author = {Alejandro Catala and Patricia Pons and Javier Jaen and Jose A. Mocholi and Elena Navarro},
keywords = {Ambient intelligence, Customization, Dataflow, Visual language, Rule, Event based, Non-expert programmer, Smart home},
abstract = {A considerable part of the behavior in smart environments relies on event-driven and rule specification. Rules are the mechanism most often used to enable user customization of the environment. However, the expressiveness of the rules available to users in editing and other tools is usually either limited or the available rule editing interfaces are not designed for end-users with low skills in programming. This means we have to look for interaction techniques and new ways to define user customization rules. This paper describes a generic and flexible meta-model to support expressive rules enhanced with data flow expressions that will graphically support the definition of rules without writing code. An empirical study was conducted on the ease of understanding of the visual data flow expressions, which are the key elements in our rule proposal. The visual dataflow language was compared to its corresponding textual version in terms of comprehension and ease of learning by teenagers in exercises involving calculations, modifications, writing and detecting equivalences in expressions in both languages. Although the subjects had some previous experience in editing mathematical expressions on spreadsheets, the study found their performance with visual dataflows to be significantly better in calculation and modification exercises. This makes our dataflow approach a promising mechanism for expressing user-customized reactive behavior in Ambient Intelligence (AmI) environments. The performance of the rule matching processor was validated by means of two stress tests to ensure that the meta-model approach adopted would be able to scale up with the number of types and instances in the space.}
}
@incollection{2011315,
title = {Appendix A - Extended Definitions for the IT Architectural Catalogs},
editor = {Charles T. Betz},
booktitle = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {315-408},
year = {2011},
isbn = {978-0-12-385017-1},
doi = {https://doi.org/10.1016/B978-0-12-385017-1.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780123850171000055}
}
@incollection{1999365,
title = {Subject Index},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {49},
pages = {365-373},
year = {1999},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)60290-9},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808602909}
}
@article{RODRIGUEZ2017263,
title = {Continuous deployment of software intensive products and services: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {123},
pages = {263-291},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215002812},
author = {Pilar Rodríguez and Alireza Haghighatkhah and Lucy Ellen Lwakatare and Susanna Teppola and Tanja Suomalainen and Juho Eskeli and Teemu Karvonen and Pasi Kuvaja and June M. Verner and Markku Oivo},
keywords = {Continuous deployment, Software development, Systematic mapping study},
abstract = {The software intensive industry is moving towards the adoption of a value-driven and adaptive real-time business paradigm. The traditional view of software as an item that evolves through releases every few months is being replaced by the continuous evolution of software functionality. This study aims to classify and analyse the literature related to continuous deployment in the software domain in order to scope the phenomenon, provide an overview of the state-of-the-art, investigate the scientific evidence in the reported results and identify areas suitable for further research. We conducted a systematic mapping study and classified the continuous deployment literature. The benefits and challenges related to continuous deployment were also analysed. RESULTS: The systematic mapping study includes 50 primary studies published between 2001 and 2014. An in-depth analysis of the primary studies revealed ten recurrent themes that characterize continuous deployment and provide researchers with directions for future work. In addition, a set of benefits and challenges of which practitioners may take advantage were identified. CONCLUSION: Overall, although the topic area is very promising, it is still in its infancy, thus offering a plethora of new opportunities for both researchers and software intensive companies.}
}
@incollection{BETZ2007227,
title = {Chapter 4 - A Supporting Systems Architecture},
editor = {Charles T. Betz},
booktitle = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {227-305},
year = {2007},
isbn = {978-0-12-370593-8},
doi = {https://doi.org/10.1016/B978-012370593-8/50031-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123705938500319},
author = {Charles T. Betz},
abstract = {Publisher Summary
As discussed at the outset of this chapter, companies often turn to the research organizations, the vendors, or both. However, the vendors are clearly self-interested, and the research organizations receive much of their funding from the vendors and are not articulating a comprehensive vision for how IT enablement tooling must interoperate. Hence, this chapter. Enablement tools and architectures are important and must be given their due. This chapter has several goals. First, it discusses the generic categories of internal IT enablement systems and potential interactions and overlaps. One issue in particular in this area is the proliferation of single-point systems. Providing a framework to support such efforts is a primary goal of this chapter. Finally, the use of a common logical process and data model can assist in the integration of the vendor packages available, and in particular the master data subjects be clearly established with defined systems of record so that the diverse systems can be aligned.}
}
@incollection{SITTIG20171,
title = {1 - Category Definitions},
editor = {Dean F. Sittig},
booktitle = {Clinical Informatics Literacy},
publisher = {Academic Press},
pages = {1-170},
year = {2017},
isbn = {978-0-12-803206-0},
doi = {https://doi.org/10.1016/B978-0-12-803206-0.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128032060000018},
author = {Dean F. Sittig}
}
@article{2002501,
title = {Subject index},
journal = {Journal of the American College of Cardiology},
volume = {39},
pages = {501-575},
year = {2002},
issn = {0735-1097},
doi = {https://doi.org/10.1016/S0735-1097(02)82076-9},
url = {https://www.sciencedirect.com/science/article/pii/S0735109702820769}
}
@article{MOSTERMAN2009376,
title = {Towards Computational Hybrid System Semantics for Time-Based Block Diagrams},
journal = {IFAC Proceedings Volumes},
volume = {42},
number = {17},
pages = {376-385},
year = {2009},
note = {3rd IFAC Conference on Analysis and Design of Hybrid Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20090916-3-ES-3003.00065},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015307916},
author = {Pieter J. Mosterman and Justyna Zander and Gregoire Hamon and Ben Denckla},
keywords = {Computational methods, Computer simulation, Computer-aided control system design, Embedded systems, Numerical simulation, Synchronous data flow, Systems design, Variable-structure systems, Verification, Zero crossings},
abstract = {At the core of Model-Based Design, computational models have caused an autocatalytic trend to use computation in design by unlocking the potential of model transformations. Precisely specifying a computational transformation requires well-defined semantics of the source and target representations. In this regard, continuous-time behavior is an essential aspect of time-based block diagrams that is typically approximated by numerical integration. The corresponding theory, however, is mostly concerned with local error and the mathematical semantics of long time behavior fails to be sufficiently precise from a computational perspective. In this work, first a computational semantics is developed based on a multi-stage variablestep solver. Next, the computational semantics of the discrete and continuous parts of hybrid systems and their interaction are formalized in a unifying framework. The framework exploits a successful functional approach to defining discrete-time and discrete-event behavior established in other work. Unification is then achieved by developing a computational representation of the continuous-time behavior as pure functions on streams.}
}
@article{1992947,
title = {Genomes and evolution},
journal = {Current Opinion in Genetics & Development},
volume = {2},
number = {6},
pages = {947-986},
year = {1992},
issn = {0959-437X},
doi = {https://doi.org/10.1016/S0959-437X(05)80122-4},
url = {https://www.sciencedirect.com/science/article/pii/S0959437X05801224}
}
@article{MARCILIO2020110671,
title = {SpongeBugs: Automatically generating fix suggestions in response to static code analysis warnings},
journal = {Journal of Systems and Software},
volume = {168},
pages = {110671},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110671},
url = {https://www.sciencedirect.com/science/article/pii/S016412122030128X},
author = {Diego Marcilio and Carlo A. Furia and Rodrigo Bonifácio and Gustavo Pinto},
keywords = {Static code analysis, Automatic fix suggestion},
abstract = {Static code analysis tools such as FindBugs and SonarQube are widely used on open-source and industrial projects to detect a variety of issues that may negatively affect the quality of software. Despite these tools’ popularity and high level of automation, several empirical studies report that developers normally fix only a small fraction (typically, less than 10% (Marcilio et al., 2019) of the reported issues—so-called “warnings”. If these analysis tools could also automatically provide suggestions on how to fix the issues that trigger some of the warnings, their feedback would become more actionable and more directly useful to developers. In this work, we investigate whether it is feasible to automatically generate fix suggestions for common warnings issued by static code analysis tools, and to what extent developers are willing to accept such suggestions into the codebases they are maintaining. To this end, we implemented SpongeBugs, a Java program transformation technique that fixes 11 distinct rules checked by two well-known static code analysis tools (SonarQube and SpotBugs). Fix suggestions are generated automatically based on templates, which are instantiated in a way that removes the source of the warnings; templates for some rules are even capable of producing multi-line patches. Based on the suggestions provided by SpongeBugs, we submitted 38 pull requests, including 946 fixes generated automatically by our technique for various open-source Java projects, including Eclipse UI – a core component of the Eclipse IDE – and both SonarQube and SpotBugs. Project maintainers accepted 87% of our fix suggestions (97% of them without any modifications). We further evaluated the applicability of our technique on software written by students and on a curated collection of bugs. All results indicate that our approach to generating fix suggestions is feasible, flexible, and can help increase the applicability of static code analysis tools.}
}
@incollection{ALI201723,
title = {Chapter Two - Uncertainty-Wise Testing of Cyber-Physical Systems},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {107},
pages = {23-94},
year = {2017},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300189},
author = {Shaukat Ali and Hong Lu and Shuai Wang and Tao Yue and Man Zhang},
keywords = {Uncertainty-wise testing, Cyber-Physical System, Belief Test Ready Model, Model evolution, Model-based testing},
abstract = {As compared with classical software/system testing, uncertainty-wise testing explicitly addresses known uncertainty about the behavior of a System Under Test (SUT), its operating environment, and interactions between the SUT and its operational environment, across all testing phases, including test design, test generation, test optimization, and test execution, with the aim to mainly achieve the following two goals. First, uncertainty-wise testing aims to ensure that the SUT deals with known uncertainty adequately. Second, uncertainty-wise testing should be also capable of learning new (previously unknown) uncertainties such that the SUT's implementation can be improved to guard against newly learned uncertainties during its operation. The necessity to integrate uncertainty in testing is becoming imperative because of the emergence of new types of intelligent and communicating software-based systems such as Cyber-Physical Systems (CPSs). Intrinsically, such systems are exposed to uncertainty because of their interactions with highly indeterminate physical environments. In this chapter, we provide our understanding and experience of uncertainty-wise testing from the aspects of uncertainty-wise model-based testing, uncertainty-wise modeling and evolution of test ready models, and uncertainty-wise multiobjective test optimization, in the context of testing CPSs under uncertainty. Furthermore, we present our vision about this new testing paradigm and its plausible future research directions.}
}
@article{CORNEBIZE2022111,
title = {Simulation-based optimization and sensibility analysis of MPI applications: Variability matters},
journal = {Journal of Parallel and Distributed Computing},
volume = {166},
pages = {111-125},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522000806},
author = {Tom Cornebize and Arnaud Legrand},
keywords = {Simulation, Validation, Sensibility analysis, SimGrid, HPL},
abstract = {Finely tuning MPI applications and understanding the influence of key parameters (number of processes, granularity, collective operation algorithms, virtual topology, and process placement) is critical to obtain good performance on supercomputers. With the high consumption of running applications at scale, doing so solely to optimize their performance is particularly costly. Having inexpensive but faithful predictions of expected performance could be a great help for researchers and system administrators. The methodology we propose decouples the complexity of the platform, which is captured through statistical models of the performance of its main components (MPI communications, BLAS operations), from the complexity of adaptive applications by emulating the application and skipping regular non-MPI parts of the code. We demonstrate the capability of our method with High-Performance Linpack (HPL), the benchmark used to rank supercomputers in the TOP500, which requires careful tuning. We briefly present (1) how the open-source version of HPL can be slightly modified to allow a fast emulation on a single commodity server at the scale of a supercomputer. Then we present (2) an extensive (in)validation study that compares simulation with real experiments and demonstrates our ability to predict the performance of HPL within a few percent consistently. This study allows us to identify the main modeling pitfalls (e.g., spatial and temporal node variability or network heterogeneity and irregular behavior) that need to be considered. Last, we show (3) how our “surrogate” allows studying several subtle HPL parameter optimization problems while accounting for uncertainty on the platform.}
}
@article{SANGIOVANNIVINCENTELLI2012217,
title = {Taming Dr. Frankenstein: Contract-Based Design for Cyber-Physical Systems*},
journal = {European Journal of Control},
volume = {18},
number = {3},
pages = {217-238},
year = {2012},
issn = {0947-3580},
doi = {https://doi.org/10.3166/ejc.18.217-238},
url = {https://www.sciencedirect.com/science/article/pii/S0947358012709433},
author = {Alberto Sangiovanni-Vincentelli and Werner Damm and Roberto Passerone},
keywords = {Contract, cyber-physical, design methodologies, platform-based, correctness},
abstract = {Cyber-physical systems combine a cyber side (computing and networking) with a physical side (mechanical, electrical, and chemical processes). In many cases, the cyber component controls the physical side using sensors and actuators that observe the physical system and actuate the controls. Such systems present the biggest challenges as well as the biggest opportunities in several large industries, including electronics, energy, automotive, defense and aerospace, telecommunications, instrumentation, industrial automation. Engineers today do successfully design cyber-physical systems in a variety of industries. Unfortunately, the development of systems is costly, and development schedules are difficult to stick to. The complexity of cyber-physical systems, and particularly the increased performance that is offered from interconnecting what in the past have been separate systems, increases the design and verification challenges. As the complexity of these systems increases, our inability to rigorously model the interactions between the physical and the cyber sides creates serious vulnerabilities. Systems become unsafe, with disastrous inexplicable failures that could not have been predicted. Distributed control of multi-scale complex systems is largely an unsolved problem. A common view that is emerging in research programs in Europe and the US is “enabling contract-based design (CBD),” which formulates a broad and aggressive scope to address urgent needs in the systems industry. We present a design methodology and a few examples in controller design whereby contract-based design can be merged with platform-based design to formulate the design process as a meet-in-the-middle approach, where design requirements are implemented in a subsequent refinement process using as much as possible elements from a library of available components. Contracts are formalizations of the conditions for correctness of element integration (horizontal contracts), for lower level of abstraction to be consistent with the higher ones, and for abstractions of available components to be faithful representations of the actual parts (vertical contracts).}
}
@article{2007A-832,
title = {Society for Surgery of the Alimentary Tract (SSAT) Abstracts},
journal = {Gastroenterology},
volume = {132},
number = {4, Supplement 2},
pages = {A-832-A-894},
year = {2007},
note = {Annual Abstract Supplement},
issn = {0016-5085},
doi = {https://doi.org/10.1016/S0016-5085(07)60011-0},
url = {https://www.sciencedirect.com/science/article/pii/S0016508507600110}
}
@article{2005S203,
title = {Posters displayed on Tuesday 10 May 2005},
journal = {Clinica Chimica Acta},
volume = {355},
pages = {S203-S318},
year = {2005},
note = {Focus on the Patient: 16th IFCC - FESCC European Congress of Clinical Biochemistry and Laboratory Medicine},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cccn.2005.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0009898105001221}
}
@article{2021I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {14},
number = {2},
pages = {I-CCVIII},
year = {2021},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(21)00040-1},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X21000401}
}
@article{TIBERMACINE2010815,
title = {A family of languages for architecture constraint specification},
journal = {Journal of Systems and Software},
volume = {83},
number = {5},
pages = {815-831},
year = {2010},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2009.11.736},
url = {https://www.sciencedirect.com/science/article/pii/S016412120900315X},
author = {Chouki Tibermacine and Régis Fleurquin and Salah Sadou},
keywords = {Architecture constraint, Constraint language, ADL, Software component, MOF, OCL, Constraint transformation},
abstract = {During software development, architecture decisions should be documented so that quality attributes guaranteed by these decisions and required in the software specification could be persisted. An important part of these architectural decisions is often formalized using constraint languages which differ from one stage to another in the development process. In this paper, we present a family of architectural constraint languages, called ACL. Each member of this family, called a profile, can be used to formalize architectural decisions at a given stage of the development process. An ACL profile is composed of a core constraint language, which is shared with the other profiles, and a MOF architecture metamodel. In addition to this family of languages, this paper introduces a transformation-based interpretation method of profiles and its associated tool.}
}
@incollection{HURTARTE200765,
title = {Chapter 7 - Intellectual Property},
editor = {Jeorge S. Hurtarte and Evert A. Wolsheimer and Lisa M. Tafoya},
booktitle = {Understanding Fabless IC Technology},
publisher = {Newnes},
address = {Burlington},
pages = {65-121},
year = {2007},
isbn = {978-0-7506-7944-2},
doi = {https://doi.org/10.1016/B978-075067944-2/50008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780750679442500084},
author = {Jeorge S. Hurtarte and Evert A. Wolsheimer and Lisa M. Tafoya},
abstract = {Publisher Summary
This chapter explains the semiconductor intellectual property (SIP) overview, business environment, sourcing products, evaluating business models, product enablers, licensing products, and the provider and buyer perspectives. SIP has existed since the advent of the semiconductor industry. SIP business practices include elements similar to those found in the traditional semiconductor or application-specific integrated circuit (ASIC), electronic design automation (EDA), and design services markets. Outsourcing is unlikely if the SIP product is seen by the potential buyer as a core competency or a key differentiator in its product, or if its use requires third-party access to the buyer's patents or trade secrets. SIP License Agreements may require a significant effort depending upon the business objectives of the parties, the intended use of the SIP Product, the nature of the SIP product, and the risks associated with using the SIP product in the end application. The scope of license, warranty, indemnity, and limitation of liability provisions usually consume the majority of the effort in negotiating an SIP License Agreement. However, IC developers are finding more low-cost solutions at their fingertips, and the semiconductor supply chain has become increasingly stratified.}
}
@incollection{1992247,
title = {4 - Reliability Prediction from Stress-Strength Models},
editor = {Krishna B. MISRA},
series = {Fundamental Studies in Engineering},
publisher = {Elsevier},
volume = {15},
pages = {247-316},
year = {1992},
booktitle = {Reliability Analysis and Prediction},
issn = {1572-4433},
doi = {https://doi.org/10.1016/B978-0-444-89606-3.50011-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444896063500117}
}
@article{SINGH20182129,
title = {A systematic literature review: Refactoring for disclosing code smells in object oriented software},
journal = {Ain Shams Engineering Journal},
volume = {9},
number = {4},
pages = {2129-2151},
year = {2018},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2017.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2090447917300412},
author = {Satwinder Singh and Sharanpreet Kaur},
keywords = {Code smells, Anti-patterns, Refactoring},
abstract = {Context
Reusing a design pattern is not always in the favor of developers. Thus, the code starts smelling. The presence of “Code Smells” leads to more difficulties for the developers. This racket of code smells is sometimes called Anti-Patterns.
Objective
The paper aimed at a systematic literature review of refactoring with respect to code smells. However the review of refactoring is done in general and the identification of code smells and anti-patterns is performed in depth.
Method
A systematic literature survey has been performed on 238 research items that includes articles from leading Conferences, Workshops and premier journals, theses of researchers and book chapters.
Results
Several data sets and tools for performing refactoring have been revealed under the specified research questions.
Conclusion
The work done in the paper is an addition to prior systematic literature surveys. With the study of paper the attentiveness of readers about code smells and anti-patterns will be enhanced.}
}
@article{MACCABE199112646,
title = {Delta-(L-alpha-aminoadipyl)-L-cysteinyl-D-valine synthetase from Aspergillus nidulans. Molecular characterization of the acvA gene encoding the first enzyme of the penicillin biosynthetic pathway},
journal = {Journal of Biological Chemistry},
volume = {266},
number = {19},
pages = {12646-12654},
year = {1991},
issn = {0021-9258},
doi = {https://doi.org/10.1016/S0021-9258(18)98948-9},
url = {https://www.sciencedirect.com/science/article/pii/S0021925818989489},
author = {A.P. MacCabe and H. {van Liempt} and H. Palissa and S.E. Unkles and M.B. Riach and E. Pfeifer and H. {von Döhren} and J.R. Kinghorn},
abstract = {The Aspergillus nidulans gene (acvA) encoding the first catalytic steps of penicillin biosynthesis that result in the formation of delta-(L-alpha-aminoadipyl)-L-cysteinyl-D-valine (ACV), has been positively identified by matching a 15-amino acid segment of sequence obtained from an internal CNBr fragment of the purified amino-terminally blocked protein with that predicted from the DNA sequence. acvA is transcribed in the opposite orientation to ipnA (encoding isopenicillin N synthetase), with an intergenic region of 872 nucleotides. The gene has been completely sequenced at the nucleotide level and found to encode a protein of 3,770 amino acids (molecular mass, 422,486 Da). Both fast protein liquid chromatography and native gel estimates of molecular mass are consistent with this predicted molecular weight. The enzyme was identified as a glycoprotein by means of affinity blotting with concanavalin A. No evidence for the presence of introns within the acvA gene has been found. The derived amino acid sequence of ACV synthetase (ACVS) contains three homologous regions of about 585 residues, each of which displays areas of similarity with (i) adenylate-forming enzymes such as parsley 4-coumarate-CoA ligase and firefly luciferase and (ii) several multienzyme peptide synthetases, including bacterial gramicidin S synthetase 1 and tyrocidine synthetase 1. Despite these similarities, conserved cysteine residues found in the latter synthetases and thought to be essential for the thiotemplate mechanism of peptide biosynthesis have not been detected in the ACVS sequence. These observations, together with the occurrence of putative 4'-phosphopantetheine-attachment sites and a putative thioesterase site, are discussed with reference to the reaction sequence leading to production of the ACV tripeptide. We speculate that each of the homologous regions corresponds to a functional domain that recognizes one of the three substrate amino acids.}
}
@incollection{BREAKFIELD200293,
title = {4 - Development Cycle},
editor = {Charles V. Breakfield and Roxanne E. Burkey},
booktitle = {Managing Systems Migrations and Upgrades},
publisher = {Digital Press},
address = {Woburn},
pages = {93-144},
year = {2002},
isbn = {978-1-55558-256-2},
doi = {https://doi.org/10.1016/B978-155558256-2/50005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781555582562500053},
author = {Charles V. Breakfield and Roxanne E. Burkey}
}
@article{KARAM2008855,
title = {A product-line architecture for web service-based visual composition of web applications},
journal = {Journal of Systems and Software},
volume = {81},
number = {6},
pages = {855-867},
year = {2008},
note = {Agile Product Line Engineering},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2007.10.031},
url = {https://www.sciencedirect.com/science/article/pii/S016412120700252X},
author = {Marcel Karam and Sergiu Dascalu and Haidar Safa and Rami Santina and Zeina Koteich},
keywords = {Product line engineering, Product line architecture, Agile methods, Web services, Visual languages},
abstract = {A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.}
}
@article{KHAN2019396,
title = {Landscaping systematic mapping studies in software engineering: A tertiary study},
journal = {Journal of Systems and Software},
volume = {149},
pages = {396-436},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302784},
author = {Muhammad Uzair Khan and Salman Sherin and Muhammad Zohaib Iqbal and Rubab Zahid},
keywords = {Tertiary study, Systematic mapping study, Secondary study, Survey, Software engineering},
abstract = {Context
A number of Systematic Mapping Studies (SMSs) that cover Software Engineering (SE) are reported in literature. Tertiary studies synthesize the secondary studies to provide a holistic view of an area.
Objectives
We synthesize SMSs in SE to provide insights into existing SE areas and to investigate the trends and quality of SMSs.
Methodology
We use Systematic Literature Review protocol to analyze and map the SMSs in SE, till August 2017, to SE Body of Knowledge (SWEBOK).
Results
We analyze 210 SMSs and results show that: (1) Software design and construction are most active areas in SE; (2) Some areas lack SMSs, including mathematical foundations, software configuration management, and SE tools; (3) The quality of SMSs is improving with time; (4) SMSs in journals have higher quality than SMSs in conferences and are cited more often; (5) Low quality in SMSs can be attributed to a lack of quality assessment in SMSs and not reporting information about the primary studies.
Conclusion
There is a potential for more SMSs in some SE areas. A number of SMSs do not provide the required information for an SMS, which leads to a low quality score.}
}
@article{2013S1,
title = {Abstracts from the XXI World Congress of the International Society for Heart ResearchJune 30-July 4, 20132013, San Diego, California, USA},
journal = {Journal of Molecular and Cellular Cardiology},
volume = {65},
pages = {S1-S162},
year = {2013},
note = {Abstracts from the XXI World Congress of the International Society for Heart Research, June 30-July 4, 2013, 2013, San Diego, California, USA},
issn = {0022-2828},
doi = {https://doi.org/10.1016/j.yjmcc.2013.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0022282813003106}
}
@article{HUUCK20153,
title = {Technology transfer: Formal analysis, engineering, and business value},
journal = {Science of Computer Programming},
volume = {103},
pages = {3-12},
year = {2015},
note = {Selected papers from the First International Workshop on Formal Techniques for Safety-Critical Systems (FTSCS 2012)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2014.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642314005358},
author = {Ralf Huuck},
keywords = {Static analysis, Model checking, SMT solving, Industrial application, Experience report},
abstract = {In this work we report on our experiences on developing and commercializing Goanna, a source code analyzer for detecting software bugs and security vulnerabilities in C/C++ code. Goanna is based on formal software analysis techniques such as model checking, static analysis and SMT solving. The commercial version of Goanna is currently deployed in a wide range of organizations around the world. Moreover, the underlying technology is licensed to an independent software vendor with tens of thousands of customers, making it possibly one of the largest deployments of automated formal methods technology. This paper explains some of the challenges as well as the positive results that we encountered in the technology transfer process. In particular, we provide some background on the design decisions and techniques to deal with large industrial code bases, we highlight engineering challenges and efforts that are typically outside of a more academic setting, and we address core aspects of the bigger picture for transferring formal techniques into commercial products, namely, the adoption of such technology and the value for purchasing organizations. While we provide a particular focus on Goanna and our experience with that underlying technology, we believe that many of those aspects hold true for the wider field of formal analysis and verification technology and its adoption in industry.}
}
@article{EASLEY1998175,
title = {Financial analysts and information-based trade},
journal = {Journal of Financial Markets},
volume = {1},
number = {2},
pages = {175-201},
year = {1998},
issn = {1386-4181},
doi = {https://doi.org/10.1016/S1386-4181(98)00002-0},
url = {https://www.sciencedirect.com/science/article/pii/S1386418198000020},
author = {David Easley and Maureen O'Hara and Joseph Paperman},
keywords = {Microstructure, Financial analysts, Trade},
abstract = {In this research, we investigate the informational role of financial analysts. Using a trade-based empirical technique, we estimate the probability of information-based trading for a sample of NYSE stocks that differ in analyst coverage. We determine how this probability differs across stocks followed by many analysts, and we investigate whether analysts increase or create the flow of information. We also determine the `normal' level of noise trading in each sample stock, thereby giving us the ability to assess the depth of the market for stocks with differing analysts followings. Our most important empirical result is that the number of financial analysts is not a good proxy for information-based trading.}
}
@article{MESBAH20082194,
title = {A component- and push-based architectural style for ajax applications},
journal = {Journal of Systems and Software},
volume = {81},
number = {12},
pages = {2194-2209},
year = {2008},
note = {Best papers from the 2007 Australian Software Engineering Conference (ASWEC 2007), Melbourne, Australia, April 10-13, 2007},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2008.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0164121208000769},
author = {Ali Mesbah and Arie {van Deursen}},
keywords = {, Web architectural style, Web engineering, Single page interface, Rich internet application},
abstract = {A new breed of web application, dubbed ajax, is emerging in response to a limited degree of interactivity in large-grain stateless Web interactions. At the heart of this new approach lies a single page interaction model that facilitates rich interactivity. Also push-based solutions from the distributed systems are being adopted on the web for ajax applications. The field is, however, characterized by the lack of a coherent and precisely described set of architectural concepts. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. We have studied and experimented with several ajax frameworks trying to understand their architectural properties. In this paper, we summarize four of these frameworks and examine their properties and introduce the spiar architectural style which captures the essence of ajax applications. We describe the guiding software engineering principles and the constraints chosen to induce the desired properties. The style emphasizes user interface component development, intermediary delta-communication between client/server components, and push-based event notification of state changes through the components, to improve a number of properties such as user interactivity, user-perceived latency, data coherence, and ease of development. In addition, we use the concepts and principles to discuss various open issues in ajax frameworks and application development.}
}
@article{AZIZI2020110675,
title = {SEET: Symbolic Execution of ETL Transformations},
journal = {Journal of Systems and Software},
volume = {168},
pages = {110675},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110675},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301308},
author = {Banafsheh Azizi and Bahman Zamani and Shekoufeh Kolahdouz-Rahimi},
keywords = {Model-Driven Engineering (MDE), Epsilon Transformation Language (ETL), Verification of model transformations, Metamodel footprint, Symbolic execution},
abstract = {Model transformations are known as the main pillar of model-driven approaches. A model transformation is a program, written in a transformation language, to convert a model into another model or code. Similar to any other program, model transformations need to be verified. The problem is that some transformation errors, e.g., logical errors, can only be detected via execution. Our focus in this research is on the Epsilon Transformation Language (ETL), one of the most extensively used model transformation languages. Lack of approaches to detecting logical errors in ETL transformations is a gap which needs to be addressed. In this paper, we present an approach to symbolic execution of ETL transformations and detecting logical errors. The approach uses a constraint solver to assess the satisfiability of a path condition and generates a symbolic metamodel footprint which can be used to detect errors. The approach is corroborated by a tool that is integrated with Eclipse. To evaluate the approach, the precision and recall are calculated for two well-known case studies. The scalability is evaluated via nine experiments. The usefulness and usability aspects are evaluated in a subjective manner. The results show the improvement in the field of verifying ETL transformations.}
}
@article{PEREIRA2021111044,
title = {Learning software configuration spaces: A systematic literature review},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111044},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111044},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001412},
author = {Juliana Alves Pereira and Mathieu Acher and Hugo Martin and Jean-Marc Jézéquel and Goetz Botterweck and Anthony Ventresque},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems},
abstract = {Most modern software systems (operating systems like Linux or Android, Web browsers like Firefox or Chrome, video encoders like ffmpeg, x264 or VLC, mobile and cloud applications, etc.) are highly configurable. Hundreds of configuration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, security, energy consumption, etc. Due to the combinatorial explosion and the cost of executing software, it is quickly impossible to exhaustively explore the whole configuration space. Hence, numerous works have investigated the idea of learning it from a small sample of configurations’ measurements. The pattern “sampling, measuring, learning” has emerged in the literature, with several practical interests for both software developers and end-users of configurable systems. In this systematic literature review, we report on the different application objectives (e.g., performance prediction, configuration optimization, constraint mining), use-cases, targeted software systems, and application domains. We review the various strategies employed to gather a representative and cost-effective sample. We describe automated software techniques used to measure functional and non-functional properties of configurations. We classify machine learning algorithms and how they relate to the pursued application. Finally, we also describe how researchers evaluate the quality of the learning process. The findings from this systematic review show that the potential application objective is important; there are a vast number of case studies reported in the literature related to particular domains or software systems. Yet, the huge variant space of configurable systems is still challenging and calls to further investigate the synergies between artificial intelligence and software engineering.}
}
@incollection{REED199189,
title = {AN ENGINEERING COST AND POLICY ANALYSIS OF INTRODUCING FIBER INTO THE RESIDENTIAL SUBSCRIBER LOOP},
editor = {MARTIN C.J. ELTON},
booktitle = {Integrated Broadband Networks},
publisher = {North-Holland},
address = {Amsterdam},
pages = {89-134},
year = {1991},
isbn = {978-0-444-89068-9},
doi = {https://doi.org/10.1016/B978-0-444-89068-9.50013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444890689500135},
author = {David P. Reed and Marvin A. Sirbu},
abstract = {Publisher Summary
This chapter presents an analysis of the principal engineering and economic issues that have emerged as telephone companies consider rewiring the nation's residences with fiber optics. Assuming significant future reductions in component costs, running fiber optic to the home is likely to remain more expensive than copper, where current loop plant costs are roughly $920 per subscriber. To realize the introduction of a fiber Integrated Broadband Networks (IBN), it must be justified on the basis of additional revenue producing services, such as the delivery of entertainment video. Fiber optic network capable of providing both voice and video services to the home can be constructed for $1800 to $2500 per home passed. Future developments in microelectronics, lasers, photodetectors, and powering architectures will greatly influence the economics of network evolution.}
}
@article{DEB2016265,
title = {Extracting finite state models from i* models},
journal = {Journal of Systems and Software},
volume = {121},
pages = {265-280},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216300048},
author = {Novarun Deb and Nabendu Chaki and Aditya Ghose},
keywords = {i model, Model transformation, Model checking},
abstract = {i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm.}
}
@article{ALBAGHAJATI2022111261,
title = {A co-evolutionary genetic algorithms approach to detect video game bugs},
journal = {Journal of Systems and Software},
volume = {188},
pages = {111261},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111261},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000292},
author = {Aghyad Albaghajati and Moataz Ahmed},
keywords = {Video game, Automated software testing, Genetic algorithms, Colored Petri nets, Reachability analysis},
abstract = {Video games systems are known for their complexity, concurrency and non-determinism, which makes them prone to challenging tacit bugs. Video games development is costly and the corresponding verification process is tiresome. Testing the nondeterministic and concurrent behaviors of video games systems is not only crucial but also challenging, especially when the game state space is huge. Accordingly, typical software testing approaches are neither suitable nor effective to find related bugs. Novel automated approaches to support video game testing are needed. This problem has caught researchers’ attention recently. Approaches found in the literature have tried to address two sub problems: modeling and uncovering bugs. Colored Petri nets is known to support modeling and verifying concurrent and nondeterministic systems. Search approaches have been used in the literature to check the availability of faulty states through exploring state spaces. However, these approaches tend to lack adaptability to test different video games systems due to the limitations of the defined fitness functions, in addition to difficulties in searching huge state spaces due to exhaustive and unguided search. The availability of automated approaches that guide and direct the process of bugs finding is mandatory. Thus, in this study we address this problem as we present a solution for automated software testing using collaborative work of two genetic algorithms (i.e. co-evolutionary) agents, where our approach is applied to colored Petri nets representations of the software workflow. The results of our experiments have shown the potential of the proposed approach in effectively finding bugs automatically.}
}
@article{GRAN201475,
title = {NorNet Core – A multi-homed research testbed},
journal = {Computer Networks},
volume = {61},
pages = {75-87},
year = {2014},
note = {Special issue on Future Internet Testbeds – Part I},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.bjp.2013.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S1389128613004489},
author = {Ernst Gunnar Gran and Thomas Dreibholz and Amund Kvalbein},
keywords = {NN C, Testbed, Multi-homing, Routing, Transport, Applications},
abstract = {Over the last decade, the Internet has grown at a tremendous speed in both size and complexity. Nowadays, a large number of important services – for instance e-commerce, healthcare and many others – depend on the availability of the underlying network. Clearly, service interruptions due to network problems may have a severe impact. On the long way towards the Future Internet, the complexity will grow even further. Therefore, new ideas and concepts must be evaluated thoroughly, and particularly in realistic, real-world Internet scenarios, before they can be deployed for production networks. For this purpose, various testbeds – for instance PlanetLab, GpENI or G-Lab – have been established and are intensively used for research. However, all of these testbeds lack the support for so-called multi-homing. Multi-homing denotes the connection of a site to multiple Internet service providers, in order to achieve redundancy. Clearly, with the need for network availability, there is a steadily growing demand for multi-homing. The idea of the NorNet Core project is to establish a Future Internet research testbed with multi-homed sites, in order to allow researchers to perform experiments with multi-homed systems. Particular use cases for this testbed include realistic experiments in the areas of multi-path routing, load balancing, multi-path transport protocols, overlay networks and network resilience. In this paper, we introduce the NorNet Core testbed as well as its architecture.}
}
@article{PANUNZIO2014770,
title = {An architectural approach with separation of concerns to address extra-functional requirements in the development of embedded real-time software systems},
journal = {Journal of Systems Architecture},
volume = {60},
number = {9},
pages = {770-781},
year = {2014},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2014.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1383762114000824},
author = {Marco Panunzio and Tullio Vardanega},
keywords = {Embedded real-time systems, Extra-functional properties, Software architecture, Component-based software engineering, Separation of concerns},
abstract = {A large proportion of the requirements on embedded real-time systems stems from the extra-functional dimensions of time and space determinism, dependability, safety and security, and it is addressed at the software level. The adoption of a sound software architecture provides crucial aid in conveniently apportioning the relevant development concerns. This paper takes a software-centered interpretation of the ISO 42010 notion of architecture, enhancing it with a component model that attributes separate concerns to distinct design views. The component boundary becomes the border between functional and extra-functional concerns. The latter are treated as decorations placed on the outside of components, satisfied by implementation artifacts separate from and composable with the implementation of the component internals. The approach was evaluated by industrial users from several domains, with remarkably positive results.}
}
@incollection{RODRIGUEZ2019135,
title = {Chapter Four - Advances in Using Agile and Lean Processes for Software Development},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {113},
pages = {135-224},
year = {2019},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0065245818300299},
author = {Pilar Rodríguez and Mika Mäntylä and Markku Oivo and Lucy Ellen Lwakatare and Pertti Seppänen and Pasi Kuvaja},
keywords = {Software processes, Agile software development, Lean software development, Lean thinking, Leagility, Rapid releases, Continuous delivery, Continuous deployment, DevOps, Lean startup, Metrics, Technical debt},
abstract = {Software development processes have evolved according to market needs. Fast changing conditions that characterize current software markets have favored methods advocating speed and flexibility. Agile and Lean software development are in the forefront of these methods. This chapter presents a unified view of Agile software development, Lean software development, and most recent advances toward rapid releases. First, we introduce the area and explain the reasons why the software development industry begun to move into this direction in the late 1990s. Section 2 characterizes the research trends on Agile software development. This section helps understand the relevance of Agile software development in the research literature. Section 3 provides a walk through the roots of Agile and Lean thinking, as they originally emerged in manufacturing. Section 4 develops into Agile and Lean for software development. Main characteristics and most popular methods and practices of Agile and Lean software development are developed in this section. Section 5 centers on rapid releases, continuous delivery, and continuous deployment, the latest advances in the area to get speed. The concepts of DevOps, as a means to take full (end-to-end) advantage of Agile and Lean, and Lean start-up, as an approach to foster innovation, are the focus of the two following 6 DevOps, 7 The Lean Startup Movement. Finally, Section 8 focuses on two important aspects of Agile and Lean software development: (1) metrics to guide decision making and (2) technical debt as a mechanism to gain business advantage. To wrap up the chapter, we peer into future directions in the area.}
}
@article{HUANG1984153,
title = {Some alternative tests of forward exchange rates as predictors of future spot rates},
journal = {Journal of International Money and Finance},
volume = {3},
number = {2},
pages = {153-167},
year = {1984},
issn = {0261-5606},
doi = {https://doi.org/10.1016/0261-5606(84)90003-2},
url = {https://www.sciencedirect.com/science/article/pii/0261560684900032},
author = {Roger D. Huang},
abstract = {The paper provides empirical analysis on the issue of forward premiums as predictors of future exchange depreciations. The need to specify an alternative to the null hypothesis, other than its complement is emphasized. Two such alternatives are considered: the random walk model and the possibility of excessive or insufficient exchange rate volatility to accord with the efficiency of exchange markets.}
}
@incollection{BARBIER2010365,
title = {Chapter 14 - MoDisco, a Model-Driven Platform to Support Real Legacy Modernization Use Cases},
editor = {William M. Ulrich and Philip H. Newcomb},
booktitle = {Information Systems Transformation},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {365-400},
year = {2010},
series = {The MK/OMG Press},
isbn = {978-0-12-374913-0},
doi = {https://doi.org/10.1016/B978-0-12-374913-0.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123749130000147},
author = {Gabriel Barbier and Hugo Bruneliere and Frédéric Jouault and Yves Lennon and Frédéric Madiot},
abstract = {Publisher Summary
The case study presented in this chapter concentrates on a two-phase discussion of model-driven modernization. To deal with the myriad of technological combinations found in modernization roadmaps, model-driven approaches, and tools offer the requisite abstraction level to build up mature and flexible modernization solutions. This chapter presents the initial collaboration between AtlanMod and Sodifrance, which led to a model-driven legacy modernization approach. The use of model-driven technologies on major migration projects has proven the benefit of this approach. Based on metamodeling standards, the tools used to extract the knowledge from existing applications, to transform the knowledge into new paradigms and architecture, and to regenerate the application according to specific technical platforms and patterns are more flexible and reusable. The new MoDisco platform brings all these benefits into an open extensible framework containing model-driven, reverse-engineering tools and components. The current process and tools used by Sodifrance on its modernization projects are elaborated in the discussion. An illustration drawn from a real migration project carried out for Amadeus Hospitality is also included. A new Eclipse initiative capitalizing on this experience to deliver a model-driven platform for the development of legacy modernization tools is presented.}
}
@incollection{2007433,
title = {Glossary},
editor = {Wayne Wolf},
booktitle = {High-Performance Embedded Computing},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {433-466},
year = {2007},
isbn = {978-0-12-369485-0},
doi = {https://doi.org/10.1016/B978-012369485-0/50009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123694850500099}
}
@article{201237,
title = {ELECTRONIC POSTER SESSION},
journal = {Journal of Pathology Informatics},
volume = {3},
number = {1},
pages = {37},
year = {2012},
issn = {2153-3539},
doi = {https://doi.org/10.1016/S2153-3539(22)00611-3},
url = {https://www.sciencedirect.com/science/article/pii/S2153353922006113}
}
@article{BINGOL201985,
title = {NURBS-Python: An open-source object-oriented NURBS modeling framework in Python},
journal = {SoftwareX},
volume = {9},
pages = {85-94},
year = {2019},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352711018301778},
author = {Onur Rauf Bingol and Adarsh Krishnamurthy},
keywords = {Curve and surface modeling, Non-uniform rational B-splines, Object-oriented programming, Python},
abstract = {We introduce NURBS-Python, an object-oriented, open-source, Pure Python NURBS evaluation library with no external dependencies. The library is capable of evaluating single or multiple NURBS curves and surfaces, provides a customizable visualization interface, and enables importing and exporting data using popular CAD file formats. The library and the implemented algorithms are designed to be portable and extensible via their abstract base interfaces. The design principles used in NURBS-Python allows users to access, use, and extend the library without any tedious software compilation steps or licensing concerns.}
}
@incollection{KHALIL2018145,
title = {Chapter Four - Optimizing the Symbolic Execution of Evolving Rhapsody Statecharts},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {108},
pages = {145-281},
year = {2018},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300487},
author = {Amal Khalil and Juergen Dingel},
keywords = {Model-driven engineering, Symbolic execution, Incremental verification, State-based behavioral models, State machines, Memoization, Dependency analysis, Model-based analysis},
abstract = {Model-driven engineering (MDE) is an iterative and incremental software development process. Supporting the analysis and the verification of software systems developed following the MDE paradigm requires to adopt incrementality when carrying out these crucial tasks in a more optimized way. Communicating state machines are one of the various formalisms used in MDE tools to model and describe the behavior of distributed, concurrent, and real-time reactive systems (e.g., automotive and avionics systems). Modeling the overall behavior of such systems is carried out in a modular way and on different levels of abstraction (i.e., it starts with modeling the behavior of the individual objects in the system first then modeling the interaction between these objects). Similarly, analyzing and verifying the correctness of the developed models to ensure their quality and their integrity is performed on two main levels. The intralevel is used to analyze the correctness of the individual models in isolation of the others, while the interlevel is used to analyze the overall interoperability of those that are communicating with each other. One way to facilitate the analysis of the overall behavior of a system of communicating state machines is to build the global state space (also known as the global reachability tree) of the system. This process is very expensive and in some cases it may suffer from the state explosion problem. Symbolic execution is a technique that can be used to construct an abstract and a bounded version of the system global state space that is known as a symbolic execution tree (SET), yet the size of the generated trees can be very large especially with big and complex systems that are composed of multiple objects. As the system evolves, one way to avoid regenerating the entire SET and repeating any SET-based analyses that have been already conducted is to utilize the previous SET and its analysis results in optimizing the process of generating the SET of the system after the change. In this chapter, we propose two optimization techniques to direct the successive runs of the symbolic execution technique toward the impacted parts of an evolving state machine model using memoization (MSE) and dependency analysis (DSE), respectively. The evaluation results of both techniques showed significant reduction in some cases compared with the standard symbolic execution technique.}
}
@article{HLADIK2021111033,
title = {Hippo: A formal-model execution engine to control and verify critical real-time systems},
journal = {Journal of Systems and Software},
volume = {181},
pages = {111033},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111033},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001308},
author = {Pierre-Emmanuel Hladik and Félix Ingrand and Silvano Dal Zilio and Reyyan Tekin},
keywords = {Verifiable implementation, Formal toolchain, Robotic case study},
abstract = {The design of embedded real-time systems requires specific toolchains to guarantee time constraints and safe behavior. These tools and their artifacts need to be managed in a coherent way all along the design process and need to address timing constraints and execution semantic in a holistic way during the system’s modeling, verification, and implementation phases. However, modeling languages used by these tools do not always share a common semantic. This can introduce a dangerous gap between what designers want to express, what is verified and the behavior of the final executable code. In order to address this problem, we propose a new toolchain, called Hippo, that integrates tools for design, verification and execution built around a common formalism. Our approach is based on an extension of the Fiacre specification language with runtime features, such as asynchronous function calls and synchronization with events. We formally define the behavior of these additions and describe a compiler to generate both an executable code and a verifiable model from the same high-level specification. The execution of the resulting code is supported by a dedicated execution engine that guarantees real-time behavior and that reduces the semantic gap between high-level models and executable code. We illustrate our approach with a non-trivial use case: the autonomous navigation of a Segway RMP440 robotic platform. We describe how we derive a Hippo model from an initial specification of the system based on the robotics programming framework . We also show how to use the Hippo runtime to control this robot, and how to use formal verification in order to check critical properties on this system.}
}
@article{MAO2022111085,
title = {User behavior pattern mining and reuse across similar Android apps},
journal = {Journal of Systems and Software},
volume = {183},
pages = {111085},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111085},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001825},
author = {Qun Mao and Weiwei Wang and Feng You and Ruilian Zhao and Zheng Li},
keywords = {Android apps, Behavior pattern reuse, Semantic-based event fuzzy matching, GUI model},
abstract = {Nowadays, Android apps have penetrated all aspects of our lives. Despite their popularity, understanding their behaviors is still a challenging task. Considering that many Android apps are in the same category and share similar workflows, in this paper, we propose a user behavior pattern mining and reuse approach across similar Android apps, thereby reducing the cost of understanding new apps. Particularly, for a specific new app, to figure out its typical behaviors, the behavior patterns that refer to the frequently-occurring workflows can be obtained from another similar app and transferred to this app. Moreover, to reuse the behavior patterns on this app, a semantic-based event fuzzy matching strategy and continuous workflow generation strategy are raised to generate workflows for this app. To evaluate our approach’s effectiveness and rationality, we conduct a series of experiments on 25 Android apps in five categories. Furthermore, the experimental results show that 88.3% of behavior patterns can be completely reused on similar apps, and the generated workflows cover 89.1% of the top 20% of important states.}
}
@article{BENNETT199174,
title = {Automated support of software maintenance},
journal = {Information and Software Technology},
volume = {33},
number = {1},
pages = {74-85},
year = {1991},
issn = {0950-5849},
doi = {https://doi.org/10.1016/0950-5849(91)90026-8},
url = {https://www.sciencedirect.com/science/article/pii/0950584991900268},
author = {KH Bennett},
keywords = {software maintenance, reengineering, reverse engineering, software process, tools},
abstract = {Software maintenance is the general name given to the set of activities undertaken on a software system following its release for operational use. Surveys have shown that for many projects, software maintenance consumes the majority of the overall software life-cycle costs, and there are indications that the proportion is increasing. Inability to cope with software maintenance can also result in a backlog of application modifications. Despite the importance of software maintenance, it has acquired the reputation of being a second-class area in which to work. The paper defines in more detail the term software maintenance, and then addresses the issues of maintaining existing code, and producing maintainable systems, stressing the role of reengineering. Three projects that focus on software maintenance are then summarized. All three aim to provide automated assistance to the software maintainer, but in contrasting ways. The ReForm project is based on a formal method to extract specifications from code using transformations. MACS and REDO are both transnational European projects funded by the Esprit collaborative programme of research; the former uses expert system technology to assist the maintainer, while REDO aims to provide a set of integratable tools within a single environment, to support the reverse engineering process.}
}
@article{BENAVIDESNAVARRO2011127,
title = {Detecting and Coordinating Complex Patterns of Distributed Events with KETAL},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {281},
pages = {127-141},
year = {2011},
note = {Proceedings of the 2011 Latin American Conference in Informatics (CLEI)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2011.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S1571066111001794},
author = {Luis Daniel {Benavides Navarro} and Andrés Barrera and Kiyoshige Garcés and Hugo Arboleda},
keywords = {Distributed event model, event patterns, causality, automata},
abstract = {This paper presents an event-based kernel library designed to explicitly construct and coordinate complex interactions and communication patterns in distributed applications. The library integrates facilities for explicitly defining complex event patterns, detecting events in distributed systems, and validating sequences of events having into account causal ordering. Concretely we present the following contributions: i) An analysis of non trivial scenarios found in distributed applications in order to formulate a set of requirements and restrictions for a kernel event-based library, ii) the design and implementation of the library supporting the detection and coordination of complex event patterns and the support of causal manipulation of distributed events, iii) a qualitative evaluation of our approach showing how this library can be used to build a sophisticated distributed aspect oriented language.}
}
@article{DANGLOT2019110398,
title = {A snowballing literature study on test amplification},
journal = {Journal of Systems and Software},
volume = {157},
pages = {110398},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.110398},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301736},
author = {Benjamin Danglot and Oscar Vera-Perez and Zhongxing Yu and Andy Zaidman and Martin Monperrus and Benoit Baudry},
keywords = {Test amplification, Test augmentation, Test optimization, Test regeneration, Automatic testing},
abstract = {The adoption of agile approaches has put an increased emphasis on testing, resulting in extensive test suites. These suites include a large number of tests, in which developers embed knowledge about meaningful input data and expected properties as oracles. This article surveys works that exploit this knowledge to enhance manually written tests with respect to an engineering goal (e.g., improve coverage or refine fault localization). While these works rely on various techniques and address various goals, we believe they form an emerging and coherent field of research, which we coin “test amplification”. We devised a first set of papers from DBLP, searching for all papers containing “test” and “amplification” in their title. We reviewed the 70 papers in this set and selected the 4 papers that fit the definition of test amplification. We use them as the seeds for our snowballing study, and systematically followed the citation graph. This study is the first that draws a comprehensive picture of the different engineering goals proposed in the literature for test amplification. We believe that this survey will help researchers and practitioners entering this new field to understand more quickly and more deeply the intuitions, concepts and techniques used for test amplification.}
}
@incollection{KALMAN2003301,
title = {Extranets},
editor = {Hossein Bidgoli},
booktitle = {Encyclopedia of Information Systems},
publisher = {Elsevier},
address = {New York},
pages = {301-312},
year = {2003},
isbn = {978-0-12-227240-0},
doi = {https://doi.org/10.1016/B0-12-227240-4/00069-1},
url = {https://www.sciencedirect.com/science/article/pii/B0122272404000691},
author = {Deborah Bayles Kalman}
}
@article{GAROUSI2015148,
title = {A survey of software engineering practices in Turkey},
journal = {Journal of Systems and Software},
volume = {108},
pages = {148-177},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215001314},
author = {Vahid Garousi and Ahmet Coşkunçay and Aysu Betin-Can and Onur Demirörs},
keywords = {Software engineering, Industry practices, Turkey},
abstract = {Understanding the types of software engineering (SE) practices and techniques used in industry is important. There is a wide spectrum in terms of the types and maturity of SE practices conducted in industry. Turkey has a vibrant software industry and it is important to characterize and understand the state of its SE practices. Our objective is to characterize and grasp a high-level view on type of SE practices in the Turkish software industry. To achieve this objective, we systematically designed an online survey with 46 questions based on our past experience in the Canadian and Turkish contexts and using the Software Engineering Body of Knowledge (SWEBOK). Two hundred and two practicing software engineers from the Turkish software industry participated in the survey. The survey results reveal important and interesting findings about SE practices in Turkey and beyond. They also help track the profession of SE, and suggest areas for improved training, education and research. Among the findings are the followings: (1) The military and defense software sectors are quite prominent in Turkey, especially in the capital Ankara region, and many SE practitioners work for those companies. (2) 54% of the participants reported not using any software size measurement methods, while 33% mentioned that they have measured lines of code (LOC). (3) In terms of effort, after the development phase (on average, 31% of overall project effort), software testing, requirements, design and maintenance phases come next and have similar average values (14%, 12%, 12% and 11% respectively). (4) Respondents experience the most challenge in the requirements phase. (5) Waterfall, as a rather old but still widely used lifecycle model, is the model that more than half of the respondents (53%) use. The next most preferred lifecycle models are incremental and Agile/lean development models with usage rates of 38% and 34%, respectively. (6) The Waterfall and Agile methodologies have slight negative correlations, denoting that if one is used in a company, the other will less likely to be used. The results of our survey will be of interest to SE professionals both in Turkey and world-wide. It will also benefit researchers in observing the latest trends in SE industry identifying the areas of strength and weakness, which would then hopefully encourage further industry–academia collaborations in those areas.}
}
@article{LIASKOS2012767,
title = {Behavioral adaptation of information systems through goal models},
journal = {Information Systems},
volume = {37},
number = {8},
pages = {767-783},
year = {2012},
note = {Special Issue: Advanced Information Systems Engineering (CAiSE'11)},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2012.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437912000737},
author = {Sotirios Liaskos and Shakil M. Khan and Marin Litoiu and Marina Daoud Jungblut and Vyacheslav Rogozhkin and John Mylopoulos},
keywords = {Information systems engineering, Goal modeling, Software customization, Adaptive systems},
abstract = {Customizing software to perfectly fit individual needs is becoming increasingly important in information systems engineering. Users want to be able to customize software behavior through reference to terms familiar to their diverse needs and experience. We present a requirements-driven approach to behavioral customization of software systems. Goal models are constructed to represent alternative behaviors that users can exhibit to achieve their goals. Customization information is then added to restrict the space of possibilities to those that fit specific users, contexts, or situations. Meanwhile, elements of the goal models are mapped to units of source code. This way, customization preferences posed at the requirements level are directly translated into system customizations. Our approach, which we apply to an on-line shopping cart system and an automated teller machine simulator, does not assume adoption of a particular development methodology, platform, or variability implementation technique and keeps the reasoning computation overhead from interfering with the execution of the configured application.}
}
@article{BARENGHI2015195,
title = {Parallel parsing made practical},
journal = {Science of Computer Programming},
volume = {112},
pages = {195-226},
year = {2015},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167642315002610},
author = {Alessandro Barenghi and Stefano {Crespi Reghizzi} and Dino Mandrioli and Federica Panella and Matteo Pradella},
keywords = {Parallel parsing algorithms, Syntax analysis, Parallel parser, Operator precedence grammar},
abstract = {The property of local parsability allows to parse inputs through inspecting only a bounded-length string around the current token. This in turn enables the construction of a scalable, data-parallel parsing algorithm, which is presented in this work. Such an algorithm is easily amenable to be automatically generated via a parser generator tool, which was realized, and is also presented in the following. Furthermore, to complete the framework of a parallel input analysis, a parallel scanner can also combined with the parser. To prove the practicality of a parallel lexing and parsing approach, we report the results of the adaptation of JSON and Lua to a form fit for parallel parsing (i.e. an operator-precedence grammar) through simple grammar changes and scanning transformations. The approach is validated with performance figures from both high performance and embedded multicore platforms, obtained analyzing real-world inputs as a test-bench. The results show that our approach matches or dominates the performances of production-grade LR parsers in sequential execution, and achieves significant speedups and good scaling on multi-core machines. The work is concluded by a broad and critical survey of the past work on parallel parsing and future directions on the integration with semantic analysis and incremental parsing.}
}
@incollection{MEILLOUR2003509,
title = {17 - Biochemistry and diversity of insect odorant-binding proteins},
editor = {Gary Blomquist and Richard Vogt},
booktitle = {Insect Pheromone Biochemistry and Molecular Biology},
publisher = {Academic Press},
address = {San Diego},
pages = {509-537},
year = {2003},
isbn = {978-0-12-107151-6},
doi = {https://doi.org/10.1016/B978-012107151-6/50019-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780121071516500190},
author = {Patricia Nagnan-Le Meillour and Emmanuelle Jacquin-Joly},
abstract = {Publisher Summary
Odorant-binding proteins (OBPs) are abundant in the sensillar lymph of insect antennae and participate in olfactory perireceptor events, such as transport of the hydrophobic odorant through the aqueous medium, presentation of the odor to olfactory receptors, and deactivation of the signal. This chapter describes the biochemistry and molecular biology of pheromone detection in the noctuid moth, Mamestra brassicae. The OBPs are diverse ranging from numerous species of several insect orders including Lepidoptera, Diptera, Coleoptera, and Hymenoptera, Hemiptera, and Phasmatodea. Classification defines OBP-Type 1 and OBP-Type 2 based on phylogeny, tissue localization, and structural features. The advantage of the functional approach to characterize the diversity of OBPs inside species is characterized. The construction of EST antennal libraries with a high EST number to be representative of the sequence diversity has identified not only new OBPs, but also SAPs, pheromone-degrading enzymes, sensory-neuron-membrane-protein-like sequences, and other elements involved in the pheromone transduction process.}
}
@article{PETERSON201868,
title = {Overview of the incompressible Navier–Stokes simulation capabilities in the MOOSE framework},
journal = {Advances in Engineering Software},
volume = {119},
pages = {68-92},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0965997817310591},
author = {John W. Peterson and Alexander D. Lindsay and Fande Kong},
abstract = {The Multiphysics Object Oriented Simulation Environment (MOOSE) framework is a high-performance, open source, C++ finite element toolkit developed at Idaho National Laboratory. MOOSE was created with the aim of assisting domain scientists and engineers in creating customizable, high-quality tools for multiphysics simulations. While the core MOOSE framework itself does not contain code for simulating any particular physical application, it is distributed with a number of physics “modules” which are tailored to solving e.g. heat conduction, phase field, and solid/fluid mechanics problems. In this report, we describe the basic equations, finite element formulations, software implementation, and regression/verification tests currently available in MOOSE’s navier_stokes module for solving the Incompressible Navier-Stokes (INS) equations.}
}
@incollection{AARNO2015161,
title = {Chapter 6 - Building virtual platforms},
editor = {Daniel Aarno and Jakob Engblom},
booktitle = {Software and System Development using Virtual Platforms},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {161-210},
year = {2015},
isbn = {978-0-12-800725-9},
doi = {https://doi.org/10.1016/B978-0-12-800725-9.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128007259000068},
author = {Daniel Aarno and Jakob Engblom},
keywords = {DML, device model, TLM, SystemC, modeling},
abstract = {Chapter 6 introduces the reader to how to best perform transaction-level modeling of individual devices and how such models are built in Simics. It covers the Device Modeling Language (DML), as well as device modeling in C, C++, Python, and SystemC. Chapter 6 provides detailed step-by-step instructions for how to create a simple device model in Simics.}
}
@article{OCHOA2018511,
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
journal = {Journal of Systems and Software},
volume = {144},
pages = {511-532},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.07.054},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301511},
author = {Lina Ochoa and Oscar González-Rojas and Alves Pereira Juliana and Harold Castro and Gunter Saake},
keywords = {Extended product line, Product configuration, Systematic literature review},
abstract = {Product line engineering has become essential in mass customisation given its ability to reduce production costs and time to market, and to improve product quality and customer satisfaction. In product line literature, mass customisation is known as product configuration. Currently, there are multiple heterogeneous contributions in the product line configuration domain. However, a secondary study that shows an overview of the progress, trends, and gaps faced by researchers in this domain is still missing. In this context, we provide a comprehensive systematic literature review to discover which approaches exist to support the configuration process of extended product lines and how these approaches perform in practice. Extend product lines consider non-functional properties in the product line modelling. We compare and classify a total of 66 primary studies from 2000 to 2016. Mainly, we give an in-depth view of techniques used by each work, how these techniques are evaluated and their main shortcomings. As main results, our review identified (i) the need to improve the quality of the evaluation of existing approaches, (ii) a lack of hybrid solutions to support multiple configuration constraints, and (iii) a need to improve scalability and performance conditions.}
}
@article{KIRRANE2021100659,
title = {Intelligent software web agents: A gap analysis},
journal = {Journal of Web Semantics},
volume = {71},
pages = {100659},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100659},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000342},
author = {Sabrina Kirrane},
keywords = {Intelligent software agents, Agent architectures, Intelligent software web agents, Web standards},
abstract = {Semantic web technologies have shown their effectiveness, especially when it comes to knowledge representation, reasoning, and data integration. However, the original semantic web vision, whereby machine readable web data could be automatically actioned upon by intelligent software web agents, has yet to be realised. In order to better understand the existing technological opportunities and challenges, in this paper we examine the status quo in terms of intelligent software web agents, guided by research with respect to requirements and architectural components, coming from the agents community. We use the identified requirements to both further elaborate on the semantic web agent motivating use case scenario, and to summarise different perspectives on the requirements from the semantic web agent literature. We subsequently propose a hybrid semantic web agent architecture, and use the various components and subcomponents in order to provide a focused discussion in relation to existing semantic web standards and community activities. Finally, we highlight open research opportunities and challenges and take a broader perspective of the research by discussing the potential for intelligent software web agents as an enabling technology for emerging domains, such as digital assistants, cloud computing, and the internet of things.}
}
@article{2020334,
title = {Abstracts},
journal = {Fuel and Energy Abstracts},
volume = {61},
number = {4},
pages = {334-433},
year = {2020},
issn = {0140-6701},
doi = {https://doi.org/10.1016/j.fueleneab.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140670120300187}
}
@article{MEYRAND2013411,
title = {Comparison of milk oligosaccharides between goats with and without the genetic ability to synthesize αs1-casein},
journal = {Small Ruminant Research},
volume = {113},
number = {2},
pages = {411-420},
year = {2013},
issn = {0921-4488},
doi = {https://doi.org/10.1016/j.smallrumres.2013.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0921448813000990},
author = {M. Meyrand and D.C. Dallas and H. Caillat and F. Bouvier and P. Martin and D. Barile},
keywords = {, Fucose, Genetic polymorphisms, Goat milk, Mass spectrometry, Oligosaccharides},
abstract = {Milk oligosaccharides (OS)—free complex carbohydrates—confer unique health benefits to the nursing neonate. Though human digestive enzymes cannot degrade these sugars, they provide nourishment to specific commensal microbes and act as decoys to prevent the adhesion of pathogenic micro-organisms to gastrointestinal cells. At present, the limited quantities of human milk oligosaccharides (HMO) impede research on these molecules and their potential applications in functional food formulations. Considerable progress has been made in the study of OS structures; however, the synthetic pathways leading to their synthesis in the mammary gland are poorly understood. Recent studies show that complex OS with fucose and N-acetyl neuraminic acid (key structural elements of HMO bioactivity) exist in goat milk. Polymorphisms in the CSN1S1 locus, which is responsible for synthesis of αs1-casein, affect lipid and casein micelle structure in goat milk. The present study sought to determine whether CSN1S1 polymorphisms also influence goat milk oligosaccharide (GMO) production and secretion. The GMO compositions of thirty-two goat milk samples, half of which were from genotype A/A (αs1-casein producers) and half from genotype O/O (αs1-casein non-producers), were determined with nanoflow liquid chromatography high-accuracy mass spectrometry. This study represents the most exhaustive characterization of GMO to date. A systematic and comprehensive GMO library was created, consolidating information available in the literature with the new findings. Nearly 30 GMO, 11 of which were novel, were confirmed via tandem mass spectrometric analyses. Six fucosylated OS were identified; 4 of these matched HMO compositions and three were identified for the first time in goat milk. Importantly, multivariate statistical analysis demonstrated that the OS profiles of the A/A and O/O genotype milks could be discriminated by the fucosylated OS. Quantitative analysis revealed that the goat milk samples contained 1.17g/L of OS; however, their concentration in milks from A/A and O/O genotypes was not different. This study provides evidence of a genetic influence on specific OS biosynthesis but not total OS production. The presence of fucosylated GMO suggests that goat milk represents a potential source of bioactive milk OS suitable as a functional food ingredient.}
}
@article{SCHERMANN201841,
title = {We’re doing it live: A multi-method empirical study on continuous experimentation},
journal = {Information and Software Technology},
volume = {99},
pages = {41-57},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302136},
author = {Gerald Schermann and Jürgen Cito and Philipp Leitner and Uwe Zdun and Harald C. Gall},
keywords = {Release engineering, Continuous deployment, Continuous experimentation, Empirical study},
abstract = {Context
Continuous experimentation guides development activities based on data collected on a subset of online users on a new experimental version of the software. It includes practices such as canary releases, gradual rollouts, dark launches, or A/B testing.
Objective
Unfortunately, our knowledge of continuous experimentation is currently primarily based on well-known and outspoken industrial leaders. To assess the actual state of practice in continuous experimentation, we conducted a mixed-method empirical study.
Method
In our empirical study consisting of four steps, we interviewed 31 developers or release engineers, and performed a survey that attracted 187 complete responses. We analyzed the resulting data using statistical analysis and open coding.
Results
Our results lead to several conclusions: (1) from a software architecture perspective, continuous experimentation is especially enabled by architectures that foster independently deployable services, such as microservices-based architectures; (2) from a developer perspective, experiments require extensive monitoring and analytics to discover runtime problems, consequently leading to developer on call policies and influencing the role and skill sets required by developers; and (3) from a process perspective, many organizations conduct experiments based on intuition rather than clear guidelines and robust statistics.
Conclusion
Our findings show that more principled and structured approaches for release decision making are needed, striving for highly automated, systematic, and data- and hypothesis-driven deployment and experimentation.}
}
@incollection{20051363,
title = {Index},
editor = {Robert K. Jackler and Derald E. Brackmann},
booktitle = {Neurotology (Second Edition)},
publisher = {Mosby},
edition = {Second Edition},
address = {Philadelphia},
pages = {1363-1411},
year = {2005},
isbn = {978-0-323-01830-2},
doi = {https://doi.org/10.1016/B978-0-323-01830-2.50092-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323018302500924}
}
@article{DIFRANCESCO201977,
title = {Architecting with microservices: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {150},
pages = {77-97},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300019},
author = {Paolo {Di Francesco} and Patricia Lago and Ivano Malavolta},
keywords = {Microservices, Software architecture, Systematic mapping study},
abstract = {Context
A microservice architecture is composed of a set of small services, each running in its own process and communicating with lightweight mechanisms. Many aspects on architecting with microservices are still unexplored and existing research is still far from being crispy clear.
Objective
We aim at identifying, classifying, and evaluating the state of the art on architecting with microservices from the following perspectives: publication trends, focus of research, and potential for industrial adoption.
Method
We apply the systematic mapping methodology. We rigorously selected 103 primary studies and we defined and applied a classification framework to them for extracting key information for subsequent analysis. We synthesized the obtained data and produced a clear overview of the state of the art.
Results
This work contributes with (i) a classification framework for research studies on architecting with microservices, (ii) a systematic map of current research of the field, (iii) an evaluation of the potential for industrial adoption of research results, and (iv) a discussion of emerging findings and implications for future research.
Conclusion
This study provides a solid, rigorous, and replicable picture of the state of the art on architecting with microservices. Its results can benefit both researchers and practitioners of the field.}
}
@incollection{2004665,
editor = {Joseph F Dyro},
booktitle = {Clinical Engineering Handbook},
publisher = {Academic Press},
address = {Burlington},
pages = {665-674},
year = {2004},
series = {Biomedical Engineering},
isbn = {978-0-12-226570-9},
doi = {https://doi.org/10.1016/B978-012226570-9/50159-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780122265709501599}
}
@incollection{2005501,
title = {INDEX},
editor = {Heinz Züllighoven},
booktitle = {Object-Oriented Construction Handbook},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {501-520},
year = {2005},
isbn = {978-1-55860-687-6},
doi = {https://doi.org/10.1016/B978-155860687-6/50014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781558606876500141}
}
@incollection{2002775,
title = {Index},
editor = {Tony Kenyon},
booktitle = {Data Networks},
publisher = {Digital Press},
address = {Burlington},
pages = {775-807},
year = {2002},
isbn = {978-1-55558-271-5},
doi = {https://doi.org/10.1016/B978-155558271-5/50038-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781555582715500389}
}
@article{FECKO2005841,
title = {XML-based requirements engineering for an electronic clearinghouse},
journal = {Information and Software Technology},
volume = {47},
number = {13},
pages = {841-858},
year = {2005},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2005.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584905000145},
author = {Mariusz A. Fecko and Christopher M. Lott},
keywords = {GUI, Message-processing, XML, Business rules, Requirements engineering},
abstract = {We present methods and tools to support XML-based requirements engineering for an electronic clearinghouse that connects trading partners in the telecommunications area. The original semi-structured requirements, locally known as business rules, were written as message specifications in a non-standardized and error-prone format using MS Word. To remedy the resulting software failures and faults, we first formalized the requirements by designing an W3C XML Schema for the precise definition of the requirements structure. The schema allows a highly structured representation of the essential information in eXtensible Markup Language (XML). Second, to offer the requirements engineers the ability to edit the XML documents in a friendly way while preserving their information structure, we developed a custom editor called XLEdit. Third, by developing a converter from MS Word to the target XML format, we helped the requirements engineers to migrate the existing business rules. Fourth, we developed translators from the structured requirements to schema languages, which enabled automated generation of message-validation code. The increase in customer satisfaction and clearinghouse-service efficiency are primary gains from the investment in the technology for structured requirements editing and validation.}
}
@article{IZADI2018191,
title = {Tolerance induction by surface immobilization of Jagged-1 for immunoprotection of pancreatic islets},
journal = {Biomaterials},
volume = {182},
pages = {191-201},
year = {2018},
issn = {0142-9612},
doi = {https://doi.org/10.1016/j.biomaterials.2018.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0142961218305714},
author = {Zhila Izadi and Ensiyeh Hajizadeh-Saffar and Jamshid Hadjati and Mahdi Habibi-Anbouhi and Mohammad Hossein Ghanian and Hamid Sadeghi-Abandansari and Mohammad Kazemi Ashtiani and Zakieh Samsonchi and Mohammad Raoufi and Maedeh Moazenchi and Mahmoud Izadi and Anava sadat Sadr Hashemi Nejad and Haideh Namdari and Yaser Tahamtani and Seyed Nasser Ostad and Hamid Akbari-Javar and Hossein Baharvand},
keywords = {Type 1 diabetes, Immunomodulation, Surface immobilization, Jagged-1, Islet PEGylation},
abstract = {Although transplantation of pancreatic islets is a promising approach for treatment of type 1 diabetes mellitus, the engraftment efficiency of these islets is limited by host immune responses. Extensive efforts have been made to immunoisolate these islets by introducing barriers on the islet surface. To date, these barriers have not successfully protected islets from attack by the immune system. In addition, the inevitable permeability of an islet capsule cannot prevent filtration by proinflammatory cytokines and islet self-antigens. Thus, we have developed a surface engineering approach for localized immonumodulation of the islet microenvironment. Jagged-1 (JAG-1), as a potent immunomodulatory factor, was immobilized on the islet surface by mediation of a double-layer of heterobifunctional poly (ethylene glycol) (PEG). Immobilization and functionality of JAG-1 on PEGylated islet surfaces were established. When co-cultured with splenocytes, the JAG-1 conjugated islets induced a significant increase in regulatory T cells and regulated the cytokine levels produced by immune cells. The results demonstrated that JAG-1 immobilization could improve immunoprotection of pancreatic islets by localized modulation of the immune milieu from an inflammatory to an anti-inflammatory state. We also evaluated the effects of surface modification of these islets by JAG-1 in a xenotransplantation model. The transplanted JAG-1/PEG/islets group showed a significantly reduced blood glucose levels compared with the control group of diabetic mice during the acute phase of the immune response to the transplanted islets. Our results demonstrated that surface modification has the potential to shift the immune system from an inflammatory to anti-inflammatory milieu and may offer a new prospective for immunoprotection of pancreatic islets.}
}
@incollection{2017171,
title = {Index},
editor = {Dean F. Sittig},
booktitle = {Clinical Informatics Literacy},
publisher = {Academic Press},
pages = {171-231},
year = {2017},
isbn = {978-0-12-803206-0},
doi = {https://doi.org/10.1016/B978-0-12-803206-0.18001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128032060180010}
}
@article{JAMRO20181,
title = {Agile and hierarchical round-trip engineering of IEC 61131-3 control software},
journal = {Computers in Industry},
volume = {96},
pages = {1-9},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517304165},
author = {Marcin Jamro and Dariusz Rzonca},
keywords = {Control software, IEC 61131-3, Implementation, Modeling, Round-trip engineering, Synchronization, Testing},
abstract = {The control software often performs complex, important, and responsible operations in industrial manufacturing systems. The size and complexity of such software are still increasing, thus it is important to provide engineers with methods and tools that simplify the development process. The situation can be improved by modeling using the Model-Driven Development (MDD) approach, standardized implementation process, as well as various testing methods. In the paper, the authors propose the further step, which consists of the comprehensive agile hierarchical round-trip engineering approach dedicated to the IEC 61131-3 control software. It divides the project into four connected parts – model, configuration, implementation, and tests. Such a solution allows developers to work independently and iteratively on various project parts, because changes are discovered automatically and are propagated to suitable views inside the project. The synchronization mechanism has been introduced into the CPDev engineering environment for programming industrial controllers.}
}
@article{NISHIDA2014992,
title = {Japan׳s prefectural digital divide: A multivariate and spatial analysis},
journal = {Telecommunications Policy},
volume = {38},
number = {11},
pages = {992-1010},
year = {2014},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2014.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0308596114000937},
author = {Tetsushi Nishida and James B. Pick and Avijit Sarkar},
keywords = {Japan, Digital divide, Information and communication technologies, ICT use factors, Theoretical model, Regression, Spatial autocorrelation, Cluster analysis, ICT policy implications},
abstract = {This study of the digital divide within Japan utilizes data from the country׳s 47 prefectures for multivariate and spatial analysis of distributions of information and communication technology (ICT) variables. The paper constructs an exploratory conceptual model of technology utilization and expenditures in Japan, induced from prior literature. Ten dependent ICT utilization and expenditure factors are posited to be related to 12 independent demographic, economic, infrastructure, education, innovation and openness factors. The relationship of the independent to dependent factors is moderated by analysis of spatial patterns of technology utilization to examine proximities and reduce spatial bias. Based on the model, a multivariate analysis identifies correlates of the nation׳s digital divide, including patents registered by Japanese citizens, newspaper circulation, students and pupils per capita, household expenditures on education, rural/urban status, and Japan׳s aged population structure which has wide generational gaps. Spatial clusters and outliers of ICTs in prefectures are analyzed, with attention to their policy impacts. Findings suggest modifications to the conceptual model. Implications of findings for the country׳s official national technology planning policies are considered and recommendations made to expand them.}
}
@article{VOGELHEUSER201735,
title = {Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies},
journal = {Journal of Systems and Software},
volume = {131},
pages = {35-62},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.05.051},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300985},
author = {Birgit Vogel-Heuser and Juliane Fischer and Stefan Feldmann and Sebastian Ulewicz and Susanne Rösch},
keywords = {Factory automation, Automated production systems, Maturity, Modularity, Control software, Programmable logic controller},
abstract = {Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies’ solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry.}
}
@article{PINHEIRO2020102418,
title = {Mutating code annotations: An empirical evaluation on Java and C# programs},
journal = {Science of Computer Programming},
volume = {191},
pages = {102418},
year = {2020},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2020.102418},
url = {https://www.sciencedirect.com/science/article/pii/S0167642320300290},
author = {Pedro Pinheiro and José Carlos Viana and Márcio Ribeiro and Leo Fernandes and Fabiano Ferrari and Rohit Gheyi and Baldoino Fonseca},
keywords = {Mutation testing, Code annotations, Mining bugs},
abstract = {Mutation testing injects code changes to check whether tests can detect them. Mutation testing tools use mutation operators that modify program elements such as operators, names, and entire statements. Most existing mutation operators focus on imperative and object-oriented language constructs. However, many current projects use meta-programming through code annotations. In a previous work, we have proposed nine mutation operators for code annotations focused on the Java programming language. In this article, we extend our previous work by mapping the operators to the C# language. Moreover, we enlarge the empirical evaluation. In particular, we mine Java and C# projects that make heavy use of annotations to identify annotation-related faults. We analyzed 200 faults and categorized them as “misuse,” when the developer did not appear to know how to use the code annotations properly, and “wrong annotation parsing” when the developer incorrectly parsed annotation code (by using reflection, for example). Our operators mimic 95% of the 200 mined faults. In particular, three operators can mimic 82% of the faults in Java projects and 84% of the faults in C# projects. In addition, we provide an extended and improved repository hosted on GitHub with the 200 code annotation faults we analyzed. We organize the repository according to the type of errors made by the programmers while dealing with code annotations, and to the mutation operator that can mimic the faults. Last but not least, we also provide a mutation engine, based on these operators, which is publicly available and can be incorporated into existing or new mutation tools. The engine works for Java and C#. As implications for practice, our operators can help developers to improve test suites and parsers of annotated code.}
}
@incollection{VALLI1993101,
title = {CHAPTER 2 - The Hematopoietic System},
editor = {K.V.F. JUBB and PETER C. KENNEDY and NIGEL PALMER},
booktitle = {Pathology of Domestic Animals (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
address = {San Diego},
pages = {101-265},
year = {1993},
isbn = {978-0-12-391607-5},
doi = {https://doi.org/10.1016/B978-0-12-391607-5.50010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123916075500108},
author = {V.E.O. Valli and B.W. Parry}
}
@incollection{19991,
title = {Subject Index},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {50},
pages = {1-420},
year = {1999},
booktitle = {Index Part I Subject Index Volumes 1-49},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)60717-2},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808607172}
}
@article{TSAI2016108,
title = {Integrated fault detection and test algebra for combinatorial testing in TaaS (Testing-as-a-Service)},
journal = {Simulation Modelling Practice and Theory},
volume = {68},
pages = {108-124},
year = {2016},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2016.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X16302210},
author = {Wei-Tek Tsai and Guanqiu Qi},
keywords = {Combinatorial testing, TaaS, Concurrent testing, Test algebra, Adaptive reasoning},
abstract = {Testing-as-a-Service (TaaS) is a software testing service in a cloud that can leverage the computation power provided by the cloud. Specifically, a TaaS can be scaled to large and dynamic workloads, executed in a distributed environment with hundreds of thousands of processors, and these processors may support concurrent and distributed test execution and analysis. This paper proposes a TaaS system based on Adaptive Reasoning (AR) and Test Algebra (TA) for Combinatorial Testing (CT). AR performs testing and identifies faulty interactions, and TA eliminates related configurations from testing and there can be carried out concurrently. By combining these two, it is possible to perform large CT that were not possible before. Specifically, we performed experiments with 250 components with 2.83*1087 6-way interactions with about 21.1×1015 configurations, and this may be the largest CT experimentation as 2014. 98.6% of configurations have been eliminated out of total number of configurations.}
}
@article{FABRY2016528,
title = {AspectJ code analysis and verification with GASR},
journal = {Journal of Systems and Software},
volume = {117},
pages = {528-544},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216300279},
author = {Johan Fabry and Coen {De Roover} and Carlos Noguera and Steffen Zschaler and Awais Rashid and Viviane Jonckers},
keywords = {Aspect oriented programming, Source code analysis, Logic program querying},
abstract = {Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked.}
}
@incollection{LOBO2022838,
title = {40 - Infertility: Etiology, Diagnostic Evaluation, Management, Prognosis},
editor = {David M. Gershenson and Gretchen M. Lentz and Fidel A. Valea and Roger A. Lobo},
booktitle = {Comprehensive Gynecology (Eighth Edition)},
publisher = {Elsevier},
edition = {Eighth Edition},
address = {St. Louis (MO)},
pages = {838-860.e5},
year = {2022},
isbn = {978-0-323-65399-2},
doi = {https://doi.org/10.1016/B978-0-323-65399-2.00049-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323653992000498},
author = {Roger A. Lobo}
}
@article{2016S4,
title = {Poster Abstracts},
journal = {Neuromuscular Disorders},
volume = {26},
pages = {S4-S42},
year = {2016},
note = {Abstracts of the UK Neuromuscular Translational Research Conference 2016},
issn = {0960-8966},
doi = {https://doi.org/10.1016/S0960-8966(16)30108-0},
url = {https://www.sciencedirect.com/science/article/pii/S0960896616301080}
}
@article{HUEBER2016274,
title = {Statistical conversion of silent articulation into audible speech using full-covariance HMM},
journal = {Computer Speech & Language},
volume = {36},
pages = {274-293},
year = {2016},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2015.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0885230815000340},
author = {Thomas Hueber and Gérard Bailly},
keywords = {Silent speech interface, GMM, HMM, Ultrasound, Articulatory–acoustic mapping},
abstract = {This article investigates the use of statistical mapping techniques for the conversion of articulatory movements into audible speech with no restriction on the vocabulary, in the context of a silent speech interface driven by ultrasound and video imaging. As a baseline, we first evaluated the GMM-based mapping considering dynamic features, proposed by Toda et al. (2007) for voice conversion. Then, we proposed a ‘phonetically-informed’ version of this technique, based on full-covariance HMM. This approach aims (1) at modeling explicitly the articulatory timing for each phonetic class, and (2) at exploiting linguistic knowledge to regularize the problem of silent speech conversion. Both techniques were compared on continuous speech, for two French speakers (one male, one female). For modal speech, the HMM-based technique showed a lower spectral distortion (objective evaluation). However, perceptual tests (transcription and XAB discrimination tests) showed a better intelligibility of the GMM-based technique, probably related to its less fluctuant quality. For silent speech, a perceptual identification test revealed a better segmental intelligibility for the HMM-based technique on consonants.}
}
@incollection{HALSTEAD1979119,
title = {Advances in Software Science},
editor = {Marshall C. Yovits},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {18},
pages = {119-172},
year = {1979},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)60583-5},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808605835},
author = {M.H. Halstead},
abstract = {Publisher Summary
The chapter presents the overview of the present status of software science. Software science is an intellectually exciting discipline currently undergoing rapid development. Software science can be treated as a proper basis or foundation for the field of software engineering, but not as synonymous with it. This is not unlike other branches, in that the engineering usually preceded and indeed stimulated the development of the underlying science. It is interesting to note, however, that it is only after the development of thermodynamics for power engineering, electrodynamics for electrical engineering, or statics, dynamics, and strength-of-materials for mechanical engineering that those branches could be considered quasi complete, highly competent, and dependable engineering disciplines. Such a goal for software engineering clearly motivates much of the work in software science. Despite the fact that there are no theorems, and perhaps never can be any, in the field of software science, one basic attribute shared by the equations in this field has become quite noticeable. This is the total and complete lack of arbitrary constants or unknown coefficients among the basic equations. It cannot yet be predicted what impact this discipline may be expected to have on the field of software engineering, or of computer programming in general in the future, but perhaps its greatest impact may result from one conclusion that seems inescapable. This conclusion is that natural laws govern language and the mental activity of using it far more strictly than previously recognized.}
}
@incollection{LACKNER2017157,
title = {Chapter Four - Advances in Testing Software Product Lines},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {107},
pages = {157-217},
year = {2017},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300311},
author = {Hartmut Lackner and Bernd-Holger Schlingloff},
keywords = {Software product lines, Cyber physical systems, Model-based testing, Test generation, Variant management, Feature modeling, Domain analysis, Fault injection, Product sampling, Test case assignment},
abstract = {In this chapter, we describe some recent techniques and results in model-based testing of software product lines. Presently, more and more software-based products and services are available in many different variants to choose from. However, this brings about challenges for the software quality assurance processes. Since only few of all possible variants can be tested at the developer's site, several questions arise. How shall the variability be described in order to make sure that all features are being tested? Is it better to test selected variants on a concrete level, or shall the whole software product line be tested abstractly? What is the quality of a test suite for a product line, anyway? If it is impossible to test all possible variants, which products should be selected for testing? Given a certain product, which test cases are appropriate for it, and given a test case, which products can be tested with it? We address these questions from an empirical software engineering point of view. We sketch modeling formalisms for software product lines. Then, we compare domain-centered and application-centered approaches to software product line testing. We define mutation operators for assessing software product line test suites. Subsequently, we analyze methods for selecting product variants on the basis of a given test suite. Finally, we show how model checking can be used to determine whether a certain test case is applicable for a certain product variant. For all our methods we describe supporting tools and algorithms. Currently, we are integrating these in an integrated tool suite supporting several aspects of model-based testing for software product lines.}
}
@article{JIA2016206,
title = {5W+1H pattern: A perspective of systematic mapping studies and a case study on cloud software testing},
journal = {Journal of Systems and Software},
volume = {116},
pages = {206-219},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.01.058},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215000370},
author = {Changjiang Jia and Yan Cai and Yuen Tak Yu and T.H. Tse},
keywords = {5W+1H pattern, Cloud software testing, Systematic mapping study},
abstract = {A common type of study used by researchers to map out the landscape of a research topic is known as mapping study. Such a study typically begins with an exploratory search on the possible ideas of the research topic, which is often done in an unsystematic manner. Hence, the activity of formulating research questions in mapping studies is ill-defined, rendering it difficult for researchers who are new to the topic. There is a need to guide them kicking off a mapping study of an unfamiliar domain. This paper proposes a 5W+1H pattern to help investigators systematically examine a generic set of dimensions in a mapping study toward the formulation of research questions before identifying, reading, and analyzing sufficient articles of the topic. We have validated the feasibility of our proposal by conducting a case study of a mapping study on cloud software testing, that is, software testing for and on cloud computing platforms. The case study reveals that the 5W+1H pattern can lead investigators to define a set of systematic, generic, and complementary research questions, enabling them to kick off and expedite the mapping study process in a well-defined manner. We also share our experiences and lessons learned from our case study on the use of the 5W+1H pattern in mapping studies.}
}
@incollection{2004703,
title = {Subject Index},
editor = {Hossein Bidgoli},
booktitle = {Encyclopedia of Information Systems},
publisher = {Elsevier},
address = {New York},
pages = {703-807},
year = {2004},
isbn = {978-0-12-227240-0},
doi = {https://doi.org/10.1016/B0-12-227240-4/00202-1},
url = {https://www.sciencedirect.com/science/article/pii/B0122272404002021}
}
@article{1981vi,
title = {Preface},
journal = {IFAC Proceedings Volumes},
volume = {14},
number = {3},
pages = {vi},
year = {1981},
note = {IFAC/IFIP Workshop on Real Time Programming, Kyoto, Japan, 31 August-2 September 1981},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)63427-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701763427X}
}
@incollection{BETZ201133,
title = {Chapter 2 - Architecture Approach},
editor = {Charles T. Betz},
booktitle = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {33-149},
year = {2011},
isbn = {978-0-12-385017-1},
doi = {https://doi.org/10.1016/B978-0-12-385017-1.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012385017100002X},
author = {Charles T. Betz},
abstract = {Publisher Summary
This chapter establishes a series of related principles that both support the architectural analysis and serve as high-level scoping and requirements for the value system. The chapter attempts to build a new kind of process framework for large scale IT management in the enterprise. The concept of an overall IT value chain and the four major, long-lived value streams of large-scale enterprise information technology (application services, infrastructure services, IT assets, and IT technology products) are discussed in the chapter. The shorter IT processes are also elaborated. The process architecture is represented as a set of larger-grained lifecycles that are acted upon by a series of well- understood IT processes with clear beginnings and endings. Value streams and processes are the key means by which value is delivered in IT, as in any business. A relentless attention to reducing unnecessary activity along these activity sequences is essential to increasing value. Synchronization points, dependencies, and critical paths would all be of interest, and constitute the foundation of IT value stream analysis. Much in IT is related to nonprocess concepts: quality concepts such as capacity and availability, or the actual functional areas that own the various lifecycles and processes. An overall systems architecture is presented for the business of IT, discussing fundamentals of enterprise application architecture and the major classes of systems encountered in IT management. Summary matrices are introduced in this discussion as a means of showing how the various architectural elements may interact.}
}
@article{KARAPANTAZIS20092050,
title = {VoIP: A comprehensive survey on a promising technology},
journal = {Computer Networks},
volume = {53},
number = {12},
pages = {2050-2090},
year = {2009},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2009.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1389128609001200},
author = {Stylianos Karapantazis and Fotini-Niovi Pavlidou},
keywords = {VoIP, IP Telephony, Voice quality, Voice codecs, Signaling protocols, Call admission control, Security},
abstract = {The Internet has burgeoned into a worldwide information superhighway during the past few years, giving rise to a host of new applications and services. Among them, Voice over IP (VoIP) is the most prominent one. Beginning more as a frolic among computer enthusiasts, VoIP has set off a feeding frenzy in both the industrial and scientific communities and has the potential to radically change telephone communications. In this article, we survey all these aspects that have the greatest impact on the quality of voice communications over IP networks. The survey begins with the merits and demerits of VoIP, followed by the Quality of Service (QoS) requirements that voice imposes and a description of test methods for the assessment of speech quality. We then proceed with a delineation of the issues related to the conversion of analog voice to packets, namely we spell out the details of the most well-known voice codecs, while light is also thrown on voice activity detection and voice packetization. Header compression schemes receive intense scrutiny as well. We also provide an overview of the signaling protocols that are tailored to the needs of VoIP, and we continue with the comparison of the call admission schemes that are geared towards the QoS constraints of VoIP. The pivotal issue of security is then discussed, pointing out potential threats as well as approaches for tackling them. Finally, the survey concludes with a discussion on the feasibility of providing VoIP over challenging satellite links.}
}
@incollection{KUHN20151,
title = {Chapter One - Combinatorial Testing: Theory and Practice},
editor = {Atif Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {99},
pages = {1-66},
year = {2015},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2015.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065245815000352},
author = {D. Richard Kuhn and Renee Bryce and Feng Duan and Laleh Sh. Ghandehari and Yu Lei and Raghu N. Kacker},
keywords = {Algorithms, Combinatorial testing, Constraints, Covering array, Fault localization, Interaction testing, Sequence testing, Software faults, Software testing, Test suite prioritization},
abstract = {Combinatorial testing has rapidly gained favor among software testers in the past decade as improved algorithms have become available and practical success has been demonstrated. This chapter reviews the theory and application of this method, focusing particularly on research since 2010, with a brief background providing the rationale and development of combinatorial methods for software testing. Significant advances have occurred in algorithm performance, and the critical area of constraint representation and processing. In addition to these foundational topics, we take a look at advances in specialized areas including test suite prioritization, sequence testing, fault localization, the relationship between combinatorial testing and structural coverage, and approaches to very large testing problems.}
}
@incollection{THOMASIAN2022385,
title = {Chapter 8 - Database parallelism, big data and analytics, deep learning},
editor = {Alexander Thomasian},
booktitle = {Storage Systems},
publisher = {Morgan Kaufmann},
pages = {385-491},
year = {2022},
isbn = {978-0-323-90796-5},
doi = {https://doi.org/10.1016/B978-0-32-390796-5.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323907965000176},
author = {Alexander Thomasian},
keywords = {Multiprocessor, data sharing, shared disk, shared nothing, Tandem Corp., Teradata Corp., Stratus Corp., Dell Corp., HP Corp., Inspur Corp., Lenovo Corp., benchmarking, TPC - Transaction Processing Council, SPC - Storage Performance Council, SPEC - Standard Performance Evaluation Corp., sort benchmarks, Data Base Machines - DBMs, head per track disks, Rotating Associative Processor - RAP, active disk, SmartSTOR project, high dimensional indexing, dimensionality reduction, Singular Value Decomposition, Graphics Processing Units - GPUs, Field Programmable Gate Arrays - FPGAs, Tensor Processing Units - TPUs, Deep Neural Nets - DNNs, benchmarking machine learning, Cerebras},
abstract = {Early Data Base Machines - DBMs were mainframes running database management systems - DBMSs. Data mining on mainframes was considered too costly, since results obtained by data mining were considered of questionable value. The advent of powerful low-cost microprocessors allowed the building of DBMs affording a high degree of parallelism, such as the Teradata DBC/1012, which has been used for data mining and data warehousing. Active disks process higher priority disk accesses for OnLine Transaction Processing - OLTP, while processing data mining requests as no cost freeblock accesses. Disks with a processor per track capability, such as the Relational Associative Processor - RAP, are no longer feasible because of high track densities, but the concept of associating processing power has been applied to flash storage and DRAM. Systems combining OLTP and Online Analytic Processing - OLAP are discussed.}
}
@incollection{FISHER2005443,
title = {Chapter 10 - Application Design and Customization},
editor = {Joseph A. Fisher and Paolo Faraboschi and Cliff Young},
booktitle = {Embedded Computing},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {443-492},
year = {2005},
isbn = {978-1-55860-766-8},
doi = {https://doi.org/10.1016/B978-155860766-8/50014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978155860766850014X},
author = {Joseph A. Fisher and Paolo Faraboschi and Cliff Young},
abstract = {Publisher Summary
One of the most important differentiators between embedded and general-purpose computing is that embedded systems typically run one single application or set of applications exclusively. This affects many aspects of computing, including code development. Although in most respects, embedded application development resembles general code development, special languages can play a role when the application under development will be embedded in a product because of different lifetime, user community, and underlying hardware considerations. Similarly, the process of making code perform can well change. Techniques that might be unacceptable in the general-purpose world can be almost essential in the embedded world, in which a product may be of no use if it cannot meet a speed or power requirement. Embedded computing is unique in that the hardware too is built to run a single application, and thus methodologies for customization become relevant. This topic is part of the larger area of hardware/software codesign. Embedded designers face issues at the hardware/software boundary that are closed to designers of general-purpose systems. Customized hardware presents an opportunity to vastly speed up individual applications. This opportunity has led to a lively research area and the foundation of more than a few startup companies.}
}
@incollection{WATSON2013233,
title = {Chapter 7 - IT Infrastructure},
editor = {David Watson and Andrew Jones},
booktitle = {Digital Forensics Processing and Procedures},
publisher = {Syngress},
address = {Boston},
pages = {233-312},
year = {2013},
isbn = {978-1-59749-742-8},
doi = {https://doi.org/10.1016/B978-1-59749-742-8.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781597497428000078},
author = {David Watson and Andrew Jones},
keywords = {process, incident, change, release, hardware, software, capacity, service, management},
abstract = {This chapter looks at the policies and issues related to the IT infrastructure within the laboratory. It looks at the hardware, the software, and the infrastructure in some detail. It then looks at process management, addressing issues including incident and problem management; change control; Release Management; and configuration, capacity, and service management.}
}
@article{CHEN2011344,
title = {A systematic review of evaluation of variability management approaches in software product lines},
journal = {Information and Software Technology},
volume = {53},
number = {4},
pages = {344-362},
year = {2011},
note = {Special section: Software Engineering track of the 24th Annual Symposium on Applied Computing},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002223},
author = {Lianping Chen and Muhammad {Ali Babar}},
keywords = {Software product line, Variability management, Systematic literature reviews, Empirical studies},
abstract = {Context
Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated.
Objective
The objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches.
Method
We carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007.
Results
We selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects.
Conclusion
The findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.}
}
@article{CARETTE2011349,
title = {Multi-stage programming with functors and monads: Eliminating abstraction overhead from generic code},
journal = {Science of Computer Programming},
volume = {76},
number = {5},
pages = {349-375},
year = {2011},
note = {Special Issue on Generative Programming and Component Engineering (Selected Papers from GPCE 2004/2005)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2008.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S016764230800110X},
author = {Jacques Carette and Oleg Kiselyov},
keywords = {MetaOCaml, Linear algebra, Genericity, Generative, Staging, Functor, Symbolic},
abstract = {We use multi-stage programming, monads and Ocaml’s advanced module system to demonstrate how to eliminate all abstraction overhead from generic programs, while avoiding any inspection of the resulting code. We demonstrate this clearly with Gaussian Elimination as a representative family of symbolic and numeric algorithms. We parameterize our code to a great extent–over domain, input and permutation matrix representations, determinant and rank tracking, pivoting policies, result types, etc.–at no run-time cost. Because the resulting code is generated just right and not changed afterward, MetaOCaml guarantees that the generated code is well-typed. We further demonstrate that various abstraction parameters (aspects) can be made orthogonal and compositional, even in the presence of name-generation for temporaries, and “interleaving” of aspects. We also show how to encode some domain-specific knowledge so that “clearly wrong” compositions can be rejected at or before generation time, rather than during the compilation or running of the generated code.}
}
@article{LOPEZHERREJON201533,
title = {A systematic mapping study of search-based software engineering for software product lines},
journal = {Information and Software Technology},
volume = {61},
pages = {33-51},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000166},
author = {Roberto E. Lopez-Herrejon and Lukas Linsbauer and Alexander Egyed},
keywords = {Software product line, Systematic mapping study, Search based software engineering, Evolutionary algorithm, Metaheuristics},
abstract = {Context
Search-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces.
Objective
The main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published.
Method
A systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014.
Results
The most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation.
Conclusions
Our study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.}
}
@article{MARTINEZ201736,
title = {Model-based analysis of Java EE web security misconfigurations},
journal = {Computer Languages, Systems & Structures},
volume = {49},
pages = {36-61},
year = {2017},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1477842416301348},
author = {Salvador Martínez and Valerio Cosentino and Jordi Cabot},
keywords = {Model-driven engineering, Security, Reverse-engineering},
abstract = {The Java EE framework, a popular technology of choice for the development of web applications, provides developers with the means to define access-control policies to protect application resources from unauthorized disclosures and manipulations. Unfortunately, the definition and manipulation of such security policies remains a complex and error prone task, requiring expert-level knowledge on the syntax and semantics of the Java EE access-control mechanisms. Thus, misconfigurations that may lead to unintentional security and/or availability problems can be easily introduced. In response to this problem, we present a (model-based) reverse engineering approach that automatically evaluates a set of security properties on reverse engineered Java EE security configurations, helping to detect the presence of anomalies. We evaluate the efficacy and pertinence of our approach by applying our prototype tool on a sample of real Java EE applications extracted from GitHub.}
}
@article{HOEY2009168,
title = {DLL4 Blockade Inhibits Tumor Growth and Reduces Tumor-Initiating Cell Frequency},
journal = {Cell Stem Cell},
volume = {5},
number = {2},
pages = {168-177},
year = {2009},
issn = {1934-5909},
doi = {https://doi.org/10.1016/j.stem.2009.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S1934590909002288},
author = {Timothy Hoey and Wan-Ching Yen and Fumiko Axelrod and Jesspreet Basi and Lucas Donigian and Scott Dylla and Maureen Fitch-Bruhns and Sasha Lazetic and In-Kyung Park and Aaron Sato and Sanjeev Satyal and Xinhao Wang and Michael F. Clarke and John Lewicki and Austin Gurney},
keywords = {STEMCELL, CELLCYCLE},
abstract = {Summary
Previous studies have shown that blocking DLL4 signaling reduced tumor growth by disrupting productive angiogenesis. We developed selective anti-human and anti-mouse DLL4 antibodies to dissect the mechanisms involved by analyzing the contributions of selectively targeting DLL4 in the tumor or in the host vasculature and stroma in xenograft models derived from primary human tumors. We found that each antibody inhibited tumor growth and that the combination of the two antibodies was more effective than either alone. Treatment with anti-human DLL4 inhibited the expression of Notch target genes and reduced proliferation of tumor cells. Furthermore, we found that specifically inhibiting human DLL4 in the tumor, either alone or in combination with the chemotherapeutic agent irinotecan, reduced cancer stem cell frequency, as shown by flow cytometric and in vivo tumorigenicity studies.}
}
@article{BADROS2000159,
title = {JavaML: a markup language for Java source code},
journal = {Computer Networks},
volume = {33},
number = {1},
pages = {159-177},
year = {2000},
issn = {1389-1286},
doi = {https://doi.org/10.1016/S1389-1286(00)00037-2},
url = {https://www.sciencedirect.com/science/article/pii/S1389128600000372},
author = {Greg J Badros},
keywords = {Java, XML, Abstract syntax tree representation, Software-engineering analysis, Jikes compiler},
abstract = {The classical plain-text representation of source code is convenient for programmers but requires parsing to uncover the deep structure of the program. While sophisticated software tools parse source code to gain access to the program's structure, many lightweight programming aids such as grep rely instead on only the lexical structure of source code. I describe a new XML application that provides an alternative representation of Java source code. This XML-based representation, called JavaML, is more natural for tools and permits easy specification of numerous software-engineering analyses by leveraging the abundance of XML tools and techniques. A robust converter built with the Jikes Java compiler framework translates from the classical Java source code representation to JavaML, and an XSLT stylesheet converts from JavaML back into the classical textual form.}
}
@article{EBEID201811,
title = {A survey of Open-Source UAV flight controllers and flight simulators},
journal = {Microprocessors and Microsystems},
volume = {61},
pages = {11-20},
year = {2018},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0141933118300930},
author = {Emad Ebeid and Martin Skriver and Kristian Husum Terkildsen and Kjeld Jensen and Ulrik Pagh Schultz},
keywords = {Unmanned Aerial Vehicle (UAV), Drones, Flight controllers, Drone simulators, Open platforms, Survey},
abstract = {The current disruptive innovation in civilian drone (UAV) applications has led to an increased need for research and development in UAV technology. The key challenges currently being addressed are related to UAV platform properties such as functionality, reliability, fault tolerance, and endurance, which are all tightly linked to the UAV flight controller hardware and software. The lack of standardization of flight controller architectures and the use of proprietary closed-source flight controllers on many UAV platforms, however, complicates this work: solutions developed for one flight controller may be difficult to port to another without substantial extra development and testing. Using open-source flight controllers mitigates some of these challenges and enables other researchers to validate and build upon existing research. This paper presents a survey of the publicly available open-source drone platform elements that can be used for research and development. The survey covers open-source hardware, software, and simulation drone platforms and compares their main features.}
}
@article{BARREA2017293,
title = {Influence of nutrition on somatotropic axis: Milk consumption in adult individuals with moderate-severe obesity},
journal = {Clinical Nutrition},
volume = {36},
number = {1},
pages = {293-301},
year = {2017},
issn = {0261-5614},
doi = {https://doi.org/10.1016/j.clnu.2015.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0261561415003453},
author = {Luigi Barrea and Carolina {Di Somma} and Paolo Emidio Macchia and Andrea Falco and Maria Cristina Savanelli and Francesco Orio and Annamaria Colao and Silvia Savastano},
keywords = {Environmental factors, Milk consumption, Nutrition, Somatotropic axis, Obesity},
abstract = {Summary
Background & aims
Nutrition is the major environmental factor that influences the risk of developing pathologies, such as obesity. Although a number of recent reviews pinpoint a protective effects of milk on body weight and obesity related co-morbidities, an inaccurate estimate of milk might contribute to hamper its beneficial effects on health outcomes. Seven-day food records provide prospective food intake data, reducing recall bias and providing extra details about specific food items. Milk intake stimulates the somatotropic axis at multiple levels by increasing both growth hormone (GH) and insulin-like growth factor-1 (IGF-1) secretion. On the other hand, obesity is associated with reduced spontaneous and stimulated GH secretion and basal IGF-1 levels. Aim of this study was to evaluate the milk consumption by using the 7-days food record in obese individuals and to investigate the association between milk intake and GH secretory status in these subjects.
Methods
Cross-sectional observational study carried out on 281 adult individuals (200 women and 81 men, aged 18–74 years) with moderate-severe obesity (BMI 35.2–69.4 kg/m2). Baseline milk intake data were collected using a 7 day food record. Anthropometric measurements and biochemical profile were determined. The GH/IGF-1 axis was evaluated by peak GH response after GHRH + ARGININE and IGF-1 standard deviation score (SDS).
Results
The majority of individuals (72.2%) reported consuming milk; 250 mL low-fat milk was the most frequently serving of milk consumed, while no subjects reported to consume whole milk. Milk consumers vs no milk consumers presented the better anthropometric measurements and metabolic profile. At the bivariate proportional odds ratio model, after adjusting for BMI, age and gender, milk consumption was associated the better GH status (OR = 0.60; p < 0.001). Among milk consumers, subjects consuming 250 mL reduced-fat milk vs 250 mL low-fat milk presented the better anthropometric measurements and metabolic profile. At the bivariate proportional odds ratio model, after adjusting for BMI, age and gender, the consume of 250 mL reduced-fat milk was associated better GH status (OR = 0.54; p = 0.003).
Conclusions
A novel positive association between milk consumption, GH status, and metabolic profile in obese individuals was evidenced. Regardless of the pathogenetic mechanisms, this novel association might be relevant in a context where commonly obese individuals skip breakfast, and suggests the need of a growing cooperation between Nutritionists and Endocrinologists in the management of the obese patients.}
}
@article{HU2016449,
title = {Robust Cyber–Physical Systems: Concept, models, and implementation},
journal = {Future Generation Computer Systems},
volume = {56},
pages = {449-475},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15002071},
author = {Fei Hu and Yu Lu and Athanasios V. Vasilakos and Qi Hao and Rui Ma and Yogendra Patil and Ting Zhang and Jiang Lu and Xin Li and Neal N. Xiong},
keywords = {Cyber–Physical Systems, Stability, Security, Sensors and actuators, Survey},
abstract = {In this paper we comprehensively survey the concept and strategies for building a resilient and integrated cyber–physical system (CPS). Here resilience refers to a 3S-oriented design, that is, stability, security, and systematicness: Stability means the CPS can achieve a stable sensing-actuation close-loop control even though the inputs (sensing data) have noise or attacks; Security means that the system can overcome the cyber–physical interaction attacks; and Systematicness means that the system has a seamless integration of sensors and actuators. We will also explain the CPS modeling issues since they serve as the basics of 3S design. We will use two detailed examples from our achieved projects to explain how to achieve arobust, systematic CPS design: Case study 1 is on the design of a rehabilitation system with cyber (sensors) and physical (robots) integration. Case Study 2 is on the implantable medical device design. It illustrates the nature of CPS security principle. The dominant feature of this survey is that it has both principle discussions and practical cyber–physical coupling design.}
}
@incollection{2002767,
title = {Index},
editor = {David R. Mirza Ahmad and Ido Dubrawsky and Hal Flynn and Joseph “Kingpin” Grand and Robert Graham and Norris L. Johnson and Dan “Effugas” Kaminsky and F. William Lynch and Steve W. Manzuik and Ryan Permeh and Ken Pfeil and Rain Forest Puppy},
booktitle = {Hack Proofing Your Network (Second Edition)},
publisher = {Syngress},
edition = {Second Edition},
address = {Burlington},
pages = {767-789},
year = {2002},
isbn = {978-1-928994-70-1},
doi = {https://doi.org/10.1016/B978-192899470-1/50022-7},
url = {https://www.sciencedirect.com/science/article/pii/B9781928994701500227}
}
@incollection{1984233,
title = {SECTION 13 - Directory of UK Companies, Societies, Institutes, and Organizations},
editor = {R S Sharpe and J West and D S Dean and D A Tyler and H A Cole},
booktitle = {Quality Technology Handbook (Fourth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Fourth Edition},
pages = {233-379},
year = {1984},
isbn = {978-0-408-01331-4},
doi = {https://doi.org/10.1016/B978-0-408-01331-4.50015-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780408013314500155}
}
@article{2005575,
title = {Society for Development Biology 64th Annual Meeting},
journal = {Developmental Biology},
volume = {283},
number = {2},
pages = {575-707},
year = {2005},
issn = {0012-1606},
doi = {https://doi.org/10.1016/j.ydbio.2005.04.040},
url = {https://www.sciencedirect.com/science/article/pii/S0012160605002940}
}
@article{2022I,
title = {Full Issue},
journal = {JACC: Cardiovascular Imaging},
volume = {15},
number = {2},
pages = {I-CCIV},
year = {2022},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(22)00044-4},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X22000444}
}
@article{2005S81,
title = {Posters displayed on Monday 9 May 2005},
journal = {Clinica Chimica Acta},
volume = {355},
pages = {S81-S201},
year = {2005},
note = {Focus on the Patient: 16th IFCC - FESCC European Congress of Clinical Biochemistry and Laboratory Medicine},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cccn.2005.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0009898105001233}
}
@incollection{20031,
title = {Subject Index},
editor = {Robert A. Meyers},
booktitle = {Encyclopedia of Physical Science and Technology (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {New York},
pages = {1-344},
year = {2003},
isbn = {978-0-12-227410-7},
doi = {https://doi.org/10.1016/B0-12-227410-5/09009-8},
url = {https://www.sciencedirect.com/science/article/pii/B0122274105090098}
}
@article{2020I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {13},
number = {12},
pages = {I-CCXXII},
year = {2020},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(20)30988-8},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20309888}
}
@incollection{2009289,
title = {Subject Index},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {289-586},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.09007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080449104090076}
}
@article{2002217,
title = {Joint Meeting of the European Pancreatic Club (EPC) and the International Association of Pancreatology (IAP)},
journal = {Pancreatology},
volume = {2},
number = {3},
pages = {217-361},
year = {2002},
issn = {1424-3903},
doi = {https://doi.org/10.1159/000058036},
url = {https://www.sciencedirect.com/science/article/pii/S1424390302800047}
}