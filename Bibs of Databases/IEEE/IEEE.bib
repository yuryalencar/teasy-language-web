@INPROCEEDINGS{7133548, 
author={M. Rahman and J. Gao}, 
booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
title={A Reusable Automated Acceptance Testing Architecture for Microservices in Behavior-Driven Development}, 
year={2015}, 
volume={}, 
number={}, 
pages={321-325}, 
abstract={Cloud Computing and Mobile Cloud Computing are reshaping the way applications are being developed and deployed due to their unique needs such as massive scalability, guaranteed fault tolerance, near zero downtime, etc. and also daunting challenges such as security, reliability, continuous deployment and update capability. Microservices architecture, where application is composed of a set of independently deployable services, is increasingly becoming popular due to its capability to address most of these needs and challenges. In recent years, the Behavior-Driven Development (BDD) has become one of the most popular agile software development processes, and frequently used in microservices development. The key to success of BDD is the executable acceptance tests that describe the expected behavior of a feature and its acceptance criteria in the form of scenarios using simple and business people readable syntax. The reusability, auditability, and maintainability become some of the major concerns when BDD test framework is applied for each microservice repository and no previous research addresses these concerns. In this paper, we present a reusable automated acceptance testing architecture to address all these concerns.}, 
keywords={cloud computing;mobile computing;program testing;software prototyping;reusable automated acceptance testing architecture;cloud computing;mobile cloud computing;behavior-driven development;agile software development process;BDD test framework;Data structures;Boolean functions;Business;Testing;Software;Maintenance engineering;executable automated acceptance testing; Gherkin; functional testing; behavior-driven development; microservice}, 
doi={10.1109/SOSE.2015.55}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5069055, 
author={D. Talby}, 
booktitle={2009 ICSE Workshop on Automation of Software Test}, 
title={The perceived value of authoring and automating acceptance tests using a model driven development toolset}, 
year={2009}, 
volume={}, 
number={}, 
pages={154-157}, 
abstract={One approach to applying keyword driven testing in a model-driven development environment is by defining a domain specific language for test cases. The toolset then provides test editors, versioning, validation, reporting and hyperlinks across models - in addition to enabling automated test execution. This case study evaluates the effectiveness of such a solution as perceived by two teams of professional testers, who used it to test several products over a two year period. The results suggest that in addition to the expected benefits of automation, the solution reduces the time and effort required to write tests, maintain tests and plan the test authoring and execution efforts - at the expense of requiring longer training and a higher bar for recruiting testers.}, 
keywords={authoring systems;program testing;specification languages;perceived value;acceptance tests;model driven development toolset;keyword driven testing;domain specific language;automated test execution;test authoring;Automatic testing;DSL;Logic testing;Domain specific languages;Application software;Automation;Recruitment;Programming;Metamodeling;Context modeling}, 
doi={10.1109/IWAST.2009.5069055}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8387650, 
author={V. Pinkevich and A. Platunov}, 
booktitle={2018 IEEE Industrial Cyber-Physical Systems (ICPS)}, 
title={Model-driven functional testing of cyber-physical systems using deterministic replay techniques}, 
year={2018}, 
volume={}, 
number={}, 
pages={141-146}, 
abstract={Specialized embedded computer systems are one of the core technologies of modern industrial cyber-physical systems. They implement application algorithms and perform data acquisition, processing and transfer. The article presents the original approach to functional testing of embedded computer systems to overcome a number of restrictions imposed by specifics of the process of their design and development.}, 
keywords={cyber-physical systems;embedded systems;production engineering computing;program testing;application algorithms;embedded computer systems;deterministic replay techniques;model-driven functional testing;industrial cyber-physical systems;cyber-physical systems;embedded systems;record and deterministic replay;computer architecture;high-level modeling;functional testing;verification;debug}, 
doi={10.1109/ICPHYS.2018.8387650}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4464015, 
author={J. Diaz and A. Yague and P. P. Alarcon and J. Garbajosa}, 
booktitle={Seventh International Conference on Composition-Based Software Systems (ICCBSS 2008)}, 
title={A Generic Gateway for Testing Heterogeneous Components in Acceptance Testing Tools}, 
year={2008}, 
volume={}, 
number={}, 
pages={110-119}, 
abstract={Acceptance testing tools and Systems Under Test (SUT) require a gateway that will set up the communication link between them. Nevertheless, SUTs are often large systems composed of heterogeneous components that are executed in heterogeneous networks and platforms. Therefore, a non trivial communication problem between testing tools and these SUT heterogeneous components arises. A significant effort is invested in designing and implementing gateways for each specific component interface to cope with heterogeneity. This problem may be addressed through the use of middleware technologies that hide heterogeneity. However, this solution is too specific for each SUT domain. It may require a noteworthy effort to support the wide range of currently available interface standards that are provided by the different platforms and networks. An approach for testing heterogeneous components based on a generic gateway is presented in this paper. The generic gateway implements a service-oriented middleware named OSGi (Open Service Gateway initiative). OSGi helps to solve the heterogeneity problem and reduces the impact of designing a gateway for each specific SUT domain. The solution has been validated using the acceptance testing tool TOPEN (Test and Operation ENvironment) in a home automation scenario.}, 
keywords={distributed processing;middleware;object-oriented methods;program testing;software tools;generic gateway;heterogeneous component testing;acceptance testing tools;systems under test;communication link;heterogeneous networks;SUT heterogeneous components;interface standards;service-oriented middleware;OSGi;Open Service Gateway initiative;heterogeneity problem;SUT domain;TOPEN;Test and Operation ENvironment;home automation scenario;System testing;Automatic testing;Middleware;Software systems;Software testing;Home automation;Embedded system;Intelligent sensors;Computer architecture;Software tools;test automation;middleware;complex systems testing;acceptance testing tools;OSGI;TOPEN;gateway;validation}, 
doi={10.1109/ICCBSS.2008.31}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8449605, 
author={E. C. Dos Santos and P. Vilain and D. Hiura Longo}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={Poster: A Systematic Literature Review to Support the Selection of User Acceptance Testing Techniques}, 
year={2018}, 
volume={}, 
number={}, 
pages={418-419}, 
abstract={User Acceptance Testing (UAT) aims to determine whether or not a software satisfies users acceptance criteria. Although some studies have used acceptance tests as software requirements, no previous study has collected information about available UAT techniques and established a comparison of them, to support an organization in the selection of one over another. This work presents a Systematic Literature Review on UAT to find out available techniques and compare their main features. We selected 80 studies and found out 21 UAT techniques. As result, we created a comparative table summarizing these techniques and their features.}, 
keywords={Testing;Software;Natural languages;Tools;Bibliographies;Software engineering;Systematics;User acceptance testing;techniques;classification;features}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@INPROCEEDINGS{6569751, 
author={A. Törsel}, 
booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
title={A Testing Tool for Web Applications Using a Domain-Specific Modelling Language and the NuSMV Model Checker}, 
year={2013}, 
volume={}, 
number={}, 
pages={383-390}, 
abstract={Test case generation from formal models using model checking software is an established method. This paper presents a model-based testing approach for web applications based on a domain-specific language model. It is shown how the domain-specific language is transformed into the input language of the NuSMV model checker and how the resulting traces are converted into executable test scripts for various test automation tools. The presented approach has been implemented with comprehensive automation in a research tool which architecture is outlined.}, 
keywords={automatic programming;formal verification;Internet;program testing;specification languages;Web applications;domain-specific modelling language;NuSMV model checker software;test case generation;model-based testing approach;test automation tools;executable test scripts;DSL;Automation;Adaptation models;Software;Web pages;Model checking;web applications;model-based testing;model checking;test automation}, 
doi={10.1109/ICST.2013.54}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{7367051, 
author={R. Kumar and V. Kumar}, 
booktitle={2015 World Congress on Information Technology and Computer Applications (WCITCA)}, 
title={Process optimization for testing of domain specific languages in industrial automation}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Software testing is essential part of software development. The goal of the testing process is not only to enhance the quality and robustness of the software but also verify the correctness and non functional requirements of the software under all working conditions. Large software has their large testing suites to verify the stability of legacy features. Testing processes have huge challenges to maintain effectiveness and efficiency of the legacy test cases. There are many different processes and techniques available all technique or processes have their advantages and limitations. A tailored testing process has been tried to utilize all technique together to improvise the benefits and efficiency of testing in the industrial automation domain. This paper tries to explain a customized approach of utilizing the available testing techniques in such a way that it enhances the effectiveness and efficiency of regression testing, thus improving the time to market of large product-line Industrial automation software.}, 
keywords={factory automation;production engineering computing;program testing;program verification;software maintenance;software quality;process optimization;domain specific language testing;software testing;software development;software quality;software verification;legacy test cases;regression testing;product-line industrial automation software;Automation;Software testing;Fault detection;Software engineering;Software maintenance;Regression Testing;Test Effectiveness;Test suite optimization;Software Testing process;Test Automation;Industial automation}, 
doi={10.1109/WCITCA.2015.7367051}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{5540750, 
author={W. Vanderbauwhede and M. Margala and S. R. Chalamalasetti and S. Purohit}, 
booktitle={ASAP 2010 - 21st IEEE International Conference on Application-specific Systems, Architectures and Processors}, 
title={A C++-embedded Domain-Specific Language for programming the MORA soft processor array}, 
year={2010}, 
volume={}, 
number={}, 
pages={141-148}, 
abstract={MORA is a novel platform for high-level FPGA programming of streaming vector and matrix operations, aimed at multimedia applications. It consists of soft array of pipelined low-complexity SIMD processors-in-memory (PIM). We present a Domain-Specific Language (DSL) for high-level programming of the MORA soft processor array. The DSL is embedded in C++, providing designers with a familiar language framework and the ability to compile designs using a standard compiler for functional testing before generating the FPGA bitstream using the MORA toolchain. The paper discusses the MORA-C++ DSL and the compilation route into the assembly for the MORA machine and provides examples to illustrate the programming model and performance.}, 
keywords={C++ language;field programmable gate arrays;multimedia computing;parallel processing;pipeline processing;program compilers;specification languages;C++ embedded domain specific language;MORA soft processor array programming;high level FPGA programming;vector streaming;matrix operations;multimedia;pipelined low complexity SIMD processors-in-memory;DSL;MORA machine;compiler;Domain specific languages;Field programmable gate arrays;Parallel processing;DSL;Streaming media;Application specific integrated circuits;Parallel programming;Concurrent computing;Algorithm design and analysis;Programming profession;Reconfigurable Processor;Soft Processor Array;Multimedia Processing;Domain-Specific Language}, 
doi={10.1109/ASAP.2010.5540750}, 
ISSN={1063-6862}, 
month={July},}
@INPROCEEDINGS{7570913, 
author={P. Pandit and S. Tahiliani and M. Sharma}, 
booktitle={2016 Symposium on Colossal Data Analysis and Networking (CDAN)}, 
title={Distributed agile: Component-based user acceptance testing}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Testing is conducted at multiple levels during the development of software. User Acceptance Testing conforms that the software meets user's criteria. In this paper, User Acceptance Testing is automatically conducted based on acceptance criteria. The acceptance criteria are written in the form of Given-When-Then Template. These acceptance criteria are broken down into steps and numbered. The dependencies among the steps are determined as Given-&gt;When-&gt;Then. Henceforth, the steps are arranged in a dependency graph. This graph further leads to the creation of a decision table in which the outcome of one step leads to the outcomes of its dependent steps. The decision table forms the basis of generation of a binary weighted dependency tree. This tree becomes the means to form test coverage (number of combinations to test) which forms the basis of generation of acceptance test cases.}, 
keywords={decision tables;human factors;object-oriented programming;program testing;software prototyping;trees (mathematics);distributed agile;component-based user acceptance testing;software development;acceptance criteria;Given-When-Then template;dependency graph;decision table;binary weighted dependency tree;acceptance test case generation;Testing;Online banking;Data analysis;Credit cards;Software;Algorithm design and analysis}, 
doi={10.1109/CDAN.2016.7570913}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5463692, 
author={R. Chatley and J. Ayres and T. White}, 
booktitle={2010 Third International Conference on Software Testing, Verification, and Validation Workshops}, 
title={LiFT: Driving Development Using a Business-Readable DSL for Web Testing}, 
year={2010}, 
volume={}, 
number={}, 
pages={460-468}, 
abstract={This paper describes the development and evolution of LiFT, a framework for writing automated tests in a style that makes them very readable, even for non-programmers. We call this style 'literate testing'. By creating a domain-specific language embedded within Java, we were able to write automated tests that read almost like natural language, allowing business requirements to be expressed very clearly. This allows development to be driven from tests that are created by developers and customers together, helping give all stakeholders confidence that the right things are being tested and hence a correct system being built. We discuss the experiences of a team using these tools and techniques in a large commercial project, and the lessons learned from the experience.}, 
keywords={business data processing;Internet;Java;LiFT;driving development;business readable DSL;Web testing;literate testing;domain specific language;Java;natural language;DSL;Automatic testing;System testing;Writing;Software testing;Domain specific languages;Java;Natural languages;Business communication;Formal specifications;TDD;acceptance testing;DSL}, 
doi={10.1109/ICSTW.2010.12}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6200141, 
author={E. Borjesson}, 
booktitle={2012 IEEE Fifth International Conference on Software Testing, Verification and Validation}, 
title={Industrial Applicability of Visual GUI Testing for System and Acceptance Test Automation}, 
year={2012}, 
volume={}, 
number={}, 
pages={475-478}, 
abstract={The software market is becoming more challenging as demands for faster time-to-market and higher software quality continue to grow. These challenges are embedded in all areas of Software Engineering, including Verification and Validation where they are proposed as solvable with automated testing. However, most automated testing techniques focus on low system level testing and are not suitable for high level tests, i.e. System and Acceptance tests, leaving industrial needs for test automation unfulfilled. In this paper we present a research plan to evaluate a novel automated testing technique, called visual GUI testing, based on image recognition algorithms and scripts that interact through the system GUI to automate complex scenario based tests. The technique has been evaluated at the company Saab AB where industrial, safety critical, scenario based, test cases were automated showing the industrial applicability of the technique. However, many factors are still unknown regarding the techniques industrial applicability, i.e. script maintenance costs, usability and learn ability, etc. Our research aims to uncover these unknown factors with the final research goal to show that visual GUI testing is a viable and cost-effective technique that will fill the gap in industry for a cost-effective, simple, robust, high-level test automation technique.}, 
keywords={graphical user interfaces;program testing;program verification;software houses;software quality;time to market;industrial applicability;visual GUI testing;system test automation;acceptance test automation;software market;time-to-market;software quality;software engineering;verification;validation;automated testing;Testing;Graphical user interfaces;Visualization;Industries;Automation;Robustness;Maintenance engineering;V&V;Automated testing;Visual GUI testing;Image recognition;Scripted testing}, 
doi={10.1109/ICST.2012.129}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{8411756, 
author={B. Elodie and A. Fabrice and L. Bruno and B. Arnaud}, 
booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={Lightweight Model-Based Testing for Enterprise IT}, 
year={2018}, 
volume={}, 
number={}, 
pages={224-230}, 
abstract={Model-Based Testing (MBT) popularity in IT is growing at a very slow pace. A recent survey stated that no more than 14% of respondents use MBT in their projects. Our experience, presented in this paper, demonstrates that the complexity in use of the current MBT approaches for the average tester is the main reason for this low dissemination. Then we introduce a lightweight MBT approach and a tool, called Yest, dedicated to business process-based testing of enterprise information systems. This tool uses a workflow-based graphical representation linked with decision tables to be used by functional testers without requiring any kind of modeling skill (such as UML for example). These approach and tool are dedicated to a particular class of applications (i.e. enterprise IT applications such as ERP and bespoke business applications). This focus strongly helps to simplify the approach and to adapt the tooling to the targeted users (namely IT functional testers). Finally, we discuss the way MBT may support emerging Acceptance Test Driven Development practices in agile.}, 
keywords={business data processing;enterprise resource planning;information systems;information technology;program testing;Unified Modeling Language;business process-based testing;MBT popularity;Yest;IT functional testers;model-based testing;enterprise IT;average tester;Acceptance Test Driven Development practices;business applications;modeling skill;functional testers;decision tables;workflow-based graphical representation;enterprise information systems;Tools;Testing;Unified modeling language;Task analysis;Password;Electronic mail;Model-based-testing;Lightweight MBT;MBT tool;Test cases generation;Business process-based testing}, 
doi={10.1109/ICSTW.2018.00053}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7128889, 
author={N. Visic and H. Fill and R. A. Buchmann and D. Karagiannis}, 
booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
title={A domain-specific language for modeling method definition: From requirements to grammar}, 
year={2015}, 
volume={}, 
number={}, 
pages={286-297}, 
abstract={The core process a modeling method engineer needs to accomplish starts with the acquisition of domain knowledge and requirements, and ends with the deployment of a usable modeling tool. In between, a key intermediate deliverable of this process is the modeling method specification which, ideally, should be platform independent. On one hand, it takes input from a structured understanding of the application domain and scenarios; on the other hand, it provides sufficiently structured input to support the implementation of tool support for modeling activities. It is quite common that such modeling methods are domain-specific, in the sense that they provide concepts from the domain as “first-class modeling citizens”. However, for the purposes of this paper, we raise the level of abstraction for “domain specificity” and consider “modeling method engineering” as the application domain. Consequently, we raise several research questions - whether a domain-specific language can support this domain, and what would be its requirements, properties, constructs and grammar. We propose an initial draft of such a language - one that abstracts away from meta-modeling platforms by establishing a meta2layer of abstraction where a modeling method can be defined in a declarative manner, then the final modeling tool is generated by automated compilation of the method definition for the meta-modeling environment of choice.}, 
keywords={formal specification;grammars;knowledge based systems;domain-specific language;grammar;domain knowledge;modeling method specification;domain specificity;modeling method engineering;metamodeling platform;Unified modeling language;Analytical models;DSL;Metamodeling;Semantics;Computational modeling;Domain specific languages;domain-specific language;modeling method;meta-modeling;modeling tool}, 
doi={10.1109/RCIS.2015.7128889}, 
ISSN={2151-1349}, 
month={May},}
@INPROCEEDINGS{5254116, 
author={A. Miller and B. Kumar and A. Singhal}, 
booktitle={2009 33rd Annual IEEE International Computer Software and Applications Conference}, 
title={Photon: A Domain-Specific Language for Testing Converged Applications}, 
year={2009}, 
volume={2}, 
number={}, 
pages={269-274}, 
abstract={Automated testing of converged applications can be complex, as it is rare for a single testing tool to provide a single solution for all access points which a given application supports. As such, testing teams often create customized testing frameworks, which integrate several different testing tools, and a myriad of programming languages and scripting tools. When an applicationpsilas unique set of access points changes, or a new testing tool comes to market which offers a competitive advantage over existing test tools, the cost of updating these customized frameworks can be difficult to justify. This paper provides a solution to this problem by introducing ldquoPhotonese,rdquo a domain-specific language which testers can use to compose automation scripts which are independent of the test tool used for automation. In this way, the tester creates reusable testing assets in a framework which is reusable across multiple projects.}, 
keywords={authoring languages;program testing;domain-specific language;testing converged application;automated testing;customized testing framework;programming language;scripting tool;Photonese;automation script;reusable testing asset;Domain specific languages;Automatic testing;Automation;Software testing;Life testing;Telephony;Books;Application software;Displays;Computer applications;Software quality;Software reusability;Software testing}, 
doi={10.1109/COMPSAC.2009.143}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{6228998, 
author={N. Hallenberg and P. L. Carlsen}, 
booktitle={2012 7th International Workshop on Automation of Software Test (AST)}, 
title={Declarative automated test}, 
year={2012}, 
volume={}, 
number={}, 
pages={96-102}, 
abstract={Automated tests at the business level can be expensive to develop and maintain. One common approach is to have a domain expert instruct a QA developer to implement what she would do manually in the application. Though there exist record-replay tools specifically developed for this, these tend to scale poorly for more complicated test scenarios. We present a different solution: An Embedded Domain Specific Language (EDSL) in F#, containing the means to model the user interface, and the various manipulations of it. We hope that this DSL will bridge the gap between the business domain and technical domain of applications to such a degree that domain experts may be able to construct automatic tests without depending on QA developers, and that these tests will prove more maintainable.}, 
keywords={program testing;software quality;user interfaces;declarative automated testing;business level;domain expert;QA developer;record-replay tool;embedded domain specific language;F#;user interface;technical domain;Testing;DSL;Documentation;Phantoms;Engines;Business;User interfaces;Functional Testing;Automated Testing;Domain Specific Language;F#}, 
doi={10.1109/IWAST.2012.6228998}, 
ISSN={}, 
month={June},}
@ARTICLE{8440671, 
author={A. M. Mirza and M. N. A. Khan}, 
journal={IEEE Access}, 
title={An Automated Functional Testing Framework for Context-Aware Applications}, 
year={2018}, 
volume={6}, 
number={}, 
pages={46568-46583}, 
abstract={In the modern era of mobile computing, context-aware computing is an emerging paradigm due to its widespread applications. Context-aware applications are gaining increasing popularity in our daily lives since these applications can determine and react according to the situational context and help users to enhance usability experience. However, testing these applications is not straightforward since it poses several challenges, such as generating test data, designing context-coupled test cases, and so on. However, the testing process can be automated to a greater extent by employing model-based testing technique for context-aware applications. To achieve this goal, it is necessary to automate model transformation, test data generation, and test case execution processes. In this paper, we propose an approach for behavior modeling of context-aware application by extending the UML activity diagram. We also propose an automated model transformation approach to transform the development model, i.e., extended UML activity diagram into the testing model in the form of function nets. The objective of this paper is to automate the context-coupled test case generation and execution. We propose a functional testing framework for automated execution of keyword-based test cases. Our functional testing framework can reduce the testing time and cost, thus enabling the test engineers to execute more testing cycles to attain a higher degree of test coverage.}, 
keywords={mobile computing;program testing;Unified Modeling Language;keyword-based test cases;functional testing framework;test engineers;test coverage;context-aware computing;test data generation;test case execution processes;automated model transformation approach;context-coupled test case generation;mobile computing;UML activity diagram;Unified modeling language;Testing;Petri nets;Context-aware services;Sensors;Context modeling;Context-aware applications;model based testing;function net;petri net;model transformation}, 
doi={10.1109/ACCESS.2018.2865213}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{6470771, 
author={D. Mayrhofer and C. Huemer}, 
booktitle={2012 IEEE 14th International Conference on Commerce and Enterprise Computing}, 
title={REA-DSL: Business Model Driven Data-Engineering}, 
year={2012}, 
volume={}, 
number={}, 
pages={9-16}, 
abstract={An accounting information system (AIS) manages data about a company's financial and economic status. The contribution of this paper is closing the gap between the languages used by business domain experts and IT-experts in analyzing the relevant data. A well accepted approach for an accountability infrastructure is the Resource-Event-Agent (REA) ontology. Although REA has been based on well-established concepts of the accounting theory, its representation has not been intuitive to domain experts. In previous work, we developed the REA-DSL, a dedicated and easy-to-understand graphical domain specific modeling language for the REA ontology. Evidently, a model-driven approach requires to transform the REA-DSL artifacts to code. In this paper we present the transformation of the REA-DSL to a relational database for AIS. This approach offers the advantage that a domain expert verifies the relevant data in an "accounting language", whereas the IT expert is able to work with traditional data base structures.}, 
keywords={accounts data processing;ontologies (artificial intelligence);relational databases;simulation languages;software engineering;REA-DSL language;business model driven data-engineering;accounting information system;AIS;resource-event-agent ontology;REA ontology;accounting theory;graphical domain specific modeling language;relational database;domain expert;accounting language;Economics;Marine animals;Business;Ontologies;Unified modeling language;Marketing and sales;Analytical models;REA;domain-specific language;business models;relational schema}, 
doi={10.1109/CEC.2012.12}, 
ISSN={2378-1963}, 
month={Sept},}
@INPROCEEDINGS{7005185, 
author={J. Peltola and S. Sierla and V. Vyatkin}, 
booktitle={Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)}, 
title={Adapting Keyword driven test automation framework to IEC 61131-3 industrial control applications using PLCopen XML}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Factory Acceptance Testing should involve customer's experts and knowledge in defining, reading and validating tests, while keeping labor costs at moderate level. This involvement requires a testing approach, which hides implementation details and emphasizes domain terminology. Keyword driven testing is seen a viable test automation solution to reduce cost and enable customer involvement in acceptance testing. We propose an approach for adaptation of Keyword driven testing framework to IEC 61131-3 industrial process control applications. It utilizes importing of application elements, presented with PLCopen XML, and transforming them to proxy objects to be used as variables in test code, with domain specific names. Benefits include simplification of test and keyword specifications and hiding of implementation details from testers.}, 
keywords={automatic testing;control engineering computing;IEC standards;process control;programmable controllers;quality assurance;XML;factory acceptance testing;customer experts;keyword driven testing;test automation solution;cost reduction;IEC 61131-3 industrial process control applications;PLCopen XML;domain specific names;keyword specifications;Testing;Libraries;XML;Automation;Process control;IEC standards;Radio frequency;Industrial Process Control System;IEC 61131-3;PLCopen XML;Factory Acceptance Testing;Test Automation;Keyword Driven Testing;Test Framework}, 
doi={10.1109/ETFA.2014.7005185}, 
ISSN={1946-0740}, 
month={Sept},}
@INPROCEEDINGS{7928002, 
author={A. Dwarakanath and D. Era and A. Priyadarshi and N. Dubash and S. Podder}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Accelerating Test Automation through a Domain Specific Language}, 
year={2017}, 
volume={}, 
number={}, 
pages={460-467}, 
abstract={Test automation involves the automatic execution of test scripts instead of being manually run. This significantly reduces the amount of manual effort needed and thus is of great interest to the software testing industry. There are two key problems in the existing tools &amp; methods for test automation - a) Creating an automation test script is essentially a code development task, which most testers are not trained on, and b) the automation test script is seldom readable, making the task of maintenance an effort intensive process. We present the Accelerating Test Automation Platform (ATAP) which is aimed at making test automation accessible to non-programmers. ATAP allows the creation of an automation test script through a domain specific language based on English. The English-like test scripts are automatically converted to machine executable code using Selenium WebDriver. ATAP's English-like test script makes it easy for non-programmers to author. The functional flow of an ATAP script is easy to understand as well thus making maintenance simpler (you can understand the flow of the test script when you revisit it many months later). ATAP has been built around the Eclipse ecosystem and has been used in a real-life testing project. We present the details of the implementation of ATAP and the results from its usage in practice.}, 
keywords={program testing;specification languages;Selenium WebDriver;Eclipse ecosystem;ATAP;accelerating test automation platform;code development task;software testing industry;test script execution;domain specific language;test automation;Automation;DSL;Tools;Selenium;Natural languages;Java;Programming;Test automation;Selenium;Xtext;DSL}, 
doi={10.1109/ICST.2017.52}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8241120, 
author={A. Jumagaliyev and J. Whittle and Y. Elkhatib}, 
booktitle={2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)}, 
title={Using DSML for Handling Multi-tenant Evolution in Cloud Applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={272-279}, 
abstract={Multi-tenancy is sharing a single application's resources to serve more than a single group of users (i.e. tenant). Cloud application providers are encouraged to adopt multi-tenancy as it facilitates increased resource utilization and ease of maintenance, translating into lower operational and energy costs. However, introducing multi-tenancy to a single-tenant application requires significant changes in its structure to ensure tenant isolation, configurability and extensibility. In this paper, we analyse and address the different challenges associated with evolving an application's architecture to a multi-tenant cloud deployment. We focus specifically on multi-tenant data architectures, commonly the prime candidate for consolidation and multi-tenancy. We present a Domain-Specific Modeling language (DSML) to model a multi-tenant data architecture, and automatically generate source code that handles the evolution of the application's data layer. We apply the DSML on a representative case study of a single-tenant application evolving to become a multi-tenant cloud application under two resource sharing scenarios. We evaluate the costs associated with using this DSML against the state of the art and against manual evolution, reporting specifically on the gained benefits in terms of development effort and reliability.}, 
keywords={cloud computing;resource allocation;specification languages;DSML;resource utilization;multitenant cloud deployment;multitenant data architecture;multitenant cloud application;multitenancy;resource sharing;multitenant evolution handling;maintenance ease;Domain-Specific Modeling language;Databases;Data models;Load modeling;Computer architecture;Cloud computing;Software as a service;Business}, 
doi={10.1109/CloudCom.2017.31}, 
ISSN={2330-2186}, 
month={Dec},}
@INPROCEEDINGS{8425200, 
author={S. Petruzza and S. Treichler and V. Pascucci and P. Bremer}, 
booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
title={BabelFlow: An Embedded Domain Specific Language for Parallel Analysis and Visualization}, 
year={2018}, 
volume={}, 
number={}, 
pages={463-473}, 
abstract={The rapid growth in simulation data requires large-scale parallel implementations of scientific analysis and visualization algorithms, both to produce results within an acceptable timeframe and to enable in situ deployment. However, efficient and scalable implementations, especially of more complex analysis approaches, require not only advanced algorithms, but also an in-depth knowledge of the underlying runtime. Furthermore, different machine configurations and different applications may favor different runtimes, i.e., MPI vs Charm++ vs Legion, etc., and different hardware architectures. This diversity makes developing and maintaining a broadly applicable analysis software infrastructure challenging. We address some of these problems by explicitly separating the implementation of individual tasks of an algorithm from the dataflow connecting these tasks. In particular, we present an embedded domain specific language (EDSL) to describe algorithms using a new task graph abstraction. This task graph is then executed on top of one of several available runtimes (MPI, Charm++, Legion) using a thin layer of library calls. We demonstrate the flexibility and performance of this approach using three different large scale analysis and visualization use cases, i.e., topological analysis, rendering and compositing dataflow, and image registration of large microscopy scans. Despite the unavoidable overheads of a generic solution, our approach demonstrates performance portability at scale, and, in some cases, outperforms hand-optimized implementations.}, 
keywords={application program interfaces;data visualisation;graph theory;message passing;parallel processing;software libraries;specification languages;MPI;dataflow;embedded domain specific language;task graph abstraction;parallel analysis;large-scale parallel implementations;scientific analysis;machine configurations;hardware architectures;software infrastructure;BabelFlow;visualization algorithms;library calls;visualization use cases;Task analysis;Runtime;Software algorithms;Payloads;Software;Libraries;Rendering (computer graphics);Embedded DSL;user productivity;in situ analytics;Simulation runtime systems;programming models}, 
doi={10.1109/IPDPS.2018.00056}, 
ISSN={1530-2075}, 
month={May},}
@BOOK{7899157, 
author={Marco Brambilla and Jordi Cabot and Manuel Wimmer and Luciano Baresi}, 
booktitle={Model-Driven Software Engineering in Practice: Second Edition}, 
title={Model-Driven Software Engineering in Practice: Second Edition}, 
year={2017}, 
volume={}, 
number={}, 
pages={}, 
abstract={<p>This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE).</p> <p>MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis.</p> <p>The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away.</p> <p>The book is organized into two main parts.</p> <ul> <li>The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios, and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes.</li> <li>The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects.</li> </ul> <p>The second edition of the book features:</p> <ul> <li>a set of completely new topics, including: full example of the creation of a new modeling language (IFML), discussion of modeling issues and approaches in specific domains, like business process modeling, user interaction modeling, and enterprise architecture</li> <li>complete revision of examples, figures, and text, for improving readability, understandability, and coherence</li> <li>better formulation of definitions, dependencies between concepts and ideas</li> <li>addition of a complete index of book content</li> </ul> <p>In addition to the contents of the book, more resources are provided on the book's website http://www.mdse-book.com, including the examples presented in the book.</p>}, 
keywords={modeling;software engineering;UML;domain-specific language;model-driven engineering;code generation;reverse engineering;model transformation;MDD;MDA;MDE;MDSE;OMG;DSL;EMF;Eclipse}, 
doi={}, 
ISSN={}, 
publisher={Morgan & Claypool}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7899157},}
@INPROCEEDINGS{7862407, 
author={J. Wienke and S. Wrede}, 
booktitle={2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)}, 
title={Continuous regression testing for component resource utilization}, 
year={2016}, 
volume={}, 
number={}, 
pages={273-280}, 
abstract={Unintended changes in the utilization of resources like CPU and memory can lead to severe problems for the operation of robotics and intelligent systems. Still, systematic testing for such performance regressions has largely been ignored in this domain. We present a method to specify and execute performance tests for individual components of component-based robotics systems based on their component interfaces. The method includes an automatic analysis of each component revision against previous ones that reports potential changes to the resource usage characteristics. This informs developers about the impact of their changes. We describe the design of the framework and present evaluation results for the automatic detection of performance changes based on tests for a variety of robotics components.}, 
keywords={program testing;robots;continuous regression testing;component resource utilization;performance tests;component-based robotic system;component interfaces;automatic component revision analysis;resource usage characteristics;automatic performance change detection;robotic component;Robots;Testing;Middleware;Intelligent systems;Radiation detectors;Systematics}, 
doi={10.1109/SIMPAR.2016.7862407}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{5626417, 
author={T. Strasser and T. Peters and H. Jägle and E. Zrenner and R. Wilke}, 
booktitle={2010 Annual International Conference of the IEEE Engineering in Medicine and Biology}, 
title={An integrated domain specific language for post-processing and visualizing electrophysiological signals in Java}, 
year={2010}, 
volume={}, 
number={}, 
pages={4687-4690}, 
abstract={Electrophysiology of vision - especially the electroretinogram (ERG) - is used as a non-invasive way for functional testing of the visual system. The ERG is a combined electrical response generated by neural and non-neuronal cells in the retina in response to light stimulation. This response can be recorded and used for diagnosis of numerous disorders. For both clinical practice and clinical trials it is important to process those signals in an accurate and fast way and to provide the results as structured, consistent reports. Therefore, we developed a freely available and open-source framework in Java (http://www.eye.uni-tuebingen.de/project/idsI4sigproc). The framework is focused on an easy integration with existing applications. By leveraging well-established software patterns like pipes-and-filters and fluent interfaces as well as by designing the application programming interfaces (API) as an integrated domain specific language (DSL) the overall framework provides a smooth learning curve. Additionally, it already contains several processing methods and visualization features and can be extended easily by implementing the provided interfaces. In this way, not only can new processing methods be added but the framework can also be adopted for other areas of signal processing. This article describes in detail the structure and implementation of the framework and demonstrate its application through the software package used in clinical practice and clinical trials at the University Eye Hospital Tuebingen one of the largest departments in the field of visual electrophysiology in Europe.}, 
keywords={application program interfaces;data visualisation;electroretinography;Java;medical signal processing;neurophysiology;vision;integrated domain specific language;post-processing;electrophysiological signals;Java;vision;electroretinogram;ERG;neural cells;light stimulation;pipes-and-filters;fluent interfaces;application programming interfaces;smooth learning curve;visualization features;University Eye Hospital Tuebingen;visual electrophysiology;Lead;Java;Artificial neural networks;Hospitals;HTML;Visualization;Algorithms;Computer Graphics;Diagnosis, Computer-Assisted;Electroretinography;Humans;Programming Languages;Retinal Diseases;Software;User-Computer Interface}, 
doi={10.1109/IEMBS.2010.5626417}, 
ISSN={1094-687X}, 
month={Aug},}
@INPROCEEDINGS{6984109, 
author={F. Häser and M. Felderer and R. Breu}, 
booktitle={2014 9th International Conference on the Quality of Information and Communications Technology}, 
title={Test Process Improvement with Documentation Driven Integration Testing}, 
year={2014}, 
volume={}, 
number={}, 
pages={156-161}, 
abstract={Improving the maturity of the test process in an organization, especially but not limited to integration testing, involves obstacles and risks, such as the additional work overhead of the new process. In addition, integration testing descriptions are often too technical not addressing the language needs of the domain. In research cooperations with companies from the insurance and banking domain it turned out that test descriptions and reports are one of the most useful testing artifacts, while doing adhoc testing. This paper presents a bottom up testing approach, which first helps the integration tester in producing a semi-formal test description and report, up to be an enabler for automatic model-based testing in the very end. The presented approach is based on a textual domain specific language that is able to evolve over time. This is done by analyzing the test descriptions and reports automatically with machine learning techniques as well as manually by integration testers. Often recurring test steps or used components are integrated into the test language, making it specially tailored for a specific organization. For each test step implementations can be attached, preparing it for the next iteration. In this paper the methodology and architecture of our integration testing approach are presented together with the underlying language concepts.}, 
keywords={learning (artificial intelligence);program testing;machine learning;textual domain specific language;automatic model-based testing;documentation driven integration testing;test process improvement;Testing;Documentation;Unified modeling language;DSL;Insurance;Companies;Model-Based Integration Testing;Test Process Improvement;Regression Testing}, 
doi={10.1109/QUATIC.2014.29}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{5974321, 
author={I. S. W. B. Prasetya and J. Amorim and T. E. J. Vos and A. Baars}, 
booktitle={6th Iberian Conference on Information Systems and Technologies (CISTI 2011)}, 
title={Using Haskell to script combinatoric testing of Web Services}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The Classification Tree Method (CTM) is a popular approach in functional testing as it allows the testers to systematically partition the input domain of an SUT, and specifies the combinations they want. We have implemented the approach as a small domain specific language (DSL) embedded in the functional language Haskell. Such an embedding leads to clean syntax and moreover we can natively access Haskell's full features. This paper will explain the approach, and how it is applied for testing Web Services.}, 
keywords={pattern classification;program testing;tree data structures;Web services;Haskell;script combinatoric testing;web services;classification tree method;CTM;functional testing;SUT;domain specific language;DSL;functional language Haskell;Cities and towns;Strips;automated testing;combinatoric testing}, 
doi={}, 
ISSN={2166-0727}, 
month={June},}
@INPROCEEDINGS{7323087, 
author={J. Iber and N. Kajtazovic and A. Höller and T. Rauter and C. Kreiner}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Ubtl UML testing profile based testing language}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={The continuous increase of software complexity is one of the major problems associated with the development of today's complex technical systems. In particular, for safety-critical systems, which usually require to be thoroughly verified and validated, managing such a complexity is of high importance. To this end, industry is utilizing Model-Driven Development (MDD) in many aspects of systems engineering, including verification and validation activities. Until now many specifications and standards have been released by the MDD community to support those activities by putting models in focus. The general problem is, however, that applying those specifications is often difficult, since they comprise a broader scope than usually required to solve specific problems. In this paper we propose a domain-specific language (DSL) that allows to specify tests from the UML Testing Profile (UTP). The main contribution is that only particular aspects of UTP are captured, thereby allowing the MDD process to be narrowed to specific needs, such as supporting code generation facilities for certain types of tests or even specific statements in tests. In the end we show the application of the DSL using a simple example within a MDD process, and we report on performance of that process.}, 
keywords={program compilers;program testing;program verification;safety-critical software;software metrics;Unified Modeling Language;code generation facilities;UTP;test specification;DSL;domain-specific language;validation activity;verification activity;systems engineering;MDD;model-driven development;safety-critical systems;complex technical systems;software complexity;testing language;UML testing profile;Ubtl;Unified modeling language;Testing;Generators;Biological system modeling;DSL;Software;Concrete;UML Testing Profile;UML;Textual Domain-Specific Language;Test Specification Language;Software Testing;Model-Driven Development}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{6058739, 
author={W. J. Headrick and M. A. Bodkin and R. R. Fox and T. W. Davis and K. Dusch and D. Wolfe}, 
booktitle={2011 IEEE AUTOTESTCON}, 
title={Signal Based Domain Specific Language (SBDSL) a proposal for a next generation test}, 
year={2011}, 
volume={}, 
number={}, 
pages={240-244}, 
abstract={Signal Based Domain Specific Language (SBDSL) is a domain specific language which combines the use of ATLAS Signal statements with high-level programming language constructs. The goals of this new language are: facilitate the writing of concurrent test programs, provide a language that is easy to extend with new constructs, maintain backwards compatibility with ATLAS Family of languages, enable interoperability between test stations, and enable engineers' fresh out of college to quickly become productive with a test programming language. This paper will cover how the design of the SBDSL language, SBDSL Integrated Development Environment (IDE) and runtime executable will accomplish these goals and present results from the technology demonstration developed.}, 
keywords={open systems;program testing;specification languages;signal based domain specific language;next generation test;ATLAS signal statements;high level programming language constructs;concurrent test programs;backwards compatibility;ATLAS language family;interoperability;test programming language;SBDSL integrated development environment;Hardware;Instruments;Visualization;Syntactics;Debugging;Programming;Libraries}, 
doi={10.1109/AUTEST.2011.6058739}, 
ISSN={1558-4550}, 
month={Sept},}
@INPROCEEDINGS{802127, 
author={A. A. Reyes and D. Richardson}, 
booktitle={14th IEEE International Conference on Automated Software Engineering}, 
title={Siddhartha: a method for developing domain-specific test driver generators}, 
year={1999}, 
volume={}, 
number={}, 
pages={81-90}, 
abstract={Siddhartha applies the domain-specific language (DSL) paradigm to solve difficult problems in specification-based testing (SBT). Domain-specific test case data specifications (TestSpecs) and difficult-to-test program design styles engender difficult SBT problems, which are the essential phenomena of interest to Siddhartha. Difficult-to-test program design styles are explicitly represented by domain-specific, unit test driver reference designs that accommodate the problematic program design styles. DSLs are developed to represent both TestSpecs and Driver reference designs. A DSL language processing tool (a translator) is developed that maps TestSpecs into Drivers. We developed a prototype implementation of Siddhartha via Reasoning SDK (formerly known as Software Refinery) and developed two domain-specific TestSpec/spl rarr/Driver translators. Each translator generated Drivers that revealed new failures in a real-world digital flight control application program.}, 
keywords={formal specification;program testing;automatic testing;program interpreters;aerospace computing;Siddhartha;domain-specific language paradigm;domain-specific test driver generator development;specification-based testing;domain-specific test case data specifications;difficult-to-test program design styles;domain-specific unit test driver reference designs;TestSpecs;Driver reference designs;DSL language processing tool;translator;Reasoning SDK;real-world digital flight control application program;Automatic testing;Application software;DSL;Aerospace control;Computer science;Aerospace electronics;Domain specific languages;Prototypes;Formal specifications;Automatic control}, 
doi={10.1109/ASE.1999.802127}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4052587, 
author={C. Yilmaz and A. Porter and A. S. Krishna and A. M. Memon and D. C. Schmidt and A. S. Gokhale and B. Natarajan}, 
journal={IEEE Transactions on Software Engineering}, 
title={Reliable Effects Screening: A Distributed Continuous Quality Assurance Process for Monitoring Performance Degradation in Evolving Software Systems}, 
year={2007}, 
volume={33}, 
number={2}, 
pages={124-141}, 
abstract={Developers of highly configurable performance-intensive software systems often use in-house performance-oriented "regression testing" to ensure that their modifications do not adversely affect their software's performance across its large configuration space. Unfortunately, time and resource constraints can limit in-house testing to a relatively small number of possible configurations, followed by unreliable extrapolation from these results to the entire configuration space. As a result, many performance bottlenecks escape detection until systems are fielded. In our earlier work, we improved the situation outlined above by developing an initial quality assurance process called "main effects screening". This process 1) executes formally designed experiments to identify an appropriate subset of configurations on which to base the performance-oriented regression testing, 2) executes benchmarks on this subset whenever the software changes, and 3) provides tool support for executing these actions on in-the-field and in-house computing resources. Our initial process had several limitations, however, since it was manually configured (which was tedious and error-prone) and relied on strong and untested assumptions for its accuracy (which made its use unacceptably risky in practice). This paper presents a new quality assurance process called "reliable effects screening" that provides three significant improvements to our earlier work. First, it allows developers to economically verify key assumptions during process execution. Second, it integrates several model-driven engineering tools to make process configuration and execution much easier and less error prone. Third, we evaluate this process via several feasibility studies of three large, widely used performance-intensive software frameworks. Our results indicate that reliable effects screening can detect performance degradation in large-scale systems more reliably and with significantly less resources than conventional techniques}, 
keywords={program testing;software performance evaluation;software quality;software reliability;reliable effects screening;distributed continuous quality assurance process;performance degradation monitoring;evolving software systems;performance intensive software systems;regression testing;software performance;in house testing;performance bottlenecks;main effects screening;configuration subset;software benchmarks;tool support;process configuration;process execution;Quality assurance;Monitoring;Degradation;Software systems;Performance evaluation;Software performance;Software testing;System testing;Time factors;Extrapolation;Distributed continuous quality assurance;performance-o-ri-ented regression testing;design-of-experiments theory.}, 
doi={10.1109/TSE.2007.20}, 
ISSN={0098-5589}, 
month={Feb},}
@INPROCEEDINGS{6233406, 
author={1. Landhäußer and A. Genaid}, 
booktitle={2012 Third International Workshop on Recommendation Systems for Software Engineering (RSSE)}, 
title={Connecting User Stories and code for test development}, 
year={2012}, 
volume={}, 
number={}, 
pages={33-37}, 
abstract={User Stories are short feature descriptions from the user's point of view. Functional tests ensure that the feature described by a User Story is fully implemented. We present a tool that builds an ontology for code and links completed User Stories in natural language with the related code artifacts. The ontology also contains links to API components that were used to implement the functional tests. Preliminary results show that these links can be used to recommend reusable test steps for new User Stories.}, 
keywords={application program interfaces;functional programming;natural languages;ontologies (artificial intelligence);program testing;software prototyping;software reusability;functional testing;feature description;user story;ontology;natural language;code artifact;API component;reusable testing;Ontologies;Natural languages;Software;Data structures;Boolean functions;Compounds;Testing;code mining;functional testing;reasoning;traceability;ontology}, 
doi={10.1109/RSSE.2012.6233406}, 
ISSN={2327-0934}, 
month={June},}
@INPROCEEDINGS{8115712, 
author={L. Meftah and M. Gomez and R. Rouvoy and I. Chrisment}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={ANDROFLEET: Testing WiFi peer-to-peer mobile apps in the large}, 
year={2017}, 
volume={}, 
number={}, 
pages={961-966}, 
abstract={WiFi P2P allows mobile apps to connect to each other via WiFi without an intermediate access point. This communication mode is widely used by mobile apps to support interactions with one or more devices simultaneously. However, testing such P2P apps remains a challenge for app developers as i) existing testing frameworks lack support for WiFi P2P, and ii) WiFi P2P testing fails to scale when considering a deployment on more than two devices. In this paper, we therefore propose an acceptance testing framework, named Androfleet, to automate testing of WiFi P2P mobile apps at scale. Beyond the capability of testing point-to-point interactions under various conditions, An-drofleet supports the deployment and the emulation of a fleet of mobile devices as part of an alpha testing phase in order to assess the robustness of a WiFi P2P app once deployed in the field. To validate Androfleet, we demonstrate the detection of failing black-box acceptance tests for WiFi P2P apps and we capture the conditions under which such a mobile app can correctly work in the field. The demo video of Androfleet is made available from https://youtu.be/gJ5_Ed7XL04.}, 
keywords={mobile computing;peer-to-peer computing;program testing;wireless LAN;P2P testing;acceptance testing framework;An-drofleet;point-to-point interaction testing;WiFi peer-to-peer mobile application testing;black-box acceptance tests;Androfleet;alpha testing phase;mobile devices;Wireless fidelity;Peer-to-peer computing;Testing;Mobile communication;Androids;Humanoid robots;Mobile handsets}, 
doi={10.1109/ASE.2017.8115712}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{713476, 
author={F. H. Daou}, 
booktitle={1998 IEEE AUTOTESTCON Proceedings. IEEE Systems Readiness Technology Conference. Test Technology for the 21st Century (Cat. No.98CH36179)}, 
title={Overview of ADSL test requirement towards conformance, performance and interoperability}, 
year={1998}, 
volume={}, 
number={}, 
pages={413-420}, 
abstract={The enormous installed base of copper in the access network with the right transmission techniques present a huge potential for delivering broadband services to bandwidth hungry customers. Various Digital Subscriber line technologies (xDSL) employ various transmission methods and efficiently utilize the last available bandwidth on existing copper wires. Asymmetric Digital Subscriber Line (ADSL) delivers up to 6 Mbps to the user. This transmission of 6 Mbps is achieved using sophisticated modulation and compression techniques in a spectrum up to 1.1 MHz, pushing the physical limit on the usable bandwidth in the copper. This technology co-exists with impairments, noise intrusions, bridge taps, and other non-spectrally compatible transmissions. This paper presents an overview of xDSL Technology (which includes ADSL), the test challenges facing ADSL technology, and outlines the three areas of ADSL tests needed to ensure product conformance and cross vendor interoperability.}, 
keywords={digital subscriber lines;telecommunication equipment testing;conformance testing;automatic test equipment;automatic testing;open systems;ADSL test requirement;conformance;performance;interoperability;broadband services;available bandwidth;copper wires;asymmetric DSL;usable bandwidth;noise intrusions;impairments;bridge taps;nonspectrally compatible transmissions;test challenges;CAP/QAM spectral separation;compliance;functional tester;ATE;6 Mbit/s;Testing;Copper;Bandwidth;Bit rate;OFDM modulation;Quadrature amplitude modulation;DSL;Modulation coding;Working environment noise;Phase modulation}, 
doi={10.1109/AUTEST.1998.713476}, 
ISSN={1088-7725}, 
month={Aug},}
@INPROCEEDINGS{7437001, 
author={N. Hafidhoh and I. Liem and F. N. Azizah}, 
booktitle={2015 International Conference on Data and Software Engineering (ICoDSE)}, 
title={Source code generator for automating business rule implementation}, 
year={2015}, 
volume={}, 
number={}, 
pages={219-224}, 
abstract={Business rules can be implemented on business processes, business behavior, people, or software in an organization. Aligned with software development, business rules are captured from requirement elicitation and analysis, then designed and implemented in the software. The changes of business environment may affect business rules. The changes of the business rules may bring impact in the software, so that the software needs to be redeveloped. In this paper, we present a source code generator to automate business rule implementation using business rule approach. We propose a Domain Specific Language (DSL) of business rule to help expressing business rules in a business-friendly language. We also develop a business rule generator to generate source codes based on a DSL script for expressing the business rules. The proposed solution has been tested in two case studies. It is shown that the generator can help the implementation of the business rules in source codes and that the generated code can be used in business applications.}, 
keywords={business data processing;formal specification;source code (software);source code generator;business rule implementation automation;software development;requirement elicitation;requirement analysis;business environment;domain-specific language;business-friendly language;business rule generator;DSL script;DSL;Natural languages;Software;Generators;Organizations;Grammar;business rule;implementation;source code generator;DSL}, 
doi={10.1109/ICODSE.2015.7437001}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4600272, 
author={S. M. R. Sadri and Y. N. Harandi and M. Pirhadi and M. Y. Waskasi and A. I. Tabrizipoor and M. Mirzabaghi}, 
booktitle={2007 International Symposium on High Capacity Optical Networks and Enabling Technologies}, 
title={Test strategy for DSL broadband IP access services}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={In this paper the test methodology has been expanded for evaluation of DSL broadband services. This is a new strategy for testing and considering different aspects of test. This strategy can precisely test various features of DSL broadband equipment and services which can be delivered by them in terms of different aspects of testing such as functionality, performance, conformance, etc. It was practically executed over a designed testbed in Iran Telecommunications Research Center (ITRC) where a lab named NGN Pilot exists. The introduced strategy has a hierarchical structure from the topmost level of testing including different categories to the bottommost level including detailed test topics and test cases. This paper elaborates how the strategy was designed and how it can be applied to a desired environment to compare different DSL scenarios with each other. The strategy can then be used by a service provider to help deciding which solutions can achieve the best outcome.}, 
keywords={broadband networks;digital subscriber lines;IP networks;DSL broadband service;IP access services;DSL broadband equipment;Iran Telecommunications Research Center;NGN Pilot;hierarchical structure;service provider;Modems;Broadband communication;IP networks;Multiprotocol label switching;Throughput;Quality of service;Next generation networking;DSL;Broadband Access;Test Strategy;Testbed;Pilot}, 
doi={10.1109/HONET.2007.4600272}, 
ISSN={1949-4092}, 
month={Nov},}
@INPROCEEDINGS{7985680, 
author={R. Rolim and G. Soares and L. D'Antoni and O. Polozov and S. Gulwani and R. Gheyi and R. Suzuki and B. Hartmann}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)}, 
title={Learning Syntactic Program Transformations from Examples}, 
year={2017}, 
volume={}, 
number={}, 
pages={404-415}, 
abstract={Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.}, 
keywords={automatic programming;program processors;syntactic program transformations learning;REFAZER;programming-by-example methodology;domain-specific language;domain-specific deductive algorithms;DSL;code edits;C# open-source projects;DSL;Programming profession;Tools;C# languages;Pattern matching;Open source software;Program transformation;program synthesis;tutoring systems;refactoring}, 
doi={10.1109/ICSE.2017.44}, 
ISSN={1558-1225}, 
month={May},}
@BOOK{6813569, 
author={Marco Brambilla and Jordi Cabot and Manuel Wimmer}, 
booktitle={Model-Driven Software Engineering in Practice}, 
title={Model-Driven Software Engineering in Practice}, 
year={2012}, 
volume={}, 
number={}, 
pages={}, 
abstract={This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The book is targeted to a diverse set of readers, spanning: professionals, CTOs, CIOs, and team managers that need to have a bird's eye vision on the matter, so as to take the appropriate decisions when it comes to choosing the best development techniques for their company or team; software analysts, developers, or designers that expect to use MDSE for improving everyday work productivity, either by applying the basic modeling techniques and notations or by defining new domain-specific modeling languages and applying end-to-end MDSE practices in the software factory; and academic teachers and students to address undergrad and postgrad courses on MDSE. In addition to the contents of the book, more resources are provided on the book's website, including the examples presented in the book. Table of Contents: Introduction / MDSE Principles / MDSE Use Cases / Model-Driven Architecture (MDA) / Integration of MDSE in your Development Process / Modeling Languages at a Glance / Developing your Own Modeling Language / Model-to-Model Transformations / Model-to-Text Transformations / Managing Models / Summary}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Morgan & Claypool}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6813569},}
@INPROCEEDINGS{5261058, 
author={C. McMahon}, 
booktitle={2009 Agile Conference}, 
title={History of a Large Test Automation Project Using Selenium}, 
year={2009}, 
volume={}, 
number={}, 
pages={363-368}, 
abstract={In 2007 I started work as a tester for a company called Socialtext. When I joined the company there was already a Selenium-based test framework in place, but there were only a couple of automated test cases created; we had about 400 test steps, or individual assertions about the behavior of the application. When I left Socialtext two years later, we had just surpassed 10,000 test steps in the main set of regression tests. We also had browser-specific test sets in place, an automated test case for visually checking the application, and a Continuous-Integration-like script that ran all day and all night against the latest version of the code. At about 4000 test steps, regression bugs released to production dropped essentially to zero. The other 6000 test steps covered ongoing new features in the project, and more robust testing of the older application functions. This report discusses how I helped grow this system, and the things we learned along the way that helped it be such a successful ongoing project. The report covers initial conditions and test design; discusses issues in application feature coverage; how and when to grow the system quickly; a couple of test design smells that caused us problems along the way; how we treat Continuous Integration in a system like this; and how we coped when significant parts of the User Interface were completely re-engineered.}, 
keywords={program testing;regression analysis;large test automation project;selenium;Socialtext;regression tests;browser specific test sets;continuous-integration-like script;robust testing;application function;application feature coverage;test design;continuous integration;user interface;History;Automatic testing;System testing;Quality management;Failure analysis;User interfaces;Engineering profession;Home automation;Radio access networks;Computer bugs;UI automation;Selenium;test design;experience}, 
doi={10.1109/AGILE.2009.9}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8101621, 
author={O. Olajubu and S. Ajit and M. Johnson and S. Thomson and M. Edwards and S. Turner}, 
booktitle={2017 9th Computer Science and Electronic Engineering (CEEC)}, 
title={Automated test case generation from high-level logic requirements using model transformation techniques}, 
year={2017}, 
volume={}, 
number={}, 
pages={178-182}, 
abstract={It is not uncommon for industries to use natural language to represent high-level software requirement specifications. It is also not uncommon for these requirement specifications to be translated into design and used further for implementation and generation of test cases in the software engineering life-cycle. These requirements are often ambiguous, incorrect, and incomplete. Finding them late in the development lifecycle proves very expensive and lowers the productivity. This paper reports on the experience of applying model-based technologies from academia to a real-world problem domain in the aviation industry to improve the productivity. The paper focuses on the application of a model-based technique to automatically generate test cases to satisfy Modified Condition/Decision Coverage (MC/DC) from high-level logic requirements expressed in a Domain Specific Language (DSL).}, 
keywords={automatic testing;formal specification;program testing;specification languages;high-level software requirement specifications;software engineering life-cycle;high-level logic requirements;Domain Specific Language;automated test case generation;model transformation techniques;natural language;productivity;model-based technique;decision coverage;modified condition;Legged locomotion;DSL;Software;Testing;Industries;Productivity;Natural languages}, 
doi={10.1109/CEEC.2017.8101621}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7102636, 
author={S. Herbold and A. De Francesco and J. Grabowski and P. Harms and L. M. Hillah and F. Kordon and A. Maesano and L. Maesano and C. Di Napoli and F. De Rosa and M. A. Schneider and N. Tonellotto and M. Wendland and P. Wuillemin}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={The MIDAS Cloud Platform for Testing SOA Applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={While Service Oriented Architectures (SOAs) are for many parts deployed online, and today often in a cloud, the testing of the systems still happens mostly locally. In this paper, we want to present the MIDAS Testing as a Service (TaaS), a cloud platform for the testing of SOAs. We focus on the testing of whole SOA orchestrations, a complex task due to the number of potential service interactions and the increasing complexity with each service that joins an orchestration. Since traditional testing does not scale well with such a complex setup, we employ a Model-based Testing (MBT) approach based on the Unified Modeling Language (UML) and the UML Testing Profile (UTP) within MIDAS. Through this, we provide methods for functional testing, security testing, and usage-based testing of service orchestrations. Through harnessing the computational power of the cloud, MIDAS is able to generate and execute complex test scenarios which would be infeasible to run in a local environment.}, 
keywords={cloud computing;program testing;service-oriented architecture;Unified Modeling Language;MIDAS cloud platform;SOA application testing;service oriented architecture;MIDAS testing-as-a-service;TaaS platform;SOA orchestration;model-based testing approach;MBT approach;unified modeling language;UML testing profile;functional testing;security testing;usage-based testing;Testing;Unified modeling language;DSL;Cloud computing;Service-oriented architecture;Monitoring}, 
doi={10.1109/ICST.2015.7102636}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{5069041, 
author={T. Clark}, 
booktitle={2009 ICSE Workshop on Automation of Software Test}, 
title={Model based functional testing using pattern directed filmstrips}, 
year={2009}, 
volume={}, 
number={}, 
pages={53-61}, 
abstract={Model driven functional system testing generates test scenarios from behavioural and structural models. In order to autmatically generate tests, conditions such as invariants and pre-/post-conditions must be precisely defined. UML provides the Object Constraint Language (OCL) for this purpose; however OCL expressions can become very complex. This paper describes an approach that allows many commonly found OCL patterns to be expressed as snapshot patterns that correspond directly to the information model diagrams. Behaviour is constructed as chains of snapshots, or filmstrips. Snapshots and filmstrips are as expressive as UML behaviour models and OCL but it is argued that they are more accessible and more modular.}, 
keywords={program testing;Unified Modeling Language;pattern directed filmstrips;model driven functional system testing;Object Constraint Language;UML;Snapshots;System testing;Unified modeling language;Logic testing;Software testing;Context modeling;Software engineering;Test pattern generators;Process design;Concrete;Graphical user interfaces}, 
doi={10.1109/IWAST.2009.5069041}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7095983, 
author={M. Kayama and S. Ogata and T. Nagai and H. Yokoka and K. Masumoto and M. Hashimoto}, 
booktitle={2015 IEEE Global Engineering Education Conference (EDUCON)}, 
title={Effectiveness of Model-Driven Development in conceptual modeling education for university freshmen}, 
year={2015}, 
volume={}, 
number={}, 
pages={274-282}, 
abstract={The purpose of this study is to explore educational methods for conceptual modeling for novices. In this research, the subjects are mainly freshmen in university. Model driven development (MDD) and a domain specific language (DSL) are key factors in this study. By using MDD, learners are expected to be able to evaluate their own model by observing the target device's behavior. By using a DSL, teachers can control the difficulty of the problems given to their learners. In this paper, we describe our research approach using MDD and a DSL, then, show our experiment design and results. We also discuss the effectiveness of MDD in university freshmen courses with proposed educational methodology.}, 
keywords={computer science education;educational courses;educational institutions;model-driven development;conceptual modeling education;domain specific language;DSL;MDD;university freshmen courses;educational methodology;Unified modeling language;Object oriented modeling;DSL;Computational modeling;Robots;Conferences;Analytical models;model driven development;university freshmen;state machine diagram;model quality;achievment level}, 
doi={10.1109/EDUCON.2015.7095983}, 
ISSN={2165-9567}, 
month={March},}
@INPROCEEDINGS{6498461, 
author={E. Duclos and S. Le Digabel and Y. Guéhéneuc and B. Adams}, 
booktitle={2013 17th European Conference on Software Maintenance and Reengineering}, 
title={ACRE: An Automated Aspect Creator for Testing C++ Applications}, 
year={2013}, 
volume={}, 
number={}, 
pages={121-130}, 
abstract={We present ACRE, an Automated aspect creator, to use aspect-oriented programming (AOP) to perform memory, invariant and interferences testing for software programs written in C++. ACRE allows developers without knowledge in AOP to use aspects to test their programs without modifying the behavior of their source code. ACRE uses a domain-specific language (DSL), which statements testers insert into the source code like comments to describe the aspects to be used. The presence of DSL statements in the code does not modify the program's compilation and behavior. ACRE parses the DSL statements and automatically generates appropriate aspects that are then weaved into the source code to identify bugs due to memory leaks, incorrect algorithm implementation, or interference among threads. Thanks to the use of aspects and ACRE, testers can add or remove tests easily. Using an aspect generated by ACRE, we find a memory leak in a complex C++ software program, NOMAD, used in both industry and research. We also verify a crucial mathematical point of the algorithm behind NOMAD and collect data to find possible interference bugs, in NOMAD.}, 
keywords={aspect-oriented programming;C++ language;program compilers;program debugging;program testing;ACRE;automated aspect creator;C++ application testing;aspect-oriented programming;AOP;memory testing;invariant testing;interferences testing;software program;domain-specific language;statements tester;source code;DSL statement;program compilation;bug identification;memory leak;NOMAD;Testing;DSL;Interference;Computer bugs;Radiation detectors;Java;Weaving;AOP;C++;NOMAD;interference bug pattern;memory testing;invariant testing}, 
doi={10.1109/CSMR.2013.22}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7323106, 
author={K. Beckmann}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Integrating existing proprietary system models into a model-driven test process for an industrial automation scenario}, 
year={2015}, 
volume={}, 
number={}, 
pages={255-262}, 
abstract={The introduction of modern model-driven software development methodologies into the industrial practise still proves to be a challenge. Especially small or medium-sized enterprises (SMEs) need an incremental and continuous modernisation process, which incorporates existing projects, is customised and cost-effective. Particularly, suitable solutions for model-based or -driven testing with test automation to increase the efficiency are in demand. This paper presents an approach for integrating existing proprietary system models of an SME partner for describing industrial automation processes into a model-driven test process, utilising a domain-specific language for the test specification. The test objectives focuses on the correct implementation of the communication and synchronisation of distributed state machines. The presented approach is integrated into a test framework, which is based on the Eclipse Modelling Framework (EMF) and the Eclipse Test and Performance Tools Platform Project (TPTP) framework. To separate the possibly changeable system and DSL-specific models from the implementation of the test framework, a stable and more generic test meta model was defined.}, 
keywords={factory automation;program testing;small-to-medium enterprises;software engineering;proprietary system model;model-driven test process;industrial automation;modern model-driven software development;small or medium-sized enterprises;SME;domain-specific language;test specification;eclipse modelling framework;EMF;eclipse test and performance tools platform project;generic test meta model;Unified modeling language;Adaptation models;DSL;Object oriented modeling;Biological system modeling;Software;Automation;MDSD;DSL;Metamodelling;Testing;MDT;Model-driven Testing}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7102598, 
author={E. Alegroth and G. Bache and E. Bache}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={On the Industrial Applicability of TextTest: An Empirical Case Study}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Software systems are becoming more complex, not least in their Graphical User Interfaces (GUIs), which presents challenges for existing testing practices. Pressure to reduce time to market leaves less time for manual testing and increases the importance of test automation. Previous research has identified several generations of automated GUI-based test approaches with different cost-benefit tradeoffs. Whilst test automation provides fast quality feedback it can be associated with high costs and inability to identify defects not explicitly anticipated by the test designer. TextTest is a capture-replay tool for GUI-based testing with a novel approach that overcomes several of the challenges experienced with previous approaches. Firstly the tool supports Approval Testing, an approach where ASCII-art representations of the GUI's visual state are used to verify correct application behavior at the system level. Secondly it records and replays test scripts in a user defined domain specific language (DSL) that is readable by all stakeholders. In this paper we present a three phase industrial case study that aims to identify TextTest's applicability in industrial practice. The paper reports that the tool is associated with (1) low script development costs due to recording functionality, (2) low maintenance costs, on average 7 minutes per test case, (3) better defect finding ability than manual system testing, (4) high test case execution performance (In this case 500 test cases in 20 minutes), (5) high script readability due to DSL defined scripts, and (6) test suites that are robust to change (In this case 93 percent per iteration). However, the tool requires a higher degree of technical skill for customization work, test maintainers need skills in designing regular expressions and the tool's applicability is currently restricted to Java and Python based applications.}, 
keywords={graphical user interfaces;Java;program testing;software tools;TextTest;software systems;graphical user interfaces;test automation;automated GUI-based testing;capture-replay tool;approval testing;ASCII-art representations;user defined domain specific language;test case execution performance;DSL defined scripts;Java based applications;Python based applications;Graphical user interfaces;Testing;Maintenance engineering;DSL;Companies;Manuals;Data collection}, 
doi={10.1109/ICST.2015.7102598}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{4547343, 
author={T. Syed and S. R. Das and S. N. Biswas and E. M. Petriu}, 
booktitle={2008 IEEE Instrumentation and Measurement Technology Conference}, 
title={Developing Automated Test System for ADSL Equipment}, 
year={2008}, 
volume={}, 
number={}, 
pages={1833-1838}, 
abstract={The requirement for an automated test system has immensely increased due to the realization that manual testing is associated with additional resources and staffing constraints. In order to achieve a competitive edge, reduced development cost, timely product delivery, and product quality are mandatory in today's organization. Manual testing requires skilled operators that increase cost, time, and product delivery. The low cost computer based automated system helps to get an edge by fulfilling these organizational demands. In this paper, an automated testing system has been developed to support functional testing of Nortel Network's modem system (1-Meg SUT). The modem is an inherently complex asymmetric digital subscriber line (ADSL) product and its testing is far more complex than just verification of process faults. The complexity of ADSL system renders automated test system an important and imperative part of ADSL testing. The subject paper demonstrates the indispensable need of automated test system for ADSL testing and its advantages in providing a competitive edge for the organization.}, 
keywords={automatic testing;digital subscriber lines;modems;automated test system;ADSL equipment;product quality;timely product delivery;manual testing;functional testing;Nortel Network modem system;asymmetric digital subscriber line product;process faults;Automatic testing;System testing;Automation;Modems;Costs;DSL;Information technology;Internet;Graphical user interfaces;Software testing;Asymmetric digital subscriber line (ADSL);automated testing;hardware platform;software platform}, 
doi={10.1109/IMTC.2008.4547343}, 
ISSN={1091-5281}, 
month={May},}
@ARTICLE{8306153, 
author={M. Bures and K. Frajtak and B. S. Ahmed}, 
journal={IEEE Transactions on Reliability}, 
title={Tapir: Automation Support of Exploratory Testing Using Model Reconstruction of the System Under Test}, 
year={2018}, 
volume={67}, 
number={2}, 
pages={557-580}, 
abstract={For a considerable number of software projects, the creation of effective test cases is hindered by design documentation that is either lacking, incomplete, or obsolete. The exploratory testing approach can serve as a sound method in such situations. However, the efficiency of this testing approach strongly depends on the method, the documentation of explored parts of a system, the organization and distribution of work among individual testers on a team, and the minimization of potential (very probable) duplicities in performed tests. In this paper, we present a framework for replacing and automating a portion of these tasks. A screen-flow-based model of the tested system is incrementally reconstructed during the exploratory testing process by tracking testers' activities. With additional metadata, the model serves for an automated navigation process for a tester. Compared with the exploratory testing approach, which is manually performed in two case studies, the proposed framework allows the testers to explore a greater extent of the tested system and enables greater detection of the defects present in the system. The results show that the time efficiency of the testing process improved with the framework support. This efficiency can be increased by team-based navigational strategies that are implemented within the proposed framework, which is documented by another case study presented in this paper.}, 
keywords={program testing;automated navigation process;exploratory testing approach;model reconstruction;sound method;software projects;Testing;Navigation;Lead;Documentation;Browsers;Web pages;Automation;Functional testing;generation of test cases from model;model reengineering;model-based testing (MBT);system under test (SUT) model;web applications testing}, 
doi={10.1109/TR.2018.2799957}, 
ISSN={0018-9529}, 
month={June},}
@INPROCEEDINGS{8369563, 
author={T. Glock and B. Sillman and M. Kobold and S. Rebmann and E. Sax}, 
booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
title={Model-based validation and testing of industry 4.0 plants}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={One of the major technical aspects of Industry 4.0 (I4.0) is the decentralized task and function distribution based on a service-oriented approach. This leads to new challenges by using the existing technologies and planning methods of I4.0 industrial plants. The validation of such dynamic service-oriented architectures requires new validation methods to ensure the correct functionality of the I4.0 plant. The interactions of distributed functions and the integration of functions on several devices play an important role in planning, configuration, and validation of such plants. This work presents a new approach of a continuous model-based validation and testing method of I4.0 plant service-oriented architectures. The method allows to reuse existing functional tests of services and to adjust them with the help of the model-based information of a plant architecture model.}, 
keywords={industrial plants;production engineering computing;production planning;service-oriented architecture;distributed functions;testing method;plant architecture model;planning methods;dynamic service-oriented architectures;continuous model-based validation;industry 4.0 plants;Tools;Unified modeling language;Testing;Computer architecture;Sensors;Industries;Planning;Functional Testing of plants;industry 4.0;service-oriented;distributed systems;reuse of tests;validation of I4.0 plant systems}, 
doi={10.1109/SYSCON.2018.8369563}, 
ISSN={2472-9647}, 
month={April},}
@INPROCEEDINGS{4839232, 
author={J. Díaz and A. Yagüe and J. Garbajosa}, 
booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems}, 
title={A Systematic Process for Implementing Gateways for Test Tools}, 
year={2009}, 
volume={}, 
number={}, 
pages={58-66}, 
abstract={Test automation is facing a new challenge because tools, as well as having to provide conventional test functionalities, must be capable to interact with ever more heterogeneous complex systems under test (SUT). The number of existing software interfaces to access these systems is also a growing number. The problem cannot be analyzed only from a technical or engineering perspective; the economic perspective is as important. This paper presents a process to systematically implement gateways which support the communication between test tools and SUTs with a reduced cost. The proposed solution does not preclude any interface protocol at the SUT side. This process is supported using a generic architecture of a gateway defined on top of OSGi. Any test tool can communicate with the gateway through a unique defined interface. To communicate the gateway and the SUT, basically, the driver corresponding to the SUT software interface has to be loaded.}, 
keywords={computer testing;internetworking;network servers;software architecture;software tools;systematic process;gateways;systems under test;cost reduction;software interface protocol;OSGi;System testing;Automatic testing;Costs;Service oriented architecture;Home automation;Internet;Radiofrequency identification;Intelligent sensors;Software testing;Web services;test automation;gateway;complex systems testing;acceptance testing tools;OSGI;TOPEN}, 
doi={10.1109/ECBS.2009.40}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7980397, 
author={A. Contan and C. Dehelean and L. Miclea}, 
booktitle={2017 14th International Conference on Engineering of Modern Electric Systems (EMES)}, 
title={Applying coding systems in the process of testing software applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={127-131}, 
abstract={The challenges met during the software projects fall into any number of categories. The development and the technical solutions bring about technical challenges, but the situations one is confronted with, may also be sociological, psychological or managerial in nature. Without any knowledge in the field of social sciences, the programmers, testers and managers might interpret the social aspects of the project improperly, and such interpretations lead to the inability to fully understand the problem and, ultimately, to inefficiency in the decision-making process. Furthermore, solid knowledge of theories in the area of the social sciences is required for a better understanding of both the context in which the application runs and of the final users who will use the developed project. The understanding of and the involvement in a software acceptance testing (SAT) project, requires the combination of multiple theories and principles from different disciplines.}, 
keywords={program testing;software management;source code (software);coding systems;software application testing;software project social aspects;decision-making process;software acceptance testing project;SAT project;Encoding;Testing;Software quality;Software engineering;Computational modeling;Psychology;coding models;software testing;software testing;social science}, 
doi={10.1109/EMES.2017.7980397}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7321521, 
author={M. Schuts and J. Hooman}, 
booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
title={Using Domain Specific Languages to improve the development of a power control unit}, 
year={2015}, 
volume={}, 
number={}, 
pages={781-788}, 
abstract={To improve the design of a power control unit at Philips, two Domain Specific Languages (DSLs) have been used. The first DSL provides a concise and readable notation for the essential state transitions. It is used to generate both configuration files and analysis models. In addition, we also generate instances of a second DSL which represents test traces. This second DSL is used to generate test cases for the power control unit. The use of DSLs not only improved productivity, but also the quality of the configuration files and the test set.}, 
keywords={medical computing;domain specific language;power control unit;Philips;DSL;DSL;Power control;Hardware;Generators;X-ray imaging;Software;Voltage control}, 
doi={10.15439/2015F46}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7809817, 
author={D. Ratiu and M. Voelter}, 
booktitle={2016 IEEE/ACM 11th International Workshop in Automation of Software Test (AST)}, 
title={Automated Testing of DSL Implementations - Experiences from Building mbeddr}, 
year={2016}, 
volume={}, 
number={}, 
pages={15-21}, 
abstract={Domain specific languages promise to improve productivity and quality of software by providing problem-adequate abstractions to developers. Projectional language workbenches like JetBrains MPS allow the definition of modular and extensible domain specific languages, generators and development environments. While recent advances in language engineering have enabled the definition of DSLs and tooling in a modular and cost-effective manner, the quality assurance of their implementation is still challenging. In this paper we present our work on testing the implementation of domain specific languages and associated tools, and discuss different approaches to increase the automation of language testing. We illustrate this based on MPS and our experience with testing mbeddr, a set of domain specific languages and tools on top of C tailored to embedded software development.}, 
keywords={embedded systems;program testing;software quality;specification languages;automated testing;DSL;building mbeddr;domain specific languages;productivity;software quality;problem-adequate abstractions;projectional language workbench;JetBrains MPS;language testing;embedded software development;Testing;DSL;Generators;C languages;Software;Semantics;Automation;domain specific languages;quality assurance;automated testing}, 
doi={10.1109/AST.2016.011}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1609829, 
author={Prashant Gandhi and N. C. Haugen and M. Hill and R. Watt}, 
booktitle={Agile Development Conference (ADC'05)}, 
title={Creating a living specification using FIT documents}, 
year={2005}, 
volume={}, 
number={}, 
pages={253-258}, 
abstract={Using FIT for automated acceptance testing supports a process in which developers and customers collaborate on a single executable specification for each story, i.e. the FIT documents. By collaborating closely on the FIT documents, the developers and customers reach a shared understanding of the domain and develop the ubiquitous language of the application. Our experience with this process was ultimately successful but not completely pain free. In this experience report we highlight the benefits and pitfalls and share techniques for achieving successful developer and customer collaboration in specifying executable FIT documents.}, 
keywords={software development management;formal specification;program testing;formal specification;FIT documents;automated acceptance testing;executable specification;successful developer-customer collaboration;Collaboration;Automatic testing;Writing;Pain;Fixtures;Collaborative tools;Encoding;System testing;Automation}, 
doi={10.1109/ADC.2005.19}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{1226344, 
author={N. Caouras and M. Freda and F. Monfet and V. S. Aldea and O. Naeem and Tho Le-Ngoc and B. Champagne}, 
booktitle={CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436)}, 
title={Performance evaluation platform for xDSL deployment in a complex multi-segment environment}, 
year={2003}, 
volume={1}, 
number={}, 
pages={61-64 vol.1}, 
abstract={This paper presents a highly flexible simulation platform catering to the easy and rapid evaluation of existing and future digital subscriber line (DSL) deployments as well as DSL modem performance prediction using practical modem implementations in a complex multi-segment environment. The paper outlines the methodology employed to architect and develop the core software, followed by a description of the performance prediction hooks for a variety of current and future DSL modem technologies. The graphical user interface (GUI) abstracting the core software for the user is described in terms of the various configuration options and the quick and easy graphical design of typical and complex deployment scenarios. The proposed simulator's calculations, notably theoretical SNR margin, maximum theoretical capacity and reach, plus performance evaluation using user-designed modem models, are also outlined. To support the accuracy of the new simulator, results for some example scenarios are presented and compared against other available simulators.}, 
keywords={performance evaluation;digital subscriber lines;modems;graphical user interfaces;telecommunication computing;digital simulation;performance evaluation platform;xDSL deployment;complex multisegment environment;digital subscriber line deployments;DSL modem performance prediction;core software;performance prediction hooks;DSL modem technologies;graphical user interface;configuration options;SNR margin;maximum theoretical capacity;user-designed modem models;twisted-pair simulator;DSL;Crosstalk;Modems;Communication cables;Background noise;Electromagnetic coupling;Copper;Telephony;Frequency;Colored noise}, 
doi={10.1109/CCECE.2003.1226344}, 
ISSN={0840-7789}, 
month={May},}
@INPROCEEDINGS{4626839, 
author={B. Magro and J. Garbajosa and J. Pérez}, 
booktitle={2008 12th International Software Product Line Conference}, 
title={A Software Product Line Definition for Validation Environments}, 
year={2008}, 
volume={}, 
number={}, 
pages={45-54}, 
abstract={Functional requirements must be tested to check if the system executes as the end user expects. Validation environments must be able to test multiple kinds of applications that belong to different domains and technologies. Since this wide validation spectrum is very difficult to cope with, validation environments are usually specialized in domains, programming languages, technologies, etc. However, it is possible to identify that the validation processes for different systems share a set of commonalities and variability points. This is a perfect framework to apply the software product line approach to develop domain specific validation environments for testing specific products. In this paper we present our experience of applying software product lines to support the variability of validation environments. We illustrate our product-line experience of developing two domain-specific validation environments for two different case studies: digital TV and slots machines.}, 
keywords={product development;program testing;program verification;software reusability;software product line definition;functional requirements;validation environments;specific products testing;Graphical user interfaces;Software;Engines;Testing;Computer architecture;Computer languages;Databases;tools;CASE;acceptance testing;software product lines;validation environments;validation tools;testing tools;software engineering environments;software development environments}, 
doi={10.1109/SPLC.2008.35}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{786779, 
author={G. Nelson}, 
booktitle={IEEE ATM Workshop '99 Proceedings (Cat. No. 99TH8462)}, 
title={Testing techniques for next-generation IP networks}, 
year={1999}, 
volume={}, 
number={}, 
pages={63-68}, 
abstract={In this paper, we look at some examples of how "IP meets ATM" and then discuss some of the recent advances in IP standards. For the remainder of the paper, we examine testing techniques used in three different scenarios: functional testing of a layer 2/layer 3 switching device; class of service (CoS) contract verification in an IP network; interworking testing of an IP/ATM access device.}, 
keywords={Internet;asynchronous transfer mode;telecommunication equipment testing;quality of service;telecommunication standards;protocols;formal verification;local area networks;next-generation IP networks;ATM;IP standards;functional testing;layer 2/layer 3 switching device;class of service;CoS;contract verification;interworking;access device;LAN;Testing;Next generation networking;IP networks;Asynchronous transfer mode;Multiprotocol label switching;Routing;Telecommunication traffic;Protocols;Packet switching;Proposals}, 
doi={10.1109/ATM.1999.786779}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7999652, 
author={J. L. de Moura and A. S. Charão and J. C. D. Lima and B. de Oliveira Stein}, 
booktitle={2017 17th International Conference on Computational Science and Its Applications (ICCSA)}, 
title={Test case generation from BPMN models for automated testing of Web-based BPM applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={This article proposes an approach to generate test cases from BPMN models, for automated testing of Web applications implemented with the support of BPM suites. The work is primarily focused on functional testing and has the following objectives: (i) identify execution paths from the flow analysis in the BPMN model and (ii) generate the initial code of test scripts to be run on a given Web application testing tool. Throughout the article, we describe the design and implementation of a solution to achieve these goals, targeting automated tests using Selenium and Cucumber as tools. The approach was applied to processes from a public repository and was able to generate test scenarios from different BPMN models.}, 
keywords={Internet;program testing;test case generation;BPMN models;Web-based BPM applications;automated testing;functional testing;Web application testing tool;Selenium;Cucumber;Testing;Tools;Logic gates;Selenium;Process control;XML;Monitoring;Business Process Management;BPMN;automatic software testing;test case generation}, 
doi={10.1109/ICCSA.2017.7999652}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6569742, 
author={D. Di Nardo and N. Alshahwan and L. Briand and Y. Labiche}, 
booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
title={Coverage-Based Test Case Prioritisation: An Industrial Case Study}, 
year={2013}, 
volume={}, 
number={}, 
pages={302-311}, 
abstract={This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.}, 
keywords={program testing;regression analysis;software fault tolerance;coverage-based test case prioritisation;industrial case study;real world system;regression fault;coverage criteria;fault detection rate;Testing;Fault detection;Measurement;Software;Computer aided software engineering;Minimization;Data collection;regression testing;industrial case study;test case prioritisation}, 
doi={10.1109/ICST.2013.27}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{5954390, 
author={C. Efkemann and J. Peleska}, 
booktitle={2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops}, 
title={Model-Based Testing for the Second Generation of Integrated Modular Avionics}, 
year={2011}, 
volume={}, 
number={}, 
pages={55-62}, 
abstract={In this paper the authors present the current research and development activities regarding automated testing of Integrated Modular Avionics controllers in the European research project SCARLETT. The authors describe the goals of the SCARLETT project and explain its background of Integrated Modular Avionics. Furthermore, they explain different levels of testing of components required for certification. A domain-specific modelling language designed for the IMA platform is presented. This language is used to create models from which tests of different levels can be generated automatically. The authors expect significant improvements in terms of effort to create and maintain test procedures compared to conventional test creation.}, 
keywords={aerospace computing;avionics;program testing;simulation languages;model-based testing;integrated modular avionics;SCARLETT research project;domain-specific modelling language;Aerospace electronics;Testing;Generators;Aircraft;Europe;Random access memory;Concrete;IMA;SCARLETT;TTCN-3;avionics;domain-specific modelling;model-based testing}, 
doi={10.1109/ICSTW.2011.72}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7962331, 
author={C. Chiw and G. Kindlmann and J. Reppy}, 
booktitle={2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST)}, 
title={DATm: Diderot's Automated Testing Model}, 
year={2017}, 
volume={}, 
number={}, 
pages={45-51}, 
abstract={Diderot is a parallel domain-specific language forthe analysis and visualization of multidimensional scientific images, such as those produced by CT and MRI scanners. Diderot is designed to support algorithms that are based on differential tensor calculus and produces a higher-order mathematical model which allows direct manipulation of tensor fields. One of the main challenges of the Diderot implementation is bridging this semantic gap by effectively translating high-level mathematical notation of tensor calculus into efficient low-level code in the target language. A key question for a high-level language, such as Diderot, is how do we know that the implementation is correct. We have previously presented and defended a core set of rewriting rules, but the full translation from source to executable requires much more work. In this paper, we present DATm, Diderot's automated testing model to check the correctness of the core operations in the programming language. DATm can automatically create test programs, and predict what the outcome should be. We measure the accuracy of the computations written in the Diderot language, based on how accurately the output of the program represents the mathematical equivalent of the computations. This paper describes a model for testing a high-level language based on correctness. It introduces the pipeline for DATm, a tool that can automatically create and test tens of thousands of Diderot test programs and that has found numerous bugs. We make a case for the necessity of extensive testing by describing bugs that are deep in the compiler, and only could be found with a unique application of operations. Lastly, we demonstrate that the model can be used to create other types of tests by visual verification.}, 
keywords={parallel languages;program compilers;program debugging;program testing;program verification;rewriting systems;specification languages;tensors;DATm;Diderot automated testing model;parallel domain-specific language;multidimensional scientific image analysis;multidimensional scientific image visualization;differential tensor calculus;higher-order mathematical model;high-level language;rewriting rules;programming language;Diderot test programs;bugs;compiler;visual verification;Testing;Tensile stress;Kernel;Shape;Computational modeling;Generators;Calculus;domain specific testing;Diderot;DSL;tensor calc;visual verificaiton}, 
doi={10.1109/AST.2017.5}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7237259, 
author={E. Lavrishcheva}, 
booktitle={2015 Science and Information Conference (SAI)}, 
title={Ontological approach to the formal specification of the standard life cycle}, 
year={2015}, 
volume={}, 
number={}, 
pages={965-972}, 
abstract={Approach is offered to the formal specification of Standard Life Cycle (LC) of the program systems (PS) by the ontology facilities with purpose automation and generation of the variants LC for making the appropriate kinds process for development different PS. Ontological approach to presentation LC model of the standard ISO/IEC 12207-2007 is included the specification of general, organizational and support processes. These processes are presented in the subject-oriented DSL, which than transformed to XML for realization. One of the processes, the testing process is given in terms of Protégé systems. An eventual result of this system Protégé got generally at accepted to the XML, suitable for implementation tasks testing PS on computer.}, 
keywords={formal specification;ontologies (artificial intelligence);XML;ontological approach;standard life cycle formal specification;program systems;ISO/IEC 12207-2007;subject-oriented DSL;XML;Protégé systems;DSL;Ontologies;XML;ISO Standards;IEC Standards;Testing;ontology;the life cycle standard;model of life cycle;processes;actions;task;testing;DSL;description;Protégé;XML}, 
doi={10.1109/SAI.2015.7237259}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8456399, 
author={J. G. Quenum and S. Aknine}, 
booktitle={2018 IEEE International Conference on Services Computing (SCC)}, 
title={Towards Executable Specifications for Microservices}, 
year={2018}, 
volume={}, 
number={}, 
pages={41-48}, 
abstract={This paper presents an empirical approach for microservice automated testing. With the rise of the agile methodology, automated testing has gained momentum in software development, including using microservices as an architectural style. However, the tests are not always related to the core specifications of the system being developed. In this paper, we discuss an approach to derive the tests, especially the acceptance tests, from the specifications of the systems. To avoid any ambiguity in the specifications, we focus on the formal specifications of the system. To this end, we introduce intelligent agents as a conceptual unit to encapsulate the formal specifications of services. Indeed, a comparison of microservice tenets and the general characterization of agents reveals that both can be thought of as autonomous software entities, driven by goals and evolving within a distributed environment and communicating with one another. Using a real-world application we show how agent formal specifications can be linked to microservice automated testing.}, 
keywords={formal specification;program testing;software architecture;software prototyping;empirical approach;microservice automated testing;agile methodology;software development;core specifications;acceptance tests;microservice tenets;agent formal specifications;executable specifications;autonomous software entities;Testing;Service-oriented architecture;Intelligent agents;Business;Computer architecture;Curriculum development;Services;Testing;Intelligent Agents;Formal Specifications}, 
doi={10.1109/SCC.2018.00013}, 
ISSN={2474-2473}, 
month={July},}
@INPROCEEDINGS{7302512, 
author={S. Vinogradov and A. Ozhigin and D. Ratiu}, 
booktitle={2015 IEEE International Symposium on Systems Engineering (ISSE)}, 
title={Modern model-based development approach for embedded systems practical experience}, 
year={2015}, 
volume={}, 
number={}, 
pages={56-59}, 
abstract={Control functionality of modern rail vehicles is getting more and more complex. It contains several modules such as the traction control unit or the central control unit, as well as input and output stations, such as driver's cab terminals and process I/Os. A plethora of devices are connected to the vehicle and train bus and are able to communicate. The functions of the vehicle control and traction systems are configured by using function blocks from which loadable programs are generated. The languages used to program the control units are well established in the field. However, one-size-fits-all approach cannot adequately address the increased complexity of the software in modern trains. In this paper we describe our preliminary experience with using the multi-paradigm modeling tool “mbeddr” in the railway domain. The following aspects have been in focus during the work: (a) matching the application requirements and domain specific language used for implementation; (b) integration of model-based approach into traditional product lifecycle; (c) reengineering existing functionality using modeling and code generation capabilities of mbeddr. The system example we chose was the application logic of automated train driving system implemented in development environment of Siemens process automation framework.}, 
keywords={embedded systems;rail traffic control;traction;model-based development approach;embedded systems;rail vehicle control functionality;traction control unit;central control unit;input stations;output stations;driver cab terminals;I/O process;vehicle bus;train bus;traction systems;function blocks;loadable programs;multiparadigm modeling tool;mbeddr tool;railway domain;application requirements;domain specific language;model-based approach;product lifecycle;reengineering;modeling capabilities;code generation capabilities;automated train driving system;application logic;Siemens process automation framework;Software;Mathematical model;Complexity theory;Control systems;Domain specific languages;Formal verification;Rail transportation;model based development;language engineering}, 
doi={10.1109/SysEng.2015.7302512}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7000030, 
author={M. T. C. F. Albuquerque and G. L. Ramalho and V. Corruble and A. L. M. Santos and F. Freitas}, 
booktitle={2014 Brazilian Symposium on Computer Games and Digital Entertainment}, 
title={Helping Developers to Look Deeper inside Game Sessions}, 
year={2014}, 
volume={}, 
number={}, 
pages={31-40}, 
abstract={Game design and development activities are increasingly relying on the analysis of gamer's behavior and preferences data. Various tools are available to the developers to track and analyze general data concerning acquisition, retention and monetization aspects of game commercialization. This is good enough to give hints on where problems are, but not to enable a precise diagnosis, which demands fine-grained data. For this kind of data, there is not enough support or guidance to decide which data to capture, to write the code to capture it, to choose the best representation of it and to allow an adequate retrieval and presentation of it. This paper introduces GameGuts (GG), a framework devoted to give further assistance to developers in choosing, representing, accessing and presenting game sessions fine-grained data. As a case study, GG recorded sessions of a game platform with over a hundred thousand users. The logs were analyzed using a Visual Domain Specific Language (as a query language) and an ensemble of rules (as a compliance test). The results are encouraging, since we could - among other results - find bugs and catch cheaters, as well as spot design flaws.}, 
keywords={computer games;data analysis;query languages;visual languages;game sessions;game design;game development activities;gamer behavior analysis;preferences data;general data tracking;general data analysis;game commercialization;GameGuts;game platform;visual domain specific language;query language;rule ensemble;bugs;catch cheaters;design flaws;Games;Ontologies;Visualization;Servers;DSL;Database languages;Measurement;game analytics;knowledge representation;game data mining}, 
doi={10.1109/SBGAMES.2014.28}, 
ISSN={2159-6662}, 
month={Nov},}
@INPROCEEDINGS{5967120, 
author={G. Piho and J. Tepandi and M. Roost and M. Parman and V. Puusep}, 
booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
title={From archetypes based domain model via requirements to software: Exemplified by LIMS Software Factory}, 
year={2011}, 
volume={}, 
number={}, 
pages={570-575}, 
abstract={The Archetypes Based Development (ABD) proceeds from archetypes based domain model via requirements to software. We give an overview of ABD and exemplify its application on Laboratory Information Management Systems (LIMS) Software Factory development. ABD is guided by Zachman Framework and utilizes software engineering triptych together with archetypes and archetype patterns. For modelling of domains the Test Driven Modelling (TDM) techniques are used. TDM utilizes test driven development techniques in domain engineering. The resultant domain models serve as the Domain Specific Language for prescribing requirements. Implementation and testing of the LIMS Software Factory proves feasibility of archetypes based techniques in real life systems. ABD helps developers to better understand business requirements, to design cost effective enterprise applications through systematic reuse of archetypal components, as well as to validate and verify requirements resulting in higher quality software.}, 
keywords={business data processing;information management;laboratories;software quality;specification languages;archetypes based domain model development;LIMS software factory;software requirements;laboratory information management systems software factory development;Zachman Framework;software engineering triptych;test driven modelling techniques;domain specific language;business requirements;Software;Production facilities;Laboratories;Capability maturity model;DSL;Organizations;Archetypes;archetype patterns;domain analysis;domain model;domain modelling;software engineering;software factory;laboratory information management system (LIMS);laboratory domain model}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5630332, 
author={M. K. Agaram and B. Laird}, 
booktitle={2010 14th IEEE International Enterprise Distributed Object Computing Conference}, 
title={A Componentized Architecture for Externalized Business Rules}, 
year={2010}, 
volume={}, 
number={}, 
pages={175-183}, 
abstract={Delta Dental is the leading provider of Dental benefits in the Midwest, serving nearly 5.1 million members. Delta Dental of Michigan uses Business Rules to articulate complex claims adjudication rules, from legacy COBOL programs. These rules are entirely authored, managed, tested and governed by Subject Matter Experts with no programming background. The Business Rules Architecture incorporates a componentized approach to authoring and organizing Business Rules which promotes extensive rule reuse at several levels through layered components. To mitigate complexity, the framework provides features for sandbox testing and regression testing of the rules by the Business Users. It also provides a rich reporting and trace ability back to the Policy charter. As a result of this framework a legacy adjudication COBOL program of 50,000+ lines were reduced to 30+ reusable rules. Nearly 94 % of the claims processed are automatically adjudicated by the rules engine. The purpose of this paper is to describe in detail the insights gained from the architecture and the measurable productivity gains accomplished.}, 
keywords={COBOL;dentistry;medical administrative data processing;componentized architecture;externalized business rules;Delta Dental;complex claims adjudication rules;legacy COBOL programs;business rules architecture;sandbox testing;regression testing;Business;Dentistry;Vocabulary;Testing;Production;Runtime;Humans;Architecture;Business Rules;BRIDE;Component}, 
doi={10.1109/EDOC.2010.26}, 
ISSN={1541-7719}, 
month={Oct},}
@INPROCEEDINGS{7102627, 
author={W. Krenn and R. Schlick and S. Tiran and B. Aichernig and E. Jobstl and H. Brandl}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={MoMut::UML Model-Based Mutation Testing for UML}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Model-based mutation testing (MBMT) is a promising testing methodology that relies on a model of the system under test (SUT) to create test cases. Hence, MBMT is a so-called black-box testing approach. It also is fault based, as it creates test cases that are guaranteed to reveal certain faults: after inserting a fault into the model of the SUT, it looks for a test case revealing this fault. This turns MBMT into one of the most powerful and versatile test case generation approaches available as its tests are able to demonstrate the absence of certain faults, can achieve both, control-flow and data-flow coverage of model elements, and also may include information about the behaviour in the failure case. The latter becomes handy whenever the test execution framework is bound in the number of observations it can make and - as a consequence - has to restrict them. However, this versatility comes at a price: MBMT is computationally expensive. The tool MoMuT::UML (https://www.momut.org) is the result of a multi-year research effort to bring MBMT from the academic drawing board to industrial use. In this paper we present the current stable version, share the lessons learnt when applying two generations of MoMuT::UML in an industrial setting, and give an outlook on the upcoming, third,generation.}, 
keywords={data flow computing;program testing;Unified Modeling Language;MoMut::UML model;model-based mutation testing;MBMT;system under test;SUT;test cases;black-box testing approach;test case generation;control-flow coverage;data-flow coverage;test execution framework;Unified modeling language;Testing;Computational modeling;Integrated circuit modeling;Circuit faults;Object oriented modeling;Semantics}, 
doi={10.1109/ICST.2015.7102627}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{7880820, 
author={X. Guo and R. G. Dutta and P. Mishra and Y. Jin}, 
booktitle={2016 17th International Workshop on Microprocessor and SOC Test and Verification (MTV)}, 
title={Automatic RTL-to-Formal Code Converter for IP Security Formal Verification}, 
year={2016}, 
volume={}, 
number={}, 
pages={35-38}, 
abstract={The wide usage of hardware intellectual property (IP) cores from untrusted vendors has raised security concerns in the integrated circuit (IC) industry. Existing testing methods are designed to validate the functionality of the hardware IP cores. These methods often fall short in detecting unspecified (often malicious) logic. Formal methods like Proof-Carrying Hardware (PCH), on the other hand, can help eliminate hardware Trojans and/or design backdoors by formally proving security properties on soft IP cores despite the high proof development cost. One of the causes to the high cost is the manual conversion of the hardware design from RTL code to a domain-specific language prior to verification. To mitigate this issue and to lower the overall cost of PCH framework, we propose an automatic code converter for translating VHDL to Formal-HDL, a domain specific language for representing hardware designs in Coq language. Our code converter provides support to wide variety of hardware designs. Towards the goal of speeding up the verification procedure in our PCH framework, the code converter is the important first step. The applicability of the tool is demonstrated by converting soft IP cores of AES to its Coq equivalent code.}, 
keywords={codes;convertors;formal verification;hardware description languages;logic circuits;microprocessor chips;security;Coq equivalent code;AES;Coq language;formal-HDL;VHDL;domain-specific language;soft IP cores;security properties;hardware Trojans;PCH;proof-carrying hardware;hardware IP cores;integrated circuit industry;intellectual property;IP security formal verification;automatic RTL-to-formal code converter;Security;Hardware;IP networks;Hardware design languages;Trojan horses;Syntactics;Hardware Security;Hardware IP Protection;Formal Verification}, 
doi={10.1109/MTV.2016.23}, 
ISSN={2332-5674}, 
month={Dec},}
@INPROCEEDINGS{7958601, 
author={T. Petsios and A. Tang and S. Stolfo and A. D. Keromytis and S. Jana}, 
booktitle={2017 IEEE Symposium on Security and Privacy (SP)}, 
title={NEZHA: Efficient Domain-Independent Differential Testing}, 
year={2017}, 
volume={}, 
number={}, 
pages={615-632}, 
abstract={Differential testing uses similar programs as cross-referencing oracles to find semantic bugs that do not exhibit explicit erroneous behaviors like crashes or assertion failures. Unfortunately, existing differential testing tools are domain-specific and inefficient, requiring large numbers of test inputs to find a single bug. In this paper, we address these issues by designing and implementing NEZHA, an efficient input-format-agnostic differential testing framework. The key insight behind NEZHA's design is that current tools generate inputs by simply borrowing techniques designed for finding crash or memory corruption bugs in individual programs (e.g., maximizing code coverage). By contrast, NEZHA exploits the behavioral asymmetries between multiple test programs to focus on inputs that are more likely to trigger semantic bugs. We introduce the notion of d-diversity, which summarizes the observed asymmetries between the behaviors of multiple test applications. Based on d-diversity, we design two efficient domain-independent input generation mechanisms for differential testing, one gray-box and one black-box. We demonstrate that both of these input generation schemes are significantly more efficient than existing tools at finding semantic bugs in real-world, complex software. NEZHA's average rate of finding differences is 52 times and 27 times higher than that of Frankencerts and Mucerts, two popular domain-specific differential testing tools that check SSL/TLS certificate validation implementations, respectively. Moreover, performing differential testing with NEZHA results in 6 times more semantic bugs per tested input, compared to adapting state-of-the-art general-purpose fuzzers like American Fuzzy Lop (AFL) to differential testing by running them on individual test programs for input generation. NEZHA discovered 778 unique, previously unknown discrepancies across a wide variety of applications (ELF and XZ parsers, PDF viewers and SSL/TLS libraries), many of which constitute previously unknown critical security vulnerabilities. In particular, we found two critical evasion attacks against ClamAV, allowing arbitrary malicious ELF/XZ files to evade detection. The discrepancies NEZHA found in the X.509 certificate validation implementations of the tested SSL/TLS libraries range from mishandling certain types of KeyUsage extensions, to incorrect acceptance of specially crafted expired certificates, enabling man-in-the-middle attacks. All of our reported vulnerabilities have been confirmed and fixed within a week from the date of reporting.}, 
keywords={program debugging;program testing;programming language semantics;security of data;NEZHA;domain-independent differential testing;program testing;cross-referencing oracles;semantic bug finding;efficient input-format-agnostic differential testing framework;crash finding;memory corruption bug;code coverage maximization;program behavioral asymmetry;d-diversity;domain-independent input generation mechanism;gray-box testing;black-box testing;SSL-TLS certificate validation implementation;general-purpose fuzzers;American Fuzzy Lop;ELF testing;XZ parser;PDF viewer;SSL-TLS libraries;critical security vulnerabilities;critical evasion attacks;ClamAV;arbitrary malicious ELF/XZ files;X.509 certificate validation implementation;KeyUsage extension;expired certificates;man-in-the-middle attack;Testing;Computer bugs;Semantics;Tools;Software;Libraries;Security;nezha;differential testing;fuzzing}, 
doi={10.1109/SP.2017.27}, 
ISSN={2375-1207}, 
month={May},}
@INPROCEEDINGS{7101658, 
author={P. McCormick and C. Sweeney and N. Moss and D. Prichard and S. K. Gutierrez and K. Davis and J. Mohd-Yusof}, 
booktitle={2014 Fourth International Workshop on Domain-Specific Languages and High-Level Frameworks for High Performance Computing}, 
title={Exploring the Construction of a Domain-Aware Toolchain for High-Performance Computing}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={The push towards exascale computing has sparked a new set of explorations for providing new productive programming environments. While many efforts are focusing on the design and development of domain-specific languages (DSLs), few have addressed the need for providing a fully domain-aware toolchain. Without such domain awareness critical features for achieving acceptance and adoption, such as debugger support, pose a long-term risk to the overall success of the DSL approach. In this paper we explore the use of language extensions to design and implement the Scout DSL and a supporting toolchain infrastructure. We highlight how language features and the software design methodologies used within the toolchain play a significant role in providing a suitable environment for DSL development.}, 
keywords={high level languages;parallel processing;software engineering;domain-aware toolchain;high-performance computing;language extensions;Scout DSL;domain-specific languages;software design methodologies;DSL;Runtime;Computer architecture;Syntactics;Graphics processing units;Image color analysis}, 
doi={10.1109/WOLFHPC.2014.9}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7552103, 
author={C. K. Poon and T. Wong and Y. T. Yu and V. C. S. Lee and C. M. Tang}, 
booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)}, 
title={Toward More Robust Automatic Analysis of Student Program Outputs for Assessment and Learning}, 
year={2016}, 
volume={1}, 
number={}, 
pages={780-785}, 
abstract={Automated analysis and assessment of students' programs, typically implemented in automated program assessment systems (APASs), are very helpful to both students and instructors in modern day computer programming classes. The mainstream of APASs employs a black-box testing approach which compares students' program outputs with instructor-prepared outputs. A common weakness of existing APASs is their inflexibility and limited capability to deal with admissible output variants, that is, outputs produced by acceptable correct programs that differ from the instructor's. This paper proposes a more robust framework for automatically modelling and analysing student program output variations based on a novel hierarchical program output structure called HiPOS. Our framework assesses student programs by means of a set of matching rules tagged to the HiPOS, which produces a better verdict of correctness. We also demonstrate the capability of our framework by means of a pilot case study using real student programs.}, 
keywords={computer science education;educational administrative data processing;program testing;automatic analysis;learning;students program assessment;automated program assessment systems;APAS;computer programming classes;black-box testing;instructor-prepared outputs;admissible output variants;student program output variations;hierarchical program output structure;HiPOS;Robustness;Programming profession;Education;Computers;Testing;Natural language processing;automated assessment technology;computer science education;learning computer programming;program output variant;student program analysis}, 
doi={10.1109/COMPSAC.2016.208}, 
ISSN={0730-3157}, 
month={June},}
@ARTICLE{5441292, 
author={X. Amatriain and P. Arumi}, 
journal={IEEE Transactions on Software Engineering}, 
title={Frameworks Generate Domain-Specific Languages: A Case Study in the Multimedia Domain}, 
year={2011}, 
volume={37}, 
number={4}, 
pages={544-558}, 
abstract={We present an approach to software framework development that includes the generation of domain-specific languages (DSLs) and pattern languages as goals for the process. Our model is made of three workflows-framework, metamodel, and patterns-and three phases-inception, construction, and formalization. The main conclusion is that when developing a framework, we can produce with minimal overhead-almost as a side effect-a metamodel with an associated DSL and a pattern language. Both outputs will not only help the framework evolve in the right direction, but will also be valuable in themselves. In order to illustrate these ideas, we present a case study in the multimedia domain. For several years, we have been developing a multimedia framework. The process has produced a full-fledged domain-specific metamodel for the multimedia domain, with an associated DSL and a pattern language.}, 
keywords={multimedia computing;software engineering;specification languages;visual programming;domain-specific languages;multimedia domain;pattern languages;domain-specific metamodel;associated DSL;visual programming;software framework development;Domain specific languages;DSL;Unified modeling language;Vocabulary;Concrete;Software engineering;Computer aided software engineering;Natural languages;Metamodeling;Best practices;Domain-specific architectures;visual programming;life cycle;CASE.}, 
doi={10.1109/TSE.2010.48}, 
ISSN={0098-5589}, 
month={July},}
@INPROCEEDINGS{5464138, 
author={A. C. Noubissi and J. Iguchi-Cartigny and J. Lanet}, 
booktitle={2010 Fifth International Conference on Systems}, 
title={Incremental Dynamic Update for Java-Based Smart Cards}, 
year={2010}, 
volume={}, 
number={}, 
pages={110-113}, 
abstract={One of the most appealing feature for multi-application smart cards is their ability to dynamically download or delete applications once the card has been issued. Applications can be updated by deleting old versions and loading the new ones. Nevertheless, for system components, the update is sligthly more complex because the systems never stop. Indeed, for smart cards based on Java called JavaCard, the virtual machine has a life cycle similar to the card because persistent objects are preserved after the communication sessions with the reader have expired. We present in this paper, our research in dynamic system components updating of JavaCard. Our technique requires a lot of off-card and on-card mechanisms. Our approach uses control flow graph to determine change between versions, a domain specific language to represent the change for minimization of the download overhead throughout the communication link with the card.}, 
keywords={Java;smart cards;software engineering;virtual machines;incremental dynamic update;Java-based smart cards;multiapplication smart cards;JavaCard;virtual machine;off-card mechanism;on-card mechanism;Java;Smart cards;Virtual machining;Runtime;Application software;Domain specific languages;Aerodynamics;Embedded software;DSL;Operating systems;Smart Card;HotSwUp;Java Card;Dynamic update;e-passport}, 
doi={10.1109/ICONS.2010.27}, 
ISSN={}, 
month={April},}
@ARTICLE{6312844, 
author={F. T. Baker}, 
journal={IEEE Transactions on Software Engineering}, 
title={Structured programming in a production programming environment}, 
year={1975}, 
volume={SE-1}, 
number={2}, 
pages={241-252}, 
abstract={Discusses how structured programming methodology has been introduced into a large production programming organization using an integrated but flexible approach. It next analyzes the advantages and disadvantages of each component of the methodology and presents some quantitative results on its use. It concludes with recommendations based on this generally successful experience, which could be useful to other organizations interested in improving reliability and productivity.}, 
keywords={computer facilities;programming;top down programming;production programming environment;structured programming;Programming;DSL;Libraries;Organizations;Encoding;Standards;Guidelines;Chief programmer teams (CPT's);development support libraries (DSL's);structured coding;structured programming;top-down development;top-down programming}, 
doi={10.1109/TSE.1975.6312844}, 
ISSN={0098-5589}, 
month={June},}
@INPROCEEDINGS{5967121, 
author={G. Piho and J. Tepandi and M. Parman and V. Puusep and M. Roost}, 
booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
title={Test Driven domain modelling}, 
year={2011}, 
volume={}, 
number={}, 
pages={576-581}, 
abstract={To write software we have to know requirements; to know requirements we have to know domain; to know the domain we have to analyze and model one. We propose a methodology for applying Test Driven Modelling in engineering of domains, requirements and software. We will restrict ourselves here to enterprise information systems and therefore to business domains. As common for Software Factories, domain models (as well as all other models) are software artefacts, not only documentation artefacts. In our approach Test Driven Modelling utilizes Test Driven Development for domain modelling. Domain models engineered in this way are used as Domain Specific Language for specifying software requirements. The hypothesis is that such domain models can be used for validation of requirements and verification of software, lead developments towards Software Factories, and increase dependability of software.}, 
keywords={business data processing;DP industry;formal specification;information systems;program testing;program verification;software reliability;specification languages;system documentation;test driven domain modelling;domains engineering;requirements engineering;software engineering;enterprise information systems;business domains;software factory;domain models;software artefacts;documentation artefacts;test driven development;domain specific language;software requirements;software verification;software dependability;Software;Measurement units;Time division multiplexing;Analytical models;Business;Silicon;Production facilities;domain analysis and engineering;domain model and domain modelling;software engineering;software factory;software testing;test driven development;test driven modelling;verification and validation}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6890825, 
author={F. Wanderley and A. Silva and J. Araujo and D. S. Silveira}, 
booktitle={2014 IEEE 4th International Model-Driven Requirements Engineering Workshop (MoDRE)}, 
title={SnapMind: A framework to support consistency and validation of model-based requirements in agile development}, 
year={2014}, 
volume={}, 
number={}, 
pages={47-56}, 
abstract={Two fundamental principles and values of agile methods are customer satisfaction by rapid delivery of useful software and the improvement of the communication process by continuous stakeholders' involvement. But, how to deal with customers' satisfaction and find a better visualization model at the requirements level (which stakeholders can understand and be involved) in an agile development context? Also, how this visualization model enhancement can guarantee consistency between agile requirements artefacts (e.g., user stories and domain models)? Thus, to answer these questions, this paper presents the SnapMind framework. This framework aims to make the requirements modelling process more user-centered, through the definition of a visual requirements language, based on mind maps, model-driven and domain specific language techniques. Moreover, through these techniques, the SnapMind framework focuses on support for consistency between user stories and the domain models using a model animation technique called snapshots. The framework was applied to an industrial case study to investigate its feasibility.}, 
keywords={customer satisfaction;software prototyping;model-based requirements;agile methods;customer satisfaction;rapid delivery;useful software;communication process;agile development context;visualization model enhancement;SnapMind framework;domain specific language techniques;user stories;domain models;model animation technique;snapshots;Visualization;Software;Syntactics;Unified modeling language;Adaptation models;Business;Semantics;Agile Software Requirements;Model-Driven Engineering;Domain-Specific Languages;Mind Map;Snapshots}, 
doi={10.1109/MoDRE.2014.6890825}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6062146, 
author={A. Groce and K. Havelund and M. Smith}, 
booktitle={2010 ACM/IEEE 32nd International Conference on Software Engineering}, 
title={From scripts to specifications: the evolution of a flight software testing effort}, 
year={2010}, 
volume={2}, 
number={}, 
pages={129-138}, 
abstract={This paper describes the evolution of a software testing effort during a critical period for the flagship Mars Science Laboratory rover project at the Jet Propulsion Laboratory. Formal specification for post-run analysis of log files, using a domain-specific language, LogScope, replaced scripted real-time analysis. Log analysis addresses the key problems of on-the-fly approaches and cleanly separates specification and execution. Mining the test repository suggested the inadequacy of the scripted approach, and encouraged a partly engineer-driven development. LogScope development should hold insights for others facing the tight deadlines and reactionary nature of testing for critical projects. LogScope received a JPL Mariner Award for "improving productivity and quality of the MSL Flight Software" and has been discussed as an approach for other flight missions. We note LogScope features that most contributed to ease of adoption and effectiveness. LogScope is general and can be applied to any software producing logs.}, 
keywords={aerospace computing;data mining;formal specification;program diagnostics;program testing;software testing effort;flagship Mars Science Laboratory rover project;Jet Propulsion Laboratory;post-run analysis;log files;domain specific language;LogScope;scripted real-time analysis;log analysis;on-the-fly approach;test repository;engineer driven development;LogScope development;JPL Mariner Award;MSL flight software;flight missions;formal specification;Software;Telemetry;Space vehicles;Laboratories;Libraries;Semantics;Python;development practices;logs;runtime verification;space flight software;temporal logic;test infrastructure;testing}, 
doi={10.1145/1810295.1810314}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{5381641, 
author={T. Hartmann}, 
booktitle={2009 Testing: Academic and Industrial Conference - Practice and Research Techniques}, 
title={Model Based Testing of End-to-End Chains Using Domain Specific Languages}, 
year={2009}, 
volume={}, 
number={}, 
pages={82-91}, 
abstract={In this paper, the author explains a new approach of model based end-to-end chain testing using scenarios with original and simulated equipment. The first goal is to automatically derive test data and test cases from the model, which is defined by a domain specific language. Several solvers can be attached to the conversion to quickly create a wide variety of stimuli for the system(s) under test. Furthermore, the system under test can be stimulated by either original equipment - which is connected to the test bench - or the test bench can simulate equipment and create inputs for the tested systems. Any mixture of simulated and original equipment is possible and can be changed on the fly. In the end, the results from the system under test are collected. These results can then be displayed back in the model. This method is currently used and improved in the project "E-Cab" in which the author is involved. Passengers travelling by plane are in the focus of this project. Complete services and service chains - from the booking at home up to leaving the destination airport - are created and used by many systems communicating with each other. The author expects advantages from testing these end-to-end chains with this approach.}, 
keywords={automatic test pattern generation;model based testing;end-to-end chain;domain specific language;automatic test case generation;system under test;E-Cab;Domain specific languages;System testing;Automatic testing;Logic testing;Constraint theory;Logistics;Operating systems;Airports;Complex networks;Robustness;model based;end-to-end chain;testing;automatic test case generation;automatic test data generation}, 
doi={10.1109/TAICPART.2009.25}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{1158072, 
author={A. Saha and V. Mishra and P. Saxena and R. Mundhada and K. Katiyar and S. Kumar}, 
booktitle={15th Annual IEEE International ASIC/SOC Conference}, 
title={Design of broadband controller for residential gateway applications}, 
year={2002}, 
volume={}, 
number={}, 
pages={283-287}, 
abstract={As Internet technology becomes more pervasive, homes are getting connected to cable or DSL. The increasing user demands for "always on" service along with multiple connectivity for voice and data is gradually making the presence of a residential gateway in every household a reality. A residential gateway should have routing and bridging capabilities along with seamless connectivity to contemporary premise networking technologies. Integration of all these features on a single device essentially requires a rich architecture, smart design techniques and thorough verification. This paper describes the architecture and design of a broadband network controller which is a critical component of TI's voice and data centric residential gateway solution.}, 
keywords={home computing;internetworking;network servers;broadband networks;wide area networks;telecommunication network routing;data communication equipment;integrated voice/data communication;Internet;digital subscriber lines;integrated circuit design;integrated circuit testing;home networks;broadband networks;wide area networks;broadband controllers;household/domestic residential gateway applications;residential gateway routing/bridging capabilities;Internet technology;DSL;cable connection;always on Internet services;voice/data multiple connectivity;premise networking technologies seamless connectivity;smart design techniques;device architecture;verification;VoIP;VoATM;VoDSL;Internet;DSL;Broadband communication;Ethernet networks;Universal Serial Bus;Control systems;Instruments;Routing;Home automation;Transceivers}, 
doi={10.1109/ASIC.2002.1158072}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6462664, 
author={B. Combemale and X. Crégut and M. Pantel}, 
booktitle={2012 19th Asia-Pacific Software Engineering Conference}, 
title={A Design Pattern to Build Executable DSMLs and Associated V amp;V Tools}, 
year={2012}, 
volume={1}, 
number={}, 
pages={282-287}, 
abstract={Model executability is now a key concern in model-driven engineering, mainly to support early validation and verification (V&V). Some approaches allow to weave executability into metamodels, defining executable domain-specific modeling languages (DSMLs). Model validation can then be achieved by simulation and graphical animation through direct interpretation of the conforming models. Other approaches address model executability by model compilation, allowing to reuse the virtual machines or V&V tools existing in the target domain. Nevertheless, systematic methods are currently not available to help the language designer in the definition of such an execution semantics and related tools. For instance, simulators are mostly hand-crafted in a tool specific manner for each DSML. In this paper, we propose to reify the elements commonly used to support state-based execution in a DSML. We infer a design pattern (called Executable DSML pattern) providing a general reusable solution for the expression of the executability concerns in DSMLs. It favors flexibility and improves reusability in the definition of semantics-based tools for DSMLs. We illustrate how this pattern can be applied to ease the development of V&V tools.}, 
keywords={simulation languages;software reusability;virtual machines;executable DSML;V&V tools;model executability;model-driven engineering;early validation and verification;metamodels;domain-specific modeling languages;model compilation;virtual machine reuse;state-based execution;design pattern;Semantics;Unified modeling language;Runtime;Computational modeling;Abstracts;Animation;Concrete;Model Driven Engineering;Software Language Engineering;Validation & Verification}, 
doi={10.1109/APSEC.2012.79}, 
ISSN={1530-1362}, 
month={Dec},}
@ARTICLE{6784505, 
author={J. M. Vara and V. A. Bollati and Á. Jiménez and E. Marcos}, 
journal={IEEE Transactions on Software Engineering}, 
title={Dealing with Traceability in the MDDof Model Transformations}, 
year={2014}, 
volume={40}, 
number={6}, 
pages={555-583}, 
abstract={Traceability has always been acknowledged as a relevant topic in Software Engineering. However, keeping track of the relationships between the different assets involved in a development process is a complex and tedious task. The fact that the main assets handled in any model-driven engineering project are models and model transformations eases the task. In order to take advantage of this scenario, which has not been appropriately capitalized on by the most widely adopted model transformation languages before, this work presents MeTAGeM-Trace, a methodological and technical proposal with which to support the model-driven development of model transformations that include trace generation. The underlying idea is to start from a high-level specification of the transformation which is subsequently refined into lower-level transformation models in terms of a set of DSLs until the source code that implements the transformation can be generated. Running this transformation produces not only the corresponding target models, but also a trace model between the elements of the source and target models. As part of the proposal, an EMF-based toolkit has been developed to support the development of ATL and ETL model transformations. This toolkit has been empirically validated by conducting a set of case studies following a systematic research methodology.}, 
keywords={research and development;software engineering;source code (software);MDD;traceability;software engineering;model-driven engineering project;model transformation languages;MeTAGeM-Trace;trace generation;lower-level transformation models;DSL;source code;EMF-based toolkit;ATL model transformations;ETL model transformations;systematic research methodology;Proposals;Object oriented modeling;Software;DSL;Complexity theory;Data models;Software engineering;Model-driven engineering;model transformations;traceability}, 
doi={10.1109/TSE.2014.2316132}, 
ISSN={0098-5589}, 
month={June},}
@INPROCEEDINGS{7577380, 
author={N. Kapre and S. Bayliss}, 
booktitle={2016 26th International Conference on Field Programmable Logic and Applications (FPL)}, 
title={Survey of domain-specific languages for FPGA computing}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={High-performance FPGA programming has typically been the exclusive domain of a small band of specialized hardware developers. They are capable of reasoning about implementation concerns at the register-transfer level (RTL) which is analogous to assembly-level programming in software. Sometimes these developers are required to push further down to manage even lower levels of abstraction closer to physical aspects of the design such as detailed layout to meet critical design constraints. In contrast, software programmers have long since moved away from textual assembly-level programming towards relying on graphical integrated development environments (IDEs), high-level compilers, smart static analysis tools and runtime systems that optimize, manage and assist the program development tasks. Domain-specific languages (DSLs) can bridge this productivity gap by providing higher levels of abstraction in environments close to the domain of application expert. DSLs carefully limit the set of programming constructs to minimize programmer mistakes while also enabling a rich set of domain-specific optimizations and program transformations. With a large number of DSLs to choose from, an inexperienced FPGA user may be confused about how to select an appropriate one for the intended domain. In this paper, we review a combination of legacy and state-of-the-art DSLs available for FPGA development and provide a taxonomy and classification to guide selection and correct use of the framework.}, 
keywords={electronic engineering computing;field programmable gate arrays;parallel programming;program compilers;program diagnostics;programming environments;software engineering;domain-specific languages;FPGA computing;high-performance FPGA programming;register-transfer level;RTL;textual assembly-level programming;graphical integrated development environments;graphical IDE;high-level compilers;smart static analysis tools;runtime systems;DSL;productivity gap;domain-specific optimizations;program transformations;Field programmable gate arrays;Hardware;DSL;Hardware design languages;Software;Programming;Productivity}, 
doi={10.1109/FPL.2016.7577380}, 
ISSN={1946-1488}, 
month={Aug},}
@INPROCEEDINGS{6605922, 
author={S. Sobernig and B. Hoisl and M. Strembeck}, 
booktitle={2013 13th International Conference on Quality Software}, 
title={Requirements-Driven Testing of Domain-Specific Core Language Models Using Scenarios}, 
year={2013}, 
volume={}, 
number={}, 
pages={163-172}, 
abstract={In this paper, we present an approach for the scenario-based testing of the core language models of domain-specific modeling languages (DSML). The core language model is a crucial artifact in DSML development, because it captures all relevant domain abstractions and specifies the relations between these abstractions. In software engineering, scenarios are used to explore and to define (actual or intended) system behavior as well as to specify user requirements. The different steps in a requirements-level scenario can then be refined through detailed scenarios. In our approach, we use scenarios as a primary design artifact. Non-executable, human-understandable scenario descriptions can be refined into executable test scenarios. To demonstrate the applicability of our approach, we implemented a scenario-based testing framework based on the Eclipse Modeling Framework (EMF) and the Epsilon model-management toolkit.}, 
keywords={formal specification;program testing;specification languages;requirement-driven testing;domain-specific core language models;scenario-based testing;domain-specific modeling languages;DSML development;software engineering;user requirements;Eclipse modeling framework;Epsilon model-management toolkit;EMF;system behavior;Biological system modeling;Testing;Unified modeling language;Software;Prototypes;Metamodeling;domain-specific modeling;scenario-based testing;language engineering;metamodel testing}, 
doi={10.1109/QSIC.2013.56}, 
ISSN={1550-6002}, 
month={July},}
@INPROCEEDINGS{766632, 
author={F. Blome and A. Hamich}, 
booktitle={International Symposium on Switching}, 
title={24 months of commercial isdn experience}, 
year={1990}, 
volume={2}, 
number={}, 
pages={159-164}, 
abstract={}, 
keywords={ISDN;Switches;Telephony;Packet switching;DSL;Telecommunications;Circuits;Roads;Testing;Cities and towns}, 
doi={10.1109/ISS.1990.766632}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{916745, 
author={R. S. Lightfoot}, 
booktitle={Proceedings of the 2000 IEEE International Conference on Management of Innovation and Technology. ICMIT 2000. 'Management in the 21st Century' (Cat. No.00EX457)}, 
title={Establishing long-term viability in the information economy: the strength of the integrated systems engineering process}, 
year={2000}, 
volume={2}, 
number={}, 
pages={526-529 vol.2}, 
abstract={Information technology companies are under increasing pressure to develop high technology products and services at an accelerated pace. They are often rewarded for such efforts through increases in their stock prices only to see these gains drop due to poor quality, failed technology, and poor customer support. In considering these events the question that comes to mind is, "What is the formula for providing long term viability in the high-tech information economy?" This paper presents recommendations that can be applied to the integrated systems/software engineering process which, when implemented, will provide companies with the formula for long-term viability in this rapidly changing fast-paced environment.}, 
keywords={information technology;systems engineering;technical support services;information economy;integrated systems engineering process;long-term viability;stock prices;poor quality;failed technology;poor customer support;high-tech information economy;integrated systems/software engineering process;Information technology;Companies;Systems engineering and theory;Stock markets;IEEE news;Educational institutions;Acceleration;Software engineering;DSL;Modems}, 
doi={10.1109/ICMIT.2000.916745}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6104710, 
author={J. Liu and G. Liu}, 
booktitle={2011 4th International Conference on Intelligent Networks and Intelligent Systems}, 
title={Research and Implementation of SNMP-Based Network Management System}, 
year={2011}, 
volume={}, 
number={}, 
pages={129-132}, 
abstract={After the brief introduction of the current situation of matured system framework based on SNMP, the basic problems of network management framework design are presented in detail. This paper focuses on the construction of SNMP-based platform in terms of software framework. Finally, by the use of the medium-sized data exchange network, it achieves the function of discovering the topology.}, 
keywords={computer network management;protocols;telecommunication network topology;SNMP based network management system;network management framework design;software framework;medium sized data exchange network;topology discovery;simple network management protocol;Topology;Software;Programming;Testing;Synchronization;DSL;Scalability;Network management framework;Auto-Topology control;Scalability;Generality}, 
doi={10.1109/ICINIS.2011.39}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4299957, 
author={D. Kim and H. Lim and S. Lee}, 
booktitle={International Conference on Software Engineering Advances (ICSEA 2007)}, 
title={A Case Study on Testing Activites for KT-OSS Maintenance}, 
year={2007}, 
volume={}, 
number={}, 
pages={77-77}, 
abstract={This paper describes the testing activities for the maintenance of the KT-OSS (Korea Telecom Operations Support System). Since the KT-OSS is a large software, it is essential to continuously perform maintenance activities such as the addition of new services from business departments and new functions requested by users and operators, performance improvement of existing functions, correction of the errors found during operation of the system, and so on. To ensure the successful maintenance of the KT-OSS without any effect on the existing functions and performance, we performed various tests related to functionality, efficiency and others before the added and modified parts were applied to the KT-OSS. In this paper, we show the maintenance process, the various tests related to it, the test organization, and the test environment for controlling the quality of the KT-OSS maintenance. Through these testing activities, we were able to successfully maintain the KT-OSS.}, 
keywords={program testing;software maintenance;telecommunication computing;Korea telecom operation support system;software maintenance;software testing activity;Software maintenance;Error correction;Telecommunications;Preventive maintenance;System testing;Information management;Quality management;DSL;Paper technology;Research and development}, 
doi={10.1109/ICSEA.2007.1}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{410732, 
author={D. Kelly and Q. Hartmann and W. Gude}, 
booktitle={Sixth Annual IEEE International ASIC Conference and Exhibit}, 
title={A multi-FPGA prototype of a DS1/HDSL synchronizer and desynchronizer prior to ASIC fabrication}, 
year={1993}, 
volume={}, 
number={}, 
pages={332-335}, 
abstract={As the speed and complexity of today's ASICs continues to grow, conventional prototyping techniques for algorithm verification begin to break down. A novel implementation of DS1/HDSL synchronizer and desynchronizer utilizing an array of four FPGA devices to verify algorithm performances prior to ASIC fabrication is described. The utilization of FPGA devices for ASIC prototyping can significantly reduce the risk, cost, and time-to-market involved with complex ASIC devices.<<ETX>>}, 
keywords={high level synthesis;hardware description languages;field programmable gate arrays;logic CAD;application specific integrated circuits;subscriber loops;synchronisation;flowcharting;VLSI;programmable logic arrays;high-bit-rate digital subscriber line;VHDL;compiled datapath circuits;VLSI;fuse generation;timing analysis;multi-FPGA prototype;DS1/HDSL synchronizer;desynchronizer;algorithm performances;ASIC prototyping;risk;cost;time-to-market;Prototypes;Application specific integrated circuits;Fabrication;Timing;Field programmable gate arrays;Jitter;Frequency synchronization;Costs;DSL;Testing}, 
doi={10.1109/ASIC.1993.410732}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{739853, 
author={P. J. Pingree}, 
booktitle={17th DASC. AIAA/IEEE/SAE. Digital Avionics Systems Conference. Proceedings (Cat. No.98CH36267)}, 
title={Deep space one integration and test challenges: getting to the launch pad in the faster, better, cheaper world}, 
year={1998}, 
volume={2}, 
number={}, 
pages={H21/1-H21/8 vol.2}, 
abstract={This paper describes the integration and test challenges of verifying and validating the avionics hardware and flight software which have been experienced in meeting the New Millennium Program Deep Space One (DS1) project's faster, better, cheaper requirements. This paper gives a high level overview of the development and application of the two flight system testbeds (DSI Hotbench), the testbed activities supported the DSI Spacecraft Assembly, Test and Launch Operations (ATLO), and the testing performed to prepare for and support post-launch mission operations. In the "Faster, Better, Cheaper" environment of the New Millennium Program, DSI Integration and Test (I&T) has defined new methods and decision criteria to meet our requirements and goals while assessing and minimizing the risk in the paths we have taken. Our successes and failures are largely yet to be seen as we approach our July 1 launch date. This paper describes the challenges that have been faced and some that have been overcome during the DSI I&T phase. It presents the risks that have been accepted in our attempts to test completely the DSI Avionics system in preparation for launch and mission operations.}, 
keywords={avionics;aerospace computing;electronic equipment testing;automatic testing;integration;test;avionics hardware;flight software;Millennium Program Deep Space One project;DSI Hotbench;testbed;post-launch mission;DSI Integration and Test;failures;Atherosclerosis;Aerospace electronics;Space technology;Electronic equipment testing;Software testing;System testing;Hardware;Space vehicles;DSL;Space missions}, 
doi={10.1109/DASC.1998.739853}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{4055046, 
author={D. Kim and H. Lim and S. Lee}, 
booktitle={2006 Canadian Conference on Electrical and Computer Engineering}, 
title={Testing Activities for KT-OSS Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={2397-2400}, 
abstract={This paper describes the testing activities for the development of the KTOSS (Korea Telecom Operations Support System). In this paper, we show the test phases for performing the verification and validation activities for the development and maintenance of KT-OSS. They are based on the general software development lifecycle, with an operational test added to it as an additional phase. To ensure the successful development of the KT-OSS, we performed various tests related to functionality, efficiency and others. Also, the tests were performed for maintenance after the field release. We also show the criteria for them and deal with the test organizations and the test-bed for managing and controlling the quality of the KT-OSS in this paper. Through these testing activities, we were able to successfully develop and release the KT-OSS}, 
keywords={program testing;program verification;software maintenance;software quality;software reliability;software reusability;testing activities;KT-OSS development;Korea Telecom Operations Support System;verification activities;validation activities;software maintenance;software development lifecycle;test-bed;software quality;Life testing;Software testing;Programming;System testing;Laboratories;Performance evaluation;Quality management;DSL;Telecommunications;ISO standards;Test;Operations Support System;Verification;Validation;Software Development Lifecycle}, 
doi={10.1109/CCECE.2006.277822}, 
ISSN={0840-7789}, 
month={May},}
@INPROCEEDINGS{5479285, 
author={Guo-Ming Sung and Yen-Tang Chang and Wen-Huei Chen and Hsiang-Yuan Hsieh}, 
booktitle={2010 International Conference on Networking and Digital Society}, 
title={A new architecture of broadband network system suitable for asymmetric digital subscriber line application}, 
year={2010}, 
volume={1}, 
number={}, 
pages={624-628}, 
abstract={This paper presents the design and implementation on a new architecture of broadband network system which suitable for asymmetric digital subscriber line (ADSL) application. The main design skill is based on the cell-based digital IC design process, and is implemented in 0.18 µm 1P6M CMOS process. The main function of this chip is to build a bridge between Ethernet and ATM which is used to substitute for RISC processor, leading to enhance the broadband network switching ability and stability. Furthermore, the clock management system is adopted to manage the packages. By this technique, a small size and low cost chip will be obtained.}, 
keywords={asynchronous transfer mode;broadband networks;CMOS integrated circuits;digital subscriber lines;integrated circuit design;broadband network system;asymmetric digital subscriber line application;ADSL;cell-based digital IC design;CMOS process;Ethernet;ATM;RISC processor;clock management system;size 0.18 micron;Broadband communication;DSL;CMOS integrated circuits;CMOS digital integrated circuits;Process design;CMOS process;Bridges;Ethernet networks;Asynchronous transfer mode;Reduced instruction set computing;component;Asynchronous transfer mode (ATM);asymmetric digital subscriber line (ADSL);Broadband Network}, 
doi={10.1109/ICNDS.2010.5479285}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4696109, 
author={M. Khoshbakhtian and N. HezareMoghadam and M. Mazoochi}, 
booktitle={2008 Third International Conference on Broadband Communications, Information Technology Biomedical Applications}, 
title={Identification of Various Problems in an Environment of Multi Vendor Equipments for PSTN Services in NGN}, 
year={2008}, 
volume={}, 
number={}, 
pages={194-201}, 
abstract={This paper focuses on Iran next generation network (NGN) Pilot, the purpose of which is to achieve appropriate knowledge for a desired move through current telecommunication network of Iran to NGN. On the base of this purpose, obtaining a proper knowledge about capabilities and weaknesses of vendors' NGN equipments is necessary as well as verifying the functionalities of these equipments in a multi-vendor environment. Accordingly, a test bed has been designed with the capability of executing various tests scenarios related to NGN and processing the results. At these stage two types of tests, i.e. Functional tests and Interoperability tests have been done. Since Functional tests are performed in a single-vendor environment; the proper interoperation among multi-vendor equipments can not be guaranteed. Consequently, other types of tests called Interoperability tests have been designed. These tests are performed in a multi-vendor environment and classified into two groups which process interoperation of two Call Servers or a Call Server and Gateway from different vendors, respectively. The following paper represents the results obtained from the mentioned tests as well as the precise assessment of them.}, 
keywords={circuit switching;internetworking;open systems;packet switching;multi vendor equipments;PSTN services;NGN;Iran next generation network pilot;interoperability tests;Call Server;Gateway;Next generation networking;Testing;Cities and towns;Network servers;File servers;Databases;Performance evaluation;Modems;Access protocols;Switching circuits;interoperability;Megaco;NGN;SIP}, 
doi={10.1109/BROADCOM.2008.14}, 
ISSN={}, 
month={Nov},}
@ARTICLE{1094910, 
author={S. Ahamed and P. Bohn and N. Gottfried}, 
journal={IEEE Transactions on Communications}, 
title={A Tutorial on Two-Wire Digital Transmission in the Loop Plant}, 
year={1981}, 
volume={29}, 
number={11}, 
pages={1554-1564}, 
abstract={This paper explores the constraints on the design of twowire repeaterless digital subscriber loop (DSL) systems. Broadly categorized, the design depends on the technical feasibility of the approach used to achieve two-wire transmission, constraints related to compatibility with other systems sharing the same cable, and immunity to central office noise. Each of these varies With the choice of system parameters including the transmission rate, transmit power, choice of line codes, etc. Technical feasibility is evaluated by computer simulation studies. Compatibility with other systems is explored by crosstalk calculations. Noise immunity considerations, as they translate into digital line power levels, are also explored.}, 
keywords={Digital communications;Wire communication subscriber networks;Tutorial;Crosstalk;DSL;Echo cancellers;Wire;Repeaters;Central office;Noise cancellation;Computer simulation;Noise level}, 
doi={10.1109/TCOM.1981.1094910}, 
ISSN={0090-6778}, 
month={November},}
@INPROCEEDINGS{7102616, 
author={F. Haser and R. Breu}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Non-Intrusive Documentation-Driven Integration Testing}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-2}, 
abstract={Powerful development frameworks and adoption of agile development methods are continuously increasing release frequency, thus compress test cycles. Test automation, often relying on model based approaches, helps to reduce test time, however the introduction of related heavy weight processes is often quite challenging. In order to tackle this problem, we propose a bottom up testing approach, which in a nutshell, in the initial phase supports the integration tester in creating a semi-formal test case description and report. The approach, a textual domain specific framework, will guide the test expert in evolving the base language, in order to be tailored for a domain language of an organization. The evolved language can be linked to executable code, which enables in the long run (semi-)automated model based regression testing.}, 
keywords={program testing;software prototyping;specification languages;system documentation;documentation-driven integration testing;agile development method;test automation;bottom up testing approach;textual domain specific framework;domain language;Testing;Automation;Context;Unified modeling language;Business;Writing;Measurement}, 
doi={10.1109/ICST.2015.7102616}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{5959781, 
author={C. Miksovic and O. Zimmermann}, 
booktitle={2011 Ninth Working IEEE/IFIP Conference on Software Architecture}, 
title={Architecturally Significant Requirements, Reference Architecture, and Metamodel for Knowledge Management in Information Technology Services}, 
year={2011}, 
volume={}, 
number={}, 
pages={270-279}, 
abstract={Capturing and sharing design knowledge such as architectural decisions is becoming increasingly important in professional Information Technology (IT) services firms. Methods, models, and tools supporting explicit knowledge management strategies have been proposed in recent years. In this paper, we extend previous work in the architectural knowledge management community to satisfy the requirements of an additional user group: the designers of IT infrastructure solutions that are outsourced from one company to another. Such strategic outsourcing solutions require complex, contractually relevant design decisions concerning many different resources such as IT infrastructures, people, and real estate. In this paper, we present a reference architecture and a decision process-oriented knowledge metamodel that we synthesized from the domain-specific functional requirements and quality attributes. We also present a tool implementation of these decision modeling concepts and discuss user feedback.}, 
keywords={information services;information technology;knowledge management;outsourcing;design knowledge;architectural decision;professional information technology service firm;knowledge management strategy;architectural knowledge management community;IT infrastructure solution;strategic outsourcing solution;decision process oriented knowledge metamodel;domain-specific functional requirement;quality attribute;decision modeling concept;user feedback;Proposals;Knowledge engineering;Knowledge based systems;Computer architecture;Communities;Engines;DSL;knowledge management;outsourcing;workflow}, 
doi={10.1109/WICSA.2011.43}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7323104, 
author={C. Sanz and A. Salas and M. de Miguel and A. Alonso and J. A. de la Puente and C. Benac}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Automated model-based testing based on an agnostic-platform modeling language}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Currently multiple Domain Specific Languages (DSLs) are used for model-driven software development, in some specific domains. Software development methods, such as agile development, are test-centered, and their application in model-based frameworks requires model support for test development. We introduce a specific language to define generic test models, which can be automatically transformed into executable tests for particular testing platforms. The resulting test models represent the test plan for applications also built according to a model-based approach. The approach presented here includes some customisations for the application of the developed languages and transformation tools for some specific testing platforms. These languages and tools have been integrated with some specific DSL designed for software development.}, 
keywords={program testing;software development management;automated model;agnostic platform modeling language;multiple domain specific languages;DSL;model driven software development;software development methods;generic test models;Testing;Unified modeling language;Software;Architecture;Computer architecture;Complexity theory;Engines;Model-based Testing;Automated Testing;Agile Development}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{6737898, 
author={A. Parizad}, 
booktitle={2013 13th International Conference on Environment and Electrical Engineering (EEEIC)}, 
title={Dynamic stability Analysis for damavand power plant considering PMS functions by DIGSILENT software}, 
year={2013}, 
volume={}, 
number={}, 
pages={145-155}, 
abstract={For a number of years Power Management System (PMS) have been used to control islanded power systems. It can control system frequency and voltage as well as generated real and reactive power. The PMS would often have a load shedding facility to prevent the cascade failure of the generation system. The investigation is based on dynamics time domain simulations by DigSilent software and aims to define the main functions for the power management system (PMS) that will govern the operation of the power plant. In this paper, DigSilent Simulation Language (DSL) is used for simulation of Turbine-Governor and excitation system. Also DigSilent Program Language (DPL) is used to simulate action of PMS when it is required. Also, different dynamic contingencies related to Damavand power plant such as Trip of grid connection, Trip of One GT, Trip of load, Trip of Two GTs, Trip of Grid connection (According to governor logic, Automatic change mode acts, Solving frequency deviation by PMS) are considered. In each scenario, power plant stability is investigated and where required, PMS functions (such as load shedding and sending appropriate set points) are applied.}, 
keywords={control engineering computing;frequency control;load management;load shedding;power engineering computing;power system stability;reactive power control;voltage control;dynamic stability analysis;PMS function;islanded power system control;frequency control;reactive power;load shedding;DigSilent software;power management system;DigSilent Simulation Language;DSL;Turbine-Governor;DigSilent Program Language;DPL;Damavand power plant;Trip of One GT;Trip of load;Trip of Two GT;Trip of Grid connection;power plant stability;Power system stability;Turbines;Generators;Load modeling;Velocity control;Power system dynamics;power system stability;power management system (PMS);Digsilent Software;Turbine-Governor model}, 
doi={10.1109/EEEIC-2.2013.6737898}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8004372, 
author={A. A. Ahmad and P. Brereton and P. Andras}, 
booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={A Systematic Mapping Study of Empirical Studies on Software Cloud Testing Methods}, 
year={2017}, 
volume={}, 
number={}, 
pages={555-562}, 
abstract={Context: Software has become more complicated, dynamic, and asynchronous than ever, making testing more challenging. With the increasing interest in the development of cloud computing, and increasing demand for cloud-based services, it has become essential to systematically review the research in the area of software testing in the context of cloud environments. Objective: The purpose of this systematic mapping study is to provide an overview of the empirical research in the area of software cloud-based testing, in order to build a classification scheme. We investigate functional and non-functional testing methods, the application of these methods, and the purpose of testing using these methods. Method: We searched for electronically available papers in order to find relevant literature and to extract and analyze data about the methods used. Result: We identified 69 primary studies reported in 75 research papers published in academic journals, conferences, and edited books. Conclusion: We found that only a minority of the studies combine rigorous statistical analysis with quantitative results. The majority of the considered studies present early results, using a single experiment to evaluate their proposed solution.}, 
keywords={cloud computing;program testing;statistical analysis;software cloud testing methods;classification scheme;nonfunctional testing methods;statistical analysis;Cloud computing;Security;Software testing;Reliability;Systematics;systematic mapping study;cloud software testing methods;software testing;empirical studies}, 
doi={10.1109/QRS-C.2017.94}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{1186689, 
author={S. Brini and D. Benjelloun and F. Castanier}, 
booktitle={2003 Design, Automation and Test in Europe Conference and Exhibition}, 
title={A flexible virtual platform for computational and communication architecture exploration of DMT VDSL modems}, 
year={2003}, 
volume={}, 
number={}, 
pages={164-169 suppl.}, 
abstract={In this paper a high-level SoC architecture exploration of DMT (Discrete Multitone) VDSL transceivers (Very high speed Digital Subscriber Line) is presented. A flexible and complete virtual platform was developed for the purpose, exploiting the paradigm of "orthogonalization of concerns" (functionality independent from architecture) in the framework of Cadence VCC system level design tool. An accurate processor model, obtained through the back-annotation of profiling results on a target DSP core, allowed the exploration of different HW/SW partitioning and the study of the computational units required. A transaction-accurate VCC bus model was developed for the investigation of the on-chip bus architecture and its relevant parameters dimensioning.}, 
keywords={digital subscriber lines;modems;system-on-chip;hardware-software codesign;flexible virtual platform;DMT VDSL modems;communication architecture;high-level SoC architecture;DMT;Discrete Multitone;orthogonalization of concerns;Cadence VCC system level design tool;processor model;back-annotation;HW/SW partitioning;computational units;transaction-accurate VCC bus model;Computer architecture;OFDM modulation;Modems;Transceivers;DSL;System-level design;Digital signal processing;Quality of service;Testing;Libraries}, 
doi={10.1109/DATE.2003.1186689}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{6976605, 
author={M. A. Jiménez and Á. V. Gómez and N. M. Villegas and G. Tamura and L. Duchien}, 
booktitle={2014 IEEE 8th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems}, 
title={A Framework for Automated and Composable Testing of Component-Based Services}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={The vision of service-oriented computing has been largely developed on the fundamental principle of building systems by composing and orchestrating services in their control flow. Nowadays, software development is notably influenced by service-oriented architectures (SOAs), in which the quality of software systems is determined by the quality of the involved services and their actual composition. Despite the efforts on improving their individual quality, adding or replacing services in an evolving system can introduce failures, thus compromising the satisfaction of the system's functional and extra-functional requirements. These failures erode the trust in the SOA vision. Thus, a key issue for the industrial adoption of SOA is providing service providers, integrators, and consumers the means to build confidence that services behave according to the contracted quality conditions. In this paper we present a first version of PA SCA NI, a framework for specifying and executing test specifications for service-oriented systems. From a test specification, PA SCA NI generates a configuration of testing services compliant with the Service Component Architecture (SCA) specification, which can be composed to integrate different testing strategies, being these tests traceable in an automated way. Our evaluation results show the applicability of the framework and a substantial gain in the tester's effort for developing tests.}, 
keywords={formal specification;object-oriented programming;program testing;service-oriented architecture;software quality;service-oriented computing;control flow;software development;service-oriented architectures;software systems;SOA vision;industrial adoption;service providers;service integrators;service consumers;PASCANI;service-oriented systems;test specification;testing services;service component architecture specification;component-based service testing;system functional requirements;system extra-functional requirements;Testing;Service-oriented architecture;Computer architecture;DSL;Software systems;Software service testing;SOA testing;composable tests;SCA testing}, 
doi={10.1109/MESOCA.2014.9}, 
ISSN={2326-6937}, 
month={Sept},}
@ARTICLE{7270333, 
author={N. Mellegård and A. Ferwerda and K. Lind and R. Heldal and M. R. V. Chaudron}, 
journal={IEEE Transactions on Software Engineering}, 
title={Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study}, 
year={2016}, 
volume={42}, 
number={3}, 
pages={245-260}, 
abstract={Domain-specific modelling (DSM) is a modern software development technology that aims at enhancing productivity. One of the claimed advantages of DSM is increased maintainability of software. However, current empirical evidence supporting this claim is lacking. In this paper, we contribute evidence from a case study conducted at a software development company. We study how the introduction of DSM affected the maintenance of a legacy system. We collected data about the maintenance phase of a system that was initially developed using manual programming, but which was gradually replaced by DSM development. We performed statistical analyses of the relation between the use of DSM and the time needed to resolve defects, the defect density, and the phase in which defects were detected. The results show that after introducing DSM the defect density is lower, that defects are found earlier, but resolving defects takes longer. Other observed benefits are that the number of developers and the number of person-hours needed for maintaining the system decreased, and the portability to new platforms increased. Our findings are useful for organizations that consider introducing DSM and would like to know which benefits can be realized in software maintenance.}, 
keywords={software maintenance;statistical analysis;domain-specific modelling;software maintenance;DSM;software development technology;software maintainability;software development company;legacy system;manual programming;statistical analysis;DSL;Maintenance engineering;Unified modeling language;Business;Software maintenance;Productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity}, 
doi={10.1109/TSE.2015.2479221}, 
ISSN={0098-5589}, 
month={March},}
@INPROCEEDINGS{1253823, 
author={S. Brini and D. Benjelloun and F. Castanier}, 
booktitle={2003 Design, Automation and Test in Europe Conference and Exhibition}, 
title={A flexible virtual platform for computational and communication architecture exploration of DMT VDSL modems}, 
year={2003}, 
volume={}, 
number={}, 
pages={164-169}, 
abstract={}, 
keywords={Computer architecture;OFDM modulation;Modems;Transceivers;DSL;System-level design;Digital signal processing;Quality of service;Testing;Libraries}, 
doi={10.1109/DATE.2003.1253823}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{8428788, 
author={R. Bussenot and H. Leblanc and C. Percebois}, 
booktitle={2018 13th Annual Conference on System of Systems Engineering (SoSE)}, 
title={Orchestration of Domain Specific Test Languages with a Behavior Driven Development approach}, 
year={2018}, 
volume={}, 
number={}, 
pages={431-437}, 
abstract={An airplane is composed by many complexes and embedded systems. During the integration testing phase, the design office produces requirements of the targeted system, and the test center produces concrete test procedures to be executed on a test bench. In this context, integration tests are mostly written in natural language and manually executed step by step by a tester. In order to formalize integration tests procedures dedicated to each system with domain specific languages approved by testers, and in order to automatize integration tests, we have introduced agile practices in the integration testing phase. We have chosen a Behavior Driven Development (BDD) approach to orchestrate Domain Specific Test Languages produced for the ACOVAS FUI project.}, 
keywords={aircraft;embedded systems;formal specification;program testing;specification languages;domain specific test languages;behavior driven development approach;embedded systems;integration testing phase;targeted system;test center;test bench;domain specific languages;integration test procedures;BDD approach;ACOVAS FUI project;Testing;Software;DSL;Natural languages;Hardware;Aerospace electronics;Aircraft}, 
doi={10.1109/SYSOSE.2018.8428788}, 
ISSN={}, 
month={June},}
@ARTICLE{1519613, 
author={Young-Jae Cho and Seung-Hoon Lee}, 
journal={IEEE Transactions on Circuits and Systems I: Regular Papers}, 
title={An 11b 70-MHz 1.2-mm/sup 2/ 49-mW 0.18-/spl mu/m CMOS ADC with on-chip current/voltage references}, 
year={2005}, 
volume={52}, 
number={10}, 
pages={1989-1995}, 
abstract={This work proposes an 11b 70-MHz CMOS pipelined analog-digital converter (ADC) as one of core circuit blocks for very high speed digital subscriber line system applications. The proposed ADC for the internal use has the strictly limited number of externally connected I/O pins while the ADC employs on-chip CMOS current/voltage references and a merged-capacitor switching technique to improve ADC performances. The ADC implemented in a 0.18-/spl mu/m 1P4M CMOS technology shows the maximum signal-to-noise distortion ratio (SNDR) of 60 dB at 70 MSample/s. The ADC maintains the SNDR of 58 dB and the spurious-free dynamic resistance of 68 dB for input frequencies up to the Nyquist rate at 60 MSample/s. The measured differential and integral nonlinearities of the ADC are within /spl plusmn/0.63 and /spl plusmn/1.21 LSB, respectively. The active chip area is 1.2 mm/sup 2/ and the ADC consumes 49 mW at 70 MSample/s at 1.8 V.}, 
keywords={analogue-digital conversion;CMOS digital integrated circuits;system-on-chip;capacitor switching;digital subscriber lines;VHF circuits;radiofrequency integrated circuits;reference circuits;network topology;integrated circuit design;CMOS ADC;on-chip current reference;on-chip voltage reference;CMOS pipelined analog-digital converters;core circuit blocks;very high speed digital subscriber lines;merged-capacitor switching technique;analogue-digital conversion;CMOS digital integrated circuits;system-on-chip;switched capacitor networks;VHF circuits;radiofrequency integrated circuits;reference circuits;network topology;integrated circuit design;70 MHz;49 mW;0.18 micron;1.8 V;Analog-digital conversion;CMOS technology;CMOS digital integrated circuits;CMOS analog integrated circuits;DSL;Pins;Voltage;Frequency;Distortion measurement;Electrical resistance measurement;Analog–digital converter (ADC);CMOS;low power;on-chip references}, 
doi={10.1109/TCSI.2005.853251}, 
ISSN={1549-8328}, 
month={Oct},}
@INPROCEEDINGS{8227296, 
author={K. S. Tse and P. C. Johnson}, 
booktitle={2017 IEEE Security and Privacy Workshops (SPW)}, 
title={A Framework for Validating Session Protocols}, 
year={2017}, 
volume={}, 
number={}, 
pages={110-119}, 
abstract={Communication protocols are complex, their implementations are difficult, causing many unintended (and severe) vulnerabilities in protocol parsing. While the problem of packet parsing is solved, session parsing remains challenging. Building on existing systems that reliably parse individual messages, we present our four-component framework for implementing protocol session parsers with the goal to improve security of protocol parsing: specification of a protocol message, description of a protocol state machine, testing routines to validate implementations against fake and real data, and graph generation to visualize implementations. This framework enables the creation of a session parser, which validates individual protocol messages in the context of other messages in the same conversation. This is helpful because more secure parsers lead to more secure communication.}, 
keywords={protocols;telecommunication security;protocol parsing;protocol message;protocol state machine;session protocols;communication protocols;packet parsing;session parsing;four-component framework;protocol session parsers;Protocols;Data structures;Security;Testing;DSL;Semantics;Language-theoretic security;protocol state machine;protocol parsing;session parsing}, 
doi={10.1109/SPW.2017.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8080500, 
author={P. Mueller and T. Belschner and R. Reichel}, 
booktitle={2017 IEEE AUTOTESTCON}, 
title={Automated test artifact generation for a distributed avionics platform utilizing abstract state machines}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The development of complex and highly safety-critical avionics systems, such as fly-by-wire, is typically linked with high efforts, risks and thus costs. Especially with regard to certification the testing activities during verification are playing a major role. This paper introduces the automatization complex of the testing artifact generation by use of Abstract State Machines (ASM), which allows a unified approach for system and software testing. The baseline is the Flexible Platform technology (a platform based development approach) currently under development by the Institute of Aircraft Systems (ILS) of the University of Stuttgart. The remaining automatization complex is the automated generation of certification relevant documentation, i.e. the requirements. These three complexes establish the AAA-Process which lays the foundation for an effective total system capability for complex avionics systems while simultaneously mitigating risks and costs. The actual test artifact generation is strictly aligned to development standards used in the aviation industry. Requirements exist as classes in a textual representation as well as in a specification model, represented by ASMs. The functional behavior, as described by the models, serves as a test oracle for test case generation. For this the model is translated into a graph system, instrumented by selectable testing methods and executed. The resulting trace data is used to automatically derive test procedures under consideration of the corresponding test environment as scripts, which are directly executable within our testing infrastructure consisting of a HiL simulation. Furthermore this includes the automatic generation of the associated traceability data and test specification documentation. An initial framework has been defined to support exchangeability of individual tasks in the generation tool-chain. The feasibility of the approach has been demonstrated by testing the complete heterogeneous signal communication of an exemplary avionics system, resp. platform instance, at system level as well as at software high-level.}, 
keywords={aerospace computing;avionics;finite state machines;program testing;safety-critical software;abstract state machines;ASM;software testing;Flexible Platform technology;platform based development approach;certification relevant documentation;complex avionics systems;test oracle;test case generation;graph system;test specification documentation;HiL simulation;University of Stuttgart;safety-critical avionics systems;Institute of Aircraft Systems;distributed avionics platform;automated test artifact generation;Aerospace electronics;Software;Hardware;Tools;Aircraft;DSL;Testing}, 
doi={10.1109/AUTEST.2017.8080500}, 
ISSN={1558-4550}, 
month={Sept},}
@INPROCEEDINGS{1689533, 
author={Dae-Woo Kim and Hyun-Min Lim and Sang-Kon Lee}, 
booktitle={2006 IEEE International Symposium on Consumer Electronics}, 
title={A Case Study on Testing and Evaluation in the KT-OSS Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper describes the test and evaluation activities for the development of the KT-OSS (Korea Telecom Operations Support System). In this paper, we show the test and evaluation phases for the development and maintenance of the KT-OSS. To ensure the successful development of the KT-OSS, we performed various tests related to functionality, efficiency and others. We also show the criteria for them and deal with the test organizations and the test-bed for managing and controlling the quality of the KT-OSS. And we describe our experiences in performing these tests. Through these test and evaluation activities, we were able to successfully develop and release the KT-OSS}, 
keywords={program testing;software performance evaluation;software quality;telecommunication computing;KT-OSS development;Korea Telecom Operations Support System;quality management;test activity;evaluation activity;Computer aided software engineering;Life testing;Software testing;Programming;System testing;Performance evaluation;Quality management;Telecommunications;Information management;DSL;Test;Evaluation;Operations Support System;Software Development Lifecycle}, 
doi={10.1109/ISCE.2006.1689533}, 
ISSN={0747-668X}, 
month={June},}
@INPROCEEDINGS{8327149, 
author={L. P. Binamungu and S. M. Embury and N. Konstantinou}, 
booktitle={2018 IEEE Workshop on Validation, Analysis and Evolution of Software Tests (VST)}, 
title={Detecting duplicate examples in behaviour driven development specifications}, 
year={2018}, 
volume={}, 
number={}, 
pages={6-10}, 
abstract={In Behaviour-Driven Development (BDD), the behaviour of the software to be built is specified as a set of example interactions with the system, expressed using a “Given-When-Then” structure. The examples are written using customer language, and are readable by end-users. They are also executable, and act as tests that determine whether the implementation matches the desired behaviour or not. This approach can be effective in building a common understanding of the requirements, but it can also face problems. When the suites of examples grow large, they can be difficult and expensive to change. Duplication can creep in, and can be challenging to detect manually. Current tools for detecting duplication in code are also not effective for BDD examples. Moreover, human concerns of readability and clarity can rise. We present an approach for detecting duplication in BDD suites that is based around dynamic tracing, and describe an evaluation based on three open source systems.}, 
keywords={formal specification;program diagnostics;public domain software;duplication;BDD suites;open source systems;behaviour driven development specifications;customer language;duplicate examples detection;software behaviour;Given-When-Then structure;dynamic tracing;Tools;Production;Software;Syntactics;Semantics;Cloning;DSL;behaviour-driven development;duplication detection;dynamic tracing}, 
doi={10.1109/VST.2018.8327149}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6210038, 
author={C. G. Bianchin and R. Demonti and A. R. de Almeida and M. T. da Silva Filho and C. L. da Silva Pinto}, 
booktitle={2012 IEEE International Conference on Industrial Technology}, 
title={Development of static switch with high speed algorithm to fault detection}, 
year={2012}, 
volume={}, 
number={}, 
pages={808-814}, 
abstract={This paper presents the updated results on research for development of a static switch for operation in medium voltage. It shows the results of the algorithm that detects an outage and enables the digital processing to control the static switch. This algorithm was based on the Clarke and Park transforms. The operation of the switch consists of transferring the power supply from the main source, where an outage occurred, to an alternative power source (backup). The load stays off for a time of milliseconds. A low voltage prototype was built, to allow the evaluation of the algorithm. Then a prototype of the static switch was built in voltage of 13,8kV. The first tests - feeding resistive load - are presented in voltage of 13.8 kV.}, 
keywords={power semiconductor switches;thyristors;static switch;high speed algorithm;fault detection;medium voltage;digital processing;power supply;alternative power source;low voltage prototype;resistive load;voltage 13.8 kV;DSL;Switches;algorithm;digital processing;series-connected thyristors;medium voltage}, 
doi={10.1109/ICIT.2012.6210038}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7073229, 
author={C. Mahmoudi and F. Mourlin}, 
booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
title={Business Process Management with mobile routes}, 
year={2014}, 
volume={}, 
number={}, 
pages={420-427}, 
abstract={Business processes are milestone of the information system of any companies. Their availability is a crucial aspect. We provide a solution for the high level of availability of business processes by the use of cluster of enterprise service buses (ESB). Our approach is based on the dynamic creation of the route between the business services and the migration of a runtime context from one ESB to another one. So, we insure the management of business processes over a cluster and measure the impact of such incident. Through the use of log, we also report these events which allow the administrator for preparing updates of the information system. With the use of open source software, we guarantee the reuse of our case study with other kinds of enterprise service bus, which respect open standard exchanges like XML language and REST API.}, 
keywords={application program interfaces;business data processing;mobile computing;public domain software;service-oriented architecture;Web services;XML;business process management;mobile route;information system;enterprise service bus;ESB;business services;open source software;XML language;REST API;Business;Containers;Context;DSL;XML;Servers;Routing;SOA architecture;orchestration;cluster of bus;message routing;web service}, 
doi={10.1109/AICCSA.2014.7073229}, 
ISSN={2161-5330}, 
month={Nov},}
@INPROCEEDINGS{7203010, 
author={R. Abreu and H. Erdogmus and A. Perez}, 
booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
title={CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts}, 
year={2015}, 
volume={2}, 
number={}, 
pages={551-554}, 
abstract={Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plug in mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, Code Aware, for distributed and fine-grained artifact analysis. Code Aware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. Code ware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems, (b) the ability to perform both static and dynamic analyses on these artifacts, and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of Code Aware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges.}, 
keywords={productivity;program diagnostics;software quality;CodeAware;sensor-based fine-grained monitoring;software artifact management;continuous integration tools;CI tools;artifact analysis capabilities;plug in mechanisms;distributed artifact analysis;fine-grained artifact analysis;ecosystem;monitors;code quality improvement;team productivity improvement;static analysis;dynamic analysis;automated actuators;Probes;Software;Ecosystems;Software engineering;Monitoring;Electronic mail;DSL}, 
doi={10.1109/ICSE.2015.192}, 
ISSN={0270-5257}, 
month={May},}
@ARTICLE{1341380, 
author={M. Varsamou and T. Antonakopoulos and N. Papandreou}, 
journal={IEEE Design Test of Computers}, 
title={From protocol models to their implementation: a versatile testing methodology}, 
year={2004}, 
volume={21}, 
number={5}, 
pages={416-428}, 
abstract={The design and test of communication protocols relies extensively on formal description languages. In this protocol design and verification scheme, high-level models serve in generating simulation sequences for low-level models, and all simulation is based on directed testing. The methodology is versatile and flexible, and difficult to set up the first time.}, 
keywords={protocols;formal verification;formal specification;conformance testing;communication protocol testing;formal description languages;protocol design;formal verification;Protocols;Object oriented modeling;System testing;Mathematical model;Design methodology;Computational modeling;Appropriate technology;Process design;DSL;Automata}, 
doi={10.1109/MDT.2004.61}, 
ISSN={0740-7475}, 
month={Sept},}
@ARTICLE{4749742, 
author={P. Scully and R. Skehill and S. McGrath}, 
journal={IEEE Wireless Communications}, 
title={Mobility in an RF-isolated test platform}, 
year={2008}, 
volume={15}, 
number={6}, 
pages={8-15}, 
abstract={Understanding the practical impact of mobility in a wireless network is essential for the wireless networks of tomorrow. Mobility influences the network performance, behavior, and ability to provide seamless service across a wide area. Testing and evaluating the real impact of mobility is difficult in the field with so many variables such as interference, fading, and so on. Experimental wireless evaluation in a test environment must correspond to an actual deployment. Furthermore, it is important to achieve repeatability without sacrificing realism. This study presents a functional test platform that uses real IEEE 802.11 equipment, providing repeatability and reliability. The platform is used to test the practical impact of device movement in a WLAN cell while voice and data applications are running. The mobility characteristics of wireless devices are based on individual models used by researchers with the addition of real aspects of mobility from empirical studies to improve realism.}, 
keywords={Testing;Fading;Wireless networks;Wireless LAN;Radio transmitters;Interference;GSM;3G mobile communication;Receivers;Wireless communication}, 
doi={10.1109/MWC.2008.4749742}, 
ISSN={1536-1284}, 
month={December},}
@INPROCEEDINGS{7785748, 
author={J. Carter and W. B. Gardner}, 
booktitle={2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)}, 
title={BHive: Towards Behaviour-Driven Development Supported by B-Method}, 
year={2016}, 
volume={}, 
number={}, 
pages={249-256}, 
abstract={Behaviour-Driven Development (BDD) is an "outside-in" approach to software development built upon semi-formal mediums for specifying the behaviour of a system as it would be observed externally. Through the representation of a system as a collection of user stories and scenarios using BDD's notation, practitioners automate acceptance tests using examples of desired behaviour for the envisioned system. A formal model created in concert with BDD tests would provide valuable insight into test validity and enhance the visibility of the problem domain. This work called BHive builds upon the formal underpinnings of BDD scenarios by mapping their "Given," "When," and "Then" statements to "Precondition," "Command," and "Postcondition" constructs as introduced by Floyd-Hoare logic. We posit that this mapping allows for a B-Method representation to be created and that such a model is useful for exploring system behaviour and exposing gaps in requirements. We also outline extensions to BDD tooling required for the described integration and present benefits of the BHive approach to integrating formalism within a BDD project.}, 
keywords={formal logic;software engineering;behaviour-driven development;BHive approach;BDD;software development;B-method representation;Floyd-Hoare logic;Software;Testing;Stakeholders;Shape;Documentation;Conferences;BDD;Behaviour-Driven Development;B-Method;Agile}, 
doi={10.1109/IRI.2016.39}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7018484, 
author={B. Hois and S. Sobernig and M. Strembeck}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Natural-language scenario descriptions for testing core language models of domain-specific languages}, 
year={2014}, 
volume={}, 
number={}, 
pages={356-367}, 
abstract={The core language model is a central artifact of domain-specific modeling languages (DSMLs) as it captures all relevant domain abstractions and their relations. Natural-language scenarios are a means to capture requirements in a way that can be understood by technical as well as non-technical stakeholders. In this paper, we use scenarios for the testing of structural properties of DSML core language models. In our approach, domain experts and DSML engineers specify requirements via structured natural-language scenarios. These scenario descriptions are then automatically transformed into executable test scenarios providing forward and backward traceability of domain requirements. To demonstrate the feasibility of our approach, we used Eclipse Xtext to implement a requirements language for the definition of semi-structured scenarios. Transformation specifications generate executable test scenarios that run in our test platform which is built on the Eclipse Modeling Framework and the Epsilon language family.}, 
keywords={Testing;Unified modeling language;Vocabulary;Natural languages;Collaboration;Abstracts;Syntactics;Domain-Specific Modeling;Natural-Language Requirement;Scenario-based Testing;Metamodel Testing;Eclipse Modeling Framework;Xtext;Epsilon;EUnit}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7140371, 
author={S. Gebert and C. Schwartz and T. Zinner and P. Tran-Gia}, 
booktitle={2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
title={Continuously delivering your network}, 
year={2015}, 
volume={}, 
number={}, 
pages={766-769}, 
abstract={Softwarization and cloudification of networks through software defined networking and network functions virtualisation promise a new degree of flexibility and agility. By moving logic from device firmware into software applications and applying software development mechanisms, innovation can be introduced with less effort. Concrete ways how to operate and orchestrate such systems are not yet defined. The process of making changes to a controller software or a virtualized network function in a production network without the risk of network disruption is not covered by literature. Complexity of systems brings the risk of unexpected side-effects and has so long been a show-stopper for administrators applying changes to networking devices. This paper suggests the adaption of the successful concept of continuous delivery into the software defined networking world. Test-driven development and automatic acceptance tests demonstrate that the software engineering community already found ways to ensure that changes do not break. Applied to network engineering, the adaption of continuous delivery can be seen as an enabler for risk-free and frequent changes in production infrastructure through push button deployments.}, 
keywords={cloud computing;firmware;software defined networking;software engineering;virtualisation;network softwarization;network cloudification;software defined networking;network function virtualisation;firmware;software development mechanisms;network engineering;push button deployments;Software;Pipelines;Production;Servers;Measurement;Technological innovation}, 
doi={10.1109/INM.2015.7140371}, 
ISSN={1573-0077}, 
month={May},}
@INPROCEEDINGS{7928008, 
author={H. Ukai and X. Qu}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Test Design as Code: JCUnit}, 
year={2017}, 
volume={}, 
number={}, 
pages={508-515}, 
abstract={In a development process where testing is highly automated, there is a major challenge to cope with issues such as huge test size and test stability. In this paper, we propose a model-based testing (MBT) tool called JCUnit, which generates a test suite from a model given as a Java class. Unlike other tools, it is designed to generate small and stable test suites and supports various popular models. With this tool, developers can apply MBT approach to their products without learning domain-specific language of proprietary MBT tools. Moreover, features such as portability and pluggability make it useful in a wide range of phases from unit testing to system testing. As a result, the efforts required in practical software testing will be reduced.}, 
keywords={Java;program testing;software portability;software tools;software testing;system testing;unit testing;software pluggability;software portability;MBT tool;Java class;test suite;model-based testing;test stability;test size;test design;JCUnit;Tools;Java;Software;Graphical user interfaces;Pipelines;Software testing;automated testing;FSM spec;model-based testing;combinatorial interaction testing}, 
doi={10.1109/ICST.2017.58}, 
ISSN={}, 
month={March},}
@ARTICLE{6200023, 
author={V. Garcia-Diaz and B. C. P. G-Bustelo and O. Sanjuan-Martinez and E. R. Nunez Valdez and J. M. C. Lovelle}, 
journal={IET Software}, 
title={MCTest: towards an improvement of match algorithms for models}, 
year={2012}, 
volume={6}, 
number={2}, 
pages={127-139}, 
abstract={Owing to the increasing importance of model-driven engineering (MDE) and the changes experienced by software systems over their life cycle, the calculation, representation and visualisation of matches and differences between two different versions of the same model are becoming more necessary and useful. This study shows the need for improvement in the algorithms for calculating the relationships between models and presents a tool to test different implementations, thus reducing the effort required to measure, compare or create new algorithms. To demonstrate the need for improvement and the framework developed, the authors have created different models that conform to the metamodel of a domain-specific language. Subsequently, the authors compared these models using the algorithms of the eclipse modelling framework (EMF) Compare tool, part of the eclipse modeling project, which is the framework of reference for MDE. Thus, in the case study, the authors tool is used to measure the quality of the comparisons performed by EMF Compare.}, 
keywords={data structures;data visualisation;software engineering;specification languages;MCTest;match algorithm;model-driven engineering;software system;match calculation;match representation;match visualisation;eclipse modelling framework;EMF Compare tool}, 
doi={10.1049/iet-sen.2011.0040}, 
ISSN={1751-8806}, 
month={April},}
@INPROCEEDINGS{7886903, 
author={D. Menendez and S. Nagarakatte}, 
booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)}, 
title={Termination-Checking for LLVM Peephole Optimizations}, 
year={2016}, 
volume={}, 
number={}, 
pages={191-202}, 
abstract={Mainstream compilers contain a large number of peephole optimizations, which perform algebraic simplification of the input program with local rewriting of the code. These optimizations are a persistent source of bugs. Our recent research on Alive, a domain-specific language for expressing peephole optimizations in LLVM, addresses a part of the problem by automatically verifying the correctness of these optimizations and generating C++ code for use with LLVM. This paper identifies a class of non-termination bugs that arise when a suite of peephole optimizations is executed until a fixed point. An optimization can undo the effect of another optimization in the suite, which results in non-terminating compilation. This paper (1) proposes a methodology to detect non-termination bugs with a suite of peephole optimizations, (2) identifies the necessary condition to ensure termination while composing peephole optimizations, and (3) provides debugging support by generating concrete input programs that cause non-terminating compilation. We have discovered 184 optimization sequences, involving 38 optimizations, that cause non-terminating compilation in LLVM with Alive-generated C++ code.}, 
keywords={C++ language;program compilers;program debugging;termination-checking;LLVM peephole optimizations;nontermination bugs;nonterminating compilation;debugging;input programs;Alive-generated C++ code;Optimization;Computer bugs;C++ languages;Semantics;Concrete;Toxicology;Pattern matching;Compiler Verification;Peephole Optimization;Alive;Termination}, 
doi={10.1145/2884781.2884809}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{5190273, 
author={X. Wang and P. Xu}, 
booktitle={2009 International Conference on Information Technology and Computer Science}, 
title={Build an Auto Testing Framework Based on Selenium and FitNesse}, 
year={2009}, 
volume={2}, 
number={}, 
pages={436-439}, 
abstract={Writing auto testing is a required engineering technique that can save time and money, and help businesses better respond to changes. But if we use testing framework improperly, more problems would possibly be caused. An auto testing framework based on Selenium and FitNesse is discussed in this article which can help with those problems. The framework use Selenium APIs to get page value, DbFit to init database, FitNesse to manage the test fixture, and a special DSL to write test fixture. It could greatly reduce the line numbers of testing code and the project developing period, lower the random error rate, facilitate writing fixture table, improve the coding productivity, and the quality of final product.}, 
keywords={application program interfaces;groupware;Internet;object-oriented programming;program testing;public domain software;software engineering;FitNesse;auto testing framework;Selenium API;page value;DbFit;fixture table writing;coding productivity;Software testing;Fixtures;Java;Automatic testing;Information security;Writing;Open source software;Programming;Information technology;Computer science;Auto Testing Framework;Selenium;FitNesse}, 
doi={10.1109/ITCS.2009.228}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{5525697, 
author={E. Blondel and C. Monney}, 
booktitle={Intelec 2010}, 
title={Efficient powering of communication and IT equipments using rotating UPS}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Today, the demand for uninterrupted power supply for Telecommunication and Internet services increases drastically. Following the same trend, cooling demand explodes. Supplying sufficient power with extremely high reliability becomes even more challenging. The era of telephone exchanges using 48V power supply and 8 hours of battery backup is past. Telecom Operators are currently migrating from POTS to all IP. In addition, broadband Internet access for everyone using DSL or fibre is already reality. More and more applications like TV on demand, streaming services, online gaming or entertainment are very power hungry. For such large systems, static UPS systems are no more efficient. A better alternative is offered by rotating UPS, also known as "No-break". In addition to requiring less space and no batteries, these systems have a better power efficiency too.}, 
keywords={computer centres;computer power supplies;cooling;telecommunication power supplies;uninterruptible power supplies;communication equipment power;IT equipment power;rotating UPS;uninterrupted power supply;telecommunication power supply;Internet service power supply;cooling demand;no-break power;Uninterruptible power systems;Batteries;Alternators;Energy storage;Power quality;Power supplies;Cooling;Ventilation;Inductors;Acoustic testing}, 
doi={10.1109/INTLEC.2010.5525697}, 
ISSN={0275-0473}, 
month={June},}
@INPROCEEDINGS{5758967, 
author={E. Blondel and C. Monney}, 
booktitle={4th International Telecommunication - Energy special conference}, 
title={Efficient powering of communication and IT equipments using rotating UPS}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Today, the demand for uninterrupted power supply for Telecommunication and Internet services increases drastically. Following the same trend, cooling demand explodes. Supplying sufficient power with extremely high reliability becomes even more challenging. The era of telephone exchanges using 48V power supply and 8 hours of battery backup is past. Telecom Operators are currently migrating from POTS to all IP. In addition, broadband Internet access for everyone using DSL or fibre is already reality. More and more applications like TV on demand, streaming services, online gaming or entertainment are very power hungry. For such large systems, static UPS systems are no more efficient. A better alternative is offered by rotating UPS, also known as "No-break". In addition to requiring less space and no batteries, these systems have a better power efficiency too.}, 
keywords={Uninterruptible power systems;Electromagnetic compatibility;Batteries;Generators;Inductors;Copper}, 
doi={}, 
ISSN={}, 
month={May},}
@ARTICLE{8254315, 
author={A. Johanson and W. Hasselbring}, 
journal={Computing in Science Engineering}, 
title={Software Engineering for Computational Science: Past, Present, Future}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={While the importance of in silico experiments for the scientific discovery process increases, state-of-the-art software engineering practices are rarely adopted in computational science. To understand the underlying causes for this situation and to identify ways for improving the current situation, we conduct a literature survey on software engineering practices in computational science. As a result of our survey, we identified 13 recurring key characteristics of scientific software development that can be divided into three groups: characteristics that results (1) from the nature of scientific challenges, (2) from limitations of computers, and (3) from the cultural environment of scientific software development. Our findings allow us to point out shortcomings of existing approaches for bridging the gap between software engineering and computational science and to provide an outlook on promising research directions that could contribute to improving the current situation.}, 
keywords={Software;Scientific computing;Software engineering;Computational modeling;Computers;Productivity;Object recognition;scientific software development;domain-specific languages;software performance engineering;software testing;requirements engineering}, 
doi={10.1109/MCSE.2018.108162940}, 
ISSN={1521-9615}, 
month={},}
@INPROCEEDINGS{1387329, 
author={S. Kundu and T. M. Mak and R. Galivanche}, 
booktitle={2004 International Conferce on Test}, 
title={Trends in manufacturing test methods and their implications}, 
year={2004}, 
volume={}, 
number={}, 
pages={679-687}, 
abstract={Driven by market applications in the areas of computing, networking, storage, optical, wireless, portable, and consumer electronics, semiconductor chips today are as diverse as ever. Confluence of multiple applications and rapid integration has also driven the heterogeneity of chips. Test methods have evolved with the products. However, the basic goals in testing remain the same: quality of product, recurring and non-recurring costs and time to market. In this paper we try to catalog some commonly used test methods, identify their associated DFT requirements and trends in terms of tester requirements. Given the diversity of semiconductors chips today such as various PLDs, volatile and non-volatile memories, analog, mixed signal, FPGA, ASIC, SOC, MEMs and processors, it is impossible for a paper of this nature to be fully comprehensive. So we limit our focus on processor, ASIC and SOCs.}, 
keywords={design for testability;system-on-chip;microprocessor chips;manufacturing test methods;product quality;recurring cost;nonrecurring cost;time to market;DFT requirements;tester requirements;semiconductors chips;volatile memory;nonvolatile memory;PLD;FPGA;ASIC;SOC;MEM;processors;Manufacturing;Testing;Optical computing;Application specific integrated circuits;Semiconductor device manufacture;Computer networks;Portable computers;Optical fiber networks;Consumer electronics;Costs}, 
doi={10.1109/TEST.2004.1387329}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6676515, 
author={A. S. Nascimento and C. M. F. Rubira and F. Castor}, 
booktitle={2013 IEEE 7th International Conference on Self-Adaptive and Self-Organizing Systems}, 
title={Using CVL to Support Self-Adaptation of Fault-Tolerant Service Compositions}, 
year={2013}, 
volume={}, 
number={}, 
pages={261-262}, 
abstract={We present a dynamic software product line to support fault-tolerant service compositions. Architectural variability is specified and resolved by Common Variability Language (CVL). CVL is a generic variability modeling language that enables the transformation of a product line model into a configured, new product model. At runtime, whenever it is necessary to determine a fault tolerance technique more adapted to the context (i.e. a new product) the correspondent product model is dynamically generated by executing CVL model-to-model transformation. Based on the comparison of the reflection model with the target product model, the adaptation process is fully automated.}, 
keywords={product development;self-adjusting systems;simulation languages;software fault tolerance;software reusability;fault-tolerant service composition self-adaptation;dynamic software product line;architectural variability;Common Variability Language;generic variability modeling language;product line model;runtime;CVL model-to-model transformation;reflection model;Adaptation models;Fault tolerance;Fault tolerant systems;Software;Unified modeling language;Runtime;Quality of service;Fault-tolerant Systems;Self-Adaptation;CVL}, 
doi={10.1109/SASO.2013.34}, 
ISSN={1949-3673}, 
month={Sept},}
@ARTICLE{715185, 
author={}, 
journal={IEEE Spectrum}, 
title={What's ahead for design on the web}, 
year={1998}, 
volume={35}, 
number={9}, 
pages={53-63}, 
abstract={Panel discussion: Experts: the Web offers better design collaboration and a path toward greater tool interoperability.}, 
keywords={Internet;Computer aided engineering;Design engineering;Design automation;Computer science;Circuits;Process design;Web sites;Electrical engineering;Workstations}, 
doi={10.1109/MSPEC.1998.715185}, 
ISSN={0018-9235}, 
month={Sept},}
@INPROCEEDINGS{7107423, 
author={J. Gmeiner and R. Ramler and J. Haslinger}, 
booktitle={2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={Automated testing in the continuous delivery pipeline: A case study of an online company}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Companies running an online business need to be able to frequently push new features and bug fixes from development into production. Successful high-performance online companies deliver code changes often several times per day. Their continuous delivery model supports the business needs of the online world. At the same time, however, such practices increase the risk of introducing quality issues and unwanted side effects. Rigorous test automation is therefore a key success factor for continuous delivery. In this paper we describe how automated testing is used in the continuous delivery pipeline of an Austrian online business company. The paper illustrates the complex technical and organizational challenges involved and summarizes the lessons from more than six years of practical experience in establishing and maintaining an effective continuous delivery pipeline.}, 
keywords={electronic commerce;program debugging;program testing;automated testing;continuous delivery pipeline;Austrian online business company;bug fixing;test automation;Pipelines;Testing;Databases;Companies;Production;Software;automated testing;continuous integration;continuous delivery;continusous deployment;dev ops}, 
doi={10.1109/ICSTW.2015.7107423}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6263963, 
author={A. K. Maji and F. A. Arshad and S. Bagchi and J. S. Rellermeyer}, 
booktitle={IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012)}, 
title={An empirical study of the robustness of Inter-component Communication in Android}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={Over the last three years, Android has established itself as the largest-selling operating system for smartphones. It boasts of a Linux-based robust kernel, a modular framework with multiple components in each application, and a security-conscious design where each application is isolated in its own virtual machine. However, all of these desirable properties would be rendered ineffectual if an application were to deliver erroneous messages to targeted applications and thus cause the target to behave incorrectly. In this paper, we present an empirical evaluation of the robustness of Inter-component Communication (ICC) in Android through fuzz testing methodology, whereby, parameters of the inter-component communication are changed to various incorrect values. We show that not only exception handling is a rarity in Android applications, but also it is possible to crash the Android runtime from unprivileged user processes. Based on our observations, we highlight some of the critical design issues in Android ICC and suggest solutions to alleviate these problems.}, 
keywords={Linux;mobile computing;operating system kernels;program testing;security of data;smart phones;virtual machines;intercomponent communication;Android;operating system;smartphones;Linux-based robust kernel;security-conscious design;virtual machine;ICC;fuzz testing methodology;Androids;Humanoid robots;Smart phones;Robustness;Testing;Runtime;Receivers;android;fuzz;security;smartphone;robustness;exception}, 
doi={10.1109/DSN.2012.6263963}, 
ISSN={2158-3927}, 
month={June},}
@INPROCEEDINGS{6229805, 
author={T. W. Schiller and B. Lucia}, 
booktitle={2012 Second International Workshop on Developing Tools as Plug-Ins (TOPI)}, 
title={Playing cupid: The IDE as a matchmaker for plug-ins}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={We describe a composable, data-driven, plug-in ecosystem for IDEs. Inspired by Unix's and Windows Power-Shell's pipeline communication models, each plug-in declares data-driven capabilities. Developers can then seamlessly mix, match, and combine plug-in capabilities to produce new insight, without modifying the plug-ins. We formalize the architecture using the polymorphic lambda calculus, with special types for source and source locations; the type system prevents nonsensical plug-in combinations, and helps to inform the design of new tools and plug-ins. To illustrate the power of the formalism, we describe several synergies between existing plug-ins (and tools) made possible by the ecosystem.}, 
keywords={ecology;lambda calculus;pipeline processing;software architecture;type theory;Unix;playing cupid;IDE;plug-ins matchmaker;plug-in ecosystem;Unix;Windows;Power-Shell pipeline communication models;data-driven capability;plug-in capability;polymorphic lambda calculus;source locations;type system;nonsensical plug-in combinations;Java;Pipelines;Biological system modeling;Contracts;Cloning;Ecosystems;Debugging}, 
doi={10.1109/TOPI.2012.6229805}, 
ISSN={2327-0748}, 
month={June},}
@ARTICLE{6772080, 
author={T. J. Aprille and D. V. Gupta and P. G. St. Amand}, 
journal={The Bell System Technical Journal}, 
title={D4 Digital Channel Bank Family: Dataport — Channel units for digital data system subrates}, 
year={1982}, 
volume={61}, 
number={9}, 
pages={2721-2740}, 
abstract={The single-channel dataports are a series of D4 channel units that convert the digital signal derived from one T-facility time slot by the D4 common circuits to an appropriate format at speeds of 64, 9.6, 4.8, or 2.4 kb/s for use in the Digital Data System (DDS). They come in two formats, the first being the DDS bipolar format for 64 kb/s and the second, for the remaining three speeds, being an EIA RS-449 format. Their error-correction feature ensures 10-8error-rate performance for a 10-3error-rate transmission channel. Advances in large-scale integration (LSI) technology have allowed the packaging of all the digital circuit functions needed into the space of a single channel unit. An on-board power converter unit generates the additional current required by the dataports over that needed by regular analog channel units. The local loop side of each channel unit uses integrated technology to achieve signal equalization and timing recovery. Standard DDS remote maintenance features are provided. The dataport channel units are easily installed and removed; they supply economical digital transmission.}, 
keywords={}, 
doi={10.1002/j.1538-7305.1982.tb03449.x}, 
ISSN={0005-8580}, 
month={Nov},}
@INPROCEEDINGS{8116418, 
author={I. Jimenez and A. Arpaci-Dusseau and R. Arpaci-Dusseau and J. Lofstead and C. Maltzahn and K. Mohror and R. Ricci}, 
booktitle={2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
title={PopperCI: Automated reproducibility validation}, 
year={2017}, 
volume={}, 
number={}, 
pages={450-455}, 
abstract={This paper introduces PopperCI, a continous integration (CI) service hosted at UC Santa Cruz that allows researchers to automate the end-to-end execution and validation of experiments. PopperCI assumes that experiments follow Popper, a convention for implementing experiments and writing articles following a DevOps approach that has been proposed recently. PopperCI runs experiments on public, private or government-fundend cloud infrastructures in a fully automated way. We describe how PopperCI executes experiments and present a use case that illustrates the usefulness of the service.}, 
keywords={cloud computing;government data processing;private government-fundend cloud;automated reproducibility validation;continous integration service;UC Santa Cruz;end-to-end execution;public government-fundend cloud;PopperCI;Tools;Runtime;Measurement;Conferences;Manuals;Writing}, 
doi={10.1109/INFCOMW.2017.8116418}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6550482, 
author={S. Esnaashari and I. Welch and P. Komisarczuk}, 
booktitle={2013 27th International Conference on Advanced Information Networking and Applications Workshops}, 
title={Determining Home Users' Vulnerability to Universal Plug and Play (UPnP) Attacks}, 
year={2013}, 
volume={}, 
number={}, 
pages={725-729}, 
abstract={Universal Plug and Play (UPnP) technology is used worldwide since it has simplified the installation and management of the devices. As a result, many devices are now equipped with UPnP capabilities. Unfortunately using UPnP in home routers puts routers at risk of abuse. For example, it is easier for hackers to discover the devices and use device vulnerabilities in order to make malicious attacks to cause financial or reputation damage to the users. In this paper, we have analyzed the UPnP protocol and its different vulnerabilities. Furthermore, we have emphasized how common the problem is with the home users' devices. Hence, we suggest a tool to achieve transparency in the health of the Internet by detecting UPnP enabled devices which are likely to be attacked on home networks. The tool will look for UPnP based attacks when people's routers have been compromised. The tool is easy to install and use for novice home users and maintains their privacy too. This project aims not only to implement a tool for a user to determine whether his/her system is vulnerable to a particular attack, but also to measure the prevalence of vulnerabilities at national or global level. Thus a larger framework is required to collect and manage the results from individual users.}, 
keywords={computer crime;computer network management;computer network security;home computing;Internet;protocols;telecommunication network routing;home users vulnerability;universal plug and play attacks;UPnP attacks;UPnP technology;device installation;device management;UPnP capability;home routers;hackers;device vulnerability;malicious attacks;financial damage;reputation damage;UPnP protocol;home users devices;Internet;UPnP enabled devices;home networks;UPnP based attacks;Servers;Protocols;Ports (Computers);Logic gates;Plugs;IP networks;Internet;Security;Network Measurement;UPnP}, 
doi={10.1109/WAINA.2013.225}, 
ISSN={}, 
month={March},}
@INBOOK{5396736, 
author={Liming Xiu}, 
booktitle={VLSI Circuit Design Methodology Demystified: A Conceptual Taxonomy}, 
title={CellBased ASIC Design Methodology}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

What are the major tasks and personnel required in a chip design project?

What are the major steps in ASIC chip construction?

What is the ASIC design flow?

What are the two major aspects of ASIC design flow?

What are the characteristics of good design flow?

What is the role of market research in an ASIC project?

What is the optimal solution of an ASIC project?

What is system-level study of a project?

What are the approaches for verifying design at the system level?

What is register-transfer-level (RTL) system-level description?

What are methods of verifying design at the register-transfer-level?

What is a test bench?

What is code coverage?

What is functional coverage?

What is bug rate convergence?

What is design planning?

What are hard macro and soft macro?

What is hardware description language (HDL)?

What is register-transfer-level (RTL) description of hardware?

What is standard cell? What are the differences among standard cell, gate-array, and sea-of-gate approaches?

What is an ASIC library?

What is logic synthesis?

What are the optimization targets of logic synthesis?

What is schematic or netlist?

What is the gate count of a design?

What is the purpose of test insertion during logic synthesis?

What is the most commonly used model in VLSI circuit testing?

What are controllability and observability in a digital circuit?

What is a testable circuit?

What is the aim of scan insertion?

What is fault coverage? What is defect part per million (DPPM)?

Why is design for testability important for a product's financial success?

What is chip power usage analysis?

What are the major components of CMOS power consumption?

What is power optimization?

What is VLSI physical design?

What are the problems that make VLSI physical design so challenging?

What is floorplanning?

What is the placement process?

What is the routing process?

What is a power network?

What is clock distribution?

What are the key requirements for constructing a clock tree?

What is the difference between time skew and length skew in a clock tree?

What is scan chain?

What is scan chain reordering?

What is parasitic extraction?

What is delay calculation?

What is back annotation?

What kind of signal integrity problems do place and route tools handle?

What is cross-talk delay?

What is cross-talk noise?

What is IR drop?

What are the major netlist formats for design representation?

What is gate-level logic verification before tapeout?

What is equivalence check?

What is timing verification?

What is design constraint?

What is static timing analysis (STA)?

What is simulation approach on timing verification?

What is the logical-effort-based timing closure approach?

What is physical verification?

What are design rule check (DRC), design verification (DV), and geometry verification (GV)?

What is schematic verification (SV) or layout versus schematic (LVS)?

What is automatic test pattern generation (ATPG)?

What is tapeout?

What is yield?

What are the qualities of a good IC implementation designer?

}, 
keywords={}, 
doi={10.1002/9780470199114.ch4}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5396736},}
@INPROCEEDINGS{1607414, 
author={J. White and D. C. Schmidt}, 
booktitle={13th Annual IEEE International Symposium and Workshop on Engineering of Computer-Based Systems (ECBS'06)}, 
title={FireAnt: a tool for reducing enterprise product line architecture deployment, configuration, and testing costs}, 
year={2006}, 
volume={}, 
number={}, 
pages={2 pp.-508}, 
abstract={Product-line architectures (PLA)s are a paradigm for developing software families by customizing and composing reusable artifacts, rather than handcrafting software from scratch. Extensive testing is required to develop reliable PLAs. Each PLA may have hundreds of valid variants that can be constructed from the architecture's components. It is crucial that each of these variants be thoroughly tested to ensure the quality of these applications on multiple OS platforms and hardware configurations. Setting up test environments and running tests can become extremely complex and expensive as the number of variants and the complexity of their deployment and configuration increases. Once a variant is deemed ready for deployment and configuration in a production environment, it is crucial that these activities be done identically to the tested configurations and upholds the assumptions of the component developers. Rapidly setting up numerous distributed test environments and ensuring that they are deployed and configured correctly is hard. This poster paper presents FireAnt, which is a tool for the model-driven development (MDD) of PLA deployment plans}, 
keywords={cost reduction;program testing;software architecture;software reusability;software tools;cost reduction;enterprise product line architecture deployment;enterprise product line architecture configuration;enterprise product line architecture testing;software family development;software reusability;model-driven development;Costs;Programmable logic arrays;Computer architecture;Software reusability;Software testing;Application software;Hardware;Production;Software packages;Packaging}, 
doi={10.1109/ECBS.2006.43}, 
ISSN={}, 
month={March},}
@ARTICLE{4657364, 
author={A. Mattsson and B. Lundell and B. Lings and B. Fitzgerald}, 
journal={IEEE Transactions on Software Engineering}, 
title={Linking Model-Driven Development and Software Architecture: A Case Study}, 
year={2009}, 
volume={35}, 
number={1}, 
pages={83-93}, 
abstract={A basic premise of model driven development (MDD) is to capture all important design information in a set of formal or semi-formal models which are then automatically kept consistent by tools. The concept however is still relatively immature and there is little by way of empirically validated guidelines. In this paper we report on the use of MDD on a significant real-world project over several years. Our research found the MDD approach to be deficient in terms of modelling architectural design rules. Furthermore, the current body of literature does not offer a satisfactory solution as to how architectural design rules should be modelled. As a result developers have to rely on time-consuming and error-prone manual practices to keep a system consistent with its architecture. To realise the full benefits of MDD it is important to find ways of formalizing architectural design rules which then allow automatic enforcement of the architecture on the system model. Without this, architectural enforcement will remain a bottleneck in large MDD projects.}, 
keywords={formal verification;software architecture;systems analysis;model-driven development;software architecture;formal models;semi-formal models;architectural design rules;Joining processes;Software architecture;Computer architecture;Guidelines;Context modeling;Computer industry;Computer errors;Programming;Keyword search;Portals;Software Architecture;Model-Driven Development;Case Study Research;Software Architecture;Model-Driven Development;Case Study Research}, 
doi={10.1109/TSE.2008.87}, 
ISSN={0098-5589}, 
month={Jan},}
@INPROCEEDINGS{781322, 
author={S. P. Harbison}, 
booktitle={Proceedings 1999 Design Automation Conference (Cat. No. 99CH36361)}, 
title={System-level hardware/software trade-offs}, 
year={1999}, 
volume={}, 
number={}, 
pages={258-259}, 
abstract={Operating systems and development tools can impose overly general requirements that prevent an embedded system from achieving its hardware performance entitlement. It is time for embedded processor designers to become more involved with system software and tools.}, 
keywords={microprocessor chips;digital signal processing chips;real-time systems;operating systems (computers);instruction sets;optimising compilers;system-level hardware/software trade-offs;embedded system;instruction set architecture;real-time OS;real-time analysis;debugging;DSP;embedded processors;Hardware;Software systems;System software;Operating systems;Computer architecture;Systolic arrays;Assembly;Real time systems;Embedded software;Digital signal processing}, 
doi={10.1109/DAC.1999.781322}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6614743, 
author={C. Dorn and A. Egyed}, 
booktitle={2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE)}, 
title={Towards collaboration-centric pattern-based software development support}, 
year={2013}, 
volume={}, 
number={}, 
pages={109-112}, 
abstract={Software engineering activities tend to be loosely coupled to allow for flexibly reacting to unforeseen development complexity, requirements changes, and progress delays. This flexibility comes a the price of hidden dependencies among design and code artifacts that make it difficult or even impossible to assess change impact. Incorrect change propagation subsequently results in costly errors. This position paper proposes a novel approach based on monitoring engineering activities for subsequent high-level pattern detection. Patterns of (i) collaboration structures, (ii) temporal action sequences, and (iii) artifact consistency constraints serve as input to recommendation and automatic reconfiguration algorithms for ultimately avoiding and correcting artifact inconsistencies.}, 
keywords={groupware;software development management;collaboration centric pattern-based software development;code artifact;design artifact;engineering activity monitoring;pattern detection;collaboration structure;temporal action sequence;artifact inconsistency constraint;automatic reconfiguration algorithm;Software;Collaboration;Computer architecture;Unified modeling language;Uncertainty;Adaptation models;Software engineering;monitoring;pattern detection;software engineering;recommendation;collaboration structures}, 
doi={10.1109/CHASE.2013.6614743}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{889571, 
author={D. Federici and P. Bisgambiglia and J. -. Santucci}, 
booktitle={Proceedings IEEE International High-Level Design Validation and Test Workshop (Cat. No.PR00786)}, 
title={High level fault simulation: experiments and results on ITC'99 benchmarks}, 
year={2000}, 
volume={}, 
number={}, 
pages={118-123}, 
abstract={In this paper we present our approach for performing Behavioral Fault Simulation (BFS). This approach involves three main steps (i) the definition of an internal modeling of behavioral descriptions, and the determination of a fault model; (ii) the definition of a fault simulation technique; (iii) the implementation of this technique. Finally, this paper deals with experiments conducted on ITC'99 benchmarks in order to validate a VHDL behavioral fault simulator (BFS). The effectiveness of the BFS software is clearly demonstrated through the obtained results.}, 
keywords={fault simulation;high level synthesis;hardware description languages;high level fault simulation;ITC'99 benchmarks;behavioral fault simulation;internal modeling;behavioral descriptions;fault simulation;VHDL;Circuit faults;Circuit simulation;Circuit testing;Benchmark testing;Very large scale integration;Test pattern generators;Electrical fault detection;Fault detection;Data structures;Software tools}, 
doi={10.1109/HLDVT.2000.889571}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4293629, 
author={N. Kitiyakara and J. Graves}, 
booktitle={Agile 2007 (AGILE 2007)}, 
title={Growing a Build Management System from Seed}, 
year={2007}, 
volume={}, 
number={}, 
pages={401-407}, 
abstract={This paper describes the authors' experiences creating a full build management system from a simple version control system. We will explore how the XP values of simplicity, feedback, communication, courage and respect play into making a system that provides the developers, testers and customer with excellent value showing how various XP principles (like baby steps and mutual benefit) come into play. We will also demonstrate how we are able to remain true to our XP values while still achieving ISO 9001- 2001 and CMM Level II certifications. Finally, we compare the build management system with other systems we have encountered that were not developed in accordance with XP values.}, 
keywords={building management systems;configuration management;build management system;version control system;CMM Level II certifications;ISO 9001-2001;System testing;Communication system control;Control systems;Feedback;Pediatrics;ISO standards;Coordinate measuring machines;Certification;Control system synthesis;Writing}, 
doi={10.1109/AGILE.2007.32}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4012600, 
author={D. Ayers}, 
journal={IEEE Internet Computing}, 
title={The Shortest Path to the Future Web}, 
year={2006}, 
volume={10}, 
number={6}, 
pages={76-79}, 
abstract={This column's title could suggest that there is only one best path forward for the Web. The path begins with document metadata and travels through the world of microformats and embedded data. A waypoint is a semantic Web that leverages these approaches, along with those offered by an environment more capable of managing first-class data directly. This is only one path, however, and it probably isn't the shortest. The Internet is a rich environment with billions of active agents. Natural selection, mutation, and genetic breeding of sorts all happen to software systems, together with a significantly higher proportion of "intelligent design" than found in the real world. The net effect is that many different evolutionary paths are being explored simultaneously, and several could lead to a better Web}, 
keywords={document handling;semantic Web;document metadata;semantic Web;Access protocols;HTML;Computer networks;Humans;Semantic Web;Network servers;Web server;Pediatrics;Internet;Electronic mail;Semantic Web;Web 2.0;Web programming}, 
doi={10.1109/MIC.2006.137}, 
ISSN={1089-7801}, 
month={Nov},}
@ARTICLE{1021118, 
author={E. Chou and B. Sheu}, 
journal={IEEE Circuits and Devices Magazine}, 
title={Nanometer mixed-signal system-on-a-chip design}, 
year={2002}, 
volume={18}, 
number={4}, 
pages={7-17}, 
abstract={A mixed-signal system-on-a-chip (SoC) design methodology and the supporting CAD tools are presented. A known tools set is identified for illustration purposes and some alternative tools can equally accomplish the task.}, 
keywords={mixed analogue-digital integrated circuits;integrated circuit layout;circuit layout CAD;hardware-software codesign;product development;design for testability;mixed-signal system-on-a-chip design;CAD tools;integrated-circuit design;design methodology;nanometer system-on-a-chip;design tools;planning stage;R&D prototype stage;product development stage;testability;functional model;floating point system model;front-end design flow;bit-true model;behavioral model;layout optimization;signal integrity effects;top-down design;System-on-a-chip;Integrated circuit modeling;Design methodology;Integrated circuit testing;Circuit testing;Design automation;System testing;Research and development;Design engineering;Prototypes}, 
doi={10.1109/MCD.2002.1021118}, 
ISSN={8755-3996}, 
month={July},}
@INPROCEEDINGS{6113487, 
author={C. Ren and D. Jiang}, 
booktitle={2011 International Conference of Information Technology, Computer Engineering and Management Sciences}, 
title={A New Ontology of Resource Specification for Wireless Sensor Networks}, 
year={2011}, 
volume={2}, 
number={}, 
pages={138-140}, 
abstract={The practical usage of ontology for resource specification is for effective resource control in wireless sensor networks. In order to support geographically and logically distinct resources to be co-scheduled and co-allocated when test beds are federated, we build a new ontology. It provides a simple schema that can be used to present a clear overview of network and the relation between sensing elements. Our ontology has been actively integrated with the common-used control framework. The ontology also provides service abstraction between service providers and users, which defines generalized resource request for users to describe the desired resources on different test beds. It has been shown to be useful for describing heterogeneous networked sensing substrate and be feasible for allocating resources across various test beds.}, 
keywords={formal specification;ontologies (artificial intelligence);resource allocation;wireless sensor networks;ontology;resource specification;resource control;wireless sensor networks;coscheduling;common used control framework;service abstraction;service providers;generalized resource request;networked sensing substrate;resource allocation;Ontologies;Sensors;Substrates;XML;Wireless sensor networks;Semantics;Wireless communication;Wireless Senor Networks;Ontology;Virtulization}, 
doi={10.1109/ICM.2011.282}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6958413, 
author={J. S. Kracht and J. Z. Petrovic and K. R. Walcott-Justice}, 
booktitle={2014 14th International Conference on Quality Software}, 
title={Empirically Evaluating the Quality of Automatically Generated and Manually Written Test Suites}, 
year={2014}, 
volume={}, 
number={}, 
pages={256-265}, 
abstract={The creation, execution, and maintenance of tests are some of the most expensive tasks in software development. To help reduce the cost, automated test generation tools can be used to assist and guide developers in creating test cases. Yet, the tests that automated tools produce range from simple skeletons to fully executable test suites, hence their complexity and quality vary. This paper compares the complexity and quality of test suites created by sophisticated automated test generation tools to that of developer-written test suites. The empirical study in this paper examines ten real-world programs with existing test suites and applies two state-of-the-art automated test generation tools. The study measures the resulting test suite quality in terms of code coverage and fault-finding capability. On average, manual tests covered 31.5% of the branches while the automated tools covered 31.8% of the branches. In terms of mutation score, the tests generated by automated tools had an average mutation score of 39.8% compared to the average mutation score of 42.1% for manually written tests. Even though automatically created tests often contain more lines of source code than those written by developers, this paper's empirical results reveal that test generation tools can provide value by creating high quality test suites while reducing the cost and effort needed for testing.}, 
keywords={program testing;software maintenance;software metrics;software quality;source code (software);empirical software quality evaluation;manually written test suites;automatically generated test suites;software test maintenance;software test execution;software test creation;software development;cost reduction;automated test generation tools;test suite quality;test suite complexity;real-world programs;code coverage;fault-finding capability;average mutation score;Manuals;Complexity theory;Software;Testing;Writing;Standards;Java}, 
doi={10.1109/QSIC.2014.33}, 
ISSN={1550-6002}, 
month={Oct},}
@INPROCEEDINGS{7601515, 
author={M. Sroka and D. Fisch and R. Nagy}, 
booktitle={2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)}, 
title={Impact of crossover and mutation on reproduction in evolutionary test model learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={39-44}, 
abstract={Automation in the software test design process has a significant impact on the software testing process and therefore also on the overall software development in the industry. The focus of this paper is on the automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is briefly described. This paper further investigates the impact of reproduction configuration on the evolutionary learning method.}, 
keywords={automatic test software;automotive engineering;embedded systems;evolutionary computation;learning (artificial intelligence);program testing;crossover impact;mutation impact;evolutionary test model learning;software test design process automation;software development;test case design automation;model-based testing;automotive embedded software;evolutionary algorithm;reproduction configuration impact;Biological cells;Sociology;Statistics;Software;Testing;Software algorithms;Evolutionary computation}, 
doi={10.1109/SISY.2016.7601515}, 
ISSN={1949-0488}, 
month={Aug},}
@INBOOK{5732852, 
author={Kamran Etemad and Ming-Yee Lai}, 
booktitle={WiMAX Technology and Network Evolution}, 
title={Overview of WiMAX Network Architecture and Evolution}, 
year={2010}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Introduction

WiMAX Basic Network Reference Model

WiMAX Network Roadmap: Release 1.0, 1.5, 1.6, and 2.0

Overview of Major Features in Release 1.0

Overview of Major Features in Release 1.5

Major Features in Network Release 1.6

Comparison of Mobile WiMAX and 3GPP/SAE Network Architecture

Summary

}, 
keywords={WiMAX;network architecture;service provider's working group;network working group;residential gateways}, 
doi={10.1002/9780470633021.ch6}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5732852},}
@INPROCEEDINGS{4384854, 
author={J. Ferreira and A. Carvalho and J. Pimentel and M. Guedes and F. Furini and N. Silva}, 
booktitle={2007 5th IEEE International Conference on Industrial Informatics}, 
title={Modeling Engineering and Manufacturing Activity in Vehicle Development Process}, 
year={2007}, 
volume={2}, 
number={}, 
pages={675-680}, 
abstract={Designing and consequent assembly of a new vehicle is a complex process as it requires close coordination and inputs from a number of disciplines in developing a number of systems and sub-systems in the vehicle that should fit within the confined vehicle space, function and provide the customers an acceptable combination of all relevant vehicle attributes. Understanding how these processes interact and how they are aligned with other while they should support the tasks involved in the conception of a new vehicle at a minimum time and cost. The first step to achieve this goal is the definition of a new UML profile (called VDML, vehicle development modeling language) based on the extension mechanics of UML (industry standard language) to assist business process description and consequent improvements achieved by the high level vision. To show the benefits of this new language applied to this specific business we model the engineer and manufacturing activity process using VDML.}, 
keywords={automobile industry;manufacturing data processing;Unified Modeling Language;vehicles;engineering activity modeling;manufacturing activity modeling;vehicle development process;UML profile;VDML;vehicle development modeling language;business process description;automotive industry;Automotive engineering;Manufacturing processes;Virtual manufacturing;Space vehicles;Unified modeling language;Process design;Assembly systems;Costs;Manufacturing industries;Standards development}, 
doi={10.1109/INDIN.2007.4384854}, 
ISSN={1935-4576}, 
month={June},}
@ARTICLE{4042539, 
author={B. Beckert and T. Hoare and R. Hahnle and D. R. Smith and C. Green and S. Ranise and C. Tinelli and T. Ball and S. K. Rajamani}, 
journal={IEEE Intelligent Systems}, 
title={Intelligent Systems and Formal Methods in Software Engineering}, 
year={2006}, 
volume={21}, 
number={6}, 
pages={71-81}, 
abstract={Over the last few years, technologies for the formal description, construction, analysis, and validation of software - based mostly on logics and formal reasoning - have matured. We can expect them to complement and partly replace traditional software engineering methods in the future. Formal methods in software engineering are an increasingly important application area for intelligent systems. The field has outgrown the area of academic case studies, and industry is showing serious interest. We convincingly argue that we've reached the point where we can solve the problem of how to formally verify industrial-scale software. We propose program verification as a computer science Grand Challenge. Deductive software verification is a core technology of formal methods. We describe recent dramatic changes in the way it's perceived and used. Another important base technique of formal methods, besides software verification, is synthesizing software that's correct by construction because it's formally derived from its specification. We discuss recent developments and trends in this area. Surprisingly efficient decision procedures for the satisfiability modulo theories problem have recently emerged. We explain these techniques and why they're important for all formal-methods tools. We look at formal methods from an industry perspective. We explain the success of Microsoft Research's SLAM project, which has developed a verification tool for device drivers}, 
keywords={formal logic;formal specification;inference mechanisms;knowledge based systems;program verification;software tools;intelligent system;formal method;software engineering;formal software description;software construction;software analysis;software validation;formal logic;formal reasoning;industrial-scale formal software verification;program verification;deductive software verification;software synthesis;software specification;decision procedure;satisfiability modulo theories problem;Microsoft Research SLAM project;verification tool;device driver;Intelligent systems;Software engineering;Costs;Application software;Programming profession;Computer science;Computer errors;Humans;Energy management;Financial management;formal methods;software engineering;program verification;deductive software verification;satisfiability modulo theories;software synthesis}, 
doi={10.1109/MIS.2006.117}, 
ISSN={1541-1672}, 
month={Nov},}
@INPROCEEDINGS{6912285, 
author={R. Wohlrab and T. de Gooijer and A. Koziolek and S. Becker}, 
booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)}, 
title={Experience of pragmatically combining RE methods for performance requirements in industry}, 
year={2014}, 
volume={}, 
number={}, 
pages={344-353}, 
abstract={To meet end-user performance expectations, precise performance requirements are needed during development and testing, e.g., to conduct detailed performance and load tests. However, in practice, several factors complicate performance requirements elicitation: lacking skills in performance requirements engineering, outdated or unavailable functional specifications and architecture models, the specification of the system's context, lack of experience to collect good performance requirements in an industrial setting with very limited time, etc. From the small set of available non-functional requirements engineering methods, no method exists that alone leads to precise and complete performance requirements with feasible effort and which has been reported to work in an industrial setting. In this paper, we present our experiences in combining existing requirements engineering methods into a performance requirements method called PROPRE. It has been designed to require no up-to-date system documentation and to be applicable with limited time and effort. We have successfully applied PROPRE in an industrial case study from the process automation domain. Our lessons learned show that the stakeholders gathered good performance requirements which now improve performance testing.}, 
keywords={formal specification;program testing;software architecture;RE methods;end-user performance expectations;software development;performance requirements elicitation;performance requirements engineering;functional specifications;architecture models;system context specification;industrial setting;nonfunctional requirements engineering methods;PROPRE;process automation domain;performance testing;Measurement;Context;Time factors;Documentation;Adaptation models;Throughput;Testing}, 
doi={10.1109/RE.2014.6912285}, 
ISSN={1090-705X}, 
month={Aug},}
@INPROCEEDINGS{5771265, 
author={A. Madhavapeddy and S. Singh}, 
booktitle={2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines}, 
title={Reconfigurable Data Processing for Clouds}, 
year={2011}, 
volume={}, 
number={}, 
pages={141-145}, 
abstract={Reconfigurable computing in the cloud helps to solve many practical problems relating to scaling out data-centers where computation is limited by energy consumption or latency. However, for reconfigurable computing in the cloud to become practical several research challenges have to be addressed. This paper identifies some of the perquisites for reconfigurable computing systems in the cloud and picks out several scenarios made possible with immense cloud-based computing capability.}, 
keywords={cloud computing;reconfigurable architectures;reconfigurable data processing;reconfigurable computing;cloud-based computing;Field programmable gate arrays;Cloud computing;USA Councils;Hardware;Computational modeling;Programming;Operating systems;reconfigurable computing;cloud computing}, 
doi={10.1109/FCCM.2011.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7133545, 
author={J. Cheng and Y. Zhu and T. Zhang and C. Zhu and W. Zhou}, 
booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
title={Mobile Compatibility Testing Using Multi-objective Genetic Algorithm}, 
year={2015}, 
volume={}, 
number={}, 
pages={302-307}, 
abstract={Mobile compatibility testing has been identified as one urgent and challenging issue. Mobile apps are expected to work on thousand kinds of mobile devices with diverse device features and mobile platforms. So mobile compatibility testing is complex and costly, it is impossible to test mobile apps on all mobile devices and in all environments with limited test resources. Then the question is how to select test devices in cost-effective mobile app compatibility testing. This paper proposes a novel test device selection approach using multi-objective genetic algorithm. Using the proposed approach, the minimum number of mobile devices is selected, and the multiple test coverage requirements are met simultaneously. Furthermore, the case study results have successfully demonstrated that the proposed approach is effective for mobile compatibility testing.}, 
keywords={genetic algorithms;mobile computing;program testing;cost-effective mobile app compatibility testing;multiobjective genetic algorithm;mobile devices;diverse device features;mobile platforms;limited test resources;test device selection approach;multiple test coverage requirements;Mobile communication;Testing;Mobile handsets;Biological cells;Genetic algorithms;Sociology;Statistics;software testing;mobile testing;compatibility testing;clustering algorithm;test coverage}, 
doi={10.1109/SOSE.2015.36}, 
ISSN={}, 
month={March},}
@ARTICLE{4814954, 
author={P. Liggesmeyer and M. Trapp}, 
journal={IEEE Software}, 
title={Trends in Embedded Software Engineering}, 
year={2009}, 
volume={26}, 
number={3}, 
pages={19-25}, 
abstract={Software's importance in the development of embedded systems has been growing rapidly over the last 20 years. Because of current embedded systems' complexity, they require sophisticated engineering methods for systematically developing high-quality software. Embedded software development differs from IT system development in several ways. For example, IT systems developers can use standard hardware and software platforms and don't face the resource requirements that embedded systems developers must take into account. To meet embedded software's extrafunctional requirements, embedded systems development is shifting from programming to model-driven development. Another important trend is the emphasis on the quality assurance of safety-related systems.}, 
keywords={embedded systems;software quality;embedded software engineering;embedded systems;embedded software development;IT system development;safety-related systems;quality assurance;Embedded software;Object oriented modeling;Mathematical model;Embedded system;Hardware;IEC standards;Costs;Automotive engineering;Computer languages;Operating systems;embedded systems development;model-driven development;embedded software;quality assurance;safety-critical systems}, 
doi={10.1109/MS.2009.80}, 
ISSN={0740-7459}, 
month={May},}
@INPROCEEDINGS{843835, 
author={A. Bommireddy and J. Khare and S. Shaikh and S. -. Su}, 
booktitle={Proceedings 18th IEEE VLSI Test Symposium}, 
title={Test and debug of networking SoCs-a case study}, 
year={2000}, 
volume={}, 
number={}, 
pages={121-126}, 
abstract={This paper describes the test challenges faced and testability features implemented on Level One's networking System on Chip (SoC), IXE2000. The IXE2000 SoC is a 20+ million transistor Layer 2/3/4 Switch with 24 10/100 Mbps and 2 1000 Mbps Ethernet ports, and a predominantly IP-based design. The chip had constraints in terms of both design time and total system costs, which added an extra burden on test. The paper discusses how these constraints led to the current testability solutions and debug features on the chip.}, 
keywords={application specific integrated circuits;design for testability;local area networks;integrated circuit testing;built-in self test;logic testing;automatic testing;networking SoCs;testability features;Level One;IXE2000;Layer 2/3/4 Switch;Ethernet ports;IP-based design;design time;total system costs;testability solutions;debug features;10 to 1000 Mbit/s;Computer aided software engineering;Switches;Clocks;System testing;Ethernet networks;System-on-a-chip;Costs;Business communication;Design for manufacture;IP networks}, 
doi={10.1109/VTEST.2000.843835}, 
ISSN={1093-0167}, 
month={April},}
@INPROCEEDINGS{8257807, 
author={P. Perera and R. Silva and I. Perera}, 
booktitle={2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
title={Improve software quality through practicing DevOps}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={DevOps is extended from certain agile practices with a mix of patterns intended to improve collaboration between development and operation teams. The main purpose of this paper is to conduct a study on how DevOps practice has impacted to software quality. The secondary objective is to find how to improve quality efficiently. A literature survey has carried out to explore about current DevOps practices in industry. According to the literature survey, the conceptual research model was developed and five hypotheses were derived. Research objectives were accomplished by testing hypotheses using Pearson correlation. A linear model is derived based on the linear regression analysis. An online questionnaire was used to collect quantitative data whereas interviews with experts on DevOps and Quality assurance have been used to identify how to improve the quality of software by practicing DevOps. Recommendations are given based on interview feedback, hypotheses testing with regression analysis. According to the quantitative study, researchers have identified that quality of the software gets improved when practice DevOps by following CAMS (Culture, Automation, Measurement, Sharing) framework. Automation is the most critical factor to improve the software quality. As per the results of multiple regression analysis, it has proved culture, automation, measurement and sharing are important factors to consider to improve quality of the software. In conclusion it can be recommended to use DevOps to achieve high quality software.}, 
keywords={regression analysis;software development management;software prototyping;software quality;agile practices;operation teams;DevOps practice;conceptual research model;research objectives;linear regression analysis;Quality assurance;practice DevOps;high quality software;software quality;quantitative data collection;Companies;Automation;Software quality;Testing;Software measurement;DevOps;CAMS Framework;Quality;ISO 9126;Automation}, 
doi={10.1109/ICTER.2017.8257807}, 
ISSN={2472-7598}, 
month={Sept},}
@INPROCEEDINGS{7589821, 
author={C. M. Tang and J. Keung and Y. T. Yu and W. K. Chan}, 
booktitle={2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
title={DFL: Dual-Service Fault Localization}, 
year={2016}, 
volume={}, 
number={}, 
pages={412-422}, 
abstract={In engineering a service, software developers often construct and deploy a newer (forthcoming) version of the service to replace the current version. A forthcoming version is often placed online for users to consume and report feedback. In the case of observed failures, the forthcoming version should be debugged and further evolved. In this paper, we propose the model of dual-service fault localization (DFL) to aid this evolution process. Many prior research studies on spectrum-based fault localization (SBFL) consider each version separately. The DFL model correlates the dynamic execution spectra of the current and the forthcoming versions of the same service placed for live test of the forthcoming version, and dynamically generates an adaptive fault localization formula to estimate the code regions in the forthcoming service responsible for the observed failures. We report an experiment in which we initialized the DFL model into six instances, each using an ensemble technique dynamically composed from 11 existing SBFL formulas, and applied the model to four benchmarks. The results show that DFL is feasible and multiple instances are statistically more effective than, if not as effective as, the best of these individual SBFL formulas on each benchmark.}, 
keywords={program debugging;software maintenance;dual-service fault localization;evolution process;DFL model;dynamic execution spectra;adaptive fault localization;code region estimation;ensemble technique;SBFL formulas;spectrum-based fault localization;Software;Debugging;Computer bugs;Adaptation models;Benchmark testing;Production;Companies;debugging;spectrum-based fault localization;ensemble techniques;dual-service fault localization}, 
doi={10.1109/QRS.2016.53}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{794352, 
author={B. Smith and W. Millar and J. Dunphy and Yu-Wen Tung and P. Nayak and E. Gamble and M. Clark}, 
booktitle={1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403)}, 
title={Validation and verification of the remote agent for spacecraft autonomy}, 
year={1999}, 
volume={1}, 
number={}, 
pages={449-468 vol.1}, 
abstract={The six-day Remote Agent Experiment (RAX) on the Deep Space 1 mission will be the first time that an artificially intelligent agent will control a NASA spacecraft. Successful completion of this experiment will open the way for AI-based autonomy technology on future missions. An important validation objective for RAX is implementation of a credible validation and verification strategy for RAX that also "scales up" to missions that make full use of spacecraft autonomy. Autonomous flight software presents novel and difficult testing challenges that traditional flight software (FSW) does not face. Since autonomous software must respond robustly in an immense number of situations, the all-paths testing approaches used for traditional FSW is not feasible. Instead, we advocate a combination of scenario-based testing and model-based validation. This paper describes the testing challenges faced by autonomous spacecraft commanding software, discusses the testing strategies and model-validation methods that we found effective for RAX, and argues that these methods will "scale up" to missions that make full use of spacecraft autonomy. Among the key challenges for validating autonomous systems such as the RAX are ensuring adequate coverage for scenario-based tests, developing methods for specifying the expected behavior, and developing automated tools for verifying the observed behavior against those specifications. Another challenge, also faced by traditional FSW, is the scarcity of high-fidelity test-beds. The test plan must be designed to take advantage of lower-fidelity test-beds without compromising test effectiveness.}, 
keywords={aerospace computing;program testing;program verification;automatic test software;space research;aerospace expert systems;knowledge verification;software architecture;remote agent;spacecraft autonomy;Deep Space 1 mission;artificially intelligent agent;NASA spacecraft;verification;autonomous flight software;autonomous software;scenario-based testing;model-based validation;test effectiveness;Space vehicles;Space technology;Software testing;Automatic testing;Space missions;Propulsion;Laboratories;Intelligent agent;NASA;Robustness}, 
doi={10.1109/AERO.1999.794352}, 
ISSN={}, 
month={March},}
@INBOOK{5238155, 
author={Mostafa Hashem Sherif}, 
booktitle={Managing Projects in Telecommunication Services}, 
title={Standards and Innovation in Telecommunication Services}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

The Two Dimensions of Telecommunication Projects

Innovation in Telecommunication Services

Phasic Relation Between Equipment and Services

Standardization for Telecommunication Services

Summary

}, 
keywords={}, 
doi={10.1002/9780470047682.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5238155},}
@ARTICLE{182763, 
author={}, 
journal={IEEE Std 610}, 
title={IEEE Standard Computer Dictionary: A Compilation of IEEE Standard Computer Glossaries}, 
year={1991}, 
volume={}, 
number={}, 
pages={1-217}, 
abstract={Identifies terms currently in use in the computer field. Standard definitions for thoseterms are established. Compilation of IEEE Stds IEEE Std 1084, IEEE Std 610.2, IEEE Std 610.3, IEEE Std 610.4, IEEE Std 610.5 and IEEE Std 610.12}, 
keywords={glossaries;dictionary;glossary;computer field;definitions;Terminology;terminology;computer;applications;glossary;definitions;dictionary;610}, 
doi={10.1109/IEEESTD.1991.106963}, 
ISSN={}, 
month={Jan},}
@INBOOK{8040815, 
author={Sauming Pang}, 
booktitle={Successful Service Design for Telecommunications: A comprehensive guide to design and implementation}, 
title={Glossary}, 
year={2009}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={10.1002/9780470741207.gloss}, 
ISSN={}, 
publisher={Wiley}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8040815},}
@INPROCEEDINGS{217601, 
author={M. R. Lyu and J. -. Chen and A. Avizienis}, 
booktitle={[1992] Proceedings. The Sixteenth Annual International Computer Software and Applications Conference}, 
title={Software diversity metrics and measurements}, 
year={1992}, 
volume={}, 
number={}, 
pages={69-78}, 
abstract={The authors define and formalize the concept of software diversity which characterizes N-Version software (NVS) from four different points of view that are designated as structural diversity, fault diversity, tough-spot diversity, and failure diversity. The goals are to find a way to quantify software diversity and to investigate the measurements which can be applied during the life cycle of NVS to gain confidence that operation will be dependable when NVS is actually used. The versions from a six-language N-Version programming project for fault-tolerant flight control software were used in the software diversity measurement.<<ETX>>}, 
keywords={software metrics;software reliability;software diversity metrics;N-Version software;structural diversity;fault diversity;tough-spot diversity;failure diversity;fault-tolerant flight control software;Software measurement;Fault tolerant systems;Aerospace control;Fault tolerance;Gain measurement;Functional programming;Error correction;Delay;Software algorithms;Quality control}, 
doi={10.1109/CMPSAC.1992.217601}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{878373, 
author={J. E. Polk and R. Y. Kakuda and J. R. Anderson and J. R. Brophy and V. K. Rawlin and J. Sovey and J. Hamley}, 
booktitle={2000 IEEE Aerospace Conference. Proceedings (Cat. No.00TH8484)}, 
title={In-flight performance of the NSTAR ion propulsion system on the Deep Space One mission}, 
year={2000}, 
volume={4}, 
number={}, 
pages={123-148 vol.4}, 
abstract={Deep Space 1 is the first interplanetary spacecraft to use an ion propulsion system for the primary delta-v maneuvers. The purpose of the mission is to validate a number of technologies, including ion propulsion and a high degree of spacecraft autonomy, on a flyby of an asteroid and two comets. The ion propulsion system has operated for a total of 3500 hours at engine power levels ranging from 0.48 to 1.94 kW and has completed the encounter with the asteroid 1992KD and the first set of deterministic burns required for a 2001 encounter with comet Wilson-Harrington. The system has worked extremely well after an initial grid short was cleared after launch. Operation during this primary mission phase has demonstrated all ion propulsion system and autonomous navigation functions. All propulsion system operating parameters are very close to the expected values with the exception of the thrust at higher power levels, which is about 2 percent lower than that calculated from the electrical parameters. This paper provides an overview of the system and presents the first flight validation data on an ion propulsion system in interplanetary space.}, 
keywords={aerospace propulsion;ion engines;space vehicles;navigation;aerospace control;NSTAR ion propulsion system;in-flight performance;Deep Space 1 mission;interplanetary spacecraft;primary delta-v maneuvers;spacecraft autonomy;asteroid flyby;comets flyby;deterministic burns;primary mission phase;autonomous navigation function;operating parameters;flight validation data;ion thruster;throttle table;decontamination;3500 hour;0.48 to 1.94 kW;Propulsion;Space missions;Space technology;Space vehicles;Engines;NASA;Xenon;Instruments;Control systems;Feeds}, 
doi={10.1109/AERO.2000.878373}, 
ISSN={1095-323X}, 
month={March},}
@INPROCEEDINGS{7755285, 
author={M. Jain and D. Gopalani}, 
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, 
title={Testing application security with aspects}, 
year={2016}, 
volume={}, 
number={}, 
pages={3161-3165}, 
abstract={For the purpose of security of the computer systems, organizations now a days plan a lot of things like firewalls, network scanning tools, secure sockets layer (SSL) etc. However security bugs present at the application layer (code level) caused by unawareness or mistakes of the developers are usually ignored. Such security bugs can lead to unauthorized privileges on a computer system. For example most web applications connect back to databases which contain sensitive information. Malicious input can allow the attacker to alter the flow of the web application and provide unauthorized access to the confidential database. Hence proper security tests are required to be conducted in order to assess the security of applications. In this paper, we propose the use of Aspect Oriented Programming (AOP) for the purpose of security testing of Java applications. With the examples of fuzz testing and servlet testing using aspects, we will show how AOP can be used for detection of security bugs in Java applications by creeping inside the program without making any changes to the source code.}, 
keywords={aspect-oriented programming;authorisation;Internet;Java;program debugging;program testing;source code (software);source code;Java application security bug detection;servlet testing;fuzz testing;AOP;aspect oriented programming;confidential database;unauthorized access;Web applications;code level security bugs;application layer security bugs;SSL;secure socket layer;network scanning tools;firewalls;computer system security;application security testing;Security;Testing;Java;Databases;Computer bugs;HTML;Programming;Aspect Oriented Programming;Security Testing;Software Testing;Aspects;AOP}, 
doi={10.1109/ICEEOT.2016.7755285}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7546497, 
author={G. Argyros and I. Stais and A. Kiayias and A. D. Keromytis}, 
booktitle={2016 IEEE Symposium on Security and Privacy (SP)}, 
title={Back in Black: Towards Formal, Black Box Analysis of Sanitizers and Filters}, 
year={2016}, 
volume={}, 
number={}, 
pages={91-109}, 
abstract={We tackle the problem of analyzing filter and sanitizer programs remotely, i.e. given only the ability to query the targeted program and observe the output. We focus on two important and widely used program classes: regular expression (RE) filters and string sanitizers. We demonstrate that existing tools from machine learning that are available for analyzing RE filters, namely automata learning algorithms, require a very large number of queries in order to infer real life RE filters. Motivated by this, we develop the first algorithm that infers symbolic representations of automata in the standard membership/equivalence query model. We show that our algorithm provides an improvement of x15 times in the number of queries required to learn real life XSS and SQL filters of popular web application firewall systems such as mod-security and PHPIDS. % Active learning algorithms require the usage of an equivalence oracle, i.e. an oracle that tests the equivalence of a hypothesis with the target machine. We show that when the goal is to audit a target filter with respect to a set of attack strings from a context free grammar, i.e. find an attack or infer that none exists, we can use the attack grammar to implement the equivalence oracle with a single query to the filter. Our construction finds on average 90% of the target filter states when no attack exists and is very effective in finding attacks when they are present. For the case of string sanitizers, we show that existing algorithms for inferring sanitizers modelled as Mealy Machines are not only inefficient, but lack the expressive power to be able to infer real life sanitizers. We design two novel extensions to existing algorithms that allow one to infer sanitizers represented as single-valued transducers. Our algorithms are able to infer many common sanitizer functions such as HTML encoders and decoders. Furthermore, we design an algorithm to convert the inferred models into BEK programs, which allows for further applications such as cross checking different sanitizer implementations and cross compiling sanitizers into different languages supported by the BEK backend. We showcase the power of our techniques by utilizing our black-box inference algorithms to perform an equivalence checking between different HTML encoders including the encoders from Twitter, Facebook and Microsoft Outlook email, for which no implementation is publicly available.}, 
keywords={automata theory;context-free grammars;firewalls;hypermedia markup languages;information filtering;Internet;learning (artificial intelligence);query processing;social networking (online);SQL;black box analysis;filter programs;sanitizer programs;regular expression filters;RE filters;string sanitizers;machine learning;automata learning algorithms;membership/equivalence query model;real life XSS;SQL filters;Web application;firewall systems;mod-security;PHPIDS;context free grammar;Mealy machines;equivalence checking;HTML encoders;Twitter;Facebook;Microsoft Outlook email;Algorithm design and analysis;Machine learning algorithms;Learning automata;Transducers;Grammar;HTML;Inference algorithms;sanitizers;filters;automata;learning;web security}, 
doi={10.1109/SP.2016.14}, 
ISSN={2375-1207}, 
month={May},}
@INPROCEEDINGS{1541840, 
author={M. Kajko-Mattsson and P. Meyer}, 
booktitle={2005 International Symposium on Empirical Software Engineering, 2005.}, 
title={Evaluating the acceptor side of EM/sup 3/: release management at SAS}, 
year={2005}, 
volume={}, 
number={}, 
pages={10 pp.-}, 
abstract={Today, there are no detailed standard process models encompassing the overall release management activities. To remedy this, we have created an individual release management process model, called EM/sup 3/: release management. In this paper, we evaluate its acceptor side against an industrial release management process performed at Scandinavian Airline Systems (SAS). We have observed some similarities and differences. Some of the observed differences provide feedback for the improvement and further extension of EM/sup 3/: release management.}, 
keywords={software maintenance;travel industry;software development management;EM/sup 3/ release management;acceptor side evaluation;Scandinavian Airline Systems;standard process model;industrial release management process;software maintenance;Synthetic aperture sonar;Technology management;Control systems;Software maintenance;Laboratories;Software standards;Performance evaluation;Feedback;Software systems;Lead}, 
doi={10.1109/ISESE.2005.1541840}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{1498464, 
author={J. Ott and D. Kutscher}, 
booktitle={Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Communications Societies.}, 
title={A disconnection-tolerant transport for drive-thru Internet environments}, 
year={2005}, 
volume={3}, 
number={}, 
pages={1849-1862 vol. 3}, 
abstract={Today's mobile, wireless, and ad-hoc communications often exhibit extreme characteristics challenging assumptions underlying the traditional way of end-to-end communication protocol design in the Internet. One specific scenario is Internet access from moving vehicles on the road as we are researching in the drive-thru Internet project. Using wireless LAN as a broadly available access technology leads to intermittent - largely unpredictable and usually short-lived - connectivity, yet providing high performance while available. To allow Internet applications to deal reasonably well with such intermittent connectivity patterns, we have introduced a supportive drive-thru architecture. A key component is a "session" protocol offering persistent end-to-end communications even in the presence of interruptions. In this paper, we present the design of the persistent connectivity management protocol (PCMP) and report on findings from our implementation.}, 
keywords={Internet;mobile radio;protocols;computer network management;mobile computing;disconnection-tolerant transport;drive-thru Internet;ad-hoc communication;wireless communication;mobile communication;end-to-end communication protocol;wireless LAN;connectivity pattern;session protocol;persistent connectivity management protocol;Internet;Access protocols;Mobile communication;Transport protocols;Road vehicles;Wireless LAN;Wireless networks;Performance loss;Wireless application protocol;Vehicle driving}, 
doi={10.1109/INFCOM.2005.1498464}, 
ISSN={0743-166X}, 
month={March},}
@INPROCEEDINGS{7980399, 
author={A. Contan and L. Miclea and C. Dehelean}, 
booktitle={2017 14th International Conference on Engineering of Modern Electric Systems (EMES)}, 
title={Automated testing framework development based on social interaction and communication principles}, 
year={2017}, 
volume={}, 
number={}, 
pages={136-139}, 
abstract={The speed of development of the IT industry as well as the computational power which are increasing exponentially, create great competitiveness in the process of development but also in the launching of software products on the market. Automated testing comes to help with these challenges by trying to increase the speed of development by offering fast feedback and trustworthy quality by means of repeated runs of the implemented tests. This isn't a problem just on a technical level, but also on a social level, especially in the area of communication and understanding the requirements of the client. This work presents the implementation of an automated testing framework which also addresses the social problems. BDD or “Behavior Driven Development” includes an approach which would like to line up the area of client requests to the technical area, offering a uniform platform of collaboration and development. The implementation of this principle is applied in an MVP (Minimum Viable Product) type project which is meant to demonstrate the technical solution which may draw together, both socially and communication wise, the business teams and the technical implementation teams.}, 
keywords={program testing;software development management;software quality;automated testing framework development;social interaction;communication principles;IT industry;computational power;software products;trustworthy quality;fast feedback;social level;BDD;automated testing framework;behavior driven development;MVP type project;minimum viable product;business teams;technical implementation teams;Testing;Software;Business;Documentation;Collaboration;Automation;Libraries;testing process;BDD;automated testing;Gherkin language}, 
doi={10.1109/EMES.2017.7980399}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1677379, 
author={C. Smidts}, 
booktitle={RAMS '06. Annual Reliability and Maintainability Symposium, 2006.}, 
title={Research in software reliability engineering}, 
year={2006}, 
volume={}, 
number={}, 
pages={228-233}, 
abstract={Our research has focused on development of an approach to predicting software reliability based on a systematic identification of software process failure modes and their likelihoods. A direct consequence of the approach and its supporting data collection efforts is the identification of weak areas in the software development process. A Bayes framework for the quantification of software process failure mode probabilities can be useful since it allows use of historical data that are only partially relevant to the software at hand. The approach has been applied in the context of a waterfall life-cycle and for failure modes related to the requirements phase}, 
keywords={failure analysis;software development management;software reliability;software reliability engineering;systematic identification;software process failure modes;software development process;Bayes framework;waterfall life-cycle;Software reliability;Reliability engineering;Programming;Risk management;Predictive models;Software tools;Software testing;Mechanical engineering;Computer science education;Educational programs}, 
doi={10.1109/RAMS.2006.1677379}, 
ISSN={0149-144X}, 
month={Jan},}
@INPROCEEDINGS{879426, 
author={Yu-Wen Tung and W. S. Aldiwan}, 
booktitle={2000 IEEE Aerospace Conference. Proceedings (Cat. No.00TH8484)}, 
title={Automating test case generation for the new generation mission software system}, 
year={2000}, 
volume={1}, 
number={}, 
pages={431-437 vol.1}, 
abstract={The significant expansion of autonomous control and information processing capabilities in the coming generation of mission software systems results in a qualitatively larger space of behaviors that needs to be "covered" during testing, not only at the system level but also at subsystem and unit levels. A major challenge in this area is to automatically generate a relatively small set of test cases that, collectively, guarantees a selected degree of coverage of the behavior space. This paper describes an algorithm for a parametric test case generation tool that applies a combinatorial design approach to the selection of candidate test cases. Evaluation of this algorithm on test parameters from the Deep Space One mission reveals a valuable reduction in the number of test cases, when compared to an earlier home-brewed generator.}, 
keywords={automatic test pattern generation;aerospace expert systems;program testing;software tools;software agents;combinatorial mathematics;new generation mission software system;test case generation automation;autonomous control;selected degree of coverage;behavior space;parametric test case generation tool;algorithm;combinatorial design approach;Deep Space One mission reveals;software testing;remote agent experiment;unit testing harness;reusable software component;seed cases;Automatic testing;Space missions;Automatic generation control;Process control;Control systems;Information processing;Software systems;Software testing;System testing;Algorithm design and analysis}, 
doi={10.1109/AERO.2000.879426}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{1541696, 
author={F. Karayannis and J. Serrat-Fernandez and J. Baliosian and J. Rubio-Loyola and K. G. Vaxevanakis and G. Pagomenos and T. B. Zahariadis}, 
journal={IEEE Communications Magazine}, 
title={In-field evaluation of a managed IP/MPLS over WDM provisioning solution}, 
year={2005}, 
volume={43}, 
number={11}, 
pages={S26-S33}, 
abstract={This article demonstrates results and experiences gained in the area of multilayer internetworking, with emphasis on bandwidth on-demand provisioning as well as resource and restoration management. Behavioral characteristics and numerical results were obtained from a management. system prototype implemented and tested in an appropriately adapted commercial WDM environment enhanced with multivendor gigabit IP routers. The management solution, the testbed environment, and a representative evaluation scenario are presented as a means of explaining in detail the results that finally allow a global system assessment.}, 
keywords={computer network management;IP networks;multiprotocol label switching;wavelength division multiplexing;optical fibre networks;bandwidth allocation;telecommunication network routing;managed IP-MPLS;WDM provisioning solution;multilayer internetworking;bandwidth on-demand provisioning;restoration management;multivendor Gigabit IP routers;Multiprotocol label switching;Wavelength division multiplexing;Telecommunication network management;Testing;Optical scattering;Biomedical optical imaging;Asynchronous transfer mode;Information management;Inventory management;Synchronous digital hierarchy}, 
doi={10.1109/MCOM.2005.1541696}, 
ISSN={0163-6804}, 
month={Nov},}
@ARTICLE{1075437, 
author={K. Maxham and J. Dugan and M. McDonald and C. Hogge}, 
journal={Journal of Lightwave Technology}, 
title={1.13-Gbit/lightwave transmission system}, 
year={1987}, 
volume={5}, 
number={10}, 
pages={1510-1517}, 
abstract={A new high capacity lightwave transmission system has been developed using GaAs semicustom logic arrays and a DFB single-mode laser, and is presently in production. The architecture of this product is designed for in-service upgrade of a 565-Mbit/s product. This paper reviews the technical characteristics and design considerations of the Rockwell LTS-21130 lightwave transmission system.}, 
keywords={Distributed feedback (DFB) lasers;Logic arrays;Optical fiber communication;Optical fiber transmitters, lasers;Optical transmitters;Protection;Gallium arsenide;Multiplexing;Circuits;Condition monitoring;Logic arrays;Production systems;High speed optical techniques;Optical receivers}, 
doi={10.1109/JLT.1987.1075437}, 
ISSN={0733-8724}, 
month={October},}
@INPROCEEDINGS{6912254, 
author={P. Pruski and S. Lohar and R. Aquanette and G. Ott and S. Amornborvornwong and A. Rasin and J. Cleland-Huang}, 
booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)}, 
title={TiQi: Towards natural language trace queries}, 
year={2014}, 
volume={}, 
number={}, 
pages={123-132}, 
abstract={One of the surprising observations of traceability in practice is the under-utilization of existing trace links. Organizations often create links in order to meet compliance requirements, but then fail to capitalize on the potential benefits of those links to provide support for activities such as impact analysis, test regression selection, and coverage analysis. One of the major adoption barriers is caused by the lack of accessibility to the underlying trace data and the lack of skills many project stakeholders have for formulating complex trace queries. To address these challenges we introduce TiQi, a natural language approach, which allows users to write or speak trace queries in their own words. TiQi includes a vocabulary and associated grammar learned from analyzing NL queries collected from trace practitioners. It is evaluated against trace queries gathered from trace practitioners for two different project environments.}, 
keywords={grammars;natural language processing;query processing;regression analysis;natural language trace queries;TiQi;trace links;organizations;compliance requirements;impact analysis;test regression selection;coverage analysis;trace data;natural language approach;associated grammar;Unified modeling language;Databases;Vocabulary;Natural languages;Software;Speech;Hazards;Traceability;Queries;Speech Recognition;Natural Language Processing}, 
doi={10.1109/RE.2014.6912254}, 
ISSN={1090-705X}, 
month={Aug},}
@INPROCEEDINGS{4626838, 
author={R. Lutz}, 
booktitle={2008 12th International Software Product Line Conference}, 
title={Enabling Verifiable Conformance for Product Lines}, 
year={2008}, 
volume={}, 
number={}, 
pages={35-44}, 
abstract={NASA is, with the rest of industry, turning to product-line engineering to reduce costs and improve quality by effectively managing reuse. Experience in industry has shown that it is the verifiable conformance of each system to the product-line specifications that makes or breaks the product-line practice. Verification that the software for each project satisfies its intended product-line constraints is thus essential. This paper reports early results from aneffort to assemble from previous, industrial experience a set of enablers of verifiable conformance for use in the application engineering of NASA product lines. Lessons learned may be useful for developers of safety-critical, long-lived, or highly autonomous productlines, as well as for companies that integrate product line subsystems developed by multiple contractors.}, 
keywords={aerospace computing;conformance testing;formal verification;safety-critical software;software reusability;verifiable conformance;product-line engineering;cost reduction;managing reuse;product-line specifications;product-line practice;software verification;product-line constraints;NASA product lines;safety-critical productlines;long-lived productlines;highly autonomous productlines;product line subsystems;NASA;Software;Computer architecture;Organizations;Industries;Evolution (biology);Safety;software product line;application engineering;verifying conformance;experience}, 
doi={10.1109/SPLC.2008.12}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{404523, 
author={E. L. Parrella and Sin-Min Chang}, 
booktitle={Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit}, 
title={Four channel DS1 framer}, 
year={1994}, 
volume={}, 
number={}, 
pages={445-448}, 
abstract={A four channel DS1 framer chip has been developed for deployment in multichannel T1 systems, SONET add-drop multiplexers, T3 multiplexes, and ATM over T1 applications. Area reduction was realized through the use of a high speed clock, permitting use of shared resources and construction of simple arbiters for single port RAM. For further gate reduction, a state-machine based framing algorithm utilizing RAM as next state memory was developed.<<ETX>>}, 
keywords={multiplexing equipment;SONET;asynchronous transfer mode;digital signal processing chips;application specific integrated circuits;synchronisation;four channel DS1 framer chip;multichannel T1 systems;SONET add-drop multiplexers;T3 multiplexes;ATM over T1 applications;chip area reduction;high speed clock;shared resources;arbiters;single port RAM;gate reduction;state-machine based framing algorithm;next state memory;Clocks;SONET;Costs;Buffer storage;Switches;Jitter;Application specific integrated circuits;Read-write memory;Add-drop multiplexers;Asynchronous transfer mode}, 
doi={10.1109/ASIC.1994.404523}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{1687602, 
author={F. Caruso and D. Milham and S. Orobec}, 
booktitle={2006 IEEE/IFIP Network Operations and Management Symposium NOMS 2006}, 
title={Emerging industry standard for managing next generation transport networks: TMF MTOSI}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-15}, 
abstract={There are enormous business benefits to being able to separate the business logic from the massive technical complexity at the network level. One of the greatest challenges to being able to achieve this abstraction at the OS level has been the requirement to communicate and manage many different sets of vendor technologies. The main inhibitor to overcoming this challenge has been the lack of standards supporting both the interface from an OS to an EMS and also that between OSs. Building upon the successful multi technology network management (MTNM) CORBA/IDL interface, MTOSI has extended MTNM work to support XML/Web service interactions between various types of operations systems. MTOSI bridged the standard gap by defining a methodology and a framework to map the domain specific business activities into well defined TMF NGOSS contracts according to the service oriented architecture principles. This session introduces the key aspects of MTOSI and presents a real use case of MTOSI in BT}, 
keywords={computer network management;distributed object management;Internet;IP networks;operating systems (computers);telecommunication standards;XML;emerging industry standard;next generation transport network management;TMF MTOSI;business benefits;business logic;massive technical complexity;OS level;vendor technologies;EMS;multi technology network management;CORBA-IDL interface;XML-Web service interactions;operations systems;domain specific business activities;TMF NGOSS contracts;service oriented architecture principles;Next generation networking;SOA;MDA;Web Services;TMF NGOSS;MTOSI;MTNM}, 
doi={10.1109/NOMS.2006.1687602}, 
ISSN={1542-1201}, 
month={April},}
@ARTICLE{4392504, 
author={}, 
journal={IEEE Unapproved Draft Std P2600/D30b, Nov 2007}, 
title={IEEE Draft Standard for Information Technology: Hardcopy Device and System Security}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{7492282, 
author={S. Datta and S. Sarkar and A. S. M. Sajeev}, 
journal={IEEE Transactions on Big Data}, 
title={How Long Will This Live? Discovering the Lifespans of Software Engineering Ideas}, 
year={2016}, 
volume={2}, 
number={2}, 
pages={124-137}, 
abstract={We all want to be associated with long lasting ideas; as originators, or at least, expositors. For a tyro researcher or a seasoned veteran, knowing how long an idea will remain interesting in the community is critical in choosing and pursuing research threads. In the physical sciences, the notion of half-life is often evoked to quantify decaying intensity. In this paper, we study a corpus of 19,000+ papers written by 21,000+ authors across 16 software engineering publication venues from 1975 to 2010, to empirically determine the half-life of software engineering research topics. In the absence of any consistent and well-accepted methodology for associating research topics to a publication, we have used natural language processing techniques to semi-automatically identify and associate a set of topics with a paper. We adapted measures of half-life already existing in the bibliometric context for our study, and also defined a new measure based on publication and citation counts. We find evidence that some of the identified research topics show a mean half-life of close to 15 years, and there are topics with sustaining interest in the community. We report the methodology of our study in this paper, as well as the implications and utility of our results.}, 
keywords={citation analysis;software engineering;software engineering ideas;software engineering publication;software engineering research topics;citation counts;publication counts;Software engineering;Big data;Measurement;Collaboration;Context;Special issues and sections;Software;Big data;software engineering;research;half-life}, 
doi={10.1109/TBDATA.2016.2580541}, 
ISSN={2332-7790}, 
month={June},}
@INPROCEEDINGS{6614319, 
author={M. Saadatmand and M. Sjödin}, 
booktitle={2013 10th International Conference on Information Technology: New Generations}, 
title={On Combining Model-Based Analysis and Testing}, 
year={2013}, 
volume={}, 
number={}, 
pages={260-266}, 
abstract={Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.}, 
keywords={program diagnostics;systems analysis;model-based analysis;model-based testing;computer system testing;static analysis;model-based development;nonfunctional requirements;model-guided testing;trade-off analysis;Unified modeling language;Testing;Analytical models;Software;Security;Timing;Batteries;Model-based development;static analysis;model-based testing;non-functional requirements;test-case prioritization}, 
doi={10.1109/ITNG.2013.42}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7338271, 
author={S. Ali and T. Yue}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={Formalizing the ISO/IEC/IEEE 29119 Software Testing Standard}, 
year={2015}, 
volume={}, 
number={}, 
pages={396-405}, 
abstract={Model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in the literature and in the practice. However, all of the techniques have been developed using their own concepts and terminology of MBT, which are very often different than other techniques and at times have conflicting semantics. Moreover, while working on MBT projects with our industrial partners in the last several years, we were unable to find a unified way of defining MBT techniques based on standard terminology. To precisely define MBT concepts with the aim of providing common understanding of MBT terminology across techniques, we formalize a small subset of the recently released ISO/IEC/IEEE 29119 Software Testing Standard as a conceptual model (UML class diagrams) together with OCL constraints. The conceptual model captures all the necessary concepts based on the standard terminology that are mandatory or optional in the context of MBT techniques and can be used to define new MBT tools and techniques. To validate the conceptual model, we instantiated its concepts for various MBT techniques previously developed in the context of our industrial partners. Such instantiation automatically enforces the specified OCL constraints. This type of validation provided us feedback to further refine the conceptual model. Finally, we also provide our experiences and lessons learnt for such formalization and validation.}, 
keywords={IEC standards;IEEE standards;ISO standards;program testing;software standards;Unified Modeling Language;ISO/IEC/IEEE 29119 software testing standard;model-based testing;UML class diagram;OCL constraint;Unified modeling language;Concrete;Testing;Terminology;Data models;ISO Standards;Model-Based Testing;ISO/IEC/IEEE 29119;UML;Test Case Generation;Modeling Methodology}, 
doi={10.1109/MODELS.2015.7338271}, 
ISSN={}, 
month={Sept},}
@ARTICLE{8383962, 
author={F. Basciani and M. D'Emidio and D. Di Ruscio and D. Frigioni and L. Iovino and A. Pierantonio}, 
journal={IEEE Transactions on Software Engineering}, 
title={Automated Selection of Optimal Model Transformation Chains via Shortest-Path Algorithms}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Conventional wisdom on model transformations in Model-Driven Engineering (MDE) suggests that they are crucial components in modeling environments to achieve superior automation, whether it be refactoring, simulation, or code generation. While their relevance is well-accepted, model transformations are challenging to design, implement, and verify because of the inherent complexity that they must encode. Thus, defining transformations by chaining existing ones is key to success for enhancing their reusability. This paper proposes an approach, based on well-established algorithms, to support modellers when multiple transformation chains are available to bridge a source metamodel with a target one. The all-important goal of selecting the optimal chain has been based on the quality criteria of coverage and information loss. The feasibility of the approach has been demonstrated by means of experiments operated on chains obtained from transformations borrowed from a publicly available repository.}, 
keywords={Unified modeling language;Adaptation models;Bridges;Analytical models;Model driven engineering;Ecosystems;Model-driven engineering;Model Transformation Composition;Graph Algorithms;Shortest Paths}, 
doi={10.1109/TSE.2018.2846223}, 
ISSN={0098-5589}, 
month={},}
@ARTICLE{1653662, 
author={Pei Hsia and Petry}, 
journal={Computer}, 
title={A Systematic Approach to Interactive Programming}, 
year={1980}, 
volume={13}, 
number={6}, 
pages={27-34}, 
abstract={Designed for program development in an interactive environment, this framework can help create a new generation of programmers with an invaluable disciplined approach to software development.}, 
keywords={Programming profession;Software engineering;Software design;Automatic programming;Programming environments;Production systems;Software quality}, 
doi={10.1109/MC.1980.1653662}, 
ISSN={0018-9162}, 
month={June},}
@INPROCEEDINGS{6984580, 
author={P. Oliveira and M. Souza and R. Braga and R. Britto and R. L. Rabêlo and P. S. Neto}, 
booktitle={2014 IEEE 26th International Conference on Tools with Artificial Intelligence}, 
title={Athena: A Visual Tool to Support the Development of Computational Intelligence Systems}, 
year={2014}, 
volume={}, 
number={}, 
pages={950-959}, 
abstract={Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.}, 
keywords={artificial intelligence;software product lines;visual programming;Athena;Computational Intelligence Systems;software product line;test case selection;CI techniques;trivial activity;drag-and-drop approach;CI-as-a-Service;CIaaS;visual programming;prioritization;visual tool;Visualization;Productivity;Remuneration;Computational modeling;Algorithm design and analysis;Computational intelligence;Resource management;Computational Intelligence;Artificial Intelligence;Visual Programming;Tool;Service}, 
doi={10.1109/ICTAI.2014.144}, 
ISSN={1082-3409}, 
month={Nov},}
@INPROCEEDINGS{4702753, 
author={T. Schavey and S. Duba}, 
booktitle={2008 IEEE/AIAA 27th Digital Avionics Systems Conference}, 
title={Streamlining IMA integration through model-driven methodologies}, 
year={2008}, 
volume={}, 
number={}, 
pages={1.B.3-1-1.B.3-5}, 
abstract={Avionics systems integration is an inherently complex undertaking. In addition to ensuring that basic functionality is satisfied, the systems integrator must maximize the systempsilas flexibility and reliability while minimizing weight and cost of change. With the introduction of integrated modular architectures (IMA) based on open standards, many traditional integration issues have been greatly improved. However, additional integration responsibilities arise due to having a large number of functions developed by independent suppliers all sharing the same physical resources. This paper describes the benefits of incorporating model-driven methodologies and associated tools into the systems integration process through Model Driven Integration (MDI). It describes how a system model can not only streamline the additional responsibilities but also establish a highly productive systems development environment and allow for virtual integration. In addition, this paper discusses a number of side- benefits that grow out of having a modeling tool platform such as enhanced team communication and automation opportunities. The analysis of avionics architectures and the use of model-driven methodologies to increase IMA manageability is based upon the authors' experience in developing platform computing systems at GE Aviation. GE Aviation has developed open system IMA architectures for both commercial aircraft and military aircraft.}, 
keywords={aerospace computing;avionics;computer architecture;avionics systems integration;integrated modular architectures;IMA;Aerospace electronics;Hardware;Cost function;Computer architecture;Humans;Biological system modeling;Manufacturing;Military aircraft;Muscles;Automotive engineering}, 
doi={10.1109/DASC.2008.4702753}, 
ISSN={2155-7195}, 
month={Oct},}
@INBOOK{8044359, 
author={William A. Flanagan}, 
booktitle={VoIP and Unified Communications: Internet Telephony and the Future Voice Network}, 
title={Index}, 
year={2011}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={10.1002/9781118166048.index}, 
ISSN={}, 
publisher={Wiley}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8044359},}
@INPROCEEDINGS{7102628, 
author={E. Rodrigues and M. Bernardino and L. Costa and A. Zorzo and F. Oliveira}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={PLeTsPerf - A Model-Based Performance Testing Tool}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Performance testing is a highly specialized task, since it requires that a performance engineer knows the application to be tested, its usage profile, and the infrastructure where it will execute. Moreover, it requires that testing teams expend a considerable effort and time on its automation. In this paper, we present the PLeTsPerf, a model-based performance testing tool to support the automatic generation of scenarios and scripts from application models. PLetsPerf is a mature tool, developed in collaboration with an IT company, which has been used in several works, experimental studies and pilot studies. We present an example of use to demonstrate the process of generating test scripts and scenarios from UML models to test a Web application. We also present the lessons learned and discuss our conclusions about the use of the tool.}, 
keywords={Internet;program testing;PLeTsPerf tool;model-based performance testing tool;scenario generation;script generation;UML model;Unified Modeling Language;Web application;Unified modeling language;Testing;Load modeling;Software;Companies;Generators;Visualization}, 
doi={10.1109/ICST.2015.7102628}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{7546576, 
author={M. Jain and D. Gopalani}, 
booktitle={2016 Second International Conference on Computational Intelligence Communication Technology (CICT)}, 
title={Aspect Oriented Programming and Types of Software Testing}, 
year={2016}, 
volume={}, 
number={}, 
pages={64-69}, 
abstract={Software testing is a process to determine that a software product satisfies the specified requirements. Software testing spans over all phases of the Software Development Life Cycle namely, requirement specification, analysis, designing, development, deployment and maintenance of a software. Software testing is important to point out the defects in the software and to ensure that the developed software works fine in the real environment with different operating systems, devices, browsers and concurrent users. Further software testing can be classified into various types based on the objective of testing, level at which the testing is performed, knowledge of the system or the degree of automation. In this paper, we examine the suitability of Aspect Oriented Programming (AOP) for the purpose of performing various types of software testing. AOP is a programming paradigm which modularizes the crosscutting concerns into units called aspects and separates them from the modules implementing the primary business logic. This leads to a system that is easier to understand and simpler to maintain. The basis of the idea behind using AOP for software testing is that aspects in AOP can be used to capture execution points within the program's modules and thus we can test components where we suspect bugs without even modifying the source code.}, 
keywords={aspect-oriented programming;formal specification;program debugging;program testing;software maintenance;aspect oriented programming;AOP;software testing;software development life cycle;requirement specification;software analysis;software design;software deployment;software maintenance;bugs;Computational intelligence;Communications technology;Aspect Oriented Programming;Types of Software Testing;Aspects;AOP}, 
doi={10.1109/CICT.2016.22}, 
ISSN={}, 
month={Feb},}
@ARTICLE{4067154, 
author={}, 
journal={IEEE Std P1175.2/D12.2, Jul 2006}, 
title={IEEE Draft Recommended Practice for Case Tool Interconnection-Characterization of Interconnections}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{6823865, 
author={H. Lackner and M. Thomas and F. Wartenberg and S. Weißleder}, 
booktitle={2014 IEEE Seventh International Conference on Software Testing, Verification and Validation}, 
title={Model-Based Test Design of Product Lines: Raising Test Design to the Product Line Level}, 
year={2014}, 
volume={}, 
number={}, 
pages={51-60}, 
abstract={System quality assurance techniques like testing are important for high-quality products and processes. The effort for applying them is usually high, but can be reduced using automation. Automated test design is possible by using models to specify test-relevant aspects and by generating tests on this basis. Testing multiple variants of a system like, e.g., a product line of a German car manufacturer, results in a significant, additional effort. In this paper, we deal with model-based testing of product lines. We combine feature models that are used to describe product lines and models that are used for automated model-based test design. Our main contribution is the definition of a test generation approach on the product line level, i.e., that does not depend on resolving single product variants. Furthermore, we compare our approach to other test generation approaches and evaluate it using our tool chain SPLTestbench for some product line examples.}, 
keywords={program testing;quality assurance;software product lines;software quality;model-based test design;product line level design;system quality assurance techniques;high-quality products;automated model-based test design;test generation approach;chain SPLTestbench tool;Unified modeling language;Testing;Biological system modeling;Standards;Security;Quality assurance;Credit cards;Software Product Lines;Quality Assurance;Software Testing;Model-Based Testing;Software Reuse;Domain Level Testing}, 
doi={10.1109/ICST.2014.16}, 
ISSN={2159-4848}, 
month={March},}
@INBOOK{5238094, 
author={Mostafa Hashem Sherif}, 
booktitle={Managing Projects in Telecommunication Services}, 
title={Index}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={
No abstract.

}, 
keywords={Indexes}, 
doi={10.1002/9780470047682.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5238094},}
@INBOOK{5273158, 
author={Steven M. Kaplan}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={A}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={
}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch1}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273158},}
@INPROCEEDINGS{4259241, 
author={R. Van Roijen and C. Collins and J. Ayala and K. Barker and H. Boiselle and S. Catlett and K. Dezfulian and R. Logan and J. Maxson and R. Ramachandran and B. Rawlins and S. Ruegsegger and T. Rust and J. Shepard and R. Singh}, 
booktitle={2007 IEEE/SEMI Advanced Semiconductor Manufacturing Conference}, 
title={Reducing Time-to-Respond in a Modern Manufacturing Environment}, 
year={2007}, 
volume={}, 
number={}, 
pages={29-33}, 
abstract={The complexity of modern manufacturing processes has sharply increased the number of steps affecting device and circuit performance. We discuss a number of critical steps, their control methodology and how to minimize the time to detect. Product test results and data-mining are used to identify critical steps and to determine which inline signals require most attention. The last section is devoted to optimizing the analysis of inline electrical signals and their application to tool control.}, 
keywords={data mining;process control;semiconductor device manufacture;semiconductor device testing;manufacturing process control;product testing;data-mining;semiconductor device manufacture;time-to-respond;Implants;Manufacturing processes;Temperature control;Manufacturing automation;Rapid thermal annealing;Microelectronics;Circuit optimization;Circuit testing;Signal processing;Signal analysis;300mm manufacturing;SOI;Process control;Manufacturing automation}, 
doi={10.1109/ASMC.2007.375075}, 
ISSN={1078-8743}, 
month={June},}
@INPROCEEDINGS{5463662, 
author={M. Palviainen}, 
booktitle={2010 Third International Conference on Software Testing, Verification, and Validation Workshops}, 
title={A Dynamic Behaviour and Reliability Evaluation Method for Applications That Are Based on Asynchronous Processing Nodes}, 
year={2010}, 
volume={}, 
number={}, 
pages={309-318}, 
abstract={Many embedded and distributed applications are based on processing nodes that perform parallel processing tasks. Unfortunately, it is difficult to evaluate the overall behaviour of this kind of applications because the overall behaviour consists of 1) the execution-paths of asynchronous processing nodes and of 2) messages that either activate or deactivate processing nodes to perform parallel processing tasks. In order to facilitate behaviour and reliability evaluation of applications doing parallel processing, we developed a method that: 1) is capable of composing an overall representation for parallel behaviours and recognizing both the defined use cases and undetermined behaviours from this representation and 2) supports calculation of use case-specific reliability values for components. In this paper, we describe the method, present a ComponentBee tool that implements the method and supports behaviour and reliability evaluation of multithreaded Java applications, and finally demonstrate the use of the method with a case study.}, 
keywords={Java;multi-threading;parallel processing;software reliability;software tools;dynamic behaviour;reliability evaluation;asynchronous processing node;parallel processing;parallel behaviour;defined use case;case-specific reliability value;ComponentBee tool;multithreaded Java application;Parallel processing;Application software;Software measurement;Java;Software systems;Software testing;Performance evaluation;Concurrent computing;Predictive models;behaviour evaluation;reliability evaluation;ComponentBee;parallel processing}, 
doi={10.1109/ICSTW.2010.45}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8432197, 
author={R. Rana and T. Lagercrantz and M. Staron}, 
booktitle={2018 IEEE International Conference on Software Architecture Companion (ICSA-C)}, 
title={Building an Effective Software Issues Scorecard: An Action Research Report from the Automotive Domain}, 
year={2018}, 
volume={}, 
number={}, 
pages={136-143}, 
abstract={A large number of mature software companies use data and analytic for status monitoring of their projects and to help improve their decision making at different levels within the organization. Dashboards or scorecards also provide common platform for different stakeholders to access information they need for tracking the status of projects of their interest. Further data from software issues database can provide real and observable indicators to track the quality of given product during its development and testing. The study presented here reports on distinct and evolution of information needs of different stakeholder groups interested in tracking such data. The action research report documents the evolution of software issues scorecard as it is extended to meet information need of specific user groups. A roadmap for future into how such scorecard can be made more effective is also presented.}, 
keywords={decision making;insurance data processing;software development management;action research report documents;observable indicators;software issues database;access information;common platform;decision making;status monitoring;mature software companies;automotive domain;effective software issues scorecard;Software;Testing;Stakeholders;Monitoring;Automotive engineering;Companies;Automobiles;software issue;scorecard;software development;action research;information need;defect database}, 
doi={10.1109/ICSA-C.2018.00042}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6934560, 
author={S. Agnelli and P. Feltz and P. Griffiths and D. Roth}, 
booktitle={2014 7th Advanced Satellite Multimedia Systems Conference and the 13th Signal Processing for Space Communications Workshop (ASMS/SPSC)}, 
title={Satellite's role in the penetration of broadband connectivity within the European Union}, 
year={2014}, 
volume={}, 
number={}, 
pages={306-311}, 
abstract={The European Commission (EC) has recently acknowledged that broadband coverage for all - the 2013 target of the Digital Agenda for Europe (DAE) - has been achieved thanks to satellite broadband services, such as Tooway™, the consumer-grade Internet access at 20 Mbps via the Eutelsat KA-SAT satellite. However, broadband take-up in the European Union (EU) is still far from being satisfactory, notably in rural and remote areas where satellite solutions are ideally suited.}, 
keywords={Internet;satellite communication;satellite role;broadband connectivity penetration;European union;European commission;EC;broadband coverage;digital agenda for Europe;satellite broadband services;Internet access;Eutelsat KA-SAT satellite;Broadband communication;Satellites;Europe;Internet;Investment;Multimedia systems;Signal processing;Broadband Coverage;Broadband Take-Up;Digital Divide;Digital Agenda for Europe;European Union;KA-SAT;SABER Project;Voucher Schemes}, 
doi={10.1109/ASMS-SPSC.2014.6934560}, 
ISSN={2329-7093}, 
month={Sept},}
@ARTICLE{5389542, 
author={T. B. Brodnax and R. V. Billings and S. C. Glenn and P. T. Patel}, 
journal={IBM Journal of Research and Development}, 
title={Implementation of the PowerPC 601 microprocessor}, 
year={1994}, 
volume={38}, 
number={5}, 
pages={621-632}, 
abstract={To produce a marketable PowerPC™ microprocessor on a short development schedule, the logic had to be designed in a manner flexible enough to allow quick modifications without sacrificing high performance and density when customized cells were required. This was accomplished for the PowerPC 601™ microprocessor (601) with a high-level design-language description, which was synthesized for a gate-level implementation and simulated for functional verification. In a similar way, the physical design strategy for the 601 struck an attractive balance between a highly automated, flexible floorplan and the additional density that had to be available for limited, well-conceived manual placements. Finally, a rigorous test strategy was implemented, which has proved very useful in analyzing the processor and in assembling 601-based systems. Careful adherence to this methodology led to a successful first-pass physical implementation, leaving the second iteration for additional customer requests.}, 
keywords={}, 
doi={10.1147/rd.385.0621}, 
ISSN={0018-8646}, 
month={Sept},}
@INPROCEEDINGS{7473044, 
author={P. Zawistowski}, 
booktitle={2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)}, 
title={The Method of Measurement and Control Systems Design and Validation with Use of BRMS Systems}, 
year={2016}, 
volume={}, 
number={}, 
pages={324-332}, 
abstract={Quality of software has always been a problem in every area of software use. Lack of software can cause problems during software execution and lead to different failures. Measurement and control systems (MCS) are such a group of software which use laboratory devices to obtain measurement data or to control e.g. production processes. Bugs in software can lead to variety of problems, from incorrect measurement data to device damages. For this reason it is important to deliver software of good quality. Software development should be considered a process which consists of a sequence of steps and is supported by a tool or set of tools that interoperate to make the work with the software easier. The problem is that there is no such approach nor tools for MCS systems. For this reason, a suitable method of design and validation of this group of systems has been developed. Moreover, the proper solution for supporting the proposed method has been implemented and tested. The solution consists of Business Rule Management System (BMRS) used for validating software execution data in an efficient way.}, 
keywords={formal verification;program debugging;software quality;BRMS system;software quality;measurement-and-control system;software development;business rule management system;Software;Software measurement;Standards;Software engineering;Computer languages;Nickel;Control systems;BRMS;Drools;LabVIEW;measurement and control systems;software design;software quality assurance;software runtime validation}, 
doi={10.1109/SOSE.2016.61}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7313460, 
author={C. Berger and D. Block and C. Hons and S. Kühnel and A. Leschke and D. Plotnikov and B. Rumpe}, 
booktitle={2015 IEEE 18th International Conference on Intelligent Transportation Systems}, 
title={Large-Scale Evaluation of an Active Safety Algorithm with EuroNCAP and US NCAP Scenarios in a Virtual Test Environment -- An Industrial Case Study}, 
year={2015}, 
volume={}, 
number={}, 
pages={2280-2286}, 
abstract={Context: Recently, test protocols from organizations like European New Car Assessment Programme (EuroNCAP) were extended to also cover active safety systems. Objective: The official EuroNCAP test protocol for Autonomous Emergency Braking (AEB)/Forward Collision Warning (FCW) systems explicitly defines to what extent a Vehicle-Under-Test (VUT) is allowed to vary in its lateral position. In addition, the United States New Car Assessment Programme (US NCAP) test protocol has broader tolerance ranges. The goal for automotive OEMs is to understand the impact of such allowed variations on a the overall vehicle's performance. Method: A simulation-based approach is outlined that allows systematic, large-scale analysis of such influences to effectively plan time-consuming and resource-intense real-world vehicle tests. Our models allow a profound analysis of an AEB algorithm by modeling and conducting more than 3,000 simulation runs with EuroNCAP's dynamic CCRm and CCRb scenarios including those with adopted USNCAP parameters. Results: Our structured analysis of such test procedures involving dynamic actors is the first of its kind in a relevant industrial setting. Several anomalies were unveiled under US NCAP conditions to support real-world test runs. Hence, we could show that the proposed method supports all possible scenarios in AEB consumer tests and scales as we had to timely process approx. 7.7GB of simulation data. Conclusion: To achieve the expected performance and to study a system's behavior in potential misuse cases from a functional point of view, large scale, model-based simulations complement traditional testing on proving ground.}, 
keywords={automobile industry;automobiles;automotive engineering;braking;digital simulation;driver information systems;mechanical testing;road safety;large-scale evaluation;active safety algorithm;US NCAP Scenario;virtual test environment;industrial case study;European New Car Assessment Programme;active safety system;official EuroNCAP test protocol;autonomous emergency braking;forward collision warning;FCW system;vehicle-under-test;VUT;lateral position;United States New Car Assessment Programme test protocol;US NCAP test protocol;automotive OEM;vehicle performance;simulation-based approach;systematic large-scale analysis;vehicle test;USNCAP parameters;test procedure;AEB consumer test;system behavior;potential misuse cases;model-based simulation;Vehicles;Safety;Data models;Protocols;Vehicle dynamics;Trajectory;Systematics}, 
doi={10.1109/ITSC.2015.368}, 
ISSN={2153-0017}, 
month={Sept},}
@INPROCEEDINGS{4221614, 
author={A. Bertolino}, 
booktitle={Future of Software Engineering (FOSE '07)}, 
title={Software Testing Research: Achievements, Challenges, Dreams}, 
year={2007}, 
volume={}, 
number={}, 
pages={85-103}, 
abstract={Software engineering comprehends several disciplines devoted to prevent and remedy malfunctions and to warrant adequate behaviour. Testing, the subject of this paper, is a widespread validation approach in industry, but it is still largely ad hoc, expensive, and unpredictably effective. Indeed, software testing is a broad term encompassing a variety of activities along the development cycle and beyond, aimed at different goals. Hence, software testing research faces a collection of challenges. A consistent roadmap of the most relevant challenges to be addressed is here proposed. In it, the starting point is constituted by some important past achievements, while the destination consists of four identified goals to which research ultimately tends, but which remain as unreachable as dreams. The routes from the achievements to the dreams are paved by the outstanding research challenges, which are discussed in the paper along with interesting ongoing work.}, 
keywords={program testing;program verification;software testing;software engineering;widespread validation approach;Software testing;Software engineering;Laboratories;Software systems;Software quality;Councils;Computer industry;Quality assurance;Feedback;State estimation}, 
doi={10.1109/FOSE.2007.25}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5070969, 
author={A. Jaaskelainen and M. Katara and A. Kervinen and M. Maunumaa and T. Paakkonen and T. Takala and H. Virtanen}, 
booktitle={2009 31st International Conference on Software Engineering - Companion Volume}, 
title={Automatic GUI test generation for smartphone applications - an evaluation}, 
year={2009}, 
volume={}, 
number={}, 
pages={112-122}, 
abstract={We present the results of an evaluation where we studied the effectiveness of automatic test generation for graphical user interface (GUI) testing of smartphone applications. To describe the context of our evaluation, the tools and the test model library we have developed for the evaluation are also presented. The library contains test models for basic S60 applications, such as camera, contacts, etc. The tools include an on-line test generator that produces sequences of so called keywords to be executed on the test targets. In our evaluation, we managed to find over 20 defects from applications that had been on the market for several months. We also describe the problems we faced during the evaluation.}, 
keywords={automatic testing;graphical user interfaces;program testing;smartphone applications;automatic GUI test generation;graphical user interface;test model library;S60 applications;Graphical user interfaces;Automatic testing;Software testing;System testing;Application software;Software systems;Libraries;Cameras;Optical character recognition software;Context modeling}, 
doi={10.1109/ICSE-COMPANION.2009.5070969}, 
ISSN={}, 
month={May},}
@ARTICLE{7997723, 
author={C. Raibulet and F. Arcelli Fontana and M. Zanoni}, 
journal={IEEE Access}, 
title={Model-Driven Reverse Engineering Approaches: A Systematic Literature Review}, 
year={2017}, 
volume={5}, 
number={}, 
pages={14516-14542}, 
abstract={This paper explores and describes the state of the art for what concerns the model-driven approaches proposed in the literature to support reverse engineering. We conducted a systematic literature review on this topic with the aim to answer three research questions. We focus on various solutions developed for model-driven reverse engineering, outlining in particular the models they use and the transformations applied to the models. We also consider the tools used for model definition, extraction, and transformation and the level of automation reached by the available tools. The model-driven reverse engineering approaches are also analyzed based on various features such as genericity, extensibility, automation of the reverse engineering process, and coverage of the full or partial source artifacts. We describe in detail and compare fifteen approaches applying model-driven reverse engineering. Based on this analysis, we identify and indicate some hints on choosing a model-driven reverse engineering approach from the available ones, and we outline open issues concerning the model-driven reverse engineering approaches.}, 
keywords={reverse engineering;model-driven reverse engineering;model definition;model extraction;model transformation;genericity;extensibility;Reverse engineering;Object oriented modeling;Tools;Systematics;Analytical models;Bibliographies;Search engines;Models;reverse engineering;model-driven reverse engineering;model transformation;legacy system}, 
doi={10.1109/ACCESS.2017.2733518}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8115704, 
author={C. Pietsch and M. Ohrndorf and U. Kelter and T. Kehrer}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Incrementally slicing editable submodels}, 
year={2017}, 
volume={}, 
number={}, 
pages={913-918}, 
abstract={Model slicers are tools which provide two services: (a) finding parts of interest in a model and (b) displaying these parts somehow or extract these parts as a new, autonomous model, which is referred to as slice or sub-model. This paper focuses on the creation of editable slices, which can be processed by model editors, analysis tools, model management tools etc. Slices are useful if, e.g., only a part of a large model shall be analyzed, compared or processed by time-consuming algorithms, or if sub-models shall be modified independently. We present a new generic incremental slicer which can slice models of arbitrary type and which creates slices which are consistent in the sense that they are editable by standard editors. It is built on top of a model differencing framework and does not require additional configuration data beyond those available in the differencing framework. The slicer can incrementally extend or reduce an existing slice if model elements shall be added or removed, even if the slice has been edited meanwhile. We demonstrate the usefulness of our slicer in several scenarios using a large UML model. A screencast of the demonstrated scenarios is provided at http://pi.informatik.uni-siegen.de/projects/SiLift/ase2017.}, 
keywords={formal specification;object-oriented programming;program slicing;program visualisation;Unified Modeling Language;UML model;autonomous model;editable slices;model editors;analysis tools;model management tools;generic incremental slicer;model differencing framework;model elements;incrementally sliced editable submodels;Unified modeling language;Adaptation models;Tools;Servers;Load modeling;Computational modeling;Data models}, 
doi={10.1109/ASE.2017.8115704}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8452099, 
author={A. Steffens and H. Lichter and J. S. Döring}, 
booktitle={2018 IEEE/ACM 4th International Workshop on Rapid Continuous Software Engineering (RCoSE)}, 
title={Designing a Next-Generation Continuous Software Delivery System: Concepts and Architecture}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Continuous Integration and Continuous Delivery are established practices in modern agile software development. The DevOps movement adapted theses practices and places the deployment pipeline at its heart as one of the main requirements to automate the software development process and to deliver and operate software in a more robust way with higher quality. Over the time a lot of systems and tools has been developed to implement the deployment pipeline and to support continuous delivery. But software development is complex, its process even more and due to the individual organization of software vendors no real all-in-one solution for CD exists. Literature identified a lot of challenges when adopting CD and DevOps in an organization. This paper presents a conceptual model and fundamental design decisions for a new generation of software delivery systems tackling some of these issues. Our approach focuses on two specific challenges for adopting CD. The first is the lack of flexibility and maintainability of software delivery systems. The second is the insufficient user support to model and manage delivery processes and pipelines. We introduce an automated mechanism to ease the effort for developers and other stakeholders. Based on these results this paper introduces an architectural proposal for a next-generation continuous software delivery system."}, 
keywords={Software;Pipelines;Logic gates;Computer architecture;Tools;Computational modeling;Next generation networking;Continuous Delivery;Continuous Integration;DevOps;Software Architecture;Microservices;Domain Driven Design;Domain Model;Software Delivery;Continuous Software Engineering;Software Design}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4148978, 
author={J. Liu and J. Dehlinger and H. Sun and R. Lutz}, 
booktitle={14th Annual IEEE International Conference and Workshops on the Engineering of Computer-Based Systems (ECBS'07)}, 
title={State-Based Modeling to Support the Evolution and Maintenance of Safety-Critical Software Product Lines}, 
year={2007}, 
volume={}, 
number={}, 
pages={596-608}, 
abstract={Changes to safety-critical product lines can jeopardize the safety properties that they must ensure. Thus, evolving software product lines must consider the impact that changes to requirements may have on the existing systems and their safety. The contribution of this work is a systematic, tool-supported technique to support safe evolution of product-line requirements using a model-based approach. We show how the potential feature interactions that need to be modeled are scoped and identified with the aid of product-line software fault tree analysis. Further, we show how reuse of the state-based models is effectively exploited in the evolution phase of product-line engineering. To illustrate this approach, we apply our technique to the evolution of a safety-critical cardiac pacemaker product line}, 
keywords={formal specification;safety-critical software;software maintenance;software prototyping;state-based modeling;safety-critical software product line evolution;safety-critical software product line maintenance;product-line requirements;product-line engineering;Software maintenance;Software safety;Product safety;Fault trees;Pacemakers;Software tools;Systems engineering and theory;Costs;Reliability engineering;Computer science}, 
doi={10.1109/ECBS.2007.66}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5979373, 
author={S. Wang and Y. Wu and M. Lu and H. Li}, 
booktitle={The Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety}, 
title={Software reliability modeling based on test coverage}, 
year={2011}, 
volume={}, 
number={}, 
pages={665-671}, 
abstract={In order to incorporate the effect of test coverage, two novel software reliability growth models (SRGMs) are proposed in this paper using failure data and test coverage simultaneously. One is continuous using testing time, and the other is discrete with respect to the number of executed test cases instead of testing time. Since one of the most important factors of the coverage-based SRGMs is the test coverage function (TCF), we first discuss a discrete TCF based on Beta function. Then we develop mean value functions (MVF) of the two models integrating test coverage and imperfect debugging. Finally the proposed TCF and MVFs are evaluated and validated on actual software reliability data collected from real software development projects. The results demonstrate clearly that both the proposed TCF and SRGMs provide better estimation and fitting for the data sets under comparisons.}, 
keywords={program debugging;program testing;software fault tolerance;software reliability;software reliability growth models;test coverage function;beta function;mean value functions;software debugging;software development projects;Software reliability;Testing;Software;Mathematical model;Fault detection;Equations;Software reliability growth model;test coverage function;beta function;non-homogeneous poisson process;mean value function}, 
doi={10.1109/ICRMS.2011.5979373}, 
ISSN={}, 
month={June},}
@ARTICLE{1166666, 
author={R. Baines and D. Pulley}, 
journal={IEEE Communications Magazine}, 
title={A total cost approach to evaluating different reconfigurable architectures for baseband processing in wireless receivers}, 
year={2003}, 
volume={41}, 
number={1}, 
pages={105-113}, 
abstract={There is growing interest in the use of flexible digital signal processors for wireless systems, driven by the demands of time to market, cost pressure, the requirement for flexibility to cope with evolving standards, and rapidly increasing processing needs. Much of the discussion of these techniques involves terms like "efficient" or "cost-effective" without necessarily quantifying the terms. This article considers the various architectures applicable to a wideband CDMA node-B base station (ASIC, FPGA, traditional DSP, and two varieties of flexible DSP) and builds a quantitative total cost approach to evaluating them, including benchmarked performance data.}, 
keywords={field programmable gate arrays;application specific integrated circuits;reconfigurable architectures;radio receivers;3G mobile communication;digital signal processing chips;broadband networks;code division multiple access;total cost approach;reconfigurable architectures;baseband processing;wireless receivers;flexible digital signal processors;wideband CDMA node-B base station;ASIC;FPGA;flexible DSP;3G wideband code-division multiple access base station;WCDMA;Costs;Reconfigurable architectures;Baseband;Digital signal processing;Digital signal processors;Time to market;Wideband;Multiaccess communication;Base stations;Application specific integrated circuits}, 
doi={10.1109/MCOM.2003.1166666}, 
ISSN={0163-6804}, 
month={Jan},}
@INPROCEEDINGS{6166261, 
author={J. Edmondson and A. Gokhale and S. Neema}, 
booktitle={2011 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)}, 
title={Automating testing of service-oriented mobile applications with distributed knowledge and reasoning}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Automated testing of distributed, service-oriented applications, particularly mobile applications, is a hard problem due to challenges testers often must deal with, such as (1) heterogeneous platforms, (2) difficulty in introducing additional resources or backups of resources that fail during testing, and (3) lack of fine-grained control over test sequencing. To address these challenges, this paper describes an approach that combines portable operating system libraries with knowledge and reasoning, which together leverage the best features of centralized and decentralized testing infrastructures to support both heterogeneous systems and distributed control by reasoning on distributed testing events.}, 
keywords={automatic test software;distributed control;inference mechanisms;mobile computing;operating systems (computers);program testing;service-oriented architecture;software libraries;automated service-oriented mobile application testing;distributed knowledge;distributed reasoning;resource backups;fine-grained control;test sequencing;portable operating system libraries;decentralized testing infrastructure;centralized testing infrastructure;heterogeneous systems;distributed control;distributed testing events;Testing;Cognition;Servers;Engines;Knowledge engineering;Real time systems;Conferences;test sequencing;distributed control in testing;portability;knowledge dissemination}, 
doi={10.1109/SOCA.2011.6166261}, 
ISSN={2163-2871}, 
month={Dec},}
@INPROCEEDINGS{7107547, 
author={M. Binas}, 
booktitle={2014 IEEE 12th IEEE International Conference on Emerging eLearning Technologies and Applications (ICETA)}, 
title={Identifying web services for automatic assessments of programming assignments}, 
year={2014}, 
volume={}, 
number={}, 
pages={45-50}, 
abstract={The main objective of this article is to verify the assumption, if the web services can be used in the process of automatic assessment of programming assignments. It tries to identify general services for such processes and presents the experimental part by creating platform based on set of web services.}, 
keywords={computer science education;programming;Web services;automatic programming assignment assessment;Web services;Web services;Programming;Testing;Electronic mail;Plagiarism;Uniform resource locators;Electronic learning}, 
doi={10.1109/ICETA.2014.7107547}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{5679044, 
author={S. Gallant and C. Gaughan}, 
booktitle={Proceedings of the 2010 Winter Simulation Conference}, 
title={Systems engineering for distributed live, virtual, and constructive (LVC) simulation}, 
year={2010}, 
volume={}, 
number={}, 
pages={1501-1511}, 
abstract={Designing a distributed simulation environment across multiple domains that typically have disparate middleware transport protocols, data exchange formats and applications increases the difficulty of capturing and linking system design decisions to the resultant implementation. Systems engineering efforts for distributed simulation environments are typically based on the middleware transport used, the applications available and the constraints placed on the technical team including network, computer and personnel limitations. To facilitate community re-use, systems engineering should focus on integrated operational function decomposition. This links data elements produced within the simulation to the functional capabilities required by the user. The system design should be captured at a functional level and subsequently linked to the technical design. Doing this within a data-driven systems engineering infrastructure allows generative programming techniques to assist accurate, flexible and rapid architecture development. This paper describes the MATREX program systems engineering process, infrastructure and path forward.}, 
keywords={digital simulation;electronic data interchange;middleware;military computing;systems engineering;transport protocols;data driven system engineering;live virtual constructive simulation;LVC simulation;distributed simulation environment;middleware transport protocol;data exchange format;system design;generative programming technique;MATREX program;Computer architecture;System analysis and design;Data models;Testing;Middleware}, 
doi={10.1109/WSC.2010.5679044}, 
ISSN={1558-4305}, 
month={Dec},}
@INPROCEEDINGS{1214600, 
author={I. Gohar and A. Mirza}, 
booktitle={IEEE Students Conference, ISCON '02. Proceedings.}, 
title={Voice over asynchronous transfer modc(ATM)}, 
year={2002}, 
volume={2}, 
number={}, 
pages={11-12}, 
abstract={}, 
keywords={Asynchronous transfer mode;Software testing;Software performance;Transportation;Educational institutions;Shape;System testing;Fault detection;Speech analysis;Spine}, 
doi={10.1109/ISCON.2002.1214600}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{4641444, 
author={A. Gargantini and E. Riccobene and P. Scandurra and A. Carioni}, 
booktitle={2008 Forum on Specification, Verification and Design Languages}, 
title={Scenario-based validation of embedded systems}, 
year={2008}, 
volume={}, 
number={}, 
pages={191-196}, 
abstract={This paper describes a scenario-based methodology for system-level design validation based on the Abstract State Machines formal method. This scenario-based approach complements an existing model-driven design methodology for embedded systems based on the SystemC UML profile. It allows the designer to functionally validate system components from SystemC UML designs early at high levels of abstraction and without requiring strong skills and expertise on formal methods. A validation tool integrated into an existing model-driven co-design environment to support the proposed scenario-based validation flow is also presented.}, 
keywords={C++ language;embedded systems;finite automata;object-oriented programming;program compilers;Unified Modeling Language;scenario-based validation;embedded systems;system-level design validation;abstract state machines formal method;model-driven design;SystemC UML profile;system components;Unified modeling language;Biological system modeling;Modeling;Embedded system;Analytical models;Libraries;Design methodology}, 
doi={10.1109/FDL.2008.4641444}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{5767210, 
author={R. Mohammed and R. Sahan and Y. Xia and Y. Pang}, 
booktitle={2011 27th Annual IEEE Semiconductor Thermal Measurement and Management Symposium}, 
title={High performance air-cooled temperature margining thermal tools for silicon validation}, 
year={2011}, 
volume={}, 
number={}, 
pages={265-271}, 
abstract={Thermal tools provide temperature margining capability by varying the case temperature at silicon thermal design power (TDP). They are used for process, voltage, temperature and frequency (PVTF) testing by Intel's post-silicon validation customers across servers, desktops, mobile and graphics segments. Thermal margining tools are widely used in silicon debug validation by varying the case temperature over a wide operating range of specifications of the Silicon to i) validate the silicon, ii) accelerate fault detection, and iii) reduce escapes and identify bugs. Thermal tool is controlled by a thermal controller to provide a temperature set-point based on the device under test's (DUT's) case or junction diode temperature. Air cooled thermal tool (AC-TT) employs a controller card to achieve the margining capability by running the tool's thermoelectric cooler (TEC), a Peltier device, within the optimal temperature range. AC-TT has an active heat sink design to remove the heat dissipated by the TEC and the silicon. Although AC-TT is expected to provide narrower range of margining capability due to the limitations of air cooling, they still can be an excellent solution for some specific thermal margining applications. Therefore, a new line of AC-TTs were developed for validation customers whose needs can be addressed without requiring costly controllers and noisy chillers while enhancing the user-experience. This paper presents the design improvement strategies implemented for developing the new line of CPU, Chipset and ASIC AC-TTs. Improved designs provide wider margining capability by using i) high performance active heat sink designs, ii) high power thermo-electric cooler (TEC), iii) cold plate designs compatible to keep out volume (KOV), iv) new choice of thermal interface material (TIM), and v) new retention design. This paper discusses the details of the design process and how multiple design strategies are implemented to finalize the design and to achieve the overall performance improvement while keeping the cost of the AC-TT low. The new line of AC-TT designs have performance improvement of 44% (~25C) for 130W CPU TT compared to existing CPU AC-TT, of 32% (~19C) for 60W chipset compared to existing chipset AC-TT, and of 41% (~8C) compared to existing 15W PCH (Peripheral Component Hub) AC-TT. Design strategies provided here can be easily adapted to develop future generation of low-cost CPU, chipset, and ASIC AC-TTs with a wider margining capability.}, 
keywords={cooling;heat sinks;Peltier effect;temperature measurement;thermoelectric devices;high performance air-cooled temperature margining thermal tools;silicon validation;thermal design power;thermal controller;junction diode temperature;thermoelectric cooler;Peltier device;thermal interface material;Heat sinks;Silicon;Cold plates;Heating;Heat transfer;Thermal resistance;Thermal tool;CPU;chipset;ASIC;retention design;CFD;TEC;air cooling;TIM;temperature margining}, 
doi={10.1109/STHERM.2011.5767210}, 
ISSN={1065-2221}, 
month={March},}
@INPROCEEDINGS{7927983, 
author={C. Artho and Q. Gros and G. Rousset and K. Banzai and L. Ma and T. Kitamura and M. Hagiya and Y. Tanabe and M. Yamamoto}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Model-Based API Testing of Apache ZooKeeper}, 
year={2017}, 
volume={}, 
number={}, 
pages={288-298}, 
abstract={Apache ZooKeeper is a distributed data storage that is highly concurrent and asynchronous due to network communication, testing such a system is very challenging. Our solution using the tool "Modbat" generates test cases for concurrent client sessions, and processes results from synchronous and asynchronous callbacks. We use an embedded model checker to compute the test oracle for non-deterministic outcomes, the oracle model evolves dynamically with each new test step. Our work has detected multiple previously unknown defects in ZooKeeper. Finally, a thorough coverage evaluation of the core classes show how code and branch coverage strongly relate to feature coverage in the model, and hence modeling effort.}, 
keywords={application program interfaces;distributed databases;formal verification;program testing;storage management;model-based API testing;Apache ZooKeeper;distributed data storage;network communication;Modbat;test cases;concurrent client sessions;synchronous callbacks;asynchronous callbacks;embedded model checker;nondeterministic outcomes;coverage evaluation;core classes;branch coverage;feature coverage;modeling effort;Servers;Testing;Electronic mail;Computational modeling;Computer science;Tools;Complexity theory;Model-based testing;concurrency;asynchronous systems;networked systems;test oracle;Apache ZooKeeper}, 
doi={10.1109/ICST.2017.33}, 
ISSN={}, 
month={March},}
@INBOOK{6542510, 
author={G. Keith Cambron}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Front Matter}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={

The prelims comprise:

Half-Title Page

Title Page

Copyright Page

Dedication Page

Table of Contents

List of Figures

About the Author

Foreword

Preface

Acknowledgments

List of Acronyms

]]>}, 
keywords={}, 
doi={10.1002/9781118394519.fmatter}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6542510},}
@INPROCEEDINGS{7966874, 
author={R. Souza and A. Oliveira}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER)}, 
title={GuideAutomator: Continuous Delivery of End User Documentation}, 
year={2017}, 
volume={}, 
number={}, 
pages={31-34}, 
abstract={User guides, also known as user manuals, are a type of documentation aimed at helping a user operate a specific system. For software systems, user guides usually include screenshots that show users how to interact with the user interface. Because creating such screenshots is a slow, manual process, keeping the user guide up-to-date with changes in the user interface is challenging. We propose an approach in which the documentation writer interleaves the user guide text with source code that automates screen capturing. As a result, screenshots always reflect the latest software version, which makes the approach suitable for a project that uses continuous delivery. The approach was implemented as a prototype, called GuideAutomator.}, 
keywords={software tools;source code (software);system documentation;user interfaces;user manuals;GuideAutomator;continuous delivery;end user documentation;user guide text;software systems;user interface;source code;command-line tool;Documentation;Tools;Software;Browsers;Cascading style sheets;Programming;software documentation;automated documentation generator;literate programming;continuous delivery}, 
doi={10.1109/ICSE-NIER.2017.10}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1203468, 
author={P. De and A. Neogi and T. -. Chiueh}, 
booktitle={23rd International Conference on Distributed Computing Systems, 2003. Proceedings.}, 
title={VirtualWire: a fault injection and analysis tool for network protocols}, 
year={2003}, 
volume={}, 
number={}, 
pages={214-221}, 
abstract={The prevailing practice for testing protocol implementations is direct code instrumentation to trigger specific states in the code. This leaves very little scope for reuse of the test cases. In this paper, we present the design, implementation, and evaluation of VirtualWire, a network fault injection and analysis system designed to facilitate the process of testing network protocol implementations. VirtualWire injects user-specified network faults and matches network events against anticipated responses based on high-level specifications written in a declarative scripting language. With VirtualWire, testing requires no code instrumentation and fault specifications can be reused across versions of a protocol implementation. We illustrate the effectiveness of VirtualWire with examples drawn from testing Linux's TCP implementation and a real-time Ethernet protocol called Rether. In each case, 10 to 20 lines of script is sufficient to specify the test scenario. VirtualWire is completely transparent to the protocols under test, and additional overhead in protocol processing latency it introduces is below 10% of the normal.}, 
keywords={transport protocols;formal specification;formal verification;fault tolerant computing;local area networks;VirtualWire;network protocol;code instrumentation;fault specification;Linux TCP implementation testing;Ethernet protocol;Rether;Instruments;Access protocols;Ethernet networks;Kernel;System testing;Filters;Laboratories;Computer science;Delay;Formal verification}, 
doi={10.1109/ICDCS.2003.1203468}, 
ISSN={1063-6927}, 
month={May},}
@INPROCEEDINGS{6130706, 
author={A. Svendsen and Ø. Haugen and B. Moller-Pedersen}, 
booktitle={2011 18th Asia-Pacific Software Engineering Conference}, 
title={Using Variability Models to Reduce Verification Effort of Train Station Models}, 
year={2011}, 
volume={}, 
number={}, 
pages={348-356}, 
abstract={We show how the effort needed to verify a transformed base model can be reduced by analyzing the definition of the modification. The Common Variability Language (CVL) is a generic language for modeling variability, where a CVL model describes the increment from one base model to another (transformed) base model. Assuming that a property of the base model has been verified, we use the CVL model to reduce the effort needed to verify the property of the transformed model. Based on the CVL model, we narrow down the set of traces required to be verified, including the increment and the cascading effects. We apply CVL to several models of the Train Control Language (TCL) to illustrate how the effort of verifying safety properties of transformed train station models can be reduced.}, 
keywords={formal verification;railway engineering;railway safety;railways;safety-critical software;specification languages;variability models;verification effort;transformed base model;common variability language;generic language;variability modeling;CVL model;transformed model;cascading effects;train control language;TCL;safety property verification;transformed train station models;Safety;Biological system modeling;Analytical models;Metals;Switches;Semantics;Mathematical model;analysis;variability;safety property;Common Variability Language;Train Control Language}, 
doi={10.1109/APSEC.2011.21}, 
ISSN={1530-1362}, 
month={Dec},}
@INPROCEEDINGS{7338242, 
author={A. Kusel and J. Etzlstorfer and E. Kapsammer and W. Retschitzegger and W. Schwinger and J. Schönböck}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={Consistent co-evolution of models and transformations}, 
year={2015}, 
volume={}, 
number={}, 
pages={116-125}, 
abstract={Evolving metamodels are in the center of Model-Driven Engineering, necessitating the co-evolution of dependent artifacts like models and transformations. While model co-evolution has been extensively studied, transformation co-evolution has received less attention up to now. Current approaches for transformation co-evolution provide a fixed, restricted set of metamodel (MM) changes, only. Furthermore, composite changes are treated as monolithic units, which may lead to inconsistent co-evolution for overlapping atomic changes and prohibits extensibility. Finally, transformation co-evolution is considered in isolation, possibly inducing inconsistencies between model and transformation co-evolution. To overcome these limitations, we propose a complete set of atomic MM changes being able to describe arbitrary MM evolutions. Reusability and extensibility are supported by means of change composition, ensuring an intra-artifact consistent co-evolution. Furthermore, each change provides resolution actions for both, models and transformations, ensuring an inter-artifact consistent co-evolution. Based on our conceptual approach, a prototypical implementation is presented.}, 
keywords={software reusability;evolving metamodels;model-driven engineering;model coevolution;transformation coevolution;atomic MM changes;reusability;extensibility;change composition;intraartifact consistent coevolution;Biological system modeling;Semantics;Systematics;Feature extraction;Software;Syntactics;Companies}, 
doi={10.1109/MODELS.2015.7338242}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7430116, 
author={A. Marroquin and D. Gonzalez and S. Maag}, 
booktitle={2015 7th IEEE Latin-American Conference on Communications (LATINCOM)}, 
title={Testing distributed systems with test cases dependencies architecture}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In this work, we present a novel distributed testing architecture based on a formal definition of test cases dependencies to test the conformance of distributed systems in a black box context. Utilizing the European Telecommunication Standards Institute, Test Description Language standard, we apply our approach to a real Internet Multimedia Subsystem (IMS)/ SIP (Session Initiation Protocol) test bed and perform the tests through two use cases. This crucial activity belongs to the conformance testing context. Which aims at stimulating the communication system under test (SUT) to detect errors and unexpected behaviors with regards to the standards. When handling distributed systems, a major difficulty arises when testing these, due to the joint and linked stimulation of distributed entities. The main reason for it is the correlation of verdicts obtained from these entities.}, 
keywords={conformance testing;Internet;multimedia systems;signalling protocols;telecommunication standards;distributed systems testing;test case dependency architecture;black box context;European telecommunication standards institute;test description language standard;Internet multimedia subsystem;IMS;SIP;session initiation protocol;system under test;SUT;error detection;Testing;Protocols;Standards;Context;Synchronization;Correlation;Conformance Testing;Distributed Systems;Test Cases;SIP;TDL}, 
doi={10.1109/LATINCOM.2015.7430116}, 
ISSN={}, 
month={Nov},}
@INBOOK{5396935, 
author={}, 
booktitle={A Guide to the Wireless Engineering Body of Knowledge (WEBOK)}, 
title={Fundamental Knowledge}, 
year={2009}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Introduction

Contents

Electrical Engineering Basics for Wireless Communications

Signal Processing and Communication Systems

RF Engineering

Instruments And Measurements [Wit02]

Communication Networks

Other Communication Systems

General Engineering Management and Economics

Key References

}, 
keywords={}, 
doi={10.1002/9780470439128.ch7}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5396935},}
@INBOOK{7827467, 
author={Tyson T. Brooks}, 
booktitle={Cyber-Assurance for the Internet of Things}, 
title={Cyber-Assurance Through Embedded Security for the Internet of Things}, 
year={2017}, 
volume={}, 
number={}, 
pages={}, 
abstract={The Internet of Things (IoT) comprises billions of Internet-connected devices (ICD) or "things", each of which can sense, communicate, compute, and potentially actuate and can have intelligence, multimodal interfaces, physical/virtual identities, and attributes. Cyber-assurance is the justified confidence that networked systems are adequately secure to meet operational needs, even in the presence of attacks, failures, accidents, and unexpected events. The cyber-assurance recognition strategy is to define only the service-level interfaces and leave out domain-specific implementation details. Once the recognition of a cyber-attack has been identified from the recognition process, the fortification process takes place. Reestablishment is a means to return the ICDs to its operational condition after the cyber-attack through remapping to a different route since the ICD was under attack. When the IoT technologies are used as part of mission critical systems, the IoT services should be survivable in order to support the important missions.}, 
keywords={Embedded systems;Sensors;Protocols;Wireless sensor networks;Internet of Things;Authentication}, 
doi={10.1002/9781119193784.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7827467},}
@INPROCEEDINGS{6606719, 
author={V. Guana}, 
booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 
title={Supporting maintenance tasks on transformational code generation environments}, 
year={2013}, 
volume={}, 
number={}, 
pages={1369-1372}, 
abstract={At the core of model-driven software development, model-transformation compositions enable automatic generation of executable artifacts from models. Although the advantages of transformational software development have been explored by numerous academics and industry practitioners, adoption of the paradigm continues to be slow, and limited to specific domains. The main challenge to adoption is the fact that maintenance tasks, such as analysis and management of model-transformation compositions and reflecting code changes to model transformations, are still largely unsupported by tools. My dissertation aims at enhancing the field's understanding around the maintenance issues in transformational software development, and at supporting the tasks involved in the synchronization of evolving system features with their generation environments. This paper discusses the three main aspects of the envisioned thesis: (a) complexity analysis of model-transformation compositions, (b) system feature localization and tracking in model-transformation compositions, and (c) refactoring of transformation compositions to improve their qualities.}, 
keywords={software maintenance;software metrics;transformational code generation environments;maintenance tasks support;model-driven software development;model-transformation compositions;transformational software development;reflecting code management;software quality;Maintenance engineering;Object oriented modeling;Analytical models;Complexity theory;Software;Games;Semantics;software maintenance;transformation composition;transformation complexity;transformation refactoring}, 
doi={10.1109/ICSE.2013.6606719}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{6549304, 
author={R. Iyer and B. Chandramouleeswaran}, 
booktitle={International Conference on Software Engineering and Mobile Application Modelling and Development (ICSEMA 2012)}, 
title={Best practices and case study for open source middleware migration: Egate to apache camel migration}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Many legacy systems use message oriented middleware to communicate between themselves. Message oriented middleware are considered to be the most effective technology for enterprise integration. There are a lot of proprietary middleware solutions in the market that involve huge licensing costs, difficult maintenance procedures and niche skill sets. The usage of open source middleware to replace these proprietary solutions in a cost effective manner is an idea that can now bear fruition due to the relative maturity of such solutions. The use of open source reduces licensing cost, enables the developers to have greater insight into the working of the system and avail of the wide spread community support for such systems.}, 
keywords={middleware;public domain software;software maintenance;open source middleware migration;apache camel migration;legacy systems;message oriented middleware;enterprise integration;proprietary middleware solutions;Middleware;open source;Apache Camel;Egate;enterprise migration}, 
doi={10.1049/ic.2012.0140}, 
ISSN={}, 
month={Dec},}
@ARTICLE{5389507, 
author={P. Bose and S. Surya}, 
journal={IBM Journal of Research and Development}, 
title={Architectural timing verification of CMOS RISC processors}, 
year={1995}, 
volume={39}, 
number={1.2}, 
pages={113-129}, 
abstract={We consider the problem of verification and testing of architectural timing models ("timers") coded to predict cycles-per-instruction (CPI) performance of advanced CMOS superscalar (RISC) processors. Such timers are used for pre-hardware performance analysis and prediction. As such, these software models play a vital role in processor performance tuning as well as application-based competitive analysis, years before actual product availability. One of the key problems facing a designer, modeler, or application analyst who uses such a tool is to understand how accurate the model is, in terms of the actual design. In contrast to functional simulators, there is no direct way of testing timers in the classical sense, since the “correct” execution time (in cycles) of a program on the machine model under test is not directly known or computable from equations, truth tables, or other formal specifications. Ultimate validation (or invalidation) of such models can be achieved under actual hardware availability, by direct comparisons against measured performance. However, deferring validation solely to that stage would do little to achieve the overall purpose of accurate pre-hardware analysis, tuning, and projection. We describe a multilevel validation method which has been used successfully to transform evolving timers into highly accurate pre-hardware models. In this paper, we focus primarily on the following aspects of the methodology: a) establishment of cause-effect relationships in terms of model defects and the associated fault signatures; b) derivation of application-based test loop kernels to verify steady-state (periodic) behavior of pipeline flow, against analytically predicted signatures; and c) derivation of synthetic test cases to verify the “core” parameters characterizing the pipeline-level machine organization as implemented in the timer model. The basic tenets of the theory and its application are described in the context of an example processor, comparable in complexity to an advanced member of the PowerPC™ 6XX processor family.}, 
keywords={}, 
doi={10.1147/rd.391.0113}, 
ISSN={0018-8646}, 
month={Jan},}
@INPROCEEDINGS{5476763, 
author={B. Mathieu and P. Paris and G. L. Guelvouit and S. Rouibia}, 
booktitle={2010 Fifth International Conference on Internet and Web Applications and Services}, 
title={A Secure and Legal Network-Aware P2P VoD System}, 
year={2010}, 
volume={}, 
number={}, 
pages={194-199}, 
abstract={File sharing applications using Peer-to-Peer (P2P) networks such as Bittorrent or eDonkey rapidly attracted a lot of people and proved the efficiency and interest of this P2P technology. Distribution of video and of live contents also experienced the P2P mechanisms with success. PPLive, UUSee and others have many of customers, hundreds of channels and thousands of concurrent users. However, major content providers are reluctant to use this technology because no solution to ensure the distribution of only legal contents is provided. In the same way, network operators do not really push towards P2P content distribution because bad organization of the overlay can lead to overload the network and consume a lot of networks resources. In this paper, a secure and legal network-aware P2P video system is introduced, which aims at overcoming those two drawbacks. The design of the system and the evaluation of a prototype showed good results and let us be optimistic about a possible deployment of P2P systems for video delivery, having the support of content providers as well as network operators.}, 
keywords={computer network security;content management;peer-to-peer computing;video on demand;video streaming;P2P VoD system;file sharing;peer-to-peer network;content distribution;P2P video system;video delivery;network operator;Law;Legal factors;Peer to peer computing;Video sharing;Prototypes;Watermarking;Computer architecture;IP networks;Web and internet services;Design optimization;P2P Video Streaming;network-awareness;secure distribution;legal contents;watermarking}, 
doi={10.1109/ICIW.2010.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5381646, 
author={S. Wieczorek and A. Stefanescu and I. Schieferdecker}, 
booktitle={2009 Testing: Academic and Industrial Conference - Practice and Research Techniques}, 
title={Model-Based Integration Testing of Enterprise Services}, 
year={2009}, 
volume={}, 
number={}, 
pages={56-60}, 
abstract={The success of service-oriented architectures (SOA) depends on faultless and seamless service integration. Formal modeling of global communication protocols between services enables a model-based integration testing (MBIT) approach. In this paper we present an MBIT approach based on SAP proprietary choreography models called message choreography models (MCM). We explain how MBIT fits into the SAP testing methodology for SOA and give some insights into the experience we gained from the work.}, 
keywords={enterprise resource planning;program testing;software architecture;model-based integration testing;enterprise services;service-oriented architectures;seamless service integration;formal modeling;global communication protocols;SAP proprietary choreography models;message choreography models;enterprise resource planning;Service oriented architecture;Enterprise resource planning;Automatic testing;Software testing;Protocols;Application software;Global communication;Unified modeling language;Java;Fault detection;Model-based Testing;Integration Testing;Service Oriented Architecture}, 
doi={10.1109/TAICPART.2009.11}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6227107, 
author={N. Devos and C. Ponsard and J. Deprez and R. Bauvin and B. Moriau and G. Anckaerts}, 
booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
title={Efficient reuse of domain-specific test knowledge: An industrial case in the smart card domain}, 
year={2012}, 
volume={}, 
number={}, 
pages={1123-1132}, 
abstract={While testing is heavily used and largely automated in software development projects, the reuse of test practices across similar projects in a given domain is seldom systematized and supported by adequate methods and tools. This paper presents a practical approach that emerged from a concrete industrial case in the smart card domain at STMicroelectronics Belgium in order to better address this kind of challenge. The central concept is a test knowledge repository organized as a collection of specific patterns named QPatterns. A systematic process was followed, first to gather, structure and abstract the test practices, then to produce and validate an initial repository, and finally to make it evolve later on Testers can then rely on this repository to produce high quality test plans identifying all the functional and nonfunctional aspects that have to be addressed, as well as the concrete tests that have to be developed within the context of a new project. A tool support was also developed and integrated in a traceable way into the existing industrial test environment. The approach was validated and is currently under deployment at STMicroelectronics Belgium.}, 
keywords={program testing;project management;smart cards;software management;domain specific test knowledge;industrial case;smart card domain;software development projects;STMicroelectronics Belgium;industrial test environment;Smart cards;Testing;Libraries;Software;Security;Concrete;patterns;test;generation;smartcard}, 
doi={10.1109/ICSE.2012.6227107}, 
ISSN={1558-1225}, 
month={June},}
@INPROCEEDINGS{1321646, 
author={Hau Lam}, 
booktitle={IEEE/CPMT/SEMI 29th International Electronics Manufacturing Technology Symposium (IEEE Cat. No.04CH37585)}, 
title={New design-to-test software strategies accelerate time-to-market}, 
year={2004}, 
volume={}, 
number={}, 
pages={140-143}, 
abstract={Today's growing device complexity and new manufacturing requirements have presented significant challenges for manufacturers looking to speed time-to-market. One such challenge is the need to contain test costs, of which a major component is the time and resources required for test program development. Some test development tools that exist today can translate a device's functional events and scan patterns into test programs for targeted ATE. Identification and specification of critical timing parameters that require conversion into cycle-based ATE formats have become an increasing cost factor, which can also significantly impact test accuracy. Traditionally, timing specifications from microprocessor and IP cores, multiple bus types, and other device components can be established via published timing specifications and by a manageable, iterative process between design and test engineering. Likewise, automatic test pattern generation tools for structural test can address simple timing, and are capable of generating cycle-based timing. Today's complex SoC may consist of over 60 IP cores made more complicated by increased challenges from high-speed serial bus technology and multiple-time domain designs. Further complicating test program development is the need for compatibility with multiple ATE platforms to accommodate global manufacturing strategies. Next generation design-to-test software tools have to address these factors to help reduce the ever growing cost-of-test. Tools must support standard industry test languages such as standard test interface language (STIL), support both functional events and scan patterns, and validate outputs to ensure first-pass success of test programs pre- and post silicon, across multiple ATE platforms.}, 
keywords={electronic design automation;automatic test software;design for testability;automatic test pattern generation;built-in self test;system-on-chip;design-to-test software strategies;time-to-market;device complexity;manufacturing requirements;test program development;test development tools;timing parameters;cost factor;microprocessor;IP cores;timing specifications;iterative process;design engineering;test engineering;automatic test pattern generation tools;structural test;SoC;serial bus technology;multiple-time domain designs;ATE platforms;global manufacturing strategies;cost-of-test;standard test interface language;scan patterns;silicon;Software design;Acceleration;Time to market;Testing;Timing;Manufacturing;Costs;Automatic test pattern generation;Microprocessors;Engineering management}, 
doi={10.1109/IEMT.2004.1321646}, 
ISSN={1089-8190}, 
month={July},}
@INPROCEEDINGS{5457806, 
author={M. Bakera and C. Wagner and T. Margaria and E. Vassev and M. Hinchey and B. Steffen}, 
booktitle={2010 Seventh IEEE International Conference and Workshops on Engineering of Autonomic and Autonomous Systems}, 
title={Extracting Component-Oriented Behaviour for Self-Healing Enabling}, 
year={2010}, 
volume={}, 
number={}, 
pages={152-161}, 
abstract={Rich and multifaceted domain specific specification languages like the Autonomic System Specification Language (ASSL) help to design reliable systems with self-healing capabilities. The GEAR game-based Model Checker has been used successfully to investigate in depth properties of the ESA ExoMars Rover. We show here how to enable GEAR's game-based verification techniques for ASSL via systematic model extraction from a behavioral subset of the language, and illustrate it on a description of the Voyager II space mission. This way, we close the gap between the design-time and the run-time techniques provided in the SHADOWS platform for self-healing of concurrency, performance, and functional issues.}, 
keywords={computer games;formal verification;learning (artificial intelligence);object-oriented programming;software fault tolerance;specification languages;self healing enabling;autonomic system specification language;ASSL;GEAR game based model checker;gamebased verification;SHADOWS platform;component oriented behaviour extraction;Gears;Specification languages;Runtime;Concurrent computing;Software systems;Aerospace industry;Aerospace electronics;Conferences;Design engineering;Software engineering}, 
doi={10.1109/EASe.2010.23}, 
ISSN={2168-1864}, 
month={March},}
@ARTICLE{4152663, 
author={}, 
journal={IEEE Unapproved Std P487/D7 Feb 2007}, 
title={Unapproved IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{6805151, 
author={Y. Jarraya and T. Madi and M. Debbabi}, 
journal={IEEE Communications Surveys Tutorials}, 
title={A Survey and a Layered Taxonomy of Software-Defined Networking}, 
year={2014}, 
volume={16}, 
number={4}, 
pages={1955-1980}, 
abstract={Software-defined networking (SDN) has recently gained unprecedented attention from industry and research communities, and it seems unlikely that this will be attenuated in the near future. The ideas brought by SDN, although often described as a “revolutionary paradigm shift” in networking, are not completely new since they have their foundations in programmable networks and control-data plane separation projects. SDN promises simplified network management by enabling network automation, fostering innovation through programmability, and decreasing CAPEX and OPEX by reducing costs and power consumption. In this paper, we aim at analyzing and categorizing a number of relevant research works toward realizing SDN promises. We first provide an overview on SDN roots and then describe the architecture underlying SDN and its main components. Thereafter, we present existing SDN-related taxonomies and propose a taxonomy that classifies the reviewed research works and brings relevant research directions into focus. We dedicate the second part of this paper to studying and comparing the current SDN-related research initiatives and describe the main issues that may arise due to the adoption of SDN. Furthermore, we review several domains where the use of SDN shows promising results. We also summarize some foreseeable future research challenges.}, 
keywords={power consumption;software radio;telecommunication network management;telecommunication power management;software defined networking;SDN;layered taxonomy;revolutionary paradigm shift;programmable networks;control-data plane separation projects;simplified network management;network automation;fostering innovation;programmability;CAPEX;OPEX;power consumption;Control systems;Taxonomy;Software defined networking;Ports (Computers);Network operating systems;Software-defined networking;OpenFlow;programmable networks;controller;management;virtualization;flow}, 
doi={10.1109/COMST.2014.2320094}, 
ISSN={1553-877X}, 
month={Fourthquarter},}
@INPROCEEDINGS{7066751, 
author={A. Malini and N. Venkatesh and K. Sundarakantham and S. Mercyshalinie}, 
booktitle={International Conference on Computing and Communication Technologies}, 
title={Mobile application testing on smart devices using MTAAS framework in cloud}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Testing of mobile applications which run on smart devices is more complex due to diversity of mobile devices and computational resources. Mobile testing using emulators which doesn't include real network traffic and testing mobile applications in more than one portable device in single system was also not possible in normal testing. In order to overcome the draw backs of normal testing, in this paper we deployed a Mobile Testing as a Service (MTAAS) framework in cloud environment. By using MTAAS framework many mobile applications can be tested in different portable devices and different mobile platforms. Testing of mobile applications using MTAAS provides most realistic results since it includes real network speed. Finally, we conducted an experiment in MTAAS framework and testing results shows that MTAAS can effectively reduce the complexity of mobile testing on different smart devices.}, 
keywords={cloud computing;diversity reception;mobile radio;telecommunication traffic;mobile application testing;smart devices;mobile testing as a service;MTAAS framework;mobile device diversity;computational resources;real network traffic;cloud environment;mobile applications;real network speed;Testing;Mobile communication;Mobile computing;Performance evaluation;Smart phones;Load modeling;Cloud Testing;Mobile Application Testing;Testing as a service}, 
doi={10.1109/ICCCT2.2014.7066751}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7503833, 
author={R. João and R. Marinheiro and P. Assunçõo and L. Cruz}, 
booktitle={2016 IEEE International Conference on Communications Workshops (ICC)}, 
title={A flexible monitor for assessing 3D video QoE in real-time}, 
year={2016}, 
volume={}, 
number={}, 
pages={480-485}, 
abstract={With the evolution of 3D technology, 3D IPTV services may prove to be a common service widely distributed by operators. So it is important that they have the necessary means to easily and inexpensively monitor the Quality of Experience (QoE) of this new service. Deployment of 3D video QoE monitors anywhere in the network will enable operators to adapt their service and network infrastructure in order to guarantee a desired QoE level, e.g., in scenarios where 3D IPTV streaming is offered to users with multi-homed equipment and simultaneous access to the network by means of heterogeneous smartcells in the customer premises.}, 
keywords={data encapsulation;IPTV;quality of experience;video coding;network impairment simulation;ITU-T G.1050 recommendation;network embedded device;encapsulation scheme;coding standard;multiple use-case scenario;network infrastructure;quality of experience flexible monitoring;3D IPTV services;real-time 3D video QoE assessment;Monitoring;Streaming media;Three-dimensional displays;Real-time systems;Mathematical model;Artificial neural networks;Computational modeling;3D video;G.1050;monitor;video-plus-depth;MVC}, 
doi={10.1109/ICCW.2016.7503833}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4670300, 
author={N. Guelfi and B. Ries}, 
booktitle={Testing: Academic Industrial Conference - Practice and Research Techniques (taic part 2008)}, 
title={Selection, Evaluation and Generation of Test Cases in an Industrial Setting: A Process and a Tool}, 
year={2008}, 
volume={}, 
number={}, 
pages={47-51}, 
abstract={The test phase in safety-critical systems industry is a crucial phase of the development process. Some companies of these industries have their own test methods which do not reuse the notions available in the theory of software testing or model driven engineering. This paper reports on an experience in a testing process improvement made inside a safety-critical systems company in order to improve the quality of the test phase improvement. We present the initial situation, the objectives, the proposed process and the tools that are used to support it. In particular, we show that the most efficient improvements were achieved concerning the test process definition and in allowing a tailored and precise delimitation of the systempsilas elements to be tested.}, 
keywords={program testing;test cases;industrial setting;safety-critical systems industry;software testing;model driven engineering;System testing;Software testing;Computer industry;Embedded software;Performance evaluation;Laboratories;Software systems;Model driven engineering;Software performance;Software engineering;test selection;model-driven testing;industrial;tool-support;process}, 
doi={10.1109/TAIC-PART.2008.12}, 
ISSN={}, 
month={Aug},}
@INBOOK{6078760, 
author={Stuart Jacobs}, 
booktitle={Engineering Information Security: The Application of Systems Engineering Concepts to Achieve Information Assurance}, 
title={Index}, 
year={2011}, 
volume={}, 
number={}, 
pages={}, 
abstract={
No abstract.

}, 
keywords={}, 
doi={10.1002/9780470947913.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6078760},}
@INPROCEEDINGS{4382117, 
author={S. Ramanathan and C. Lac}, 
booktitle={2007 IEEE International Symposium on Consumer Electronics}, 
title={Use of fault tree analysis to improve residential gateway testing}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={A residential gateway, heart of the strategy of most Telcos, is a centralized intelligent device between the operator's access network and the home's network. It terminates all external access networks and enables residential services to be delivered to the consumer. Besides a plethora of useful services, the growth in market depends upon the reputation of its resilience (availability, reliability and security). This emphasizes a near zero fault design and efficient testing should be taken care before its launch into the market. This paper deals with the analysis of failures, both from test and field data, aiming to increase the efficiency of laboratory testing. Using fault tree analysis, we study the faults that have passed through the testing phase and created failures in the customer premises. With the help of defined specifications, we have identified the zones in which testing in the laboratory needs to be improved.}, 
keywords={broadband networks;fault trees;reliability;fault tree analysis;residential gateway testing;availability;reliability;security;Fault trees;Testing;Failure analysis;Laboratories;Heart;Intelligent networks;Home automation;Resilience;Availability;Data security}, 
doi={10.1109/ISCE.2007.4382117}, 
ISSN={0747-668X}, 
month={June},}
@INBOOK{5273129, 
author={Steven M. Kaplan}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={C}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={
}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch3}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273129},}
@INPROCEEDINGS{8327233, 
author={S. Chodarev and M. Bacíková}, 
booktitle={2017 IEEE 14th International Scientific Conference on Informatics}, 
title={Development of Oberon-0 using YAJCo}, 
year={2017}, 
volume={}, 
number={}, 
pages={122-127}, 
abstract={YAJCo is a tool for the development of software languages based on an annotated language model. The model is represented by Java classes with annotations defining its mapping to concrete syntax. This approach to language definition enables the abstract syntax to be central point of the development process, instead of concrete syntax. In this paper a case study of Oberon-0 programming language development is presented. The study is based on the LTDA Tool Challenge and showcases details of abstract and concrete syntax definition using YAJCo, as well as implementation of name resolution, type checking, model transformation and code generation.}, 
keywords={Java;program compilers;program verification;programming languages;specification languages;YAJCo;software languages;annotated language model;Java classes;annotations;concrete syntax;language definition;abstract syntax;Oberon-0 programming language development;LTDA Tool Challenge;model transformation;code generation;name resolution;type checking;Syntactics;Tools;Grammar;Java;Generators;Analytical models;Abstract syntax;experience report;language development;Oberon-0;parser generator;YAJCo}, 
doi={10.1109/INFORMATICS.2017.8327233}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{493448, 
author={R. B. Kieburtz and L. McKinney and J. M. Bell and J. Hook and A. Kotov and J. Lewis and D. P. Oliva and T. Sheard and I. Smith and L. Walton}, 
booktitle={Proceedings of IEEE 18th International Conference on Software Engineering}, 
title={A software engineering experiment in software component generation}, 
year={1996}, 
volume={}, 
number={}, 
pages={542-552}, 
abstract={The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.}, 
keywords={automatic programming;software reusability;military computing;human resource management;specification languages;program verification;program interpreters;command and control systems;formal specification;software engineering experiment;software component generation;program generator construction;domain-specific specification languages;reuse technology;reusable Ada program templates;message translation modules;message validation modules;military command control communications and information systems;personnel;independent contractor;message specifications;Air Force systems;productivity;software modules;average performance;confidence levels;Software engineering;Paper technology;Automatic programming;System testing;Specification languages;Military communication;Communication system control;Control systems;Information systems;Personnel}, 
doi={10.1109/ICSE.1996.493448}, 
ISSN={0270-5257}, 
month={March},}
@INPROCEEDINGS{7338996, 
author={M. Borek and K. Stenzel and K. Katkalov and W. Reif}, 
booktitle={2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
title={Abstracting security-critical applications for model checking in a model-driven approach}, 
year={2015}, 
volume={}, 
number={}, 
pages={11-14}, 
abstract={Model checking at the design level makes it possible to find protocol flaws in security-critical applications automatically. But depending on the size of the application and especially on the abstraction of the application model, model checking may need a lot of resources, primarily time. To reduce the complexity, the application models are usually highly abstracted. But in a model-driven approach with automatic generation of runnable applications the application models need to be detailed and are often too complex to check in reasonable time. In this paper we describe an approach to handle this problem by using additional UML models to restrict the protocol runs, the attacker abilities and the numbers of participants. This makes model checking of large applications in our model-driven approach called SecureMDD possible without manual abstraction of the generated specifications. For model checking we use AVANTSSAR and show how the restrictions modeled within UML are translated. We demonstrate our approach with a smart card based electronic ticketing example.}, 
keywords={program diagnostics;program verification;protocols;safety-critical software;Unified Modeling Language;security-critical application abstraction;model-driven approach;design level model checking;application model abstraction;automatic application generation;UML models;SecureMDD;AVANTSSAR;Unified modeling language;Model checking;Security;Protocols;Complexity theory;Manuals;Smart cards;UML;model checking;security-critical systems;model-driven development;transformations;SecureMDD}, 
doi={10.1109/ICSESS.2015.7338996}, 
ISSN={2327-0594}, 
month={Sept},}
@ARTICLE{4040128, 
author={}, 
journal={IEEE Std P1175.2/D11.2}, 
title={Unapproved IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections (Superseded by P1175.2_D12.2)}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{5567088, 
author={Y. Afek and U. Drepper and P. Felber and C. Fetzer and V. Gramoli and M. Hohmuth and E. Riviere and P. Stenstrom and O. Unsal and W. M. Moreira and D. Harmanci and P. Marlier and S. Diestelhorst and M. Pohlack and A. Cristal and I. Hur and A. Dragojevic and R. Guerraoui and M. Kapalka and S. Tomic and G. Korland and N. Shavit and M. Nowack and T. Riegel}, 
journal={IEEE Micro}, 
title={The Velox Transactional Memory Stack}, 
year={2010}, 
volume={30}, 
number={5}, 
pages={76-87}, 
abstract={The adoption of multi- and many-core architectures for mainstream computing undoubtedly brings profound changes in the way software is developed. In particular, the use of fine grained locking as the multi-core programmer's coordination methodology is considered by more and more experts as a dead-end. The transactional memory (TM) programming paradigm is a strong contender to become the approach of choice for replacing locks and implementing atomic operations in concurrent programming. Combining sequences of concurrent operations into atomic transactions allows a great reduction in the complexity of both programming and verification, by making parts of the code appear to execute sequentially without the need to program using fine-grained locking. Transactions remove from the programmer the burden of figuring out the interaction among concurrent operations that happen to conflict when accessing the same locations in memory. The EU-funded FP7 VELOX project designs, implements and evaluates an integrated TM stack, spanning from programming language to the hardware support, and including runtime and libraries, compilers, and application environments. This paper presents an overview of the VELOX TM stack and its associated challenges and contributions.}, 
keywords={concurrency control;multiprocessing systems;parallel architectures;parallel programming;Velox transactional memory stack;multicore architecture;many-core architecture;mainstream computing;fine grained locking;transactional memory programming;concurrent programming;atomic transaction;Velox project design;integrated TM stack;Velox TM stack;parallel programming;Libraries;Runtime;Hardware;Java;Programming;Program processors;concurrent programming;software transactional memory;hardware transactional memory;compilers;language extensions}, 
doi={10.1109/MM.2010.80}, 
ISSN={0272-1732}, 
month={Sept},}
@INPROCEEDINGS{7964337, 
author={J. O. Ringert and B. Rumpe and C. Schulze and A. Wortmann}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET)}, 
title={Teaching agile model-driven engineering for cyber-physical systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={127-136}, 
abstract={Agile development methods, model-driven engineering, and cyber-physical systems are important topics in software engineering education. It is not obvious how to teach their combination while respecting individual challenges posed to students and educators. We have devised a software project class for teaching the agile MDE for CPS. The project class was held in three different semesters. In this paper, we report on the setup of our exploratory study and its goals for teaching. We base our evaluation and insights on interviews and questionnaires. Our results show the feasibility of combination of agile MDE for CPS but also the challenges this combination poses to students and educators.}, 
keywords={computer science education;cyber-physical systems;software engineering;software prototyping;teaching;agile model-driven engineering teaching;cyber-physical systems;agile development method;software engineering education;software project class;agile MDE;CPS;Education;Unified modeling language;Educational robots;Software engineering;Model driven engineering;teaching;model-driven engineering;cyber-physical systems;case study}, 
doi={10.1109/ICSE-SEET.2017.16}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{386480, 
author={S. R. Rao and Bi-Yu Pan and J. R. Armstrong}, 
booktitle={1993 European Conference on Design Automation with the European Event in ASIC Design}, 
title={Hierarchical test generation for VHDL behavioral models}, 
year={1993}, 
volume={}, 
number={}, 
pages={175-182}, 
abstract={In this method, the VHDL model to be tested is represented by its process model graph (PMG). Test sets for individual processes of the model are precomputed and stored in the design library. The Hierarchical Behavioral Test Generator (HBTG) algorithm accepts the PMG and the precomputed tests as inputs, from which it hierarchically constructs a test sequence that tests the functionality of the model. Such an automatic test generation process relieves the modeler of the time-consuming task of developing test-benches. The test sequence generated by HBTG is then used for simulation of the model. Experimental results indicate that the tests generated exercise the model thoroughly.<<ETX>>}, 
keywords={hardware description languages;high level synthesis;design for testability;logic testing;automatic testing;hierarchical test generation;high level testability;VHDL behavioral models;process model graph;algorithm;functionality;automatic test generation process;simulation;Circuit testing;Automatic testing;Circuit faults;Libraries;Computational modeling;Circuit simulation;Computer simulation;Computer industry;Hardware;Design engineering}, 
doi={10.1109/EDAC.1993.386480}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{5541042, 
author={Yan Huan and Miao Changyun and Wu Zhigang}, 
booktitle={2010 International Conference On Computer Design and Applications}, 
title={The design of IP telephony media gateway which is based on soft-switching technology}, 
year={2010}, 
volume={5}, 
number={}, 
pages={V5-379-V5-381}, 
abstract={This paper presents a new type of design method for IP telephony gateway which is base on the soft-switch technology. It designs the hardware circuit with TMS320C5402 as its core and develops a telephone communication protocol which is based on UDP / IP. The Trunk Gateway supports MGCP protocol standards. Besides, it can fulfill lots of functions including audio processing, voice codec, and signaling tone generation and detection.}, 
keywords={access protocols;Internet telephony;IP telephony media gateway;soft-switching technology;hardware circuit;telephone communication protocol;audio processing;voice codec;signaling tone generation;Telephony;Protocols;Design methodology;Hardware;Circuits;Communication standards;Standards development;Codecs;Signal processing;Signal generators;soft-switching;IP phones gateways;Media Gateway Controller (MGC);Media Gateway (MG);signaling;protocol}, 
doi={10.1109/ICCDA.2010.5541042}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{554009, 
author={O. Tanir}, 
booktitle={Wescon/96}, 
title={Specification driven design of complex systems}, 
year={1996}, 
volume={}, 
number={}, 
pages={327-332}, 
abstract={This paper examines the benefits of applying a specification driven approach and presents a framework for environments that can support the related design activities. The paper also outlines interrelated advanced topics such as intermediate architecture languages, model libraries and model verification issues.}, 
keywords={design engineering;CAD;specification driven design;complex system;intermediate architecture language;model library;model verification;Costs;Hardware;Design engineering;Audio systems;Switches;Software performance;Software design;Libraries;Testing}, 
doi={10.1109/WESCON.1996.554009}, 
ISSN={1095-791X}, 
month={Oct},}
@INPROCEEDINGS{4700654, 
author={A. Khoche and P. Burlison and J. Rowe and G. Plowman}, 
booktitle={2008 IEEE International Test Conference}, 
title={A Tutorial on STDF Fail Datalog Standard}, 
year={2008}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Advances in technology are making it imperative to collect detailed structural IC fail data during manufacturing test to improve yield. However, there is currently no standard format for communicating and storing such structural fail data efficiently. This leads to ad-hoc tool-specific solutions, which do not offer interoperability required in a typical multi-tool, multi-vendor customer environment. These ad-hoc solutions result in unnecessary investment in development of point-to-point interface solutions that are ultimately still not integrated with the traditional data collection for a unified yield analysis data format. Expanding an established datalogging standard to accommodate the new requirements solves these issues. Standard Test Data Format (STDF) is the predominant format used today for traditional failure datalogging storage, but in its current form falls far short in handling the new high-volume structural failures for yield learning. A group of more than 20 companies from ATE, EDA, Semiconductor and Yield Management companies has been working on a new enhanced STDF standard that addresses the new requirements. This paper provides the overview of the new enhanced standard.}, 
keywords={data loggers;failure analysis;integrated circuit manufacture;integrated circuit reliability;production engineering computing;system monitoring;STDF fail datalog standard;IC fail data;standard test data format;failure datalogging storage;Tutorial;Electronic design automation and methodology;Production;Manufacturing;Silicon;Integrated circuit testing;Communication standards;Investments;Data analysis;Test pattern generators}, 
doi={10.1109/TEST.2008.4700654}, 
ISSN={1089-3539}, 
month={Oct},}
@INPROCEEDINGS{7352573, 
author={Y. Bandung and L. B. Subekti and Y. S. Gondokaryono}, 
booktitle={2015 International Conference on Electrical Engineering and Informatics (ICEEI)}, 
title={A design of teacher portal for supporting teacher's internet literacy web based solution for teacher learning}, 
year={2015}, 
volume={}, 
number={}, 
pages={618-623}, 
abstract={There are a lot of valuable materials available in the Internet to support learning and self-improvement. But in some developing countries like Indonesia, Internet penetration rate is still low and people still don't comprehend on how to get benefit from it. In this research, a web based solution to support teacher's Internet literacy is designed. This solution is a teacher portal system which provides featured contents and files management system for teacher. Some application modules have been developed to support this system those are web-based portal, file management system, chatting system, and content aggregator module as the main component of the system builder.}, 
keywords={computer aided instruction;computer literacy;file organisation;portals;teacher training;teacher portal design;teacher Internet literacy;learning improvement;self-improvement;developing countries;Indonesia;Internet penetration rate;content management system;Web-based portal;file management system;chatting system;content aggregator module;system builder;Servers;Internet;Portals;Synchronization;Feature extraction;Computer architecture;Education;Internet literacy;teacher portal;files management;information and communication technology}, 
doi={10.1109/ICEEI.2015.7352573}, 
ISSN={2155-6830}, 
month={Aug},}
@INPROCEEDINGS{6693140, 
author={A. Gambi and W. Hummer and S. Dustdar}, 
booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Automated testing of cloud-based elastic systems with AUToCLES}, 
year={2013}, 
volume={}, 
number={}, 
pages={714-717}, 
abstract={Cloud-based elastic computing systems dynamically change their resources allocation to provide consistent quality of service and minimal usage of resources in the face of workload fluctuations. As elastic systems are increasingly adopted to implement business critical functions in a cost-efficient way, their reliability is becoming a key concern for developers. Without proper testing, cloud-based systems might fail to provide the required functionalities with the expected service level and costs. Using system testing techniques, developers can expose problems that escaped the previous quality assurance activities and have a last chance to fix bugs before releasing the system in production. System testing of cloud-based systems accounts for a series of complex and time demanding activities, from the deployment and configuration of the elastic system, to the execution of synthetic clients, and the collection and persistence of execution data. Furthermore, clouds enable parallel executions of the same elastic system that can reduce the overall test execution time. However, manually managing the concurrent testing of multiple system instances might quickly overwhelm developers' capabilities, and automatic support for test generation, system test execution, and management of execution data is needed. In this demo we showcase AUToCLES, our tool for automatic testing of cloud-based elastic systems. Given specifications of the test suite and the system under test, AUToCLES implements testing as a service (TaaS): It automatically instantiates the SUT, configures the testing scaffoldings, and automatically executes test suites. If required, AUToCLES can generate new test inputs. Designers can inspect executions both during and after the tests.}, 
keywords={cloud computing;program testing;automated testing;AUToCLES;cloud-based elastic computing system;resources allocation;quality of service;business critical function;system testing technique;quality assurance;concurrent testing;test generation;system test execution;testing as a service;TaaS;SUT;Elasticity;Monitoring;Standards;System testing;Cloud computing}, 
doi={10.1109/ASE.2013.6693140}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7733581, 
author={R. Awad and G. Heppner and A. Roennau and M. Bordignon}, 
booktitle={2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={ROS engineering workbench based on semantically enriched app models for improved reusability}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this work, the ReApp Engineering Workbench and its underlying semantically enriched app models are presented. The usage of a model, which describes the apps functionality, interfaces and other attributes, allows the utilization of engineering tools for code generation and automated testing. Further, it ensures the compatibility of the generated interfaces, which in turn enhances the reusability of the developed apps in larger applications.}, 
keywords={program compilers;software reusability;ROS engineering workbench;app models;reusability;ReApp engineering workbench;apps functionality;engineering tools;code generation;automated testing;Biological system modeling;Unified modeling language;Hardware;Model driven engineering;Robot kinematics}, 
doi={10.1109/ETFA.2016.7733581}, 
ISSN={}, 
month={Sept},}
@ARTICLE{6179576, 
author={Y. A. Katsigiannis and P. S. Georgilakis and E. S. Karapidakis}, 
journal={IEEE Transactions on Sustainable Energy}, 
title={Hybrid Simulated Annealing–Tabu Search Method for Optimal Sizing of Autonomous Power Systems With Renewables}, 
year={2012}, 
volume={3}, 
number={3}, 
pages={330-338}, 
abstract={Small autonomous power systems (SAPS) that include renewable energy sources are a promising option for isolated power generation at remote locations. The optimal sizing problem of SAPS is a challenging combinatorial optimization problem, and its solution may prove a very time-consuming process. This paper initially investigates the performance of two popular metaheuristic methods, namely, simulated annealing (SA) and tabu search (TS), for the solution of SAPS optimal sizing problem. Moreover, this paper proposes a hybrid SA-TS method that combines the advantages of each one of the above-mentioned metaheuristic methods. The proposed method has been successfully applied to design an SAPS in Chania region, Greece. In the study, the objective function is the minimization of SAPS cost of energy (€/kWh), and the design variables are: 1) wind turbines size, 2) photovoltaics size, 3) diesel generator size, 4) biodiesel generator size, 5) fuel cells size, 6) batteries size, 7) converter size, and 8) dispatch strategy. The performance of the proposed hybrid optimization methodology is studied for a large number of alternative scenarios via sensitivity analysis, and the conclusion is that the proposed hybrid SA-TS improves the obtained solutions, in terms of quality and convergence, compared to the solutions provided by individual SA or individual TS methods.}, 
keywords={combinatorial mathematics;power generation dispatch;renewable energy sources;search problems;simulated annealing;wind turbines;hybrid simulated annealing-Tabu search method;small autonomous power systems;renewable energy sources;isolated power generation;remote locations;combinatorial optimization problem;time-consuming process;SA;SAPS optimal sizing problem;hybrid SA-TS method;above-mentioned metaheuristic methods;Chania region;Greece;wind turbines;photovoltaics size;diesel generator size;biodiesel generator size;fuel cell size;battery size;converter size;dispatch strategy;hybrid optimization methodology;Generators;Batteries;Fuels;Power systems;Simulated annealing;Renewable energy resources;Hybrid power systems;optimal sizing;optimization methods;power generation dispatch;renewable energy sources;simulated annealing (SA);small autonomous power systems (SAPS);solar energy;tabu search (TS);wind energy}, 
doi={10.1109/TSTE.2012.2184840}, 
ISSN={1949-3029}, 
month={July},}
@INPROCEEDINGS{1552877, 
author={Y. Fan and E. A. Kendall}, 
booktitle={IEEE International Conference on e-Business Engineering (ICEBE'05)}, 
title={A hybrid dialogue strategy for speech-enabled mobile commerce}, 
year={2005}, 
volume={}, 
number={}, 
pages={110-117}, 
abstract={Designing a dialogue strategy for speech-enabled mobile commerce is a significant challenge due to the context. This paper introduces a hybrid dialogue strategy to overcome the inflexibility of application-directed interactions while avoiding the significant recognition difficulty of a full mixed-initiative style. The system uses N-gram grammars to govern the recognition at the request segment of a dialogue, and employs an application-directed strategy at the clarification discourse segment. The paper also details generating a corpus for the N-gram grammar through a case-based reasoning approach, and constructing application-directed dialogues with decision trees. Our preliminary testing indicates the strategy is a feasible and effective solution for voice-enabling mobile commerce applications}, 
keywords={case-based reasoning;decision trees;electronic commerce;interactive systems;mobile computing;speech-based user interfaces;hybrid dialogue strategy;speech-enabled mobile commerce;application-directed interaction;N-gram grammar;case-based reasoning approach;decision trees;voice-enabling mobile commerce;Business;Natural languages;Computer networks;Speech;Cities and towns;Mobile computing;Decision trees;Testing;Standards development;Control systems}, 
doi={10.1109/ICEBE.2005.6}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4101803, 
author={S. J. Ring}, 
journal={IEEE Transactions on Aerospace and Electronic Systems}, 
title={A Distributed Intelligence Automatic Test System for PATRIOT}, 
year={1977}, 
volume={AES-13}, 
number={3}, 
pages={264-272}, 
abstract={An automatic test system supporting high volume production testing of diverse state-of-the-art electronic assemblies is described. The test complex consists of a centralized computer system communicating to a network of satellite stations, each structured as "Intelligent Test Centers" dedicated to a particular family of assemblies (e.g., analog, digital, microwave). Allocation of resources and tasks have been distributed for optimum efficiency of production testing. This paper describes the organization and characteristics of the test system. Test center operation is explained with emphasis given to unique man-machine interactive features designed for on-line generation, examination, and maintenance of Unit-Under-Test (UUT) programs. Details are presented of the test language, RATEL, used for UUT programming. Other aspects that are discussed include test data and UUT program characteristics.}, 
keywords={Intelligent systems;Automatic testing;System testing;Electronic equipment testing;Assembly systems;Production systems;Analog computers;Computer networks;Artificial satellites;Intelligent networks}, 
doi={10.1109/TAES.1977.308394}, 
ISSN={0018-9251}, 
month={May},}
@INPROCEEDINGS{6389024, 
author={P. Gananchchelvi and Jiao Yu and M. S. Pukish}, 
booktitle={IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society}, 
title={Current trends in in-vehicle electrical engineering applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={6268-6273}, 
abstract={Novel electrical engineering applications play a major role in in-vehicle technology. Today's automobile industry is transforming from mechanically driven functions to electronic and software driven functions. We can see the impact of electrical engineering technology in many parts of vehicle and in all of its operations such as control, power conversion, navigation, communication, entertainment, safety and security. In addition, today's automobile industry is showing a renewed interest in electric transportation. This paper presents a review on the recent developments in the in-vehicle electrical engineering applications classified into two sections, the embedded systems and the power and energy system, both of which can strongly influence the automobile industry in the future.}, 
keywords={automobile industry;electric vehicles;electrical engineering;embedded systems;transportation;in-vehicle electrical engineering applications;electrical engineering applications;in-vehicle technology;automobile industry;mechanically driven functions;software driven functions;electrical engineering technology;power conversion;navigation;communication;entertainment;safety;security;electric transportation;in-vehicle electrical engineering;embedded systems;energy system;power system;Field programmable gate arrays;Supercapacitors;Lead;Batteries;Digital signal processing;Frequency conversion;Frequency estimation;in-vehicle electronics;embedded systems;ECU;FPGA;EV;ultracapacitors}, 
doi={10.1109/IECON.2012.6389024}, 
ISSN={1553-572X}, 
month={Oct},}
@ARTICLE{50774, 
author={W. K. Ehrlich and S. K. Lee and R. H. Molisani}, 
journal={IEEE Software}, 
title={Applying reliability measurement: a case study}, 
year={1990}, 
volume={7}, 
number={2}, 
pages={56-64}, 
abstract={The problem of knowing when to stop testing software is considered, focusing on the strategy of stopping when a reliability level or rate of failure occurrence acceptable to the customer is reached. The system's reliability is monitored throughout the system test, and the system is released to the field only when the measured reliability is at or above this objective. This approach was applied to test-failure data collected on Remote Measurement System-Digital 1, a large telecommunications testing system that had already gone through system test and been released to the field. The RMS-D1 failure data, which consisted of command-response errors versus commands executed, had been routinely collected by the system-test organization during testing. The testing phase analyzed, the load test, was an operational-profile-driven test in which a controlled load was imposed on the system reflective of the system's busy-hour usage pattern. It was found to be feasible to apply the reliability-measurement approach in real time, to systems actually undergoing system test, given a controlled load-test environment.<<ETX>>}, 
keywords={software reliability;software reliability measurement;rate of failure occurrence;Remote Measurement System-Digital 1;telecommunications testing system;controlled load-test environment;System testing;Control systems;Software testing;Reliability;Condition monitoring;Remote monitoring;Pattern analysis;System buses;Real time systems}, 
doi={10.1109/52.50774}, 
ISSN={0740-7459}, 
month={March},}
@INPROCEEDINGS{5431981, 
author={H. Esquivel and A. Akella and T. Mori}, 
booktitle={2010 Second International Conference on COMmunication Systems and NETworks (COMSNETS 2010)}, 
title={On the effectiveness of IP reputation for spam filtering}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Modern SMTP servers apply a variety of mechanisms to stem the volume of spam delivered to users. These techniques can be broadly classified into two categories: pre-acceptance approaches, which apply prior to a message being accepted (e.g. IP reputation), and post-acceptance techniques which apply after a message has been accepted (e.g. content based signatures). We argue that the effectiveness of these measures varies based on the SMTP sender type. This paper focuses on the most light-weight pre-acceptance filtering mechanism-IP reputation. We first classify SMTP senders into three main categories: legitimate servers, end-hosts, and spam gangs, and empirically study the limits of effectiveness regarding IP reputation filtering for each category. Next, we develop new techniques that build custom IP reputation lists, which significantly improve the performance of existing IP reputation lists. In compiling these lists, we leverage a somewhat surprising fact that both legitimate domains and spam domains often use the DNS Sender Policy Framework (SPF) in an attempt to pass simple authentication checks. That is, good/bad IP addresses can be systematically compiled by collecting good/bad domains and looking up their SPF resource records. We also evaluate the effectiveness of these lists over time. Finally, we aim to understand the characteristics of the three categories of email senders in depth. Overall, we find that it is possible to construct IP reputation lists that can identify roughly 90% of all spam and legitimate mail, but some of the lists, i.e. the lists for spam gangs, must be updated on a constant basis to maintain this high level of accuracy.}, 
keywords={e-mail filters;information filtering;Internet;IP networks;unsolicited e-mail;IP reputation;spam filtering;legitimate servers;end-hosts;spam gangs;SMTP servers;DNS sender policy framework;email senders;spam mail;legitimate mail;postacceptance techniques;lightweight preacceptance filtering mechanism;Filtering;Testing;Optical filters;Authentication;Protocols;Optical character recognition software;Unsolicited electronic mail;Laboratories;Postal services;Optical recording}, 
doi={10.1109/COMSNETS.2010.5431981}, 
ISSN={2155-2487}, 
month={Jan},}
@INBOOK{7406343, 
author={Stuart Jacobs}, 
booktitle={Engineering Information Security: The Application of Systems Engineering Concepts to Achieve Information Assurance}, 
title={Index}, 
year={2016}, 
volume={}, 
number={}, 
pages={}, 
abstract={

No abstract.

}, 
keywords={}, 
doi={10.1002/9781119104728.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7406343},}
@INPROCEEDINGS{8449448, 
author={C. Kröher and S. El-Sharkawy and K. Schmid}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={KernelHaven – An Experimentation Workbench for Analyzing Software Product Lines}, 
year={2018}, 
volume={}, 
number={}, 
pages={73-76}, 
abstract={Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem. KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.}, 
keywords={Data mining;Pipelines;Data models;Feature extraction;Tools;Analytical models;Software engineering;Software product line analysis;variability extraction;static analysis;empirical software engineering}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@INPROCEEDINGS{8369576, 
author={G. Hains and A. Jakobsson and Y. Khmelevsky}, 
booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
title={Towards formal methods and software engineering for deep learning: Security, safety and productivity for dl systems development}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Deep Learning (DL) techniques are now widespread and being integrated into many important systems. Their classification and recognition abilities ensure their relevance for multiple application domains far beyond pure signal processing. As a machine-learning technique that relies on training instead of explicit algorithm programming they offer a high degree of productivity. But recent research has shown that they can be vulnerable to attacks and the verification of their correctness is only just emerging as a scientific and engineering possibility. Moreover DL tools are not integrated into classical software engineering so software tools to specify, modify and verify them would make them even more mainstream as software-hardware systems. This paper surveys recent work and proposes research directions and methodologies for this purpose.}, 
keywords={formal verification;learning (artificial intelligence);pattern classification;security of data;software engineering;software tools;formal methods;recognition abilities;multiple application;machine-learning technique;classical software engineering;software tools;software-hardware systems;systems development;DL techniques;classification abilities;signal processing;Deep Learning;Artificial neural networks;Safety;Testing;Machine learning;Tools;Security;Training;deep-learning systems;neural networks;vulnerability of deep learning;security;verification;software engineering}, 
doi={10.1109/SYSCON.2018.8369576}, 
ISSN={2472-9647}, 
month={April},}
@INPROCEEDINGS{8088307, 
author={S. Gröning and C. Rosas and C. Wietfeld}, 
booktitle={2017 IEEE International Systems Engineering Symposium (ISSE)}, 
title={Validating electric vehicle to grid communication systems based on model checking assisted test case generation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In last decades software development processes changed in order to address increasing complexity within decreasing implementation time. Hence, new practices like Kanban, Extreme Programming or Agile Software Development emerged. Model-based development is one potential option, which is more and more used to cope these new demands. However, adapting testing processes to the needs is still an open topic. This paper describes how model checking assisted test case generation can be used to integrate testing in new software development processes, focusing on a protocol implementation for electric vehicle charging communication as a case study. Therefore, it describes certain extensions made in the COMmunication Protocol vaLidation Toolchain COMFLgTg in order to enable test case generation in TTCN-3 core language using counterexamples of SPIN model checker.}, 
keywords={electric vehicle charging;electric vehicles;formal specification;power engineering computing;power grids;program testing;protocols;software prototyping;implementation time;Agile Software Development;SPIN model checker;grid communication systems;software development processes;testing processes;model checking assisted test case generation;Kanban;extreme programming;model-based development;electric vehicle vehicle charging communication;COMmunication Protocol vaLidation Toolchain;COMFLgTg;TTCN-3 core language;Unified modeling language;Protocols;Machine-to-machine communications;Model checking;Adaptation models;Grammar}, 
doi={10.1109/SysEng.2017.8088307}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4267608, 
author={P. G. Bassett}, 
journal={IEEE Software}, 
title={The Case for Frame-Based Software Engineering}, 
year={2007}, 
volume={24}, 
number={4}, 
pages={90-99}, 
abstract={Frame technology adapts generic components into custom information structures. Its facility for maximizing reuse and minimizing redundancy has demonstrated dramatic improvements across software's life cycle.}, 
keywords={object-oriented programming;software reusability;software engineering;frame technology;generic components;custom information structures;reuse maximation;redundancy minimization;software life cycle;Software engineering;Programming profession;Software maintenance;Redundancy;Software systems;Software quality;Computer industry;Humans;Statistics;Organizing;reuse models;automatic programming;evolutionary programming;software engineering process}, 
doi={10.1109/MS.2007.119}, 
ISSN={0740-7459}, 
month={July},}
@ARTICLE{6781158, 
author={P. A. Battaglia and C. C. Byers and L. A. Guth and A. Holliday and C. Spinelli and J. J. Tong}, 
journal={Bell Labs Technical Journal}, 
title={Modular platform vision and strategy}, 
year={2004}, 
volume={9}, 
number={1}, 
pages={121-142}, 
abstract={A platform is a set of hardware, software, and process building blocks that can form the basis of many different products. The goals of platforms are to reuse and/or share assets wherever possible, to save on development costs, to spread fixed development and production costs across the largest volumes, and to offer highly integrated solutions, all while maintaining critical differentiation of products. Using platforms correctly can produce substantial life-cycle cost benefits and can lead to enhanced supplier management and customer satisfaction. This paper will begin by describing the general concept of platforms. It will then consider their economic benefits to development teams, suppliers, and customers. Next, it will discuss the role of industry standards in forming the basis of a platform offer. It will consider the changing architectures of telecommunications networks, along with the contribution of platforms to these changes. Finally, the paper will outline an example set of platform building blocks and their requirements, along with some case studies of how to combine these building blocks into products.}, 
keywords={}, 
doi={10.1002/bltj.20009}, 
ISSN={1538-7305}, 
month={},}
@INPROCEEDINGS{7781923, 
author={E. Klein and A. Gschwend and A. C. Neuroni}, 
booktitle={2016 Conference for E-Democracy and Open Government (CeDEM)}, 
title={Towards a Linked Data Publishing Methodology}, 
year={2016}, 
volume={}, 
number={}, 
pages={188-196}, 
abstract={Linked open government data (LOGD) can be a catalyst in the development of value-added services and products. The vision of many Linked Open Data (LOD) projects is to make publishing and reuse of linked data as easy as possible for the end user thanks to a thriving marketplace with data publishers, developers, and consumers along the value chain. In the large scale LOD project "Fusepool P3", tourism-related applications and software components were developed that support data owners and open data enthusiasts in transforming legacy data to linked data. Based on experiences from this project, we present reflections and discuss pitfalls in drawing a linked data publishing methodology. An integrated view on all phases of the publishing process has not been described so far, for the technical phases linked data life-cycles have been identified only. The methodology developed enables stakeholders to transfer the lessons learned to other use cases and application contexts. This allows for better estimation of efforts and skills for future LOD projects.}, 
keywords={government data processing;Linked Data;publishing;Linked Data publishing methodology;linked open government data;LOGD;value-added service development;product development;Linked Data reusability;Fusepool P3 large scale LOD project;tourism-related applications;software components;legacy data;Linked Data life-cycles;Publishing;Stakeholders;Context;Government;Data models;Software;Portals;linked open data;data publishing;linked data life-cycle;publishing methodology;linked data platform}, 
doi={10.1109/CeDEM.2016.12}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{114002, 
author={L. Bonet and J. Ganger and J. Girardeau and C. Greaves and M. Pendleton and D. Yatim}, 
booktitle={Proceedings. International Test Conference 1990}, 
title={Test features of the MC145472 ISDN U-transceivers}, 
year={1990}, 
volume={}, 
number={}, 
pages={68-79}, 
abstract={The design of a single-chip implementation of a 2B1Q ISDN (integrated services digital network) U transceiver that meets the ANSI T1.601 standards has been completed. The MC145472 was designed with testability in mind and to be consistent with Motorola's design-for-manufacturability goals. The authors describe in detail the design-for-testability techniques specifically intended for the IC manufacturer production test and other ad hoc test/diagnostic structures for the customer to use in evaluating system performance. A global test strategy for testing the ISDN U transceiver is presented. The test features have been used extensively not only for testing the device in the production environment but also for conducting evaluations and design verification experiments during the chip debugging phase. The test features described are well integrated with the architecture of the chip, thus minimizing incremental cost.<<ETX>>}, 
keywords={automatic testing;electronic equipment testing;ISDN;production testing;transceivers;Motorola;MC145472 ISDN U-transceivers;2B1Q ISDN;integrated services digital network;design-for-manufacturability;design-for-testability;IC manufacturer production test;design verification;chip debugging;cost;ISDN;Transceivers;Integrated circuit testing;System testing;ANSI standards;Manufacturing;Production systems;System performance;Debugging;Costs}, 
doi={10.1109/TEST.1990.114002}, 
ISSN={}, 
month={Sept},}
@ARTICLE{5389336, 
author={B. Wile and M. P. Mullen and C. Hanson and D. G. Bair and K. M. Lasko and P. J. Duffy and E. J. Kaminski and T. E. Gilbert and S. M. Licker and R. G. Sheldon and W. D. Wollyung and W. J. Lewis and R. J. Adkins}, 
journal={IBM Journal of Research and Development}, 
title={Functional verification of the CMOS S/390 Parallel Enterprise Server G4 system}, 
year={1997}, 
volume={41}, 
number={4.5}, 
pages={549-566}, 
abstract={Verification of the S/390® Parallel Enterprise Server G4 processor and level 2 cache (L2) chips was performed using a different approach than previously. This paper describes the methods employed by our functional verification team to demonstrate that its logical system complied with the S/390 architecture while staying within the changing cost structure and time-to-market constraints. Verification proceeded at four basic levels defined by the breadth of logic being tested. The lowest level, designer macro verification, contained a single designer's hardware description language (in VHDL). Unit-level verification consisted of a logical portion of function that generally contained four or five designers' logic. The third level of verification was the chip level, in which the processor or L2 chips were individually tested. Finally, system-level verification was performed on symmetric multiprocessor (SMP) configurations that included bus-switching network (BSN) chips and I/O connection chips, designated as memory bus adaptors (MBAs), along with multiple copies of the processor and L2 chips.}, 
keywords={}, 
doi={10.1147/rd.414.0549}, 
ISSN={0018-8646}, 
month={July},}
@INPROCEEDINGS{8502653, 
author={A. S. Nezhad and J. J. Lukkien and R. H. Mak}, 
booktitle={2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Behavior-driven Development for Real-time Embedded Systems}, 
year={2018}, 
volume={1}, 
number={}, 
pages={59-66}, 
abstract={Embedded systems are a class of computer systems that are typically characterized by a tight interaction with the physical environment. Various methodologies have been adopted for the development of such systems, ranging from traditional waterfall to modern agile techniques. One of the agile techniques that has recently attracted increasing attention is Behavior-Driven Development (BDD). BDD promotes the engagement of all stakeholders in every development iteration to minimize the misunderstanding between technical and non-technical stakeholders and, consequently, to speed up the development process and lower the costs. In this paper, we investigate the application of BDD to the development of embedded systems, especially focusing on the testing of timing requirements for real-time embedded software. In particular, we extend BDD with time-related concepts and propose an approach to generate test code for the verification of timing behavior of real-time embedded systems. Our approach offers more automation for the development of test code compared to existing BDD tools, thus minimizing the risk of timing faults and reducing development costs and time-to-market.}, 
keywords={Logic gates;Testing;Timing;Embedded systems;Real-time systems;Stakeholders}, 
doi={10.1109/ETFA.2018.8502653}, 
ISSN={1946-0759}, 
month={Sept},}
@INPROCEEDINGS{7588753, 
author={L. Chen and P. James and D. Kirkwood and H. N. Nguyen and G. L. Nicholson and M. Roggenbach}, 
booktitle={2016 IEEE International Conference on Intelligent Rail Transportation (ICIRT)}, 
title={Towards integrated simulation and formal verification of rail yard designs - an experience report based on the UK East Coast Main Line}, 
year={2016}, 
volume={}, 
number={}, 
pages={347-355}, 
abstract={The development of railway systems is often supported by a range of tools, each addressing individual, but overlapping concerns such as, e.g., performance or safety analysis. However, it is a challenge for users to organise work-flows; results are often in different, non-aligning data formats; furthermore, tools work on different levels of abstraction from macro to microscopic. Thus, tool integration would be beneficial, and also allow for more playful, experimental prototyping and design. This paper reports on lessons learned from the integration of BRaVE - the Birmingham Railway Virtual Environment - and OnTrack from Swansea University. BRaVE is an easy-to-use railway simulation software for development, modelling and flow analysis. OnTrack allows for the automatic verification of scheme plans against a number of safety properties via different formal methods. We present an approach that bridges the gap that occurs from varying details in data sources through automated transformations. This integration provides a first step towards a seamless environment for prototyping, concept development, and safety analysis under ”one roof”. We demonstrate the usefulness of our approach by giving integrated simulation and verification results for the UK East Coast Main Line. This work is part of the wider RSSB's Future Traffic Regulation Optimisation research programme.}, 
keywords={formal verification;railway engineering;software engineering;rail yard designs;formal verification;UK East Coast Main Line;railway system development;nonaligning data formats;tool integration;BRaVE;Birmingham Railway Virtual Environment;Swansea University;railway simulation software;safety properties;RSSB future traffic regulation optimisation research programme;Rail transportation;Solid modeling;Analytical models;Data models;Computational modeling;Vehicles}, 
doi={10.1109/ICIRT.2016.7588753}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{4031222, 
author={J. P. Almeida and P. V. Eck and M. Iacob}, 
booktitle={2006 10th IEEE International Enterprise Distributed Object Computing Conference (EDOC'06)}, 
title={Requirements Traceability and Transformation Conformance in Model-Driven Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={355-366}, 
abstract={The variety of design artefacts (models) produced in a model-driven design process results in an intricate relationship between requirements and the various models. This paper proposes a methodological framework that simplifies management of this relationship. This framework is a basis for tracing requirements, assessing the quality of model transformation specifications, metamodels, models and realizations. We propose a notion of conformance between application models which reduces the effort needed for assessment activities. We discuss how this notion of conformance can be integrated with model transformations}, 
keywords={formal specification;systems analysis;requirements traceability;model-driven design;transformation conformance;model-driven development;model transformation specification;Process design;Application software;Testing;Design engineering;Telematics;Information technology;Buildings;Distributed computing;requirements traceability;assessment;con-formance;model transformation;model-driven design}, 
doi={10.1109/EDOC.2006.45}, 
ISSN={1541-7719}, 
month={Oct},}
@ARTICLE{7992926, 
author={B. Garcia and F. Gortazar and L. Lopez-Fernandez and M. Gallego and M. Paris}, 
journal={IEEE Communications Standards Magazine}, 
title={WebRTC Testing: Challenges and Practical Solutions}, 
year={2017}, 
volume={1}, 
number={2}, 
pages={36-42}, 
abstract={WebRTC comprises a set of novel technologies and standards that provide Real-Time Communication on Web browsers. WebRTC makes simple the embedding of voice and video communications in all types of applications. However, releasing those applications to production is still very challenging due to the complexity of their testing. Validating a WebRTC service requires assessing many functional (e.g. signaling logic, media connectivity, etc.) and non-functional (e.g. quality of experience, interoperability, scalability, etc.) properties on large, complex, distributed and heterogeneous systems that spawn across client devices, networks and cloud infrastructures. In this article, we present a novel methodology and an associated tool for doing it at scale and in an automated way. Our strategy is based on a blackbox end-to-end approach through which we use an automated containerized cloud environment for instrumenting Web browser clients, which benchmark the SUT (system under test), and fake clients, that load it. Through these benchmarks, we obtain, in a reliable and statistically significant way, both network-dependent QoS (Quality of Service) metrics and media-dependent QoE (Quality of Experience) indicators. These are fed, at a second stage, to a number of testing assertions that validate the appropriateness of the functional and non-functional properties of the SUT under controlled and configurable load and fail conditions. To finish, we illustrate our experiences using such tool and methodology in the context of the Kurento open source software project and conclude that they are suitable for validating large and complex WebRTC systems at scale.}, 
keywords={cloud computing;online front-ends;WebRTC testing;real-time communication;voice communications;video communications;WebRTC service;distributed systems;heterogeneous systems;client devices;cloud infrastructures;automated containerized cloud environment;Web browser clients;SUT;system under test;fake clients;network-dependent QoS metrics;Quality of Service metrics;media-dependent QoE indicators;Quality of Experience indicators;Kurento open source software project;complex WebRTC systems;WebRTC;Browsers;Telecommunication traffic;Media;Real-time systems;Quality of service;Internet}, 
doi={10.1109/MCOMSTD.2017.1700005}, 
ISSN={2471-2825}, 
month={},}
@INPROCEEDINGS{8109427, 
author={L. Nobach and J. Blendin and H. Kolbe and G. Schyguda and D. Hausheer}, 
booktitle={2017 IEEE 42nd Conference on Local Computer Networks (LCN)}, 
title={Bare-Metal Switches and Their Customization and Usability in a Carrier-Grade Environment}, 
year={2017}, 
volume={}, 
number={}, 
pages={649-657}, 
abstract={The current ecosystem of network elements, such as switches and appliances, is largely dominated by devices supplied and sold with a bundled operating system, and software dedicated to manage the device's forwarding hardware, however, these platforms are not open-source and cannot be arbitrarily customized, and there is no cost transparency or flexibility in choosing software different to the bundled components.,,,,In this paper, we explore the capabilities of bare-metal switches, which are equipped with commodity switching hardware components, but shipped without an operating system. We evaluate the feasibility of these commonly lower-cost devices to meet the requirements of a customized, carrier-grade network function. Therefore, we have implemented a prototype on generic hardware, re-using as much open-source software as possible. Our Broadband Remote Access Server (BRAS) prototype can lower the cost compared to proprietary network appliances, and, known to have a hardware backplane capacity of 720 Gbps, the merchant-silicon / ASIC approach can highly outperform the state of the art of current x86-based virtualized network functions, while implementing the most important BRAS features.}, 
keywords={application specific integrated circuits;broadband networks;computer centres;computer network management;Internet;network servers;operating systems (computers);public domain software;virtualisation;lower-cost devices;customized carrier-grade network function;generic hardware;open-source software;proprietary network appliances;hardware backplane capacity;virtualized network functions;bare-metal switches;customization;usability;carrier-grade environment;current ecosystem;network elements;bundled operating system;cost transparency;bundled components;commodity switching hardware components;broadband remote access server prototype;Hardware;Software;Servers;Linux;Ports (Computers);Switches;Bare-Metal Switching;Dataplanes;Network Functions;Middleboxes;Sofware-Defined Networking;Cost-Efficiency}, 
doi={10.1109/LCN.2017.104}, 
ISSN={0742-1303}, 
month={Oct},}
@INPROCEEDINGS{6030046, 
author={S. Hutchesson and J. McDermid}, 
booktitle={2011 15th International Software Product Line Conference}, 
title={Towards Cost-Effective High-Assurance Software Product Lines: The Need for Property-Preserving Transformations}, 
year={2011}, 
volume={}, 
number={}, 
pages={55-64}, 
abstract={Generative programming and model transformation techniques are becoming widely used for the development of software components for product lines. The ability to develop components with identified common and variable parts, and rapidly instantiate product-specific versions is key to many software product line approaches. However if this approach is to be truly cost effective for high assurance applications, the instantiation process must be property-preserving, any verification evidence acquired on the product-line component must be demonstrably applicable to the instantiated component. In this paper we outline an approach that uses static analysis techniques and the SPARK language that can potentially demonstrate the correctness of model transformations.}, 
keywords={software cost estimation;software reliability;cost effective high assurance software product line component;property preserving transformation;generative programming;model transformation technique;software component;product-specific version;high assurance application;instantiation process;verification evidence;static analysis technique;SPARK language;Software;Unified modeling language;Sparks;Programming;Ignition;Contracts;UML;SPARK;M2M;Safety Critical;High Integrity;Software Product Lines;Verification;Static Analysis;DO-178B/ED-12B}, 
doi={10.1109/SPLC.2011.32}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8250775, 
author={J. Johnson and A. E. Jai}, 
booktitle={2017 International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
title={Netact based test-automation framework development for IMS CMREPO}, 
year={2017}, 
volume={}, 
number={}, 
pages={518-522}, 
abstract={IP Multimedia Subsystem (IMS) is an architectural framework to provide VoLte and other multimedia services. It is based on ETSI standards like SIP for interfaces between architectural elements.[9] IMS(IP Multimedia subsysytem) was originally designed by the wireless standards body 3rd Generation Partnership Project (3GPP). IMS(IP Multimedia subsysytem) is the key element in the 3G architecture that makes it possible to provide cellular access to all the services that the Internet provides. It is considered as a bridge between[9] cellular network and internet. The introduction of Configuration Management (CM) Repository Server (CMRepo Server) is an important prerequisite for the mass roll out of network elements (NEs), such as, Call Session Control Function (CSCF) and Home Subscriber Server (HSS). It is also required to pre-administer important parameters of the NEs that are required for IMS functionality. Thus, CMRepo Server provides the central CM system to manage multiple NEs. NetAct functions as the central CM system for managing online changeable parameters (class D and class E parameters). Management of other parameters (class A, B, and C parameters) is done through the customization procedure, which is time consuming as well as complex. Management of these parameters in simplified with the introduction of CMRepo Server. NetAct is an OSS Platform. This environment giving access to statistic, performance monitoring, configuration management, user management, fault management and all OSS aspect for the overall subsystem. All tools which is available in NetAct mostly developed by Java platform. Using the Robot framework tool automating NetAct test cases in IMS(IP Multimedia subsysytem).}, 
keywords={3G mobile communication;cellular radio;computer network management;fault diagnosis;Internet;IP networks;Java;multimedia communication;network servers;telecommunication standards;netact based test-automation framework development;pre-administer important parameters;IP multimedia subsystem;configuration management repository server;ETSI standards;SIP;wireless standards;3rd Generation Partnership Project;3GPP;Internet;cellular access network;network element;NE;call session control function;CSCF;home subscriber server;HSS;OSS platform;performance monitoring;user management;fault management;Java platform;Robot framework tool;IMS CMRepo server;Robots;Servers;Testing;Protocols;Multimedia communication;Logic gates;IMS;Robot framework;Netact;VOLTE;CMRepo}, 
doi={10.1109/ICCONS.2017.8250775}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1390775, 
author={B. Hicks}, 
booktitle={The 23rd Digital Avionics Systems Conference (IEEE Cat. No.04CH37576)}, 
title={Transforming avionics architectures to support network centric warfare}, 
year={2004}, 
volume={2}, 
number={}, 
pages={8.E.3-81}, 
abstract={Network centric warfare was applied to different layers in the military force structure to enable commanders and direct combatants to monopolize information to increase lethality and survivability. The flow of information, the amount, type, and other attributes to be discussed, heavily impact the aviation sector of military operations and acquisition. This work concentrates on the impact of NCW on avionics architectures and provides insight to the changes required of aircraft systems to fully utilize the NCW tenets. The NCW concepts are described along with the properties of information necessary for network centric operations.}, 
keywords={military avionics;military aircraft;electronic warfare;military communication;avionics architectures;network centric warfare;military force structure;aviation sector;military operations;military acquisition;aircraft systems;military communication;Aerospace electronics;Information systems;Military aircraft;Privacy;Information security;Logistics;Business;Information resources;Radar;Electrooptic devices}, 
doi={10.1109/DASC.2004.1390775}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6759193, 
author={T. Clark and V. Kulkarni and B. Barn and R. France and U. Frank and D. Turk}, 
booktitle={2014 47th Hawaii International Conference on System Sciences}, 
title={Towards the Model Driven Organization}, 
year={2014}, 
volume={}, 
number={}, 
pages={4817-4826}, 
abstract={Modern organizations are faced with the need to rapidly respond to frequent changes arising from external business pressures. The effect of such continuous evolution eventually leads to organizational misalignment, that is, situations in which sub-optimal configurations of underlying systems significantly reduce an organization's ability to meet its strategic goals. Ensuring alignment of an organization's systems and its goals has been a concern of researchers and practitioners in the enterprise architecture (EA) domain. Unfortunately, current approaches do not adequately address alignment problems that modern organizations face. In this paper we propose that alignment concerns can be better addressed by making models the primary entities that stakeholders within and outside of an organization use to interact with the organization. We call an organization that maintains and uses an integrated set of models to manage alignment concerns a Model Driven Organization (MDO). In this paper we characterize the alignment problem, discuss the shortcomings of current alignment management approaches and present our MDO vision.}, 
keywords={business data processing;organisational aspects;model driven organization;external business pressures;organizational misalignment;suboptimal configurations;enterprise architecture domain;EA;alignment problems;MDO;alignment management approaches;Organizations;Analytical models;Context modeling;Adaptation models;Electronic mail;Context;Enterprise Architecture;Enterprise Modelling;Simulation}, 
doi={10.1109/HICSS.2014.591}, 
ISSN={1530-1605}, 
month={Jan},}
@INPROCEEDINGS{6602477, 
author={P. Morrison and C. Holmgreen and A. Massey and L. Williams}, 
booktitle={2013 5th International Workshop on Software Engineering in Health Care (SEHC)}, 
title={Proposing regulatory-driven automated test suites for electronic health record systems}, 
year={2013}, 
volume={}, 
number={}, 
pages={46-49}, 
abstract={In regulated domains such as finance and health care, failure to comply with regulation can lead to financial, civil and criminal penalties. While systems vary from organization to organization, regulations apply across organizations. We propose the use of Behavior-Driven-Development (BDD) scenarios as the basis of an automated compliance test suite for standards such as regulation and interoperability. Such test suites could become a shared asset for use by all systems subject to these regulations and standards. Each system, then, need only create their own system-specific test driver code to automate their compliance checks. The goal of this research is to enable organizations to compare their systems to regulation in a repeatable and traceable way through the use of BDD. To evaluate our proposal, we developed an abbreviated HIPAA test suite and applied it to three open-source electronic health record systems. The scenarios covered all security behavior defined by the selected regulation. The system-specific test driver code covered all security behavior defined in the scenarios, and identified where the tested system lacked such behavior.}, 
keywords={automatic testing;conformance testing;health care;medical information systems;open systems;program testing;security of data;regulatory-driven automated test suites;health care;financial penalties;civil penalties;criminal penalties;organization;regulations;behavior-driven-development;BDD scenarios;automated compliance test suite;interoperability;system-specific test driver code;compliance checks;HIPAA test suite;open-source electronic health record systems;security behavior;Data structures;Boolean functions;NIST;Certification;Behavior-Driven-Development Healthcare IT;Regulatory Compliance;Security;Software Engineering;Software Testing}, 
doi={10.1109/SEHC.2013.6602477}, 
ISSN={}, 
month={May},}
@ARTICLE{7206603, 
author={M. Jamro}, 
journal={IEEE Transactions on Industrial Informatics}, 
title={POU-Oriented Unit Testing of IEC 61131-3 Control Software}, 
year={2015}, 
volume={11}, 
number={5}, 
pages={1119-1129}, 
abstract={Software testing is an important part of project development. Depending on system type and size, it is performed variously. Unit testing is one of the available approaches that is used to ensure that behavior of small software parts is consistent with requirements. It allows to improve software quality and decrease overall costs. Despite the fact that such an approach is commonly judged as a vital concept, it is not usual in control software. In this paper, the comprehensive approach to test the IEC 61131-3 software using unit tests is presented. It supports to create tests in two ways-either in textual and graphical IEC 61131-3 languages or in the CPTest+ dedicated test definition language. The latter is equipped with many advanced features, such as test fixtures and inclusions, parameterized and analog signal extensions, mock objects, as well as a few kinds of suites. The overall solution runs on the developer and testing station; hence, it does not have significant impact on performance of the control program and tests are more reliable and repeatable. To explain the concept, the simple running example is presented in this paper. The described solution has been introduced in the CPDev engineering environment for programming controllers.}, 
keywords={control engineering computing;program testing;software quality;POU-oriented unit testing;IEC 61131-3 control software;software testing;software quality;textual IEC 61131-3 languages;graphical IEC 61131-3 languages;CPTest+ dedicated test definition language;CPDev engineering environment;Testing;Software;IEC Standards;Informatics;Automation;Control systems;control software;IEC 61131-3;testing;unit test;Control software;IEC 61131-3;testing;unit test}, 
doi={10.1109/TII.2015.2469257}, 
ISSN={1551-3203}, 
month={Oct},}
@INPROCEEDINGS{5501495, 
author={S. Chatterjee}, 
booktitle={2010 Seventh International Conference on Information Technology: New Generations}, 
title={Modeling, Debugging, and Tuning QoE Issues in Live Stream-Based Applications - A Case Study with VoIP}, 
year={2010}, 
volume={}, 
number={}, 
pages={1044-1050}, 
abstract={End-user acceptance criteria of live-stream based, interactive applications are different from traditional B2B or B2C applications. For example, if users sense disruptions in audio or video stream quality, they may quickly form a negative opinion. The Quality of Experience (QoE) in such live-stream applications is, thus, based on perception, and is open to subjective interpretations. QoE can be affected by hundreds of possible variables. QoE problems (QoE bugs), however, require an objective solution (fix in the product's code or tuning of product parameters). Understanding and debugging QoE bugs in such scenarios starts with designing relevant metrics and analysis tools. Thereafter, smart test-designs and strategies are required to gain insights into bottlenecks. This paper builds a discussion around these thoughts, and through a case study (sample QoE problem in a Voice over Internet Protocol (VoIP) application), collates some generic guidelines to investigate QoE bugs in live-streaming scenarios.}, 
keywords={computer debugging;Internet telephony;media streaming;program debugging;modeling QoE issues;debugging QoE issues;tuning QoE issues;live stream-based applications;VoIP;end-user acceptance criteria;audio stream quality;video stream quality;quality of experience;analysis tools;smart test-designs;voice over Internet protocol;Debugging;Streaming media;Computer bugs;Collaboration;Delay;Information technology;Videoconference;Product codes;Testing;Internet telephony;VOIP;QoE;QoS;Media Streaming;VOIP Metric;Modeling}, 
doi={10.1109/ITNG.2010.44}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{4293588, 
author={M. Fletcher and W. Bereza and M. Karlesky and G. Williams}, 
booktitle={Agile 2007 (AGILE 2007)}, 
title={Evolving into Embedded Develop}, 
year={2007}, 
volume={}, 
number={}, 
pages={150-155}, 
abstract={In late 2005 we had the opportunity to start our first embedded development project. We apply agile practices to a variety of domains from web development to desktop applications to factory floor test equipment. The challenge for this new work was not learning the environment and technology. Our challenge was applying the practices of the agile world to the small and complex world of embedded systems. The hurdles were numerous: we battled the archaic state of many embedded tool sets, the lack of integration with tools like Rake that provide easy automation, and poor support for object oriented design. We've overcome each of these difficulties. This report is about our yearlong experience in introducing our development practices to embedded development.}, 
keywords={embedded systems;object-oriented programming;program testing;embedded system;Web development;object oriented design;factory floor test equipment;agile practices;Velocity control;Microprogramming;Vehicle safety;Automatic testing;Embedded system;Vehicle driving;Automation;System testing;Sampling methods;Production facilities}, 
doi={10.1109/AGILE.2007.25}, 
ISSN={}, 
month={Aug},}
@ARTICLE{5388069, 
author={C. E. Walston and C. P. Felix}, 
journal={IBM Systems Journal}, 
title={A method of programming measurement and estimation}, 
year={1977}, 
volume={16}, 
number={1}, 
pages={54-73}, 
abstract={The present approach to productivity estimation, although useful, is far from being optimized. Based on the results of the variable analysis described in this paper, and supplemented by the results of the continued investigation of additional variables related to productivity, an experimental regression model has been developed. Preliminary results indicate that the model reduces the scatter. Further work is being done to determine the potential of regression as an estimating tool, as well as to extend the analyses of the areas of computer usage, documentation volume, duration, and staffing.}, 
keywords={}, 
doi={10.1147/sj.161.0054}, 
ISSN={0018-8670}, 
month={},}
@INPROCEEDINGS{7073242, 
author={E. Biliri and M. Petychakis and I. Alvertis and F. Lampathaki and S. Koussouris and D. Askounis}, 
booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
title={Infusing social data analytics into Future Internet applications for manufacturing}, 
year={2014}, 
volume={}, 
number={}, 
pages={515-522}, 
abstract={Today, a new age of engagement and collaboration has emerged with the proliferation of usergenerated content in social networks and generally the Web 2.0, rendering it particularly difficult for enterprises to monitor and act upon all content following conventional data mining methodologies. In this paper, we present our approach for a Future Internet enabler (FITMAN Anlzer) that provides automated, social data analytics and aims at assisting enterprises in becoming more tuned to their customer needs and gaining insights into current and future trends to early embed them into product design. The FITMAN Anlzer implementation is domainindependent and allows any manufacturer to effectively train it based on his needs and create personalized reports to timely capture the right information. Our methodology includes trend analytics, polarity detection through machine learning, data querying through flexible reports and finally informative charts to visualize the results in order to help companies in their decision making procedures.}, 
keywords={data analysis;data mining;data visualisation;decision making;groupware;Internet;learning (artificial intelligence);product design;production engineering computing;query processing;social networking (online);social data analytics infusion;Future Internet applications;manufacturing;collaboration;user generated content;social networks;Web 2.0;data mining methodologies;Future Internet enabler;FITMAN Anlzer;product design;trend analytics;polarity detection;machine learning;data querying;informative charts;result visualization;decision making procedures;Media;Sentiment analysis;Market research;Facebook;Twitter;Data mining;Context;social media monitoring;trend analysis;opinion mining;natural language processing;sentiment analysis;social data analytics}, 
doi={10.1109/AICCSA.2014.7073242}, 
ISSN={2161-5330}, 
month={Nov},}
@ARTICLE{4346539, 
author={Y. Chen and D. Bindel and H. H. Song and R. H. Katz}, 
journal={IEEE/ACM Transactions on Networking}, 
title={Algebra-Based Scalable Overlay Network Monitoring: Algorithms, Evaluation, and Applications}, 
year={2007}, 
volume={15}, 
number={5}, 
pages={1084-1097}, 
abstract={Overlay network monitoring enables distributed Internet applications to detect and recover from path outages and periods of degraded performance within seconds. For an overlay network with end hosts, existing systems either require measurements, and thus lack scalability, or can only estimate the latency but not congestion or failures. Our earlier extended abstract [Y. Chen, D. Bindel, and R. H. Katz, ldquoTomography-based overlay network monitoring,rdquo Proceedings of the ACM SIGCOMM Internet Measurement Conference (IMC), 2003] briefly proposes an algebraic approach that selectively monitors linearly independent paths that can fully describe all the paths. The loss rates and latency of these paths can be used to estimate the loss rates and latency of all other paths. Our scheme only assumes knowledge of the underlying IP topology, with links dynamically varying between lossy and normal. In this paper, we improve, implement, and extensively evaluate such a monitoring system. We further make the following contributions: i) scalability analysis indicating that for reasonably large n (e.g., 100), the growth of k is bounded as O(n log n), ii) efficient adaptation algorithms for topology changes, such as the addition or removal of end hosts and routing changes, iii) measurement load balancing schemes, iv) topology measurement error handling, and v) design and implementation of an adaptive streaming media system as a representative application. Both simulation and Internet experiments demonstrate we obtain highly accurate path loss rate estimation while adapting to topology changes within seconds and handling topology errors.}, 
keywords={algebra;computational complexity;computer network management;Internet;resource allocation;algebra-based scalable overlay network monitoring;load balancing;adaptive streaming media system;Internet architecture;Topology;Delay;Condition monitoring;IP networks;Scalability;Algorithm design and analysis;Degradation;Routing;Load management;Measurement errors;Dynamics;load balancing;network measurement and monitoring;numerical linear algebra;overlay;scalability}, 
doi={10.1109/TNET.2007.896251}, 
ISSN={1063-6692}, 
month={Oct},}
@INPROCEEDINGS{4839257, 
author={D. Dechev and B. Stroustrup}, 
booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems}, 
title={Model-Based Product-Oriented Certification}, 
year={2009}, 
volume={}, 
number={}, 
pages={295-304}, 
abstract={Future space missions such as the Mars Science Laboratory and Project Constellation suggest the engineering of some of the most complex man-rated software systems. The present process-oriented certification methodologies employed by NASA are becoming prohibitively expensive when applied to systems of such complexity. The process of software certification establishes the level of confidence in a software system in the context of its functional and safety requirements. Providing such certification evidence may require the application of a number of software development, analysis, and validation techniques. We define product-oriented certification as the process of measuring the system's reliability and efficiency based on the analysis of its design (expressed in models) and implementation (expressed in source code). In this work we introduce a framework for model-based product-oriented certification founded on the concept of source code enhancement and analysis. We describe a classification of the certification artifact types, the development and validation tools and techniques, the application domain-specific factors, and the levels of abstraction. We demonstrate the application of our certification platform by analyzing the process of model-based development of the parallel autonomic goals network, a critical component of the Jet Propulsion Laboratory's Mission Data System (MDS). We describe how we identify and satisfy seven critical certification artifacts in the process of model-driven development and validation of the MDS goal network. In the analysis of this process, we establish the relationship among the seven certification artifacts, the applied development and validation techniques and tools, and the level of abstraction of system design and development.}, 
keywords={aerospace computing;parallel processing;program verification;software reliability;software tools;model-based product-oriented certification;space missions;Mars Science Laboratory;Project Constellation;complex man-rated software systems;certification methodologies;software certification evidence;software development;validation techniques;product-oriented certification;source code enhancement;validation tools;application domain-specific factors;model-based development;Jet Propulsion Laboratory Mission Data System;Certification;Laboratories;Software systems;Software safety;Space missions;Mars;NASA;Application software;Programming;Reliability;product-oriented certification;nonblocking synchronization;semantic enhancement;concurrent real-time systems}, 
doi={10.1109/ECBS.2009.15}, 
ISSN={}, 
month={April},}
@ARTICLE{8016712, 
author={}, 
journal={ISO/IEC/IEEE 24765:2017(E)}, 
title={ISO/IEC/IEEE International Standard - Systems and software engineering--Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-541}, 
abstract={This document provides a common vocabulary applicable to all systems and software engineering work. It was prepared to collect and standardize terminology. This document is intended to serve as a useful reference for those in the information technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute. This document includes references to the active source standards for definitions so that systems and software engineering concepts and requirements can be further explored.}, 
keywords={IEC standards;IEEE standards;ISO standards;project management;software engineering;systems engineering;vocabulary;ISO/IEC/IEEE international standard;systems engineering;vocabulary;terminology standardization;information technology;ISO standards;IEEE Computer Society and Project Management Institute;software engineering concepts;software requirements;IEEE Standards;ISO Standards;IEC Standards;Informatino technology;Software engineering;Systems engineering and theoryt;Terminology;computer;dictionary;information technology;software engineering;systems engineering;24765}, 
doi={10.1109/IEEESTD.2017.8016712}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{1471346, 
author={C. Verdonck and D. Vandenameele}, 
booktitle={Proceedings of the 27th European Solid-State Circuits Conference}, 
title={A single chip configurable network processor with built in ADSL-modem in 0.18 amp;#181;m CMOS}, 
year={2001}, 
volume={}, 
number={}, 
pages={109-112}, 
abstract={The SEA ASIC integrates a complete Discrete Multi-Tone (DMT) ADSL modem with an Asynchronous Transfer Mode (ATM) switch, an IEEE 802.3 Ethernet based packet switch and an ARM microcontroller into a single 0.18 &#181;m CMOS chip. The device is a cost-effective platform for a complete range of Alcatel Customer Premises ADSL Equipment.}, 
keywords={Intelligent networks;CMOS process;Hardware;Application specific integrated circuits;Local area networks;Modems;Wide area networks;Switches;Ethernet networks;Protocols}, 
doi={}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7357239, 
author={S. Barnett and I. Avazpour and R. Vasa and J. Grundy}, 
booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={A multi-view framework for generating mobile apps}, 
year={2015}, 
volume={}, 
number={}, 
pages={305-306}, 
abstract={This paper demonstrates a multi-view framework for Rapid APPlication Tool (RAPPT). RAPPT enables rapid development of mobile applications. It employs a multilevel approach to mobile application development: a Domain Specific Visual Language to define the high level structure of mobile apps, a Domain Specific Textual Language to define behavioural concepts, and concrete source code for fine grained improvements.}, 
keywords={mobile computing;software engineering;source code (software);visual languages;mobile apps generation;multiview framework;Rapid Application Tool;RAPPT;mobile applications development;Domain Specific Visual Language;Domain Specific Textual Language;source code;high level structure;behavioural concepts;Navigation}, 
doi={10.1109/VLHCC.2015.7357239}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{5770632, 
author={P. L. M. Navarro and G. M. Pérez and D. S. Ruiz}, 
booktitle={2011 Fourth IEEE International Conference on Software Testing, Verification and Validation}, 
title={Towards Software Quality and User Satisfaction through User Interfaces}, 
year={2011}, 
volume={}, 
number={}, 
pages={415-418}, 
abstract={With this work we expect to provide the community and the industry with a solid basis for the development, integration, and deployment of software testing tools. As a solid basis we mean, on one hand, a set of guidelines, recommendations, and clues to better comprehend, analyze, and perform software testing processes, and on the other hand, a set of robust software frameworks that serve as a starting point for the development of future testing tools.}, 
keywords={program testing;software quality;user interfaces;software quality;user satisfaction;user interfaces;software testing;Graphical user interfaces;Software testing;Usability;Open source software;Computer architecture;software testing;GUI testing;automatic test case generation;usability evaluation;user experience evaluation;GUI-data verification}, 
doi={10.1109/ICST.2011.13}, 
ISSN={2159-4848}, 
month={March},}
@ARTICLE{4197854, 
author={}, 
journal={IEEE Unapproved Draft Std P487/D8, Apr 2007}, 
title={Unapproved IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations (Revision of IEEE Std 487-2000)}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{8317991, 
author={A. Johanson and W. Hasselbring}, 
journal={Computing in Science Engineering}, 
title={Software Engineering for Computational Science: Past, Present, Future}, 
year={2018}, 
volume={20}, 
number={2}, 
pages={90-109}, 
abstract={Despite the increasing importance of in silico experiments to the scientific discovery process, state-of-the-art software engineering practices are rarely adopted in computational science. To understand the underlying causes for this situation and to identify ways to improve it, the authors conducted a literature survey on software engineering practices in computational science. They identified 13 recurring key characteristics of scientific software development that are the result of the nature of scientific challenges, the limitations of computers, and the cultural environment of scientific software development. Their findings allow them to point out shortcomings of existing approaches for bridging the gap between software engineering and computational science and to provide an outlook on promising research directions that could contribute to improving the current situation.}, 
keywords={natural sciences computing;software engineering;computational science;scientific discovery process;scientific software development;software engineering practices;Scientific computing;Software engineering;Computational modeling;Software development management;survey;software engineering;computational science;software development;history of computing}, 
doi={10.1109/MCSE.2018.021651343}, 
ISSN={1521-9615}, 
month={Mar},}
@INPROCEEDINGS{6005491, 
author={T. D. Hellmann and F. Maurer}, 
booktitle={2011 Agile Conference}, 
title={Rule-Based Exploratory Testing of Graphical User Interfaces}, 
year={2011}, 
volume={}, 
number={}, 
pages={107-116}, 
abstract={This paper introduces rule-based exploratory testing, an approach to GUI testing that combines aspects of manual exploratory testing with rule-based test automation. This approach uses short, automated rules to increase the bug-detection capability of recorded exploratory test sessions. A preliminary evaluation found that this approach can be used to detect both general and application-specific bugs, but that rules for general bugs are easier to transfer between applications. Also, despite the advantages of keyword-based testing, it interferes with the transfer of rules between applications.}, 
keywords={graphical user interfaces;program debugging;program testing;rule based exploratory testing;graphical user interfaces;GUI;rule based test automation;bug detection;keyword based testing;Testing;Graphical user interfaces;Computer bugs;Humans;Manuals;Security;Fires;GUI testing;rule-based testing;exploratory testing}, 
doi={10.1109/AGILE.2011.23}, 
ISSN={}, 
month={Aug},}
@INBOOK{5273677, 
author={P. K. Bhatnagar}, 
booktitle={Engineering Networks for Synchronization, CCS 7, and ISDN: Standards, Protocols, Planning and Testing}, 
title={Testing in the ISDN}, 
year={1997}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Introduction

Layer 1 Tests

Layer 2 and Layer 3 Tests

Maintenance of ISDN Access

This chapter contains sections titled:

References

}, 
keywords={}, 
doi={10.1109/9780470544570.ch14}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273677},}
@INPROCEEDINGS{8491148, 
author={D. N. Jorge and P. D. L. Machado and E. L. G. Alves and W. L. Andrade}, 
booktitle={2018 IEEE 26th International Requirements Engineering Conference (RE)}, 
title={Integrating Requirements Specification and Model-Based Testing in Agile Development}, 
year={2018}, 
volume={}, 
number={}, 
pages={336-346}, 
abstract={In agile development, Requirements Engineering (RE) and testing have to cope with a number of challenges such as continuous requirement changes and the need for minimal and manageable documentation. In this sense, extensive research has been conducted to automatically generate test cases from (structured) natural language documents using Model-Based Testing (MBT). However, the imposed structure may impair agile practices or test case generation. In this paper, inspired by cooperation with industry partners, we propose CLARET, a notation that allows the creation of use case specifications using natural language to be used as central artifacts for both RE and MBT practices. A tool set supports CLARET specification by checking syntax of use cases structure as well as providing visualization of flows for use case revisions. We also present exploratory studies on the use of CLARET to create RE documents as well as on their use as part of a system testing process based on MBT. Results show that, with CLARET, we can document use cases in a cost-effective way. Moreover, a survey with professional developers shows that CLARET use cases are easy to read and write. Furthermore, CLARET has been successfully applied during specification, development and testing of industrial applications.}, 
keywords={Testing;Industries;Requirements engineering;Tools;Natural languages;Manuals;Syntactics;Agile Development;Requirements Engineering;Model-Based Testing;Use case specification}, 
doi={10.1109/RE.2018.00041}, 
ISSN={2332-6441}, 
month={Aug},}
@ARTICLE{7548916, 
author={D. Lübke and T. van Lessen}, 
journal={IEEE Software}, 
title={Modeling Test Cases in BPMN for Behavior-Driven Development}, 
year={2016}, 
volume={33}, 
number={5}, 
pages={15-21}, 
abstract={Testing large-scale process integration solutions is complex and cumbersome. To tackle this problem, researchers employed behavior-driven development. They used the Business Process Model and Notation language to model domain-specific test cases. These test cases can be understood by both developers and business stakeholders and can be executed automatically.}, 
keywords={business data processing;program testing;software engineering;test case modelling;Business Process Model and Notation;BPMN;behavior-driven development;Simple object access protocol;Business process management;Modeling;Testing;Software engineering;Behaviorial sciences;business processes;Business Process Model and Notation;BPMN;behavior-driven development;BDD;test-driven development;TDD;software testing;software development;software engineering}, 
doi={10.1109/MS.2016.117}, 
ISSN={0740-7459}, 
month={Sept},}
@ARTICLE{278309, 
author={}, 
journal={IEEE Std 610.13-1993}, 
title={IEEE Standard Glossary of Computer Languages}, 
year={1993}, 
volume={}, 
number={}, 
pages={i-}, 
abstract={}, 
keywords={}, 
doi={10.1109/IEEESTD.1993.119224}, 
ISSN={}, 
month={},}
@ARTICLE{7752755, 
author={}, 
journal={ISO/IEC/IEEE FDIS P24765 FDIS, October 2016}, 
title={ISO/IEC/IEEE Draft Systems and Software Engineering - Vocabulary}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-554}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;ISO Standards;IEC Standards;Software engineering;Terminology;Dictionaries}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{6972271, 
author={A. Tokmakoff and B. Sparrow and D. Turner and A. Lowe}, 
booktitle={2014 IEEE 10th International Conference on e-Science}, 
title={AusPlots Rangelands Field Data Collection and Publication: Infrastructure for Ecological Monitoring}, 
year={2014}, 
volume={1}, 
number={}, 
pages={249-255}, 
abstract={The TERN AusPlots Rangelands field data collection system has been developed to facilitate simple and efficient data collection by ecologists operating in the Australian outback. The infrastructure provides tooling for 'clean' data collection on mobile (tablet) devices, associated data storage in a cloud-based server infrastructure, facilities for data curation and management and interfaces with the Australian Ecological Knowledge and Observation System (ÆKOS) data repository for long-term data management and semantic enrichment. In this paper, we introduce the AusPlots Rangelands field data collection solution, providing a systems-level view and motivating its development through the discussion of key functional requirements. We provide an outline of the ÆKOS data repository and demonstrate that the combined system provides a unique end-to-end data collection, curation, archiving and publishing mechanism for ecological data.}, 
keywords={cloud computing;data handling;ecology;environmental science computing;mobile computing;ecological data publishing;ecological data archiving;ecological data curation;end-to-end ecological data collection;ÆKOS data repository;Australian Ecological Knowledge and Observation System;data management;cloud-based server infrastructure;data storage;mobile devices;ecological monitoring;TERN AusPlots Rangelands field data publication;TERN AusPlots Rangelands field data collection;Data collection;Vegetation;Databases;Servers;Soil;Protocols;Vegetation mapping;ecological data;mobile;data collection;data publishing}, 
doi={10.1109/eScience.2014.55}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7811570, 
author={L. C. Hoyos and C. E. Rothenberg}, 
booktitle={2016 8th IEEE Latin-American Conference on Communications (LATINCOM)}, 
title={NOn: Network function virtualization ontology towards semantic service implementation}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={A hazard of ongoing Network Function Virtualization (NFV) realizations is the lack of a common understanding in support of development, deployment and operation tasks related to Virtual Function Networks (VNFs), NFV components and interfaces. In the current state of affairs, NFV stakeholders commonly create their own terminology to define and describe NFV components, following going the specifications led by European Telecommunications Standard Institute but also adopting telecommunication- and software-centric definitions. As a consequence, portability and interoperability goals of NFV get compromised since NFV technology providers have hard times in understanding and using definitions and descriptions across different domains. Furthermore, VNF data models of operational systems and deployment configuration software need to be re-defined, re-coded, and re-compiled to make them work over different NFV platforms. In this work, we present the design and implementation of our proposed NFV Ontology (NOn) enabling Semantic nFV Services (SnS) to reduce manual intervention during the integration process of heterogeneous NFV domains and effectively overcome the costly re-work hazards of current NFV implementation approaches. We present the proof of concept implementation of a Generic Client leveraging SnS/NOn to create and consume dynamic workflows in an open source testbed based on OpenStack and OpenBaton.}, 
keywords={ontologies (artificial intelligence);open systems;semantic Web;virtualisation;network function virtualization ontology;semantic service implementation;NOn;NFV component;virtual function network;VNF data model;operational system;NFV interface;European Telecommunications Standard Institute;interoperability;portability;deployment configuration software;manual intervention reduction;open source testbed;OpenStack testbed;OpenBaton testbed;Semantics;Ontologies;Interoperability;Software;Network function virtualization;Manuals;Engines;Network Function Virtualization;NFV;Semantic Services;Ontology}, 
doi={10.1109/LATINCOM.2016.7811570}, 
ISSN={}, 
month={Nov},}
@ARTICLE{8451922, 
author={M. Sayagh and N. Kerzazi and B. Adams and F. Petrillo}, 
journal={IEEE Transactions on Software Engineering}, 
title={Software Configuration Engineering in Practice: Interviews, Survey, and Systematic Literature Review}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Modern software applications are adapted to different situations (e.g., memory limits, enabling/disabling features) by only changing few configuration option values without any source code modifications. According to several studies, this flexibility is expensive. Indeed, configuration errors represent one of the largest percentage of software errors, they are hard to debug and resolve, while comprehension of the code also is hampered by sprinkling conditional checks of configuration options. Although researchers have proposed various approaches to help debug or prevent configuration errors, especially from the end users' perspective, this paper takes a step back to understand the activities required by practitioners to engineer the software configuration options in their source code, the challenges they experience as well as best practices that they have or could adopt. By interviewing 14 software engineering experts, followed by a large survey on 229 software engineers, we identified 9 major activities related to configuration engineering, 22 challenges faced by developers, and 25 expert recommendations to improve software configuration quality. We complemented this study by a systematic literature review to enrich the experts' recommendations, and to identify possible solutions for the developers' challenges discussed and evaluated by the research community.}, 
keywords={Software systems;Interviews;Systematics;Facebook;Bibliographies;Software algorithms}, 
doi={10.1109/TSE.2018.2867847}, 
ISSN={0098-5589}, 
month={},}
@INPROCEEDINGS{7965258, 
author={D. Mazinanian and N. Tsantalis}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
title={CSSDev: Refactoring Duplication in Cascading Style Sheets}, 
year={2017}, 
volume={}, 
number={}, 
pages={63-66}, 
abstract={Cascading Style Sheets (CSS) is a widely-used language for defining the presentation of structured documents and user interfaces. Despite its popularity, CSS still lacks adequate tool support for everyday maintenance tasks, such as debugging and refactoring. In this paper, we present CSSDEV, a tool suite for analyzing CSS code to detect refactoring opportunities.(https://youtu.be/lu3oITi1XrQ).}, 
keywords={document handling;program diagnostics;software maintenance;source code (software);user interfaces;CSSDev;duplication refactoring;cascading style sheets;structured documents;user interfaces;CSS code analysis;Cascading style sheets;Tools;HTML;Browsers;Crawlers;Maintenance engineering;Runtime;Cascading Style Sheets;Preprocessors;Refactoring}, 
doi={10.1109/ICSE-C.2017.7}, 
ISSN={}, 
month={May},}
@INBOOK{8044562, 
author={Moray Rumney}, 
booktitle={LTE and the Evolution to 4G Wireless: Design and Measurement Challenges}, 
title={List of Acronyms}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={10.1002/9781118799475.oth1}, 
ISSN={}, 
publisher={Wiley}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8044562},}
@INPROCEEDINGS{5614873, 
author={F. Abbors and D. Truscan}, 
booktitle={2010 Second International Conference on Advances in System Testing and Validation Lifecycle}, 
title={Approaching Performance Testing from a Model-Based Testing Perspective}, 
year={2010}, 
volume={}, 
number={}, 
pages={125-128}, 
abstract={The paper introduces the concept of model-based performance testing, which we plan to pursue in our research. The underlying idea is to describe various performance aspects as well as functional aspects of a software system using modeling languages like UML, and from the resulting models to automatically design tests that can be used for performance testing. In our research, we also plan to focus on how the modeling and traceability of performance requirements can be achieved across the testing process.}, 
keywords={model-based reasoning;program testing;simulation languages;software performance evaluation;performance testing;model based testing perspective;software system;modeling language;UML;Unified modeling language;Testing;Adaptation model;Analytical models;Software systems;Load modeling;Model-Based Testing;Model Validation;Requirements Traceability}, 
doi={10.1109/VALID.2010.22}, 
ISSN={}, 
month={Aug},}
@INBOOK{8045719, 
author={Henning Schulzrinne and Hannes Tschofenig}, 
booktitle={Internet Protocol-based Emergency Services}, 
title={Architectures}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={Emergency services;IP networks;3GPP;Protocols;Internet;WiMAX}, 
doi={10.1002/9781119993858.ch3}, 
ISSN={}, 
publisher={Wiley}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8045719},}
@INPROCEEDINGS{7905631, 
author={F. Fourati and M. T. Bhiri and R. Robbana}, 
booktitle={2016 5th International Conference on Multimedia Computing and Systems (ICMCS)}, 
title={Verification and validation of PDDL descriptions using Event-B formal method}, 
year={2016}, 
volume={}, 
number={}, 
pages={770-776}, 
abstract={The automatic planning community of Artificial Intelligence AI have developed a de facto standard language for PDDL, producing formal modeling of Planning problems. Equally it have conceived and produced tools called planners to automatically generate plans for PDDL descriptions. But the verification and validation of PDDL descriptions is little treated topic. In this paper, we shall treat this issue through the Event-B formal method. We illustrate the contribution of the static analysis tools associated with Event-B (provers, model checker, animator, and simulator) for verification and validation of PDDL descriptions.}, 
keywords={planning (artificial intelligence);program diagnostics;program verification;PDDL description validation;PDDL description verification;automatic planning community;artificial intelligence AI;de facto standard language;formal modeling;planners;automatic plan generation;static analysis tools;Event-B formal method;model checker;provers;animator;simulator;Planning;Context;Boats;Context modeling;Poles and towers;Syntactics;Electronic mail;Artificial Intelligence;Planning and Scheduling;PDDL;Verification and Validation;Event-B;Transformation;Static and Dynamic analysis}, 
doi={10.1109/ICMCS.2016.7905631}, 
ISSN={2472-7652}, 
month={Sept},}
@INPROCEEDINGS{4766483, 
author={S. Sarkar and A. Panayappan}, 
booktitle={TENCON 2008 - 2008 IEEE Region 10 Conference}, 
title={Formal architecture modeling of business application- software maintenance case study}, 
year={2008}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Maintenance of complex business applications is challenging for software services industry. The maintenance team inherits the software with little design and implementation knowledge. The client-facing team gathers an ad-hoc architectural description of some sort and communicates the same to the geographically distributed maintenance team through informal box and line diagrams. This information is poorly understood, and the underlying architectural constraints are never enforced. This paper proposes a type system to model the architecture of a complex enterprise IT system using Acme architecture description language and reports a modeling approach to capture various architectural design decisions architects perform as a part of the architecture review. An initial field-study to evaluate the usefulness of such modeling has been encouraging.}, 
keywords={DP industry;software architecture;software development management;software maintenance;specification languages;formal architecture modeling;business application;software maintenance;software service industry;type system;complex enterprise IT system;Acme architecture description language;software development management;Computer architecture;Software maintenance;Application software;Computer industry;Architecture description languages;Software design;Documentation;Logic design;Software development management;Programming}, 
doi={10.1109/TENCON.2008.4766483}, 
ISSN={2159-3442}, 
month={Nov},}
@INPROCEEDINGS{8071326, 
author={D. Mordvinov and Y. Litvinov and T. Bryksin}, 
booktitle={2017 20th Conference of Open Innovations Association (FRUCT)}, 
title={TRIK studio: Technical introduction}, 
year={2017}, 
volume={}, 
number={}, 
pages={296-308}, 
abstract={This paper presents TRIK Studio - an environment for visual (and textual) programming of robotic kits, which is used in educational organizations across Russia and Europe. First part of the article provides overview of the system - its purpose, features, differences from similar programming environments, general difficulties of robot programming and solutions proposed by TRIK Studio. Second part presents implementation details of TRIK Studio and its most interesting components. This article combines five fields of study: robotics, domain-specific visual modeling, education, formal methods and methods of program analysis. Main contribution of this article is detailed technical description of TRIK Studio as complex and successful open-source cross-platform robot programming environment written in C++/Qt, and first part of the article can also be interesting for teachers as it provides an overview of existing robot programming tools and related problems.}, 
keywords={humanoid robots;mobile robots;robot programming;similar programming environments;TRIK Studio;domain-specific visual modeling;complex source cross-platform robot programming environment;robot programming tools;TRIK studio;visual programming;open-source cross-platform robot programming environment}, 
doi={10.23919/FRUCT.2017.8071326}, 
ISSN={2305-7254}, 
month={April},}
@INPROCEEDINGS{186190, 
author={F. Godon and D. Al-Khalili and R. Inkol}, 
booktitle={Third Annual IEEE Proceedings on ASIC Seminar and Exhibit}, 
title={Multi circular buffer controller chip for advanced ESM system}, 
year={1990}, 
volume={}, 
number={}, 
pages={P14/5.1-P14/5.4}, 
abstract={A 90 K transistor 1.5 mu m CMOS integrated circuit that operates at a data transfer rate of 20 MHz and implements an array of variable size circular buffers mapped into a high-speed RAM through physical and virtual addressing techniques is discussed. The device is fully programmable with the capability of single and block data transfers. The target application is an advanced multiprocessor ESM system.<<ETX>>}, 
keywords={buffer storage;CMOS integrated circuits;electronic warfare;storage management chips;physical addressing;advanced ESM system;CMOS integrated circuit;data transfer rate;variable size circular buffers;high-speed RAM;virtual addressing;block data transfers;multiprocessor;1.5 micron;Control systems;Buffer storage;Random access memory;Read-write memory;Pulse measurements;Counting circuits;Very large scale integration;Computer architecture;Registers;Logic arrays}, 
doi={10.1109/ASIC.1990.186190}, 
ISSN={}, 
month={Sept},}
@INBOOK{5273145, 
author={Steven M. Kaplan}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={R}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={
}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch18}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273145},}
@INPROCEEDINGS{8247727, 
author={S. Pfrang and D. Meier and V. Kautz}, 
booktitle={2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Towards a modular security testing framework for industrial automation and control systems: ISuTest}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Industrial automation and control systems (IACS) play a key role in modern production facilities. On the one hand, they provide real-time functionality to the connected field devices. On the other hand, they get more and more connected to local networks and the internet in order to facilitate use cases promoted by “Industry 4.0”. This makes IACS susceptible to cyber-attacks which exploit vulnerabilities, for example in order to interrupt the automation process. Security testing targets at discovering those vulnerabilities before they are exploited. In order to enable IACS manufacturers and integrators to perform security testing for their devices, we present ISuTest, a modular security testing framework for IACS. ISuTest is designed to be extendable regarding all kinds of automation protocols, different connection paths as well as evaluating arbitrary outputs of the tested devices. This paper describes the fundamental ideas behind ISuTest, its design and a basic evaluation in which the ISuTest framework was able to discover a vulnerability in a programmable logic controller (PLC). The paper concludes with a broad overview of the planned future work.}, 
keywords={Internet;production control;production engineering computing;production facilities;program testing;programmable controllers;security of data;security testing targets;vulnerability;integrators;modular security testing framework;automation protocols;different connection paths;tested devices;ISuTest framework;programmable logic controller;modern production facilities;connected field devices;automation process;IACS;industrial automation and control systems;Industry 4.0;cyber-attacks;PLC;Testing;Security;Automation;Protocols;Control systems;Software;Hardware}, 
doi={10.1109/ETFA.2017.8247727}, 
ISSN={1946-0759}, 
month={Sept},}
@ARTICLE{5621965, 
author={M. Schwartz}, 
journal={IEEE Communications Magazine}, 
title={X.25 Virtual Circuits - TRANSPAC IN France - Pre-Internet Data Networking [History of communications]}, 
year={2010}, 
volume={48}, 
number={11}, 
pages={40-46}, 
abstract={The following article , by Remi Despres, is the second on the history of X.25 systems to appear in this column. As noted by Dr. Despres, the previous article focused on the Canadian Datapac system. Earlier articles on packet switching in this column have included one on the history of the Arpanet/Internet and one on early British packet switching systems. What makes this article particularly distinctive, aside, of course, from the fact that it focuses on the major contributions of French engineers to the development of packet switching as well as to X.25 standardization, is that it carefully outlines the reasons for the choice of connection-oriented virtual circuits for the Transpac network, as contrasted with datagram-based packet switching adopted for Arpanet. Interestingly, Dr. Despres notes that the idea of using virtual-circuit connection-oriented packet switching in the Transpac development came from the British packet switching activity. It is to be noted that early commercial packet switching networks in the United States, such as Tymnet and Telenet, also adopted the virtual circuit paradigm.}, 
keywords={packet switching;protocols;wide area networks;communications history;X.25 systems;Canadian Datapac system;packet switching;X.25 standardization;connection-oriented virtual circuits;Transpac network;Packet switching;Virtual circuits;Protocols;Internet;History;Software}, 
doi={10.1109/MCOM.2010.5621965}, 
ISSN={0163-6804}, 
month={November},}
@INPROCEEDINGS{6075019, 
author={D. Balaretnaraja and S. Weerawarana}, 
booktitle={2011 International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
title={A framework for managing persistence in distributed systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={9-13}, 
abstract={Enterprise applications today have acquired the need to be distributed due various demanding reasons. Such systems are developed with focus on distributed concerns than on the application logic. This diverted the developers from the functional requirement of the system and burdened them with the responsibility of developing and maintaining code related to distributed concerns. The main intention of this research is to facilitate development of distributed systems without any consideration for distributed concerns. We suggest a way where the application is initially designed without them and later enabled by integrating the framework proposed in this research. We confine our interest in separating persistence and replication among other distributed concerns. The motivation for this research comes by recognizing the fact that such a framework drastically reduces the code and complexity involved to make a distributed application resilient to failures and thereby to minimize the effort necessary to debug, deploy and maintain.}, 
keywords={distributed processing;software fault tolerance;software maintenance;software management;persistence management;distributed system;enterprise application;functional requirement;code maintenance;failure resilience;debugging;Peer to peer computing;Robustness;Indexing;Servers;distributed computing;group communication framework;peer to peer overlay;JGroups;FreePastry}, 
doi={10.1109/ICTer.2011.6075019}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7332467, 
author={J. Kim and D. Batory and D. Dig}, 
booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Scripting parametric refactorings in Java to retrofit design patterns}, 
year={2015}, 
volume={}, 
number={}, 
pages={211-220}, 
abstract={Retrofitting design patterns into a program by hand is tedious and error-prone. A programmer must distinguish refactorings that are provided by an Integrated Development Environment (IDE) from those that must be realized manually, determine a precise sequence of refactorings to apply, and perform this sequence repetitively to a laborious degree. We designed, implemented, and evaluated Reflective Refactoring (R<sup>2</sup>), a Java package to automate the creation of classical design patterns (Visitor, Abstract Factory, etc.), their inverses, and variants. We encoded 18 out of 23 Gang-of-Four design patterns as R<sup>2</sup> scripts and explain why the remaining are inappropriate for refactoring engines. We evaluate the productivity and scalability of R<sup>2</sup> with a case study of 6 real-world applications. In one case, R<sup>2</sup> automatically created a Visitor with 276 visit methods by invoking 554 Eclipse refactorings in 10 minutes - an achievement that could not be done manually. R<sup>2</sup> also sheds light on why refactoring correctness, expressiveness, and speed are critical issues for scripting in next-generation refactoring engines.}, 
keywords={Java;object-oriented methods;programming environments;software maintenance;parametric refactoring scripting;Java;design pattern retrofitting;integrated development environment;IDE;reflective refactoring;R2;visitor;abstract factory;Eclipse refactoring;refactoring correctness;refactoring expressiveness;refactoring speed;next-generation refactoring engine;time 10 min;Manuals;Graphics;DVD}, 
doi={10.1109/ICSM.2015.7332467}, 
ISSN={}, 
month={Sept},}
@ARTICLE{4519427, 
author={}, 
journal={IEEE Unapproved Draft Std P11073-00101/D03, Sep 2007}, 
title={Health Informatics - Point-Of-Care Medical Device Communication - Technical Report - Guidelines for the Use of RF Wireless Technology}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={The following Guidance document addresses the use of radio frequency 1 (RF) wireless technology for the transport of medical data both to and from point-of-care (PoC) medical devices. The context of such wireless medical data transport can range from home- or mobile-based healthcare to in hospital ambulatory and stationary situations. The intent of the guidance document is to be global with respect to wireless spectrum and equipment, although working group participation and expertise have favored detail of scenarios from the US. At the time of this Guidance document several applicable RF wireless technologies exist with a range of capabilities and characteristics, and in different stages of maturity, standardization, and adoption in healthcare. It is recognized that RF technologies are rapidly evolving, and new options may become available (or sufficiently established) after the publication of this Guidance document. The recommendations, therefore, avoid being overly prescriptive and instead attempt to assist medical device manufacturers, wireless equipment manufacturers, healthcare providers, government agencies and any other end-user of this document to make reasonable judgments regarding performance and practical implementation of wireless solutions. The Guidance document defines specific use cases to estimate, compare, and contrast performance of known technologies operating on wireless personal area (WPAN), wireless local area (WLAN), wireless metropolitan area (WMAN), and wireless wide area (WWAN) networks. Major considerations are 1) the quality-of-service (QoS) requirements (reliability, latency, priority, bandwidth) associated with the data being transported, 2) the expected performance (power, link range, throughput, link establishment and maintenance) of the wireless technology, and 3) the specific needs and resources of the end user. Related issues include network architecture, EMI/EMC, coexistence with other data streams, security, cost, power consumption, and technology configurability. Performance summaries for specific wireless technologies that support defined use cases are not intended as an endorsement of optimal solution because different needs, resources, sizes, and environments cannot be comprehensively addressed. This overview document is meant to be a foundation and reference for several follow-on IEEE 11073.3.5.x standards that will profile specific classes of off-the-shelf RF wireless technologies for medical data transport. Importantly, this guidance document is not envisioned to be periodically updated, but instead will act as a source of information for the follow-on IEEE 11073-0305.x standards that will supplant it. Periodic updates will be performed on the IEEE 11073- 0305.x standards only.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{577990, 
author={S. M. Kransner and D. E. Bernard}, 
booktitle={1997 IEEE Aerospace Conference}, 
title={Integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium}, 
year={1997}, 
volume={2}, 
number={}, 
pages={409-420 vol.2}, 
abstract={Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process.}, 
keywords={aerospace computing;special purpose computers;software engineering;real-time systems;computerised navigation;computerised monitoring;autonomy technologies;embedded spacecraft system-flight software;Deep Space 1;deep-space mission;NASA;DS1 flight software;spacecraft activities;Smart Executive;mode identification;reconfiguration engine;autonomous navigation;beacon monitoring;spacecraft flight software architecture;real-time;rapid prototype;Space technology;Space vehicles;Hardware;Scheduling;Trajectory;Space missions;Image resolution;Timing;Engines;Radio navigation}, 
doi={10.1109/AERO.1997.577990}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8453176, 
author={O. Semeráth and A. S. Nagy and D. Varró}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)}, 
title={A Graph Solver for the Automated Generation of Consistent Domain-Specific Models}, 
year={2018}, 
volume={}, 
number={}, 
pages={969-980}, 
abstract={Many testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy.}, 
keywords={formal logic;formal specification;formal verification;graph theory;query processing;specification languages;consistent domain-specific models;automated model generation;software engineering;graph models;domain-specific instance models;mapping-based approach;incremental graph query evaluation;graph solver framework;back-end logic solvers;first-order logic specification;safety standards;tool qualification;systematic generation;systems engineering;Analytical models;Object oriented modeling;Tools;Biological system modeling;IP networks;Testing;Graph generation;Test generation;Domain Specific Modeling Languages;Logic Solver;Graph Solver}, 
doi={}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{5985928, 
author={S. Siegl and K. Hielscher and R. German and C. Berger}, 
booktitle={2011 12th Latin American Test Workshop (LATW)}, 
title={Automated testing of embedded automotive systems from requirement specification models}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Embedded software for modern automotive and avionic systems is increasingly complex. In early design phases, even when there is still uncertainty about the feasibility of the requirements, valuable information can be gained from models that describe the expected usage and the desired system reaction. The generation of test cases from these models indicates the feasibility of the intended solution and helps to identify scenarios for which the realization is hardly feasible or the intended system behavior is not properly defined. In this paper we present the formalization of requirements by models to simulate the expected field usage of a system. These so called usage models can be enriched by information about the desired system reaction. Thus, they are the basis for all subsequent testing activities: First, they can be used to verify the first implementation models and design decisions w.r.t. the fulfillment of requirements and second, test cases can be derived in a random or statistic manner. The generation can be controlled with operational profiles that describe different classes of field usage. We have applied our approach at a large German car manufacturer in the early development phase of active safety functionalities. Test cases were generated from the usage models to assess the implementation models in MATLAB/Simulink. The parametrization of the systems could be optimized and a faulty transition in the implementation models was revealed. These design and implementation faults had not been discovered with the established test method.}, 
keywords={automotive engineering;avionics;embedded systems;formal specification;program testing;software fault tolerance;automated testing;embedded automotive system;requirement specification model;embedded software;avionic system;early design phase;intended system;requirement formalization;German car manufacturer;safety functionality;test case;MATLAB-Simulink;faulty transition;Safety;Testing;Timing;MATLAB;Belts;Clocks}, 
doi={10.1109/LATW.2011.5985928}, 
ISSN={2373-0862}, 
month={March},}
@INPROCEEDINGS{8109258, 
author={C. Duffau and B. Grabiec and M. Blay-Fornarino}, 
booktitle={2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={Towards Embedded System Agile Development Challenging Verification, Validation and Accreditation: Application in a Healthcare Company}, 
year={2017}, 
volume={}, 
number={}, 
pages={82-85}, 
abstract={When Agile development meets critical embedded systems, verification, validation and accreditation activities are impacted. Challenges such as tests increase or accreditation documents production have to be managed in terms of time and resources. In this paper, we highlight these challenges and present a continuous integration ecosystem that aims to tackle these issues. We report on how this approach has been applied in a research and development healthcare company named AXONIC.}, 
keywords={accreditation;embedded systems;formal verification;health care;safety-critical software;software prototyping;critical embedded systems;accreditation activities;embedded system agile development;verification activities;validation activities;research and development healthcare company;continuous integration ecosystem;AXONIC;Accreditation;Testing;Embedded systems;Hardware;Companies;Ecosystems;agile development;embedded systems;justification;VV&A;continuous integration}, 
doi={10.1109/ISSREW.2017.8}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7018532, 
author={A. Ulrich and S. Jell and A. Votintseva and A. Kull}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={The ETSI Test Description Language TDL and its application}, 
year={2014}, 
volume={}, 
number={}, 
pages={601-608}, 
abstract={The wide-scale introduction of model-based testing techniques in an industrial context faces many obstacles. One of the obstacles is the existing methodology gap between informally described test purposes and formally defined test descriptions used as the starting point for test automation. The provision of an explicit test description becomes increasingly essential when integrating complex, distributed systems and providing support for conformance and interoperability tests of such systems. The upcoming ETSI standard on the Test Definition Language (TDL) covers this gap. It allows describing scenarios on a higher abstraction level than programming or scripting languages. Furthermore, TDL can be used as an intermediate representation of tests generated from other sources, e.g. simulators, test case generators, or logs from previous test runs. TDL is based on a meta-modelling approach that expresses its abstract syntax. Deploying this design approach, individual concrete syntaxes of TDL can be designed for different application domains. The paper provides an overview of TDL and discusses its application on a use case from the rail domain.}, 
keywords={program testing;specification languages;ETSI test description language;TDL language;model-based testing techniques;test purpose;test description;test automation;conformance test;interoperability test;test representation;meta-modelling approach;abstract syntax;application domain;rail domain;Testing;Unified modeling language;Concrete;Telecommunication standards;Syntactics;Semantics;Abstracts;Model-based Testing;Domain-Specific Languages;Meta-modelling;Rail Application}, 
doi={}, 
ISSN={}, 
month={Jan},}
@ARTICLE{7820199, 
author={M. Fleck and J. Troya and M. Kessentini and M. Wimmer and B. Alkhazi}, 
journal={IEEE Transactions on Software Engineering}, 
title={Model Transformation Modularization as a Many-Objective Optimization Problem}, 
year={2017}, 
volume={43}, 
number={11}, 
pages={1009-1032}, 
abstract={Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.}, 
keywords={genetic algorithms;program debugging;search problems;software maintenance;software quality;model transformations modularization;model transformation modularization;model transformation programs;transformation languages;higher-order transformations;in-place transformation engine;concrete transformation language;ATL transformations;bug fixing;transformation rules;many-objective optimization problem;metamodels version;monolithic artifacts;automated search-based approach;NSGA-III;statistical analysis;maintenance activities;Unified modeling language;Object oriented modeling;Adaptation models;Measurement;Algorithm design and analysis;Software engineering;Computer bugs;Model transformation;modularization;ATL;NSGA-III;MDE;SBSE}, 
doi={10.1109/TSE.2017.2654255}, 
ISSN={0098-5589}, 
month={Nov},}
@ARTICLE{7361678, 
author={}, 
journal={IEEE Std 487-2015 (Revision of IEEE Std 487-2007) - Redline}, 
title={IEEE Standard for the Electrical Protection of Communications Facilities Serving Electric Supply Locations -- General Considerations - Redline}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-324}, 
abstract={General considerations are presented for the electrical protection of telecommunications facilities serving electric supply locations. This standard contains material that is common to the IEEE 487(TM) family of standards (i.e., dot-series) including fundamental protection theory; basic electrical protection philosophy, concepts, and designs; protection apparatus; service types; reliability; service performance objective (SPO) classifications; and transmission considerations. In general, special protective measures, handling procedures, and administrative procedures are necessary to provide electrical protection against damage to telecommunications facilities and equipment, maintain reliability of service, and ensure the safety of personnel.}, 
keywords={IEEE Standards;Electricity supply industry;Voltage control;Power transmission lines;Power stations;electric supply locations;high-voltage tower;IEEE 487(TM);power stations;protection;wire-line telecommunications}, 
doi={}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7372061, 
author={A. Wölfl and N. Siegmund and S. Apel and H. Kosch and J. Krautlager and G. Weber-Urbina}, 
booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Generating Qualifiable Avionics Software: An Experience Report (E)}, 
year={2015}, 
volume={}, 
number={}, 
pages={726-736}, 
abstract={We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.}, 
keywords={avionics;helicopters;program compilers;software engineering;qualifiable avionics software;data-management component;NH90 helicopter;Airbus helicopter;legally-binding certification process;model-based technology;product-line technology;software repository analysis;Aerospace electronics;Helicopters;System software;Interviews;Hardware;Encoding}, 
doi={10.1109/ASE.2015.35}, 
ISSN={}, 
month={Nov},}
@INBOOK{5273147, 
author={Steven M. Kaplan}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={B}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={

}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273147},}
@INPROCEEDINGS{6597883, 
author={G. Fan and H. Yu and L. Chen and D. Liu}, 
booktitle={2013 International Symposium on Theoretical Aspects of Software Engineering}, 
title={Aspect Orientation Based Test Case Selection Strategy for Service Composition}, 
year={2013}, 
volume={}, 
number={}, 
pages={95-104}, 
abstract={Software testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may select part of their test cases so that those that are more important are run earlier in the testing process. However, the methods that can be used to select test cases for service composition and its analysis are still lacking at present. This paper proposes an aspect orientation based test case selection strategy for service composition. Aspect-orientation is used to weave testing crosscutting concerns of service composition, which includes component testing concern and testing concern of service composition, the weaving mechanism dynamically integrates these schemas into a testing enforcement model. Based on this, the test cases selection strategy for service composition is given, and abstract it as a crosscutting concern to weave into testing model, the corresponding enforcement algorithm is also given, the operation semantics and related theories of Petri nets help prove its effectiveness and feasibility. A case study explains the testing process of service composition, and a series of experiments are done to explain that the use of aspects for testing Web service is more efficient than conventional techniques, which can improve the testing quality and efficiency.}, 
keywords={aspect-oriented programming;Petri nets;program testing;programming language semantics;software maintenance;Web services;aspect orientation based test case selection strategy;service composition;software testing;software maintenance;component testing;weaving mechanism;testing enforcement model;crosscutting concern;operation semantics;Petri nets;Web service;testing quality;testing efficiency;Testing;Weaving;Computational modeling;Analytical models;Semantics;Web services;Service composition; testing model; test case; aspect orientation; Petri nets}, 
doi={10.1109/TASE.2013.21}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8051361, 
author={M. Terber}, 
booktitle={2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
title={Function-Oriented Decomposition for Reactive Embedded Software}, 
year={2017}, 
volume={}, 
number={}, 
pages={288-295}, 
abstract={Due to C's overwhelming dominance in industry, reactive embedded applications usually rely on conventional sequential programming. Adopted approaches favor event-driven paradigms which prevent function-oriented code decomposition in particular. This encourages the violation of fundamental software engineering principles. The reactive programming paradigm is proposed as a general solution. However, most reactive languages cannot keep up with C's practical advantages. It appears, that the subfamily of synchronous languages provides promising features but real-world deployments and evaluations are rarely reported in literature. On this account, we make two major contributions in this paper. First, we elaborate how the lack of function-oriented software decomposition manifests in a real-life industrial application. Second, we provide a corresponding re-implementation which illustrates the deployment and discusses the gained engineering benefits provided by the third-party, synchronous-reactive programming language Céu. We believe that our work generally reveals a practicable way of improving embedded software quality in industrial applications.}, 
keywords={C language;embedded systems;programming languages;software quality;function-oriented decomposition;reactive embedded software;sequential programming;event-driven paradigms;software engineering principles;reactive programming paradigm;reactive languages;C practical advantages;synchronous languages;function-oriented software decomposition;synchronous-reactive programming language;embedded software quality;Programming;Logic gates;Software;Software engineering;Heating systems;Mirrors;Switches;synchronous reactive programming;Céu;software decomposition;software quality}, 
doi={10.1109/SEAA.2017.42}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7323164, 
author={T. C. De}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Applying model-driven development to environment monitoring System}, 
year={2015}, 
volume={}, 
number={}, 
pages={577-584}, 
abstract={Environmental monitoring is critical in understanding whether the quality of our environment is getting better or worse. Information gathered by using an environmental monitoring system is important to make decisions. Vietnam is a vulnerable country of climate change. Specially, in the South of Vietnam, the Mekong delta is known as the region getting the most impact of sea level rise in Vietnam. That leads to a lot of problems making the worst effects to residents in the area, who are mainly still very poor. On the other hand, Vietnam is going on industrialization process that makes a strong effect on the environment. To deal with these challenges, different projects of environment management have been proposed and implemented and many monitoring systems have been built in those projects. Those systems are basically sensor networks with high cost in developing and maintaining. They are related to modern technology such as cloud, communication mobile and wireless. They provide the data for large community for different purposes. Therefore, building such a system is normally a long term project that requires an incremental and modular development for a complex system. This paper, on one hand, represents some common characteristics of an environment monitoring system that requires more study to develop a formal model and a methodology for their specifications, implementations and verification. On the other hand, we would like to adapt the formal model approach proposed for Intelligent Transport Systems (ITS) to an environmental monitoring system. The framework of Baobab is also introduced as an example for transformation from model to code.}, 
keywords={environmental monitoring (geophysics);environmental science computing;environment monitoring system;applying model driven development;vulnerable country;South of Vietnam;industrialization process;environment management;intelligent transport systems;ITS;Unified modeling language;Cities and towns;Environmental monitoring;Hardware;Mobile communication;Adaptation models;Environment Management;Environment Monitoring System;Formal Method;Model-Driven Development;Sensor System}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7872763, 
author={S. R. Akbar and W. Kurniawan and M. H. H. Ichsan and I. Arwani and M. T. Handono}, 
booktitle={2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, 
title={Pervasive device and service discovery protocol in XBee sensor network}, 
year={2016}, 
volume={}, 
number={}, 
pages={79-84}, 
abstract={Internet of Things is a novel paradigm that combined microcomputer and wireless communication technology. IoT device will be considered as a pervasive and ubiquitous device that able to interact with it user and environment autonomously with minimum human intervention. At present, wireless technology already has pervasive features in the link and the network layer. They able to do the dynamic addressing, finding a neighbor and do the routing task such as in Xbee technology. In the future, pervasive sensing will support adaptive context-aware services that are not provided by the wireless sensor network protocol in the link and the network layer. Our research proposed pervasive device and service discovery protocol at the application level by creating a protocol in the smart sensor device and the smart sensor gateway. By implementing our protocol, the sensor network gateway is able to find each of the sensor network device and service descriptions and request the on-demand service to the smart sensor device. The protocol is implemented in two Arduino Uno integrated with XBee transceiver as the smart sensor device and the raspberry pi as the smart sensor gateway. Result shows, the gateway was able to find both device and service description in the smart sensor network with 4.13 seconds average time. The average round trip time for request and response data from the gateway is 0.201 seconds.}, 
keywords={intelligent sensors;Internet of Things;internetworking;protocols;wireless sensor networks;service discovery protocol;XBee sensor network;Internet of Things;microcomputer technology;wireless communication technology;ubiquitous device;wireless technology;network layer;dynamic addressing;routing task;Xbee technology;pervasive sensing;adaptive context-aware services;wireless sensor network protocol;pervasive device discovery protocol;smart sensor device;smart sensor gateway;service description;on-demand service;Arduino Uno;XBee transceiver;Raspberry Pi;smart sensor network;round trip time;request data;response data;Logic gates;Intelligent sensors;Protocols;Machine-to-machine communications;Temperature sensors;Humidity;Wireless sensor networks;Internet of Things;pervasive discovery;sensor network}, 
doi={10.1109/ICACSIS.2016.7872763}, 
ISSN={}, 
month={Oct},}
@INBOOK{6544996, 
author={G. Keith Cambron}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Integration and Innovation}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={

This chapter contains sections titled:

Technology Integration

Lifecycle Support

Invention and Innovation

Summary

References

]]>}, 
keywords={Benchmark testing}, 
doi={10.1002/9781118394519.ch13}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6544996},}
@INPROCEEDINGS{6227131, 
author={S. Thummalapenta and S. Sinha and N. Singhania and S. Chandra}, 
booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
title={Automating test automation}, 
year={2012}, 
volume={}, 
number={}, 
pages={881-891}, 
abstract={Mention “test case”, and it conjures up the image of a script or a program that exercises a system under test. In industrial practice, however, test cases often start out as steps described in natural language. These are essentially directions a human tester needs to follow to interact with an application, exercising a given scenario. Since tests need to be executed repeatedly, such manual tests then have to go through test automation to create scripts or programs out of them. Test automation can be expensive in programmer time. We describe a technique to automate test automation. The input to our technique is a sequence of steps written in natural language, and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention. The technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets, except that there can be ambiguity in identifying segments, in identifying the action in a segment, as well as in the specification of the target of the action. The technique resolves this ambiguity by backtracking, until it can synthesize a successful sequence of calls. We present an evaluation of our technique on professionally created manual test cases for two open-source web applications as well as a proprietary enterprise application. Our technique could automate over 82% of the steps contained in these test cases with no human intervention, indicating that the technique can reduce the cost of test automation quite effectively.}, 
keywords={natural languages;program testing;automating test automation;test case;natural language;programmer time;human intervention;Manuals;Automation;Optimization;Humans;Natural languages;Programming profession}, 
doi={10.1109/ICSE.2012.6227131}, 
ISSN={1558-1225}, 
month={June},}
@INPROCEEDINGS{1617602, 
author={Mingjing Chen and H. Haggag and A. Orailoglu}, 
booktitle={24th IEEE VLSI Test Symposium}, 
title={Decision tree based mismatch diagnosis in analog circuits}, 
year={2006}, 
volume={}, 
number={}, 
pages={6 pp.-285}, 
abstract={Mismatch is a critical consideration in analog circuit design. Knowledge of mismatch locations and an understanding of their impact on circuit performance are crucial for design optimization and process improvement. We present a circuit level mismatch diagnosis methodology in this paper. The functional parameters with abnormal values are measured as manifestations of mismatch, from which reverse tracing is employed to determine the mismatch source. The methodology is implemented on a representative benchmark and its efficiency confirmed by simulation results}, 
keywords={analogue circuits;decision trees;fault diagnosis;decision tree;mismatch diagnosis;analog circuit design;design optimization;process improvement;reverse tracing;mismatch source;Decision trees;Analog circuits;Design optimization;Circuit simulation;Fabrication;Degradation;Design automation;Predictive models;Circuit testing;Circuit optimization}, 
doi={10.1109/VTS.2006.26}, 
ISSN={1093-0167}, 
month={April},}
@INPROCEEDINGS{6139107, 
author={F. Belli and A. T. Endo and M. Linschulte and A. Simao}, 
booktitle={Proceedings of 2011 IEEE 6th International Symposium on Service Oriented System (SOSE)}, 
title={Model-based testing of web service compositions}, 
year={2011}, 
volume={}, 
number={}, 
pages={181-192}, 
abstract={The use of web services integrated in different applications, especially the composition of services, brings challenges for testing due to their complex interactions. In this paper, we propose an event-based approach to test web service compositions. The approach is based on event sequence graphs which we extend by facilities to consider the specific features of web service compositions. An enterprise service bus component supports the test case execution. A case study, based on a commercial web application, demonstrates the feasibility of the approach and analyzes its characteristics. The results of empirical work suggest that the approach is a promising candidate to reach a high level of confidence and reliability.}, 
keywords={program testing;Web services;model-based testing;Web service compositions;event sequence graphs;Testing;Service oriented architecture;Business;Data models;Monitoring;enterprise service bus;event sequence graphs;model-based testing;service composition testing}, 
doi={10.1109/SOSE.2011.6139107}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6908678, 
author={M. F. Granda and N. Condori-Fernández and T. E. J. Vos and O. Pastor}, 
booktitle={2014 IEEE 1st International Workshop on Requirements Engineering and Testing (RET)}, 
title={Towards the automated generation of abstract test cases from requirements models}, 
year={2014}, 
volume={}, 
number={}, 
pages={39-46}, 
abstract={In a testing process, the design, selection, creation and execution of test cases is a very time-consuming and error-prone task when done manually, since suitable and effective test cases must be obtained from the requirements. This paper presents a model-driven testing approach for conceptual schemas that automatically generates a set of abstract test cases, from requirements models. In this way, tests and requirements are linked together to find defects as soon as possible, which can considerably reduce the risk of defects and project reworking. The authors propose a generation strategy which consists of: two meta-models, a set of transformations rules which are used to generate a Test Model, and the Abstract Test Cases from an existing approach to communication-oriented Requirements Engineering; and an algorithm based on Breadth-First Search. A practical application of our approach is included.}, 
keywords={formal specification;program testing;tree searching;automated abstract test cases generation;requirements models;model-driven testing approach;project reworking;transformations rules;communication-oriented requirements engineering;breadth-first search;Unified modeling language;Testing;Abstracts;Object oriented modeling;Analytical models;Concrete;Business;Requirements-based testing;Communication Analysis;Model-driven testing;Conceptual Schema Testing;Test Model Generation;Test Case Generation}, 
doi={10.1109/RET.2014.6908678}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{5280052, 
author={H. Krapfenbauer and D. Ertl and A. Zoitl and F. Kupzog}, 
booktitle={2009 Fourth International Multi-Conference on Computing in the Global Information Technology}, 
title={Improving Component Testing of Industrial Automation Software}, 
year={2009}, 
volume={}, 
number={}, 
pages={259-262}, 
abstract={Industrial automation systems are tested nowadays mainly via system tests at a very late stage of development. These tests are conducted manually, are time-consuming and cost-intensive. Earlier testing of automation software, e.g., component testing, is therefore desired in order to reduce the effort for system testing by detecting errors sooner. In this paper we present an improved concept for a test environment that enables developers of industrial control electronics to test the functionality of IEC 61499 software components. Components can be tested on any hardware with an IEC 61499 runtime environment, even on the target hardware. There is no need to change the automation software for testing. We propose using dynamically typed languages to implement tests because such languages have inherent properties that are useful for this task. We provide example code of a typical test case.}, 
keywords={control engineering computing;IEC standards;object-oriented programming;program testing;software engineering;industrial automation software;system testing;component testing improvement;industrial automation systems;industrial control electronics;test environment;IEC 61499 software components;target hardware;dynamically typed languages;Automatic testing;Software testing;Computer industry;Automation;System testing;Electronic equipment testing;IEC standards;Hardware;Industrial control;Industrial electronics;Industrial Automation Software;IEC 61499;Component Testing;Dynamically Typed Languages}, 
doi={10.1109/ICCGI.2009.46}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4068343, 
author={}, 
journal={IEEE Std 1175.2-2006}, 
title={IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-45}, 
abstract={This recommended practice describes interconnections that need to be understood and evaluated when buying, building, testing, or using computer-aided software engineering (CASE) tools. CASE tools are developed for use in creating computing systems. By assisting users to reach a clear understanding of the context of operation for a computing system tool, this recommended practice contributes to the effective implementation and application of computing system tools. This recommended practice does not describe the processes of evaluating, acquiring, or adopting CASE tools. This recommended practice is limited to the technical aspects of CASE tools. It does not include issues in the management, marketing, or training domains.}, 
keywords={computer aided software engineering;software tools;CASE tool interconnection;computer-aided software engineering;computing system tool;IEEE standards;Context;Computer aided software engineering;Trademarks;Standards Board;Patents;Computer-Aided Software Engineering (CASE) tools;tool communications;tool interconnections}, 
doi={10.1109/IEEESTD.2007.288641}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{4610718, 
author={M. Nicolae and R. Dobrescu and M. Dobrescu and D. Popescu}, 
booktitle={2008 6th International Symposium on Communication Systems, Networks and Digital Signal Processing}, 
title={Embedded node around a DSP core for mobile sensor networks over 802.11 infrastructure}, 
year={2008}, 
volume={}, 
number={}, 
pages={643-646}, 
abstract={Digital signal processors (DSPs) are very efficient devices to implement algorithms for signal processing and analyzing. Endowing sensorial nodes from a sensor network with such processing cores it could lead to high performance because of the possibility of parallel and distribute processing and thus reducing the quantity of information spread into the network (network load). If such a node uses for communication a mature infrastructure like 802.11 standard, will be obtained a solution for implementing mobile sensor networks with high performance end cost efficient. In the same way of obtaining high performances with low costs we suggest that on the information chain from the physical quantity to the numerical result, the acquisition part to be done by an audio codec. In this way, the entire network may look like a Voice over IP (VoIP) mobile network, yet the information exchanged will not be voice but measures and commands with quality of service (QoS) inherited from VoIP.}, 
keywords={digital signal processing chips;embedded systems;Internet telephony;parallel processing;quality of service;wireless LAN;wireless sensor networks;mobile sensor networks;digital signal processors;IEEE 802.11;parallel processing;distributed processing;voice over IP;quality of service;Digital signal processing;Computer architecture;Wireless sensor networks;Codecs;Mobile communication;Mobile computing;Hardware}, 
doi={10.1109/CSNDSP.2008.4610718}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7000095, 
author={M. Staron and R. Rana and W. Meding and M. Nilsson}, 
booktitle={2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement}, 
title={Consequences of Mispredictions of Software Reliability: A Model and its Industrial Evaluation}, 
year={2014}, 
volume={}, 
number={}, 
pages={157-162}, 
abstract={Predicting reliability of software under development is an important part of estimations in software engineering projects. In many organizations as the goal is that software products are released with no known defects, the process of finding and removing defects correlates with the effort for software projects. Software development projects estimate the resources needed to design, develop, test and release software products, and the number of defects which have to be handled. In this paper we present a model for consequence analysis of inaccurate predictions of quality in software projects. The model is a result of multiple case studies and is evaluated at two companies. The model recognizes the most common mispredictions - e.g. Over- and under-prediction, early- and late-predictions - and the combination of theses. The results from the industrial evaluation show that the consequences can be grouped according to under- and over-predictions and that the late- and early-predictions have the same consequences. The results show also that mispredicting the shape of the reliability curve has a significant consequence with regard to assessment of release readiness and resource planning.}, 
keywords={project management;software management;software reliability;software reliability;industrial evaluation;software engineering project;software product;software development project;consequence analysis;software project;reliability curve;release readiness;resource planning;Software;Testing;Software reliability;Shape;Organizations;Predictive models;Software Reliability;SRGMs;Consequence;Mispredictions;Software;Forecasting}, 
doi={10.1109/IWSM.Mensura.2014.16}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6297159, 
author={L. Nagowah and K. Doorgah}, 
booktitle={2012 International Conference on Computer Information Science (ICCIS)}, 
title={Improving test data management in record and playback testing tools}, 
year={2012}, 
volume={2}, 
number={}, 
pages={931-937}, 
abstract={It is almost impossible to prevent requirement change in the web development life cycle. Selenium despite being a widely used open source automated tool for testing web application, has its limitation when it concerns test data management. Frequent changes in requirement result in changes in the user interface which in turn requires additional effort to re-record the test script. Eventually keeping track of test data used for each test script becomes very problematic for the tester. In this paper, we analyse existing tools and provide a design of an automated testing tool, Kishanium that also manages the set of test data. A prototype was created during experimentation phase to prove the concept of the underlying ideas of the proposed tool. The prototype has been implemented based on the core technologies of DomDocument, XPath and Curl. The testing carried out proves that Kishanium is a useful automated tool that can be used on its own or in conjunction with Selenium. With a very systematic approach it automatically searches input and button objects, allows testers to add new test data, edit existing test data and delete previous test data in order to respond to frequent requirement changes. The power of Kishanium is that it is able to re-use existing test data even if there are a number of changes in the user interface. It also automatically runs the tests with the appropriate set of test data using its Poster Component. Moreover the Kishanium automated tool provides additional features such as Data generator, Spylink and Snapshot.}, 
keywords={formal specification;Internet;program testing;user interfaces;test data management;record testing tool;playback testing tool;requirement change;Web development life cycle;user interface;test script;automated testing tool;Kishanium;DomDocument;XPath;Curl;Selenium;poster component;data generator;Spylink;Snapshot;Presses;Fires;Manuals;Libraries;record and playback problem;automated testing;test data management}, 
doi={10.1109/ICCISci.2012.6297159}, 
ISSN={}, 
month={June},}
@INBOOK{6542353, 
author={G. Keith Cambron}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Disasters and Outages}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Disasters

Outages

The Vicious Cycle

Summary

References

]]>}, 
keywords={Tornadoes;Earthquakes;Copper;Storms;Power cables;Bandwidth;Organizations}, 
doi={10.1002/9781118394519.ch14}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6542353},}
@INPROCEEDINGS{7321627, 
author={Z. Bicevska and J. Bicevskis and I. Oditis}, 
booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
title={Smart technologies for improved software maintenance}, 
year={2015}, 
volume={}, 
number={}, 
pages={1533-1538}, 
abstract={Steadily increasing complexity of software systems makes them difficult to configure and use without special IT knowledge. One of the solutions is to improve software systems making them “smarter”, i.e. to supplement software systems with features of self-management, at least partially. This paper describes several software components known as smart technologies, which facilitate software use and maintenance. As to date smart technologies incorporate version updating, execution environment testing, self-testing, runtime verification and business process execution. The proposed approach has been successfully applied in several software projects.}, 
keywords={software maintenance;smart technology;improved software maintenance;software systems;software components;business process execution;software projects;execution environment testing;runtime verification;Software;Information systems;Business;Built-in self-test;Runtime;Complexity theory;Autonomic computing;smart technologies;self-managing systems;software maintenance}, 
doi={10.15439/2015F170}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7878375, 
author={H. Fangohr and M. Albert and M. Franchin}, 
booktitle={2016 IEEE/ACM International Workshop on Software Engineering for Science (SE4Science)}, 
title={Nmag Micromagnetic Simulation Tool — Software Engineering Lessons Learned}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={We review design and development decisions and their impact for the open source code Nmag from a software engineering in computational science point of view. We summarise lessons learned and recommendations for future computational science projects. Key lessons include that encapsulating the simulation functionality in a library of a general purpose language, here Python, provides great flexibility in using the software. The choice of Python for the top-level user interface was very well received by users from the science and engineering community. The from-source installation in which required external libraries and dependencies are compiled from a tarball was remarkably robust. In places, the code is a lot more ambitious than necessary, which introduces unnecessary complexity and reduces main- tainability. Tests distributed with the package are useful, although more unit tests and continuous integration would have been desirable. The detailed documentation, together with a tutorial for the usage of the system, was perceived as one of its main strengths by the community.}, 
keywords={digital simulation;graphical user interfaces;micromagnetics;physics computing;public domain software;software libraries;software packages;Nmag micromagnetic simulation tool;software engineering;open source code;general purpose language library;Python;top-level user interface;external libraries;tarball;software package;unit tests;Libraries;Software;Mathematical model;Computational modeling;Software engineering;Micromagnetics;Magnetization;Nmag;Computational Science Software Engineering;Python;Finite Elements}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6949287, 
author={R. Angmo and M. Sharma}, 
booktitle={2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)}, 
title={Performance evaluation of web based automation testing tools}, 
year={2014}, 
volume={}, 
number={}, 
pages={731-735}, 
abstract={In today's 21<sup>st</sup> century era countless software applications are written as a web based application which runs in a web browsers. With new technologies and commercialization of I.T. sector, the web based system has undergoes frequent and rapid changes. Today Softwares are coded as a web based application, which help to access data from any part of the globe. Even the economic relevance of web based enhances the control and quality of software. The quality assurance of any system depends on its test. But to do manually testing in most of the cases is time consuming, expensive and hectic. For the better business purpose and to save time and money automation testing is required. There are variety of tools are available in the market for this. One of the best known tool is selenium suite which is a combination of different automation testing tool. In this paper we will discuss about the selenium suite. It provides testers with different framework for different test cases. The main objective of this paper is to find the best tool in selenium suite and then compare it with some other tool for same task. For this purpose, performance evaluation is done on the basis of some criteria.}, 
keywords={Internet;program testing;software performance evaluation;software quality;software tools;selenium suite;software quality;Web browsers;Web based application;software applications;Web based automation testing tools;performance evaluation;Automation;Performance evaluation;Browsers;Software;Software testing;Information technology;Web applications;Selenium;Performance;Watir-webdriver;Test case;Automation testing}, 
doi={10.1109/CONFLUENCE.2014.6949287}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{1559629, 
author={J. Noll and R. Steel}, 
booktitle={2005 IEEE Aerospace Conference}, 
title={EKLOPS: An Adaptive Approach to a Mission Planning System}, 
year={2005}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={EKLOPS is the Enhanced Kernel Library for Operational Planning and Scheduling. This paper discusses the area of mission planning, and present EKLOPS as a generic mission planning solution proposed by the Mission Planning Group of Anite Systems GmbH. EKLOPS has evolved from mission planning systems that were developed under contracts of the European Space Agency. It implements an adaptive object model architecture to integrate the common elements of mission planning systems. The model of a specific satellite mission is expressed as metadata, which configure the MPS. Rules implement functions of the planning process for which a number of specific roles can be identified. The paper presents a language that has so far been utilized to express constraint-checking rules. The experience made with EKLOPS is shown using the examples of the ENVISAT and Mars Express missions. The generic nature of EKLOPS facilitates an extension of its usage outside the field of spacecraft operations planning}, 
keywords={aerospace computing;artificial satellites;planning;scheduling;space research;EKLOPS;mission planning system;enhanced kernel library for operational planning and scheduling;European Space Agency;adaptive object model architecture;satellite mission;metadata;planning process;constraint-checking rules;ENVISAT;Mars Express missions;spacecraft operations planning;Satellites;Space missions;Process planning;Programmable control;Testing;Hardware;Adaptive systems;Steel;Kernel;Libraries}, 
doi={10.1109/AERO.2005.1559629}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{4040327, 
author={}, 
journal={IEEE Std P487/Draft6}, 
title={IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={This recommended practice presents engineering design practices for special high-voltage protection systems intended to protect wire-line telecommunication facilities serving electric supply locations. The following topics are included in this document: a) A description of the electric supply locations environment, i.e., ground potential rise (GPR), induced voltages, lightning, and switching transients; b)A discussion of special high-voltage protection devices; c)Definitions of service types and service performance objectives for electric supply locations telecommunication services; d)Special protection theory and philosophy; e)Special protection system design guidelines; f)Personnel safety considerations; g)Grounding; h)Cables with metallic members. Other telecommunication alternatives such as radio and optica fiber systems are excluded from this document.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{7128891, 
author={F. Wanderley and A. Silva and J. Araújo}, 
booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
title={Evaluation of BehaviorMap: A user-centered behavior language}, 
year={2015}, 
volume={}, 
number={}, 
pages={309-320}, 
abstract={In the software development process, one of the recurring problems is to ensure that the expectations of stakeholders are being met. These expectations must match the system's behavior and be present in the requirements specifications and models. The Requirements Engineering discipline studies how to capture, specify, validate and manage requirements. However, recent empirical studies show that stakeholders do not usually understand traditional requirements models. This paper focuses on the cognitive evaluation of a user-centered language called BehaviorMap that aims to specify behavioral user scenarios in a cognitive way, based on mind map modelling. This paper describes an experimental evaluation to verify the understandability of the BehaviorMap scenarios compared to the textual ones. The experiment gathered data from 15 individuals (naïve-users), with different backgrounds, that had to analyze 8 scenarios, being 4 graphical and 4 textual. To assess the participants' cognitive effort, it was used questionnaires. Also, the time effort to perform the tasks was measured. This experiment showed promising results for the BehaviorMap scenarios.}, 
keywords={cognition;formal specification;software prototyping;user centred design;BehaviorMap evaluation;user-centered behavior language;software development process;stakeholder expectation;system behavior;requirements specifications;requirements engineering;empirical analysis;cognitive evaluation;behavioral user scenarios;mind map modelling;graphical analysis;textual analysis;Data structures;Boolean functions;Visualization;Software;Atmospheric measurements;Particle measurements;Agile Requirements;Mind Map Modelling;Behavior-Driven Design;User-Centred Requirements;Cognitive Effort}, 
doi={10.1109/RCIS.2015.7128891}, 
ISSN={2151-1349}, 
month={May},}
@INPROCEEDINGS{687914, 
author={D. E. Bernard and G. A. Dorais and C. Fry and E. B. Gamble and B. Kanefsky and J. Kurien and W. Millar and N. Muscettola and P. P. Nayak and B. Pell and K. Rajan and N. Rouquette and B. Smith and B. C. Williams}, 
booktitle={1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)}, 
title={Design of the Remote Agent experiment for spacecraft autonomy}, 
year={1998}, 
volume={2}, 
number={}, 
pages={259-281 vol.2}, 
abstract={This paper describes the Remote Agent flight experiment for spacecraft commanding and control. In the Remote Agent approach, the operational rules and constraints are encoded in the flight software. The software may be considered to be an autonomous "remote agent" of the spacecraft operators in the sense that the operators rely on the agent to achieve particular goals. The experiment will be executed during the flight of NASA's Deep Space One technology validation mission. During the experiment, the spacecraft will not be given the usual detailed sequence of commands to execute. Instead, the spacecraft will be given a list of goals to achieve during the experiment. In flight, the Remote Agent flight software will generate a plan to accomplish the goals and then execute the plan in a robust manner while keeping track of how well the plan is being accomplished. During plan execution, the Remote Agent stays on the lookout for any hardware faults that might require recovery actions or replanning. In addition to describing the design of the remote agent, this paper discusses technology-insertion challenges and the approach used in the Remote Agent approach to address these challenges. The experiment integrates several spacecraft autonomy technologies developed at NASA Ames and the Jet Propulsion Laboratory: on-board planning, a robust multi threaded executive, and model-based failure diagnosis and recovery.}, 
keywords={aerospace control;space vehicles;robust control;software agents;Remote Agent experiment;spacecraft autonomy;spacecraft control;operational rules;flight software;Deep Space One technology;validation mission;robust manner;recovery actions;technology-insertion challenges;NASA;on-board planning;multi threaded executive;model-based failure diagnosis;Space vehicles;Space technology;Robustness;NASA;Humans;Propulsion;Laboratories;Space missions;Orbital robotics;Paper technology}, 
doi={10.1109/AERO.1998.687914}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{4040044, 
author={}, 
journal={IEEE Std P1073.0.1.1/D01J}, 
title={Unapproved IEEE Draft Guide for Health Informaticspoint-Of-Care Medical Device Communicationtechnical Reportguidelines for the Use of RF Wireless Technology}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{7886987, 
author={J. Kim and D. Batory and D. Dig and M. Azanza}, 
booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)}, 
title={Improving Refactoring Speed by 10X}, 
year={2016}, 
volume={}, 
number={}, 
pages={1145-1156}, 
abstract={Refactoring engines are standard tools in today's Integrated Development Environments (IDEs). They allow programmers to perform one refactoring at a time, but programmers need more. Most design patterns in the Gang-of-Four text can be written as a refactoring script - a programmatic sequence of refactorings. In this paper, we present R3, a new Java refactoring engine that supports refactoring scripts. It builds a main-memory, non-persistent database to encode Java entity declarations (e.g., packages, classes, methods), their containment relationships, and language features such as inheritance and modifiers. Unlike classical refactoring engines that modify Abstract Syntax Trees (ASTs), R3 refactorings modify only the database; refactored code is produced only when pretty-printing ASTs that reference database changes. R3 performs comparable precondition checks to those of the Eclipse Java Development Tools (JDT) but R3's codebase is about half the size of the JDT refactoring engine and runs an order of magnitude faster. Further, a user study shows that R3 improved the success rate of retrofitting design patterns by 25% up to 50%.}, 
keywords={Java;software maintenance;refactoring speed;integrated development environment;IDE;Gang-of-Four text;refactoring script;refactoring sequence;Java refactoring engine;Java entity declarations;abstract syntax trees;AST;Eclipse Java development tools;JDT;retrofitting design patterns;Java;Engines;Databases;Graphics;Computer bugs;Graphical user interfaces;Maintenance engineering}, 
doi={10.1145/2884781.2884802}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{4577708, 
author={A. Gargantini and E. Riccobene and P. Scandurra}, 
booktitle={2008 International Symposium on Industrial Embedded Systems}, 
title={A model-driven validation amp; verification environment for embedded systems}, 
year={2008}, 
volume={}, 
number={}, 
pages={241-244}, 
abstract={This paper presents a validation and verification tool component, based on the abstract state machine formal method, that we are developing to support high level formal analysis of embedded system model-driven design. This component is integrated into a model-driven environment for HW/SW co-design that provides a graphical high-level representation of HW and SW components by means of UML profiles for SystemC/multi-thread C, and allows C/C++/SystemC code generation/back-annotation from/to graphical UML models.}, 
keywords={C++ language;embedded systems;formal verification;hardware-software codesign;multi-threading;program compilers;Unified Modeling Language;model-driven validation;model-driven verification;embedded system;abstract state machine;formal method;formal analysis;model-driven design;model-driven environment;HW/SW codesign;graphical high-level representation;SystemC;multithread C language;C++ language;graphical UML model;Embedded system;Unified modeling language;Hardware;Diffusion tensor imaging;Application software;Computer architecture;Industrial control;Standardization;Job shop scheduling;Timing}, 
doi={10.1109/SIES.2008.4577708}, 
ISSN={2150-3109}, 
month={June},}
@INPROCEEDINGS{7784207, 
author={M. Sroka and D. Fisch and R. Nagy}, 
booktitle={2016 6th International Conference on Information Communication and Management (ICICM)}, 
title={Localised mutation in evolutionary test model learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={13-18}, 
abstract={The focus of this paper is on automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is described. In an experiment the impact of localised mutational changes on the evolutionary learning method is investigated.}, 
keywords={automotive engineering;evolutionary computation;learning (artificial intelligence);program testing;localised mutation;evolutionary test model learning;test case design automation;model-based testing;automotive embedded software;evolutionary algorithm;localised mutational changes;Biological cells;Sociology;Statistics;Testing;Software;Algorithm design and analysis;Software algorithms;model-based testing;evolutionary test model learning;localised mutation;reproduction operator}, 
doi={10.1109/INFOCOMAN.2016.7784207}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6575271, 
author={N. Mulkey and B. Liu and A. Medda}, 
booktitle={2013 8th International Conference on System of Systems Engineering}, 
title={The Integrated Blast Effects Sensor Suite: A rapidly developed, complex, system of systems}, 
year={2013}, 
volume={}, 
number={}, 
pages={224-228}, 
abstract={The need for rapid development of tactical system of systems solutions for military applications requires the use of system modeling techniques and simulation and validation methods to be applied throughout the lifecycle of the system. This combined approach of development and verification is preferred to traditional approaches for risk mitigation and cost effectiveness. This paper examines the Integrated Blast Effects Sensor Suite developed at the Georgia Tech Research Institute and its architecture as a complex system of systems.}, 
keywords={explosions;military equipment;risk management;sensors;systems engineering;integrated blast effect sensor suite;complex system of systems;tactical system of systems solutions;system modeling techniques;validation methods;simulation methods;risk mitigation;Georgia Tech Research Institute;Vehicles;Systems engineering and theory;Computer architecture;Databases;Complexity theory;Explosives;Data collection;System of System;SoS lifecycle;Fast Development;High Complexity}, 
doi={10.1109/SYSoSE.2013.6575271}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6511809, 
author={F. Wanderley and D. S. da Silveria}, 
booktitle={2012 Eighth International Conference on the Quality of Information and Communications Technology}, 
title={A Framework to Diminish the Gap between the Business Specialist and the Software Designer}, 
year={2012}, 
volume={}, 
number={}, 
pages={199-204}, 
abstract={Requirements Engineering establishes the process for defining requirements as one in which elicitation, modeling and analysis are tasks which must be carried out. This process should involve different stakeholders and their different viewpoints. Among these stakeholders, there is the software designer, responsible for creating models based on the information gathered by business specialists. However, this communication channel may create some "noise" that leads to information being lost. This loss produces a semantic gap between what is desired and what will be developed. The semantic gap is characterized by inconsistencies in the requirements represented by scenarios -- user stories in a behavior-driven context -- and by the conceptual model. This paper presents an interactive approach to the agile requirements modeling, thus fostering greater consistency between the artifacts of the scenarios and the conceptual model. This consistency is ensured by using a mind model specification which will serve as a basis for transforming the definitions of the scenario and generating a conceptual model represented by a UML class diagram. The mind model represents the main role of this approach, and functions as a bond that represents the business entities, thus enabling the requirements to be more consistent with the reality of the business.}, 
keywords={formal specification;software prototyping;Unified Modeling Language;business specialist;software designer;requirements engineering;semantic gap;behavior-driven context;interactive approach;agile requirements modeling;UML class diagram;mind model specification;Agile Modeling Requirements;Behaviour Driven Development;UML;Mind Map Modeling;Domain Model}, 
doi={10.1109/QUATIC.2012.9}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6115389, 
author={T. Cruz and P. Simões and J. Almeida and J. Rodrigues and E. Monteiro and F. Bastos and A. Laranjeira}, 
booktitle={2011 IEEE 36th Conference on Local Computer Networks}, 
title={How to provision and manage off-the-shelf SIP phones in domestic and SOHO environments}, 
year={2011}, 
volume={}, 
number={}, 
pages={42-49}, 
abstract={Integrated services delivered over broadband connections are becoming the norm in domestic households, as it is the case with triple-play bundles which offer combined Voice, Television and Data services delivered using IP-based technologies and protocols. As a result, the usage of SIP-based (Session Initiation Protocol) VoIP devices has known a significant growth in domestic environments, either in the form of standalone (e.g. SIP telephones) or embedded devices (as it happens with some domestic gateways, which embed analog-to-SIP adaptors). For Internet Service Providers (ISPs), the provisioning and management of those devices is a challenge - especially standalone SIP phones, since most of them were exclusively designed for corporate LAN usage, not supporting adequate mechanisms for remote management over broadband access networks. In this paper we propose a framework which allows the integration of off-the-shelf SIP phones with the CWMP protocol suite, the prevailing standard for remote management of Customer Premises Devices (CPEs) in broadband access networks. This integration framework supports the vast majority of commercially available SIP phones whilst maintaining full compatibility with the original CWMP specification - thus allowing ISPs to reuse their CWMP management infrastructure to configure and provision off-the-shelf SIP telephones.}, 
keywords={broadband networks;Internet telephony;local area networks;protocols;telecommunication network management;session initiation protocol phones;domestic environments;SOHO environments;integrated services;domestic households;triple-play bundles;voice services;television services;data services;IP-based technologies;protocols;VoIP devices;Internet Service Providers;ISP;LAN;remote management over broadband access networks;CWMP protocol suite;customer premises devices remote management;Data models;Logic gates;Servers;Protocols;Broadband communication;Local area networks;Runtime;CWMP;VoIP;SIP;Home Networks}, 
doi={10.1109/LCN.2011.6115389}, 
ISSN={0742-1303}, 
month={Oct},}
@ARTICLE{1638205, 
author={}, 
journal={IEEE Std 1100-2005 (Revision of IEEE Std 1100-1999)}, 
title={IEEE Recommended Practice for Powering and Grounding Electronic Equipment}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-703}, 
abstract={The IEEE Emerald Book(TM) presents a collection of consensus best practices for thepowering and grounding of electronic equipment used in commercial and industrial applications.The main objective is to provide consensus recommended practices in an area where conflictinginformation and conflicting design philosophies have dominated. The recommended practicesdescribed are intended to enhance equipment performance while maintaining a safe installation. Adescription of the nature and origin of power disturbances is provided, followed by theory on thevarious parameters that impact power quality. Information on quantifying and resolving power andgrounding related concerns using measurement and diagnostic instrumentation and standardizedinvestigative procedures are included. Recommended power protection equipment and wiring andgrounding system design practices are presented. Information on telecommunications systempower protection as well as grounding, industrial system grounding, and noise control is included.Finally a selection of case studies are presented to support the recommended practices presentedthroughout the book.}, 
keywords={IEEE Standards;Commercialization;Electronic equipment;Industry applications;Power conditioning;Power system quality;Monitoring;Grounding;commercial applications;electrical power;electronic equipment;grounding;industrialapplications;power conditioning;power disturbance;power monitor;power quality}, 
doi={10.1109/IEEESTD.2006.216391}, 
ISSN={}, 
month={May},}
@ARTICLE{7891866, 
author={}, 
journal={ISO/IEC/IEEE P24765/D3:2017}, 
title={ISO/IEC/IEEE Approved Draft International Standard - Systems and Software Engineering - Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-570}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;IEC Standards;ISO Standards;Terminology;Dictionaries;Software engineering}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{4400351, 
author={L. Lengyel and T. Levendovszky and G. Mezei and T. Vajk and H. Charaf}, 
booktitle={EUROCON 2007 - The International Conference on "Computer as a Tool"}, 
title={Practical Uses of Validated Model Transformation}, 
year={2007}, 
volume={}, 
number={}, 
pages={2200-2207}, 
abstract={Model-based approaches in development are widely recognized as a potential way of increasing productivity in software engineering. Model-based development is driven by model transformations that attempt to bridge the large semantic gaps between high-level models and low-level languages. There is a demand for researching the ways in which model transformation can become more flexible, efficient, highly-configurable as well as validated. This paper addresses issues of visually defined metamodel-based model transformations that support validated model transformations. We introduce our model transformation framework, visual modeling and transformation system (VMTS), and a list of applications realized with VMTS on metamodel-based model transformation basis. Furthermore, a comprehensive comparison is given related to other model transformation approaches.}, 
keywords={data models;meta data;software engineering;validated model transformation;metamodel-based model transformations;visual modeling and transformation system;VMTS;Algorithm design and analysis;Application software;Programming;Microwave integrated circuits;Productivity;Software engineering;System analysis and design;Automation;Informatics;Electronic mail;metamodel-based model transformation;graph rewriting;validated model transformation}, 
doi={10.1109/EURCON.2007.4400351}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{232773, 
author={E. Simeu and A. Puissochet and J. L. Rainard and A. M. Tagant and M. Poize}, 
booktitle={Digest of Papers. 1992 IEEE VLSI Test Symposium}, 
title={A new tool for random testability evaluation using simulation and formal proof}, 
year={1992}, 
volume={}, 
number={}, 
pages={321-326}, 
abstract={A set of tools is described, allowing one to compute random testability measurement for combinational circuits, based on a black box worst case hypothesis. These tools provide enough information to allow circuit modification, in order to meet a prescribed testability value. The efficiency of these tools is due to the use of a statistical method combined with formal proof mechanisms. The random testability of the complete ISCAS benchmark of combinational circuits is computed. For the least testable circuits, a few modifications, guided by the testability measurements, are shown to be sufficient to make them randomly testable.<<ETX>>}, 
keywords={combinatorial circuits;logic testing;simulation;statistical analysis;random testability evaluation;simulation;combinational circuits;statistical method;ISCAS benchmark;Circuit testing;Circuit faults;Computational modeling;Circuit simulation;Switching circuits;Combinational circuits;Benchmark testing;Built-in self-test;Telecommunication computing;Semiconductor device modeling}, 
doi={10.1109/VTEST.1992.232773}, 
ISSN={}, 
month={April},}
@ARTICLE{7140715, 
author={}, 
journal={IEEE Std 2030.2-2015}, 
title={IEEE Guide for the Interoperability of Energy Storage Systems Integrated with the Electric Power Infrastructure}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-138}, 
abstract={This guide applies the smart grid interoperability reference model (SGIRM) process (IEEE Std 2030-2011) to energy storage by highlighting the information relevant to energy=storage system (ESS) interoperability with the energy power system (EPS). The process can be applied to ESS applications located on customer premises, at the distribution level, and on the transmission level (i.e., bulk storage). This guide provides useful industry-derived definitions for ESS characteristics, applications, and terminology that, in turn, simplify the task of defining system information and communications technology (ICT) requirements. As a result. these requirements can be communicated more clearly and consistently in project specifications. This guide also presents a methodology that can be used for most common ESS projects to describe the power system, communications, and information technology (IT) perspectives based on the IEEE 2030 definitions. From this framework, a seemingly complex system can be more clearly understood by all project stakeholders. Emerging cybersecurity requirements can also be incorporated into the framework as appropriate. Additionally, this guide provides the templates that can be used to develop requirements for an ESS project and goes through several real-world ESS project examples step by step.}, 
keywords={energy storage;IEEE standards;inductive power transmission;smart power grids;IEEE guide;energy storage system interoperability;electric power infrastructure;smart grid interoperability reference model;SGIRM process;IEEE Std 2030-2011;ESS interoperability;energy power system;EPS;distribution level;transmission level;system information and communications technology;ICT;cybersecurity requirements;IEEE Std 2030.2-2015;IEEE Standards;Energy storage;Batteries;Smart grids;Electric power systems;Power system reliability;battery;communications technology;electric power system;energy storage system;IEEE 2030.2(TM);information technology;interoperability;power system;Smart Grid}, 
doi={10.1109/IEEESTD.2015.7140715}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7962325, 
author={O. Liechti and J. Pasquier and R. Reis}, 
booktitle={2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST)}, 
title={Supporting Agile Teams with a Test Analytics Platform: A Case Study}, 
year={2017}, 
volume={}, 
number={}, 
pages={9-15}, 
abstract={Continuous improvement, feedback mechanisms and automated testing are cornerstones of agile methods. We introduce the concept of test analytics, which brings these three practices together. We illustrate the concept with an industrial case study and describe the experiments run by a team who had set a goal for itself to get better at testing. Beyond technical aspects, we explain how these experiments have changed the mindset and the behaviour of the team members. We then present an open source test analytics platform, later developed to share the positive learnings with the community. We describe the platform features and architecture and explain how it can be easily put to use. Before the conclusions, we explain how test analytics fits in the broader context of software analytics and present our ideas for future work.}, 
keywords={feedback;program diagnostics;program testing;public domain software;software prototyping;agile teams;feedback mechanisms;automated testing;team member behaviour;open source test analytics platform;software analytics;Testing;Software;Companies;Context;Collaboration;agile development;automated testing;gamification;feedback channels}, 
doi={10.1109/AST.2017.3}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1317496, 
author={D. Batory}, 
booktitle={Proceedings. 26th International Conference on Software Engineering}, 
title={Feature-oriented programming and the AHEAD tool suite}, 
year={2004}, 
volume={}, 
number={}, 
pages={702-703}, 
abstract={Feature oriented programming (FOP) is an emerging paradigm for application synthesis, analysis, and optimization. A target application is specified declaratively as a set of features, like many consumer products (e.g., personal computers, automobiles). FOP technology translates such declarative specifications into efficient programs.}, 
keywords={software tools;object-oriented programming;feature-oriented programming;AHEAD tool suite;application synthesis;application analysis;application optimization;target application;FOP technology;declarative specifications;Automatic programming;Algebra;Application software;Java;Software engineering;Design optimization;Query processing;Large-scale systems;Domain specific languages;Prototypes}, 
doi={10.1109/ICSE.2004.1317496}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{685775, 
author={}, 
booktitle={Proceedings. Fifth International Conference on Software Reuse (Cat. No.98TB100203)}, 
title={Subject index}, 
year={1998}, 
volume={}, 
number={}, 
pages={377-388}, 
abstract={The index contains an entry for all items that appeared in this publication.}, 
keywords={}, 
doi={10.1109/ICSR.1998.685775}, 
ISSN={1085-9098}, 
month={June},}
@INPROCEEDINGS{6821183, 
author={S. Meyer and P. Healy and T. Lynn and J. Morrison}, 
booktitle={2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing}, 
title={Quality Assurance for Open Source Software Configuration Management}, 
year={2013}, 
volume={}, 
number={}, 
pages={454-461}, 
abstract={Commonly used open source configuration management systems, such as Puppet, Chef and CFEngine, allow for system configurations to be expressed as scripts. A number of quality issues that may arise when executing these scripts are identified. An automated quality assurance service is proposed that identifies the presence of these issues by automatically executing scripts across a range of environments. Test results are automatically published to a format capable of being consumed by script catalogues and social coding sites. This would serve as an independent signal of script trustworthiness and quality to script consumers and would allow developers to be made quickly aware of quality issues. As a result, potential consumers of scripts can be assured that a script is likely to work when applied to their particular environment. Script developers can be notified of compatibility issues and take steps to address them.}, 
keywords={program compilers;public domain software;software quality;automated quality assurance service;social coding sites;script catalogues;script trustworthiness;independent signal;script consumers;system configurations;open source software configuration management;Servers;Operating systems;Communities;Linux;Quality assurance;Testing;Automated configuration;configuration management;continuous integration;automated deployment;service orchestration;assurance}, 
doi={10.1109/SYNASC.2013.66}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6616325, 
author={P. Zech and M. Felderer and M. Farwick and R. Breu}, 
booktitle={2013 IEEE Seventh International Conference on Software Security and Reliability Companion}, 
title={A Concept for Language-Oriented Security Testing}, 
year={2013}, 
volume={}, 
number={}, 
pages={53-62}, 
abstract={Today's ongoing trend towards intense usage of web service based applications in daily business and everybody's daily life poses new challenges for security testing. Additionally, such applications mostly not execute in their own runtime environment but instead are deployed in some data center, run alongside multiple other applications, and serve different purposes for sundry user domains with diverging security requirements. As a consequence, security testing also has to adapt to be able to meet the necessary requirements for each application in its domain and its specific security requirements. In addition, security testing needs to be feasible for both service providers and consumers. In our paper we identify drawbacks of existing security testing approaches and provide directions for meeting emerging challenges in future security testing approaches. We also introduce and describe the idea of language-oriented security testing, a novel testing approach building upon domain-specific languages and domain knowledge to meet future requirements in security testing.}, 
keywords={high level languages;program testing;security of data;Web services;language-oriented security testing;Web service based applications;domain-specific languages;domain knowledge;software testing;Testing;Security;Business;Cloud computing;Automation;Security Testing;Domainspecific Language;Languageoriented Programming;Servicecentric Systems}, 
doi={10.1109/SERE-C.2013.16}, 
ISSN={}, 
month={June},}
@ARTICLE{8011815, 
author={M. Bernardino and E. M. Rodrigues and A. F. Zorzo and L. Marchezan}, 
journal={IET Software}, 
title={Systematic mapping study on MBT: tools and models}, 
year={2017}, 
volume={11}, 
number={4}, 
pages={141-155}, 
abstract={Every year several contributions to the model-based testing (MBT) field are published. Therefore, to follow the evolution and trends of several tools and models available is difficult. Moreover, since the variety of models and tools that became available in recent years, choosing an approach to support the MBT process is a challenging activity. The main objective of this study is to provide an overview on MBT tools and models used by those tools. Furthermore, the authors' study can help academic researchers and companies to understand the topics involving MBT. Therefore, a systematic mapping study was conducted in which 1197 distinct papers were evaluated. At the end, 87 primary studies were selected to be analysed in a quantitative and qualitative way. As a result, they classified the tools and models that are currently used to support MBT. Moreover, they identified 70 MBT tools, as well as different domains in which MBT is already applied to. Therefore, there are some evidence that MBT continues to be a broad and `alive' research field since every year a significant number of papers presenting different kinds of contributions are published.}, 
keywords={program testing;software engineering;systematic mapping;model-based testing;MBT process;software modelling;software development}, 
doi={10.1049/iet-sen.2015.0154}, 
ISSN={1751-8806}, 
month={},}
@INPROCEEDINGS{7140503, 
author={M. de Bayser and L. G. Azevedo and R. Cerqueira}, 
booktitle={2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
title={ResearchOps: The case for DevOps in scientific applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={1398-1404}, 
abstract={DevOps (a portmanteau of “development” and “operations”) is a software development method that extends the agile philosophy to rapidly produce software products and services and to improve operations performance and quality assurance. It was born to accelerate the delivery of Web-based systems and quickly bring new value to users. Many Web-based systems evolve according to usage trends without a clear long-term goal. Before the widespread use of Web services, most software with a clear goal were delivered as packages that users installed on their own system. New versions were delivered with a much lower frequency, with periods in between versions ranging from months to years. Development cycles were divided into large design, coding and testing phases culminating in the release of a new stable version. In software development in the context of applied science, even when the goal is clear, the process to attain it is not. Hence, working releases that capture the current software state must be released frequently in order to reduce the risks for all stakeholders and to make it possible to assess the current state of a project and steer it in the right direction. This paper explores the usefulness of DevOps concepts to improve the development of software that supports scientific projects. We establish the similarities and differences between scientific projects and Web applications development, and discuss where the related methodologies need to be extended. Unique challenges are discussed herewith developed solutions, and still open questions. Lessons learned are highlighted as best practices to be followed in research projects. This discussion is rooted in our experience in real-life projects at the IBM Research Brazil Lab, which just as well apply to other research institutions.}, 
keywords={program testing;project management;research and development;scientific information systems;software development management;software product lines;software prototyping;software quality;Web services;ResearchOps;DevOps;scientific applications;software development method;software products;software services;operation performance improvement;quality assurance improvement;agile software;Web-based system delivery;Web services;software packages;software development cycles;software design phase;software coding phase;software testing phase;risks reduction;project state assessent;research projects;IBM Research Brazil Lab;Software;Testing;Prototypes;Conferences;Libraries;Servers;Production}, 
doi={10.1109/INM.2015.7140503}, 
ISSN={1573-0077}, 
month={May},}
@ARTICLE{8405633, 
author={V. Garousi and M. Felderer and Ç. M. Karapiçak and U. Yilmaz}, 
journal={IEEE Software}, 
title={What We Know about Testing Embedded Software}, 
year={2018}, 
volume={35}, 
number={4}, 
pages={62-69}, 
abstract={To cost-effectively test embedded software, practitioners and researchers have proposed many test techniques, approaches, tools, and frameworks. However, obtaining an overview of the state of the art and state of the practice in this area is challenging for practitioners or new researchers. In addition, owing to an inadequate overview of what already exists in this area, some companies often reinvent the wheel by designing a test approach that's new to them but already exists. To address these problems, the authors conducted a systematic literature review of this area that covered the testing topics, testing activities, test artifacts, and industries on which the studies focused. The results can benefit both practitioners and researchers by serving as an index to the vast body of knowledge in this important, fast-growing area.}, 
keywords={embedded systems;program testing;test techniques;test approach;testing topics;test artifacts;embedded software testing;testing activities;Testing;Unified modeling language;Automation;Automotive engineering;Embedded software;software testing;embedded systems;embedded software;systematic literature mapping;systematic literature review;software engineering;software development}, 
doi={10.1109/MS.2018.2801541}, 
ISSN={0740-7459}, 
month={July},}
@INPROCEEDINGS{687918, 
author={A. S. Aljabri and D. E. Bernard and D. L. Dvorak and G. K. Man and B. Pell and T. W. Starbird}, 
booktitle={1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)}, 
title={Infusion of autonomy technology into space missions: DS1 lessons learned}, 
year={1998}, 
volume={2}, 
number={}, 
pages={315-329 vol.2}, 
abstract={The impact of infusing breakthrough autonomy technology into a flight project was a big surprise. Valuable technical and cultural lessons, many of general applicability when introducing system-level autonomy, have been learned by infusing the Remote Agent (RA) into NASA's Deep Space 1 (DS1) spacecraft. The RA's architecture embodies system-level autonomy in three major components: planning and scheduling, execution, and fault diagnosis and reconfiguration. Lessons learned include: the architecture was confirmed; active participation by nonautonomy personnel in the development is essential; communication of new concepts is essential, difficult, and hampered by differences in terminology; giving a spacecraft system-level autonomy changes organizational roles in operating the spacecraft after launch, and hence changes roles during development; software models supporting functions traditionally handled on the ground must be developed early enough to get on-board; shortfalls in planned features must be technically and developmentally accomodatable, in particular not to threaten the launch schedule; traditional commanding must be supported; testing must be emphasized. These lessons and others, on incremental system releases and use of autocode generation, are based on 16 months of spiral development from start of project through the project's decision to reduce the role of the RA from full-time control of the spacecraft to a separable experiment.}, 
keywords={space vehicles;aerospace control;software agents;scheduling;fault diagnosis;autonomy technology;space missions;DS1;Remote Agent;NASA;Deep Space 1;system-level autonomy;planning;scheduling;fault diagnosis;spacecraft system-level autonomy;software models;launch schedule;incremental system releases;autocode generation;full-time control;Space technology;Space missions;Space vehicles;Scheduling;Cultural differences;Fault diagnosis;Computer architecture;Personnel;Terminology;Communication system software}, 
doi={10.1109/AERO.1998.687918}, 
ISSN={1095-323X}, 
month={March},}
@INPROCEEDINGS{8432020, 
author={B. Eberhardinger and H. Ponsar and G. Siegert and W. Reif}, 
booktitle={2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={Case Study: Adaptive Test Automation for Testing an Adaptive Hadoop Resource Manager}, 
year={2018}, 
volume={}, 
number={}, 
pages={513-518}, 
abstract={Coping with adaptive software systems is one of the key challenges testing is currently faced with. In our previous work, we proposed to enable the test system itself to be adaptive to the system under test as a solution. The adaptation is built up on the concepts of a self-aware test automation enabling to use this information to sequence, instantiate, or update the test suite to the current situation. In our test framework the modeling language S# allows to use a run-time model to do so in a model-based testing approach. In this paper, we demonstrate how our concepts of adaptive, self-aware test automation are applied to a real world scenario: testing an adaptive resource manager of Hadoop. We show the steps necessary to implement the approach and discuss our experiences in this case study paper.}, 
keywords={data handling;parallel processing;program testing;adaptive Hadoop resource manager;adaptive software systems;self-aware test automation;test framework;run-time model;adaptive test automation;Adaptation models;Automation;Adaptive systems;Testing;Software systems;Yarn;Task analysis;Software Testing;Hadoop;Test Automation;Adaptive Testing;Adaptive Systems}, 
doi={10.1109/QRS-C.2018.00092}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7018469, 
author={O. Badreddin and A. Forward and T. C. Lethbridge}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={A test-driven approach for developing software languages}, 
year={2014}, 
volume={}, 
number={}, 
pages={225-234}, 
abstract={Test-Driven Development (TDD) is the practice of attempting to use the software you intend to write, before you write it. The premise is straightforward, but the specifics of applying it in different domains can be complex. In this paper, we provide aTDD approach for language development. The essence is to apply TDD at each of four levels of language processing, hence we call our approach Multi-Level TDD, or MLTDD. MLTDD can be applied to programming languages, preprocessors, domain specific languages, and transformation engines. MLTDD was used to build Umple, a model-oriented programming language available for Java, Ruby, and PHP. We present two case studies where this approach was implemented to develop two other domain specific languages.}, 
keywords={Testing;Syntactics;Semantics;Java;Generators;Unified modeling language;Software;Test Driven Development;Model Oriented Programming Language;UML}, 
doi={}, 
ISSN={}, 
month={Jan},}
@ARTICLE{7839172, 
author={}, 
journal={ISO/IEC/IEEE FDIS P24765:2016(E), January 2017}, 
title={ISO/IEC/IEEE Draft Systems and Software Engineering - Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-568}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;IEC Standards;ISO Standards;Dictionaries;Terminology;Software engineering}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{4730478, 
author={S. Wieczorek and A. Roth and A. Stefanescu and A. Charfi}, 
booktitle={2008 IEEE International Symposium on Service-Oriented System Engineering}, 
title={Precise Steps for Choreography Modeling for SOA Validation and Verification}, 
year={2008}, 
volume={}, 
number={}, 
pages={148-153}, 
abstract={Service-oriented architecture (SOA) enables organizations to transform their existing IT infrastructure into a more flexible business process platform. In this architecture, decoupled components that provide standard services can be composed to form individually configured and highly flexible applications. When building such applications it is important to have a formal specification of the interaction protocols between the composed services not only because such a specification provides an accurate and unambiguous description of the interactions and their ordering but also to enable automated verification and validation. In this paper, we present a case study from the SAP context showing the interactions between two SAP service components and use that case study to derive a set of modeling requirements. This motivates a discussion about applicable techniques for service choreography modeling and whether existing choreography languages cover the identified needs.}, 
keywords={program verification;software architecture;choreography modeling;SOA validation;SOA verification;service-oriented architecture;Semiconductor optical amplifiers;Service oriented architecture;Enterprise resource planning;Marketing and sales;Software systems;Systems engineering and theory;Buildings;Formal specifications;Protocols;Context-aware services;SOA;Choreography;Service;Modeling}, 
doi={10.1109/SOSE.2008.43}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8116506, 
author={I. Jimenez and S. Hamedian and J. Lofstead and C. Maltzahn and K. Mohror and R. Arpaci-Dusseau and A. Arpaci-Dusseau and R. Ricci}, 
booktitle={2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
title={Demo abstract: PopperCI: Automated reproducibility validation}, 
year={2017}, 
volume={}, 
number={}, 
pages={952-953}, 
abstract={In this demo we illustrate the usage of PopperCI [1], a continous integration (CI) service for experiments hosted at UC Santa Cruz that allows researchers to automate the end-to-end execution and validation of experiments. PopperCI assumes that experiments follow Popper [2], a convention for implementing experiments and writing articles following a DevOps approach that has been proposed recently.}, 
keywords={cloud computing;software engineering;end-to-end execution;continous integration service;automated reproducibility validation;DevOps approach;PopperCI;Tools;Measurement;Runtime;Software;Conferences;Laboratories;Guidelines}, 
doi={10.1109/INFCOMW.2017.8116506}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7363910, 
author={R. Richardet and J. Chappelier and S. Tripathy and S. Hill}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={Agile text mining with Sherlok}, 
year={2015}, 
volume={}, 
number={}, 
pages={1479-1484}, 
abstract={The successful development of an intelligent text mining application requires the collaboration of two main stakeholders: subject matter experts and text miners. In this paper, we describe a new methodology, agile text mining to improve that collaboration. Agile text mining is characterized by short development cycles, frequent tasks redefinition and continuous performance monitoring through integration tests. We introduce Sherlok, a system supporting the development of agile text mining applications and present an application to extract mention of neurons from a very large corpus of scientific articles. The resulting code and models are publicly available.}, 
keywords={data integration;data mining;scientific information systems;text analysis;Sherlok;intelligent text mining application;subject matter experts;text miners;agile text mining;frequent tasks redefinition;continuous performance monitoring;integration tests;agile text mining applications;scientific articles;Text mining;Pipelines;Ontologies;Engines;Proteins;Collaboration;natural language processing;text mining;big data;UIMA;agile data science}, 
doi={10.1109/BigData.2015.7363910}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{4031279, 
author={B. Langlois and D. Exertier and S. Bonnet}, 
booktitle={2006 10th IEEE International Enterprise Distributed Object Computing Conference Workshops (EDOCW'06)}, 
title={Performance Improvement of MDD Tools}, 
year={2006}, 
volume={}, 
number={}, 
pages={19-19}, 
abstract={From first to mature versions of Model-Driven Development (MDD) tools, there is a gap, as for any other software applications. All functional requirements must be met, including qualities of services, at the risk of seeing MDD tools rejected by users. In this paper, we focus on performance, especially for large-scale developments. After an overview of methodological elements, we give a list of reusable practices on performance. We conclude by a set of observations and stakes in order to understand where efforts must be applied during the development process.}, 
keywords={Model driven engineering;Software tools;Scalability;Unified modeling language;Aircraft;Bridges;Software performance;Application software;Quality of service;Large-scale systems}, 
doi={10.1109/EDOCW.2006.54}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4152660, 
author={}, 
journal={IEEE Unapproved Std P11073-00101/D02J, Feb 2007}, 
title={Unapproved IEEE Draft Guide for Health Informatics-Point-Of-Care Medical Device Communication-Technical Report-Guidelines for the Use of RF Wireless Technology}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{5655567, 
author={S. Wieczorek and A. Stefanescu and A. Roth}, 
booktitle={2010 Seventh International Conference on the Quality of Information and Communications Technology}, 
title={Model-Driven Service Integration Testing - A Case Study}, 
year={2010}, 
volume={}, 
number={}, 
pages={292-297}, 
abstract={This paper presents a case study for the modeling and model-based testing (MBT) of enterprise service choreographies. Our proposed MBT approach uses proprietary models called Message Choreography Models (MCM) as test models. The case study illustrates how MCM-based service integration testing allows to formalize design decisions and enables full integration into an existing industrial test infrastructure by using the concepts of domain specific languages and model transformations. Further, the MBT tools integrated into the testing framework have been compared based on one concrete use case.}, 
keywords={formal specification;program testing;model-driven service integration testing;model-based testing;enterprise service choreographies;message choreography models;industrial test infrastructure;domain specific languages;model transformations;testing framework;Testing;Business;Service oriented architecture;Generators;Unified modeling language;Data models;Context;Model-based Testing;Enterprise Systems;Service-oriented Architecture;Case Study;Service Choreographies}, 
doi={10.1109/QUATIC.2010.49}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{6151484, 
author={A. Jabeen and S. Tariq and Q. Farooq and Z. I. Malik}, 
booktitle={2011 IEEE 14th International Multitopic Conference}, 
title={A lightweight aspect modelling approach for BPMN}, 
year={2011}, 
volume={}, 
number={}, 
pages={255-260}, 
abstract={Aspectual Business Process Modelling is not a new concept in business process based development to support the separation of the cross cutting concerns. Most of the researchers use the concept of the heavyweight extensions of the business processes to incorporate the aspects. This requires changes in the meta-models of the languages and the tool infrastructures, which is not a feasible option. Some of the researchers also provide lightweight extensions in the form of profiles, but these are mostly incomplete and do not provide solutions for modelling some important aspectual concepts like Pointcuts effectively. To overcome this issue, in this paper, we provide a lightweight extension of the business processes expressed in BPMN to incorporate the aspect specific concepts in it. We propose a profile ABPMN which uses the existing notations of the BPMN models for expressing aspectual concepts. Further, we developed a language PCDL to express the pointcuts in an effective way. The language is implemented using XText in Eclipse. To evaluate the applicability of our approach, we applied it on a case study of the E-Bidding system.}, 
keywords={business data processing;corporate modelling;simulation languages;lightweight aspect modelling approach;BPMN;aspectual business process modelling;cross cutting concerns;tool infrastructures;languages meta models;Pointcuts;ABPMN;PCDL;XText;Eclipse;e-bidding system;Weaving;Aspect-oriented Modelling;BPMN;Xtext;Metamodel;ABPMN}, 
doi={10.1109/INMIC.2011.6151484}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7943664, 
author={D. Bhatt and B. Hall and A. Murugesan and D. Oglesby and E. Bush and E. Engstrom and J. Mueller and M. Pelican}, 
booktitle={2017 IEEE Aerospace Conference}, 
title={Opportunities and challenges for formal methods tools in the certification of avionics software}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-20}, 
abstract={Formal methods tools, whose underlying principles are based on mathematics and formal logic, are considered one of the most effective and rigorous means of verifying system properties and assuring the absence of undesirable system behavior. The use of such tools seem to squarely fit the needs of those aiming to develop and certify avionic software as per the DO-178C standard, the set of objectives laid out by FAA to achieve a high level of confidence on the systems. However, our recent work on a NASA-funded research project revealed that there are practical considerations and additional complexities involved in using formal method tools to provide the level of assurance as exemplified by the DO-178C. In this paper we discuss one of the key concerns with formal tools: its soundness - the characteristic of a tool to never permit the verified system property be declared true when it is actually not true. We explored two major classes of formal methods tools - namely model checkers and static analyzers - and observed several threats to their soundness such as tool fallacies and failure modes that could lead to misplaced confidence in the verified system. We present various strategies to mitigate them, including an assurance case framework to verify that potential risks are all mitigated. The intent of this paper is not to discourage but encourage scrupulous use of formal tools to certify critical avionic software by being wary of the subtle but serious issues that may be overlooked.}, 
keywords={avionics;certification;formal verification;program diagnostics;safety-critical software;formal methods tools;avionics software;formal logic;static analyzers;model checkers;DO-178C standard;Tools;Software;Analytical models;Model checking;Aerospace electronics;Safety;Algorithm design and analysis}, 
doi={10.1109/AERO.2017.7943664}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6037562, 
author={M. K. Agaram and C. Liu}, 
booktitle={2011 IEEE 15th International Enterprise Distributed Object Computing Conference}, 
title={An Engine-Independent Framework for Business Rules Development}, 
year={2011}, 
volume={}, 
number={}, 
pages={75-84}, 
abstract={There is a compelling need for highly customized Domain Specific Languages and Business Vocabulary in certain industries such as insurance, mortgage, and finance to enable Knowledge Workers to articulate and to automate complex rules pertinent to their areas of function within their companies. Rule Engine vendors attempt to provide a solution to the problem by selling an integrated Rules Engine and Business Rules Management System. Usually, the BRMS's provided by vendors need to be customized and integrated into the overall Enterprise Architecture. This results in the Enterprise Architecture to be tightly coupled with the vendor's rule offering. Moreover, it poses a significant risk to the Enterprise as vendor solutions change between releases. The Enterprise Architecture needs a way to insulate itself from such impacts. This paper describes a framework that delivers the core BRMS functions of authoring and representation in a vendor neutral fashion. In addition, the paper sheds light on specific areas of the framework that can be standardized.}, 
keywords={business data processing;insurance data processing;knowledge based systems;mortgage processing;specification languages;vocabulary;engine-independent framework;business rules development;domain specific languages;business vocabulary;insurance;mortgage;finance;rules engine;business rules management system;enterprise architecture;BRMS functions;Vocabulary;Business;Engines;Production;Dentistry;Computer architecture;Syntactics;Business Rules;Business Rules Languages;Business Rule Components;Business Vocabulary;Domain Specific Languages}, 
doi={10.1109/EDOC.2011.20}, 
ISSN={1541-7719}, 
month={Aug},}
@INPROCEEDINGS{7329720, 
author={M. Sroka and R. Nagy and D. Fisch}, 
booktitle={2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)}, 
title={Impact of mutation intensity on evolutionary test model learning}, 
year={2015}, 
volume={}, 
number={}, 
pages={271-276}, 
abstract={Automation in the software testing process has significant impact on the overall software development in industry. The focus of this paper is on automation of test case design via model-based testing for automotive embedded software. A new method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information was designed and this paper investigates the impact of mutation intensity on the evolutionary learning process.}, 
keywords={automobiles;evolutionary computation;program testing;software engineering;traffic engineering computing;mutation intensity;evolutionary test model learning;software testing process;software development;automotive embedded software;evolutionary algorithm;evolutionary learning process;Biological cells;Sociology;Statistics;Software;Testing;Evolutionary computation;Software algorithms}, 
doi={10.1109/INES.2015.7329720}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{738516, 
author={K. H. Bennett}, 
booktitle={Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272)}, 
title={Do program transformations help reverse engineering?}, 
year={1998}, 
volume={}, 
number={}, 
pages={247-254}, 
abstract={Program transformations have been advocated as a method for accomplishing reverse engineering. The hypothesis is that the original source code can be progressively transformed into alternative forms, but with the same semantics. At the end of the process, an equivalent program is acquired, but one which is much easier to understand and more maintainable. We have been undertaking an extensive programme of research over twelve years into the design and development of transformations for the support of software maintenance. The paper very briefly explains the theory, practice and tool support for transformational systems, but does not present new theoretical results. The main results are on an analysis of the strengths and weaknesses of the approach, based on experience with case studies and industrial applications. The evaluation framework used (called DERE) is that presented in Bennett and Munro (1998). It is hoped that the results will be of benefit to industry, who might be considering using the technology; and to other researchers, interested in addressing the open problems. The overall conclusion is that transformations can help in the bottom-up analysis and manipulation of source code at approximately the 3GL level, and have proved successful in code migration, but need to be complemented by other top-down techniques to be useful at higher levels of abstraction or in more ambitious re-engineering projects.}, 
keywords={reverse engineering;software maintenance;reverse engineering;program transformations;software maintenance;evaluation framework;DERE;3GL level;top-down techniques;Reverse engineering;Libraries;Computer science;Electronic mail;Electrical capacitance tomography;Computer languages;Read only memory}, 
doi={10.1109/ICSM.1998.738516}, 
ISSN={1063-6773}, 
month={Nov},}
@INPROCEEDINGS{7813841, 
author={S. Kumar and Rajkumar}, 
booktitle={2016 International Conference on Computing, Communication and Automation (ICCCA)}, 
title={Test case prioritization techniques for software product line: A survey}, 
year={2016}, 
volume={}, 
number={}, 
pages={884-889}, 
abstract={Software product line (SPL) testing is a tougher work than testing of single systems. Still testing of each individual SPL product would be perfect but it is too costly in practice. In fact, when the number of features increases then the number of possible products also increases exponentially usually derived from a feature model. Number of features is leading to thousands of different products. Due to cost and time constraints, it is infeasible or large number of effort to run all the test cases in an existing test suite. To decrease the cost of testing, various techniques have been proposed. One of them is test case prioritization (TCP) techniques. Here we presented a survey for TCP techniques for software SPL.}, 
keywords={program testing;software product lines;software product line;SPL product;feature model;test suite;test case prioritization;TCP;Software;Software product lines;Testing;Frequency modulation;Fault detection;Automation;Libraries;Software product lines;Test Case Prioritization;Variability;Commonality;Feature Model}, 
doi={10.1109/CCAA.2016.7813841}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6209957, 
author={T. Kanstrén and O. Puolitaival and V. Rytky and A. Saarela and J. S. Keränen}, 
booktitle={2012 IEEE International Conference on Industrial Technology}, 
title={Experiences in setting up domain-specific model-based testing}, 
year={2012}, 
volume={}, 
number={}, 
pages={319-324}, 
abstract={Model-based testing is a technique for generating test cases based on a model of the system under test. Typically the model is expressed in a specific notation of the test tool, using a generic notation intended to describe any system under test. In this paper we present experiences in using a domain-specific modeling layer on top of the specific model-based testing tools. This allows for easier change of the used testing tool, while providing a more familiar modeling notation in terms of the domain concepts familiar to the user. Our experiences show how this can significantly help in adopting the model-based testing approach and provide improved test results.}, 
keywords={formal specification;program testing;software tools;domain-specific model-based testing;test case generation;system under test;test tool;generic notation;domain-specific modeling layer;specific model-based testing tools;modeling notation;Generators;Encoding;Testing}, 
doi={10.1109/ICIT.2012.6209957}, 
ISSN={}, 
month={March},}
@ARTICLE{7389278, 
author={A. Bellucci and M. Romano and I. Aedo and P. Díaz}, 
journal={IEEE Pervasive Computing}, 
title={Software Support for Multitouch Interaction: The End-User Programming Perspective}, 
year={2016}, 
volume={15}, 
number={1}, 
pages={78-86}, 
abstract={The hardware development of the past years favored the widespread diffusion of multitouch devices (such as smartphones, tablets, and interactive tabletops) to such an extent that a wide variety of users are now exploiting them to perform different activities on a daily basis. In the heterogeneous and manifold context of modern computation, it is impossible to predict, at design time, all the possible configurations of such technologies, and especially the way users will be willing to interact with them. Therefore, empowering end users with tools for developing multitouch interaction is a promising step toward the materialization of ubiquitous computing. The aim of this survey is to frame the state of the art of existing multitouch software development tools from an end-user programming (EUP) perspective.}, 
keywords={haptic interfaces;software tools;ubiquitous computing;EUP perspective;multitouch software development tools;ubiquitous computing;materialization;daily basis;multitouch devices;hardware development;end-user programming perspective;multitouch interaction;software support;Software tools;Programming profession;Graphics;Complexity theory;Sensors;coding tools and techniques;ubiquitous computing;end-user programming;pervasive computing;software engineering;graphics;mobile}, 
doi={10.1109/MPRV.2016.3}, 
ISSN={1536-1268}, 
month={Jan},}
@INPROCEEDINGS{6135869, 
author={D. F. Ferguson and E. Hadar}, 
booktitle={2011 8th International Conference Expo on Emerging Technologies for a Smarter World}, 
title={Optimizing the IT business supply chain utilizing cloud computing}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Information technology applications and systems are essential to businesses and enterprises as they implement business components of the enterprise. In some cases, IT is the business, such as with financial services. Optimizing Return-on-Investment (ROI) in the IT area is essential to the business performance. Reducing cost is one component of ROI, however the predominant value is increasing revenue. IT is essential to the enterprise agilely to exploit new business opportunities. Cloud computing is emerging as a technology for optimizing IT costs and supporting agility. Enterprises are incrementally moving to cloud computing in an exploratory, ad hoc manner. Since, enterprises think in terms of IT Services that IT provides to the business, and an IT service is interconnecting hardware and software resources, the management aspects are conceptually similar to a manufacture or retail supply chain. As a result, exploiting cloud computing is a supply chain management problem for IT services using cloud computing. This paper describes the architecture requirements and implementation of a set of components for optimizing the IT supply chain using cloud computing.}, 
keywords={business data processing;cloud computing;cost reduction;investment;supply chain management;IT business supply chain optimisation;cloud computing;information technology application;financial services;return-on-investment optimisation;cost reduction;supply chain management problem;IT services;architecture requirements;Business;Optimization;Servers;Monitoring;Cloud computing;Hardware;Automation;enterprise IT;cloud archtiecture;supply chain management;composite IT system}, 
doi={10.1109/CEWIT.2011.6135869}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4599483, 
author={P. King and C. Smith}, 
booktitle={Agile 2008 Conference}, 
title={Technical lessons Learned Turning the Agile Dials to Eleven!}, 
year={2008}, 
volume={}, 
number={}, 
pages={233-238}, 
abstract={This report outlines technical lessons learnt by about 20 of Australiapsilas most experienced agile specialists over several years across several projects within an organization which aggressively applied the agile practices with much success. In these projects the agile dials were cranked to eleven to achieve very high levels of quality. Most of the specialists involved believe that they produced the highest quality software of their careers with some of the highest productivity they have ever experienced.}, 
keywords={agile manufacturing;software quality;agile dials;agile practices;software quality;Production;Radiation detectors;Complexity theory;Testing;Libraries;Productivity;Writing;Agile;Coverage;Mocking;TDD;Pair-programming}, 
doi={10.1109/Agile.2008.15}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4040125, 
author={}, 
journal={IEEE Std P1175.2/D8.0}, 
title={Unapproved IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections (Superseded by P1175.2_D11.2)}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INBOOK{6354029, 
author={Andrzej Jajszczyk}, 
booktitle={A Guide to the Wireless Engineering Body of Knowledge (WEBOK)}, 
title={Fundamental Knowledge}, 
year={2012}, 
volume={}, 
number={}, 
pages={}, 
abstract={
This chapter contains sections titled:

Introduction

Electrical and RF Engineering

Communication Engineering

Engineering Management

]]>}, 
keywords={Microwave filters;Radio frequency;RLC circuits;Filtering theory;Wireless communication;Band-pass filters;Microwave circuits}, 
doi={10.1002/9781118444221.ch7}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6354029},}
@INPROCEEDINGS{5533639, 
author={G. Piho and J. Tepandi and M. Parman and D. Perkins}, 
booktitle={The 33rd International Convention MIPRO}, 
title={From archetypes-based domain model of clinical laboratory to LIMS software}, 
year={2010}, 
volume={}, 
number={}, 
pages={1179-1184}, 
abstract={We present our approach for developing a laboratory information management system (LIMS) software by combining Björners software triptych methodology (from domain models via requirements to software) with Arlow and Neustadt archetypes and archetype patterns based initiative. The fundamental hypothesis is that through this Archetypes Based Development (ABD) approach to domains, requirements and software, it is possible to improve the software development process as well as to develop more dependable software. We use ADB in developing LIMS software for the Clinical and Biomedical Proteomics Group (CBPG), University of Leeds.}, 
keywords={information management;medical information systems;clinical laboratory;LIMS software;laboratory information management system;Bjorners software triptych methodology;archetypes based development;Clinical and Biomedical Proteomics Group;Leeds University;Laboratories;Abstracts;Programming;Production facilities;Software engineering;Proteomics;Information management;Software systems;Application software;Customer relationship management}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4813952, 
author={F. Borchardt}, 
booktitle={2009 IET Smart Metering - Making It Happen}, 
title={Meeting the operational and logistical challenges of smart meter roll-out - the European experience (“It's not just hanging meters on walls”)}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-20}, 
abstract={A collection of slides on meeting the operational and logistical challenges of smart meter roll-out by Frank Borchardt, was given.}, 
keywords={automatic meter reading;logistics;power system management;operational challenge;logistical challenge;smart meter roll out;smart metering;business process}, 
doi={}, 
ISSN={0537-9989}, 
month={Feb},}
@ARTICLE{8352077, 
author={K. Rubinov and L. Baresi}, 
journal={Computer}, 
title={What Are We Missing When Testing Our Android Apps?}, 
year={2018}, 
volume={51}, 
number={4}, 
pages={60-68}, 
abstract={Android's broad adoption drives the development of millions of new apps. Apps on this OS are not just trivial games; many of them handle sensitive information, exhibit complex structure, and require high reliability and trustworthiness. The authors discuss the problem of testing Android apps-the results achieved with current approaches, and what is still missing and requires fresh solutions.}, 
keywords={mobile computing;program testing;sensitive information;trustworthiness;reliability;Android apps testing;Android adoption;Software testing;Androids;Humanoid robots;Software development;Graphical user interfaces;Analytical models;Runtime;Computer applications;Android;mobile;software testing;mobile computing;mobile applications;debugging}, 
doi={10.1109/MC.2018.2141024}, 
ISSN={0018-9162}, 
month={April},}
@INPROCEEDINGS{6405436, 
author={J. Lauret and H. Waeselynck and J. Fabre}, 
booktitle={2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops}, 
title={Detection of Interferences in Aspect-Oriented Programs Using Executable Assertions}, 
year={2012}, 
volume={}, 
number={}, 
pages={165-170}, 
abstract={Aspect-oriented programming (AOP) is a technique that promotes separation of concerns. Unfortunately, it still suffers from well-known composition issues, in particular from undesirable interferences when multiple concerns are applied at the same join point. In this paper we propose an approach to detect interferences side effect using executable assertions. The assertions are inserted in the aspect chain to detect various types of interferences. The implementation is based on the AIRIA resolver construct, recently introduced to better control conflicting aspects in AspectJ. Resolvers add observation points that were lacking in AspectJ. We propose to take advantage of this to implement automated detection of interferences at execution time. We study the feasibility of this approach and demonstrate it on artificial examples.}, 
keywords={aspect-oriented programming;executable assertions;aspect-oriented programming technique;AOP technique;aspect chain;AIRIA resolver construct;AspectJ;observation points;automatic interference detection;execution time;Interference;Monitoring;Instruments;Weaving;Encryption;Data structures;Programming;Aspect interference;executable assertions;verification}, 
doi={10.1109/ISSREW.2012.34}, 
ISSN={}, 
month={Nov},}
@ARTICLE{8438923, 
author={P. Parra and O. R. Polo and J. Fernandez and A. Da Silva and S. Sanchez Prieto and A. Martinez}, 
journal={IEEE Transactions on Emerging Topics in Computing}, 
title={A Platform-Aware Model-Driven Embedded Software Engineering Process Based on Annotated Analysis Models}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={In this work a platform-aware model-driven engineering process for building component-based embedded software systems using annotated analysis models is described. The process is supported by a framework, called MICOBS, that allows working with different component technologies and integrating different tools that, independently of the component technology, enable the analysis of non-functional properties based on the principles of composability and compositionality. An actor, called Framework Architect, is responsible for this integration. Three other actors take a relevant part in the analysis process. The Component Provider supplies the components, while the Component Tester is in charge of their validation. The latter also feeds MICOBS with the annotated analysis models that characterize the extra-functional properties of the components for the different platforms on which they can be deployed. The Application Architect uses these components to build new systems, performing the trade-off between different alternatives. At this stage, and in order to verify that the final system meets the extra-functional requirements, the Application Architect uses the reports generated by the integrated analysis tools. This process has been used to support the validation and verification of the on-board application software for the Instrument Control Unit of the Energetic Particle Detector of the Solar Orbiter mission.}, 
keywords={Analytical models;Tools;Biological system modeling;Computational modeling;Embedded software;Measurement;Component-based Software Engineering;Model-Driven Development;On-board Software;Schedulability Analysis}, 
doi={10.1109/TETC.2018.2866024}, 
ISSN={2168-6750}, 
month={},}
@INPROCEEDINGS{6114166, 
author={C. Ioannides and G. Barrett and K. Eder}, 
booktitle={2011 IEEE International High Level Design Validation and Test Workshop}, 
title={Introducing XCS to Coverage Directed test Generation}, 
year={2011}, 
volume={}, 
number={}, 
pages={57-64}, 
abstract={Coverage Directed test Generation (CDG) is rife with challenges and problems, despite the relative successes of machine learning methodologies over the years in automating it. This paper introduces the use of the eXtended Classifier System (XCS) in simulation-based digital design verification. It argues for the use of this novel genetics-based machine learning technique to perform effective CDG by learning the full mapping between coverage results and test generator directives. Using the resulting production rules, efficient test suites can be constructed, and inference on the validity of the verification environment can be made. There is great potential in using XCS for design verification and this paper forms an initial attempt to highlight the associated advantages. The technique requires no domain knowledge to setup and satisfies important CDG requirements. Once matured, it is expected to be utilized seamlessly in any industrial level simulation-based verification process.}, 
keywords={formal verification;learning (artificial intelligence);pattern classification;XCS;coverage directed test generation;machine learning methodologies;extended classifier system;simulation-based digital design verification;genetics-based machine learning technique;CDG requirements;industrial level simulation-based verification process;Generators;Fires;Pipelines;Genetic algorithms;Machine learning;Measurement;Learning systems;Electronic Design Automation and Methodology;Digital Simulation;Learning Systems;Learning Classifier Systems;XCS}, 
doi={10.1109/HLDVT.2011.6114166}, 
ISSN={1552-6674}, 
month={Nov},}
@ARTICLE{5525316, 
author={T. Schavey and S. Duba}, 
journal={IEEE Aerospace and Electronic Systems Magazine}, 
title={Streamlining IMA integration through model-driven methodologies}, 
year={2010}, 
volume={25}, 
number={6}, 
pages={21-24}, 
abstract={Avionics systems integration is an inherently complex undertaking. In addition to ensuring that basic functionality is satisfied, the systems integrator must maximize the system's flexibility and reliability while minimizing weight and cost of change. With the introduction of integrated modular architectures (IMA) based on open standards, many traditional integration issues have been greatly improved. However, additional integration responsibilities arise due to having a large number of functions developed by independent suppliers all sharing the same physical resources.}, 
keywords={aerospace computing;avionics;computer architecture;military aircraft;avionics systems integration;integrated modular architectures;integration issues;model-driven integration;streamline;virtual integration;IMA;commercial aircraft;military aircraft;Aerospace electronics;Cost function}, 
doi={10.1109/MAES.2010.5525316}, 
ISSN={0885-8985}, 
month={June},}
@INPROCEEDINGS{6581637, 
author={H. Neufeldt and S. Stanzel}, 
booktitle={2013 14th International Radar Symposium (IRS)}, 
title={An operational WAM in frankfurt airspace}, 
year={2013}, 
volume={2}, 
number={}, 
pages={561-566}, 
abstract={Extending the capacity of Frankfurt airport with an additional runway to the north of the existing runway system, the surveillance capability in Frankfurt Terminal Maneuvering Area (TMA) had to be adapted as well. Based on their surveillance strategy, the DFS invested in multilateration technology to establish a Precision Approach Monitor (PAM) system and integrate it into the existing surveillance infrastructure. After a phase of thorough planning and preparation and an open tender process, Thales was contracted to implement the PAM FRA system which is now going into operation. This paper reports the successful implementation and testing of an operational WAM system in one of the most congested airspaces of the world. Transponder anomalies found as well as methods and strategies to achieve required performances are presented.}, 
keywords={airports;distributed sensors;surveillance;transponder anomaly;Precision Approach Monitor system;Frankfurt terminal maneuvering area;surveillance capability;runway system;Frankfurt airport;Frankfurt airspace;wide area multilateration technology;WAM;Surveillance;Airports;Accuracy;Radar tracking;Transponders;Receivers}, 
doi={}, 
ISSN={2155-5745}, 
month={June},}
@INBOOK{6671244, 
author={Stuart Jacobs}, 
booktitle={Security Management of Next Generation Telecommunications Networks and Services}, 
title={Index}, 
year={2014}, 
volume={}, 
number={}, 
pages={}, 
abstract={
No abstract.

}, 
keywords={}, 
doi={10.1002/9781118741580.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6671244},}
@INPROCEEDINGS{658216, 
author={D. M. Murphy and D. M. Allen}, 
booktitle={IECEC-97 Proceedings of the Thirty-Second Intersociety Energy Conversion Engineering Conference (Cat. No.97CH6203)}, 
title={SCARLET development, fabrication, and testing for the Deep Space 1 spacecraft}, 
year={1997}, 
volume={4}, 
number={}, 
pages={2237-2245 vol.4}, 
abstract={An advanced version of "Solar Concentrator Arrays with Refractive Linear Element Technology" (SCARLET) is being assembled for use on the first NASA/JPL New Millennium spacecraft: Deep Space One (DS1). The array is scaled up from the first SCARLET array that was built for the METEOR satellite in 1995 and incorporates advanced technologies such as dual-junction solar cells and an improved structural design. Due to the failure of the Conestoga launch vehicle, this will be the first flight of a modular concentrator array. SCARLET will provide 2.6 kW to the DS1 spacecraft to be launched in July 1998 for a mission that includes fly-bys of the asteroid McAuliffe, Mars and the comet West-Kohoutek-Ikemura. This paper describes the SCARLET design, fabrication/assembly and testing program for the flight system.}, 
keywords={space vehicles;space vehicle power plants;solar cell arrays;solar energy concentrators;photovoltaic power systems;Deep Space One spacecraft;SCARLET array;Solar Concentrator Arrays with Refractive Linear Element Technology;development;fabrication;testing;NASA/JPL New Millennium spacecraft;dual-junction solar cells;structural design;modular concentrator array;assembly;testing programme;2.6 kW;Fabrication;Testing;Space technology;Space vehicles;Building integrated photovoltaics;Assembly;NASA;Satellites;Photovoltaic cells;Mars}, 
doi={10.1109/IECEC.1997.658216}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8411751, 
author={R. Pröll and B. Bauer}, 
booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={A Model-Based Test Case Management Approach for Integrated Sets of Domain-Specific Models}, 
year={2018}, 
volume={}, 
number={}, 
pages={175-184}, 
abstract={Due to rapid improvements in the area of embedded processing hardware, the complexity of developed systems constantly increases. In order to ensure a high quality level of such systems, related quality assurance concepts have to evolve. The introduction of Model-Based Testing (MBT) approaches has shown promising results by automating and abstracting multiple activities of the software testing life cycle. Nevertheless, there is a strong need for approaches supporting scoped test models, i.e. subsets of test cases, reflecting specific test purposes driven by risk-oriented development strategies. Therefore, we developed an integrated and model-based approach supporting test case management, which incorporates the beneficial aspects of abstract development methodologies with predominant research for test case management in non-model-based scenarios. Based on a new model artifact, the integration model, tasks like cross-domain information mapping and the integration of domain-specific KPIs derived by analyses favor the subsequently applied constraint-based mechanism for test case management. Further, a prototypical implementation of these concepts within the Architecture And Analysis Framework (A3F) is elaborated and further evaluated based on representative application scenarios. A comparative view on related work leads to a conclusive statement regarding our future work.}, 
keywords={program testing;quality assurance;software architecture;software quality;test cases;specific test purposes;risk-oriented development strategies;integrated model-based;abstract development methodologies;nonmodel-based scenarios;model artifact;integration model;cross-domain information mapping;domain-specific KPIs;subsequently applied constraint-based mechanism;Model-Based test case management approach;integrated sets;domain-specific models;embedded processing hardware;developed systems;high quality level;related quality assurance concepts;automating activities;abstracting multiple activities;software testing life cycle;scoped test models;MBT;Analytical models;Data models;Context modeling;Software;Software testing;Model-Based Testing;Test Case Management;Test Selection;Test Prioritization;Test Suite Reduction;Test Model Scoping}, 
doi={10.1109/ICSTW.2018.00048}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{5381934, 
author={C. Xin}, 
booktitle={2009 International Conference on Wireless Networks and Information Systems}, 
title={Wireless Communications Trends}, 
year={2009}, 
volume={}, 
number={}, 
pages={278-281}, 
abstract={This paper reviews the wireless communications roadmap, and discusses the trends of wireless communications, including: higher and higher data rates, ubiquity of wireless devices, smart antennas, faster, smaller, cheaper hardware, frequency congestion, and multiple-input, multiple-output systems. Finally, this paper discusses the 4G wireless evolution.}, 
keywords={4G mobile communication;adaptive antenna arrays;MIMO communication;wireless communications roadmap;4G wireless evolution;smart antennas;frequency congestion;multiple-input multiple-output systems;Wireless communication;Internet telephony;Cable TV;Communication cables;Land mobile radio;Business;Cellular phones;Web and internet services;Hardware;Wires;wireless communication;4G;telecommunication;evolution}, 
doi={10.1109/WNIS.2009.53}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7923802, 
author={R. R. d. Oliveira and R. M. Martins and A. d. S. Simao}, 
booktitle={2017 IEEE International Conference on Cloud Engineering (IC2E)}, 
title={Impact of the Vendor Lock-in Problem on Testing as a Service (TaaS)}, 
year={2017}, 
volume={}, 
number={}, 
pages={190-196}, 
abstract={Testing as a Service (TaaS) is a new business and service model that provides efficient and effective software quality assurance and enables the use of a cloud for the meeting of quality standards, requirements and consumer's needs. However, problems that limit the effective use of TaaS involve lack of standardization in writing, execution, configuration and management of tests and lack of portability and interoperability among TaaS platforms - the so-called lock-in problem. The lock-in problem is a serious threat to software testing in the cloud and may become critical when a provider decides to suddenly increase prices, or shows serious technical availability problems. This paper proposes a novel approach for solving the lock-in problem in TaaS with the use of design patterns. The aim to assist software engineers and quality control managers in building testing solutions that are both portable and interoperable and promote a more widespread adoption of the TaaS model in cloud computing.}, 
keywords={cloud computing;object-oriented programming;open systems;program testing;quality assurance;software quality;vendor lock-in problem;testing as a service;TaaS;software quality assurance;quality standards;software testing;design patterns;interoperability;cloud computing;Cloud computing;Testing;Computational modeling;Interoperability;Browsers;Context;Cloud Computing;Testing as a Service (TaaS);Design Patterns;Vendor Lock-in;Testing Service}, 
doi={10.1109/IC2E.2017.30}, 
ISSN={}, 
month={April},}
@ARTICLE{531663, 
author={}, 
journal={IEEE Std 743-1995}, 
title={IEEE Standard Equipment Requirements and Measurement Techniques for Analog Transmission Parameters for Telecommunications}, 
year={1996}, 
volume={}, 
number={}, 
pages={i-}, 
abstract={Performance requirements for test equipment that measures the analog transmission parameters of subscriber loops, message trunks, PBX trunks, and ties lines are specified. Requirements for these measurements with DS1 bit stream access are also provided. The measurement of loss, noise, and impulse noise on non-loaded cable pairs used for digital subscriber lines is also addressed.}, 
keywords={telecommunication standards;nonloaded cable pairs;measurement techniques;IEEE standard;IEEE Std 743-1995;test equipment;analog transmission parameters;subscriber loops;message trunks;PBX trunks;ties lines;DS1 bit stream access;loss;noise;impulse noise;digital subscriber lines;Communication standards}, 
doi={10.1109/IEEESTD.1996.81076}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{7081882, 
author={M. Laverdière and B. J. Berger and E. Merloz}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Taint analysis of manual service compositions using Cross-Application Call Graphs}, 
year={2015}, 
volume={}, 
number={}, 
pages={585-589}, 
abstract={We propose an extension over the traditional call graph to incorporate edges representing control flow between web services, named the Cross-Application Call Graph (CACG). We introduce a construction algorithm for applications built on the Jax-WS standard and validate its effectiveness on sample applications from Apache CXF and JBossWS. Then, we demonstrate its applicability for taint analysis over a sample application of our making. Our CACG construction algorithm accurately identifies service call targets 81.07% of the time on average. Our taint analysis obtains a F-Measure of 95.60% over a benchmark. The use of a CACG, compared to a naive approach, improves the F-Measure of a taint analysis from 66.67% to 100.00% for our sample application.}, 
keywords={data flow analysis;flow graphs;Web services;manual service compositions;control flow;Web services;cross-application call graph;Jax-WS standard;Apache CXF;JBossWS;taint analysis;CACG construction algorithm;service call targets;F-measure;Web services;Benchmark testing;Java;Security;Manuals;Algorithm design and analysis;Androids}, 
doi={10.1109/SANER.2015.7081882}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{1316702, 
author={J. Sprinkle}, 
booktitle={Proceedings. 11th IEEE International Conference and Workshop on the Engineering of Computer-Based Systems, 2004.}, 
title={Improving CBS tool development with technological spaces}, 
year={2004}, 
volume={}, 
number={}, 
pages={218-224}, 
abstract={The complexity of computer based systems (CBSs) requires that multiple levels of abstraction be available to a designer in order to facilitate their formal specification. Generating final executable code from the model of the system is preferred to hand-coding the implementation, but this is seldom done in one step-usually there are several cascading transformations that eventually result in the executable system. We explain how the concept of the technological space (TS) can be used to define and describe the layers between cascading transformations, and the transformations themselves. TSs are also shown as a categorization that better distinguishes between a domain and the technology used to store information in a domain.}, 
keywords={formal specification;software metrics;Unified Modeling Language;software architecture;computer aided software engineering;software tools;CBS tool development;computer based system;formal specification;hand-coding implementation;cascading transformation;technological space;Space technology;Software engineering;Software systems;Systems engineering and theory;Computer architecture;Software performance;Programming;Design engineering;Object oriented modeling;Large-scale systems}, 
doi={10.1109/ECBS.2004.1316702}, 
ISSN={}, 
month={May},}
@ARTICLE{4447435, 
author={}, 
journal={IEEE Unapproved Draft Std P2600_D33b, Feb 2008}, 
title={IEEE Draft Standard for Information Technology: Hardcopy Device and System Security}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={This standard defines security requirements (all aspects of security including but not limited to authentication, authorization, privacy, integrity, device management, physical security and information security) for manufacturers, users and others on the selection, installation, configuration and usage of hardcopy devices and systems including printers, copiers, and multifunction devices and the computer systems that support these devices. This standard identifies security exposures for these hardcopy devices and systems and instructs manufacturers and software developers on appropriate security capabilities to include in their devices and systems and instructs users on appropriate ways to use these security capabilities.}, 
keywords={copier;hardcopy device;information security;printer;multifunction device;MFD;MFP;All-in-One;scanner;HCD;multifunction product;fax;facsimile;hardcopy security}, 
doi={}, 
ISSN={}, 
month={},}
@INBOOK{5273597, 
author={Frank Hargrave}, 
booktitle={Hargrave's Communications Dictionary}, 
title={Index}, 
year={2001}, 
volume={}, 
number={}, 
pages={}, 
abstract={
}, 
keywords={Dictionaries;Indexes;Communication systems}, 
doi={10.1109/9780470544822.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={}, 
url={https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5273597},}
@INPROCEEDINGS{8502650, 
author={K. Meixner and S. Biffl and D. Winkler}, 
booktitle={2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Towards Flexible and Automated Testing in Production Systems Engineering Projects}, 
year={2018}, 
volume={1}, 
number={}, 
pages={169-176}, 
abstract={Automated and systematic testing of automation systems (AS) and production systems (PS) require an integrated testing tool chain for test case development, execution and reporting. In practice, the test automation tool chain cannot be fully automated because of missing links between different tools used in the test automation process. Closing these gaps typically require (high) human effort. Furthermore, domain and software testing expertise is often bundled by one (expensive) engineer who is responsible for the application domain (reflected in use cases and test cases) and software tests (software test code). This paper presents a flexible Testing Automation Framework (TAF) that enables the configuration of test processes involving different tools and various layers for test automation and enables separated roles for the application domain and software tests. We build on best-practice test automation from Software Engineering and design a test automation process for the automation systems domain. We demonstrate the feasibility with a use case, derived from production systems automation, with selected tools covering all test automation layers. First results showed the feasibility of the framework in the evaluation use case making test processes more flexible and automated. Although the successful implementation of the TAF can support the efficient configuration and execution of test processes, there is additional effort for preparing the flexible and automated tool chain.}, 
keywords={Automation;Tools;Testing;Software;Production systems;Unified modeling language;Software engineering;software and system testing;test automation framework;automation systems;production systems;test configuration;feasibility study}, 
doi={10.1109/ETFA.2018.8502650}, 
ISSN={1946-0759}, 
month={Sept},}
@ARTICLE{8360943, 
author={K. Gallaba and S. McIntosh}, 
journal={IEEE Transactions on Software Engineering}, 
title={Use and Misuse of Continuous Integration Features: An Empirical Study of Projects that (mis)use Travis CI}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Continuous Integration (CI) is a popular practice where software systems are automatically compiled and tested as changes appear in the version control system of a project. Like other software artifacts, CI specifications require maintenance effort. Although there are several service providers like Travis CI offering various CI features, it is unclear which features are being (mis)used. In this paper, we present a study of feature use and misuse in 9,312 open source systems that use Travis CI. Analysis of the features that are adopted by projects reveals that explicit deployment code is rare&#x2014;48.16% of the studied Travis CI specification code is instead associated with configuring job processing nodes. To analyze feature misuse, we propose Hansel&#x2014;an anti-pattern detection tool for Travis CI specifications. We define four anti-patterns and Hansel detects anti-patterns in the Travis CI specifications of 894 projects in the corpus (9.60%), and achieves a recall of 82.76% in a sample of 100 projects. Furthermore, we propose Gretel&#x2014;an anti-pattern removal tool for Travis CI specifications, which can remove 69.60% of the most frequently occurring anti-pattern automatically. Using Gretel, we have produced 36 accepted pull requests that remove Travis CI anti-patterns automatically.}, 
keywords={Tools;Organizations;Software;Computer languages;Control systems;Electronic mail;Feature extraction}, 
doi={10.1109/TSE.2018.2838131}, 
ISSN={0098-5589}, 
month={},}
@ARTICLE{898825, 
author={D. MacMillen and R. Camposano and D. Hill and T. W. Williams}, 
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
title={An industrial view of electronic design automation}, 
year={2000}, 
volume={19}, 
number={12}, 
pages={1428-1448}, 
abstract={The automation of the design of electronic systems and circuits [electronic design automation (EDA)] has a history of strong innovation. The EDA business has profoundly influenced the integrated circuit (IC) business and vice-versa. This paper reviews the technologies, algorithms, and methodologies that have been used in EDA tools and the business impact of these technologies. In particular, we focus on four areas that have been key in defining the design methodologies over time: physical design, simulation/verification, synthesis, and test. We then look briefly into the future. Design will evolve toward more software programmability or some other kind of field configurability like field programmable gate arrays (FPGAs). We discuss the kinds of tool sets needed to support design in this environment.}, 
keywords={formal verification;high level synthesis;integrated circuit design;circuit CAD;network routing;circuit simulation;electronic design automation;history;EDA tools;design methodologies;physical design;simulation;verification;synthesis;software programmability;field configurability;field programmable gate arrays;Electronics industry;Electronic design automation and methodology;Integrated circuit technology;Field programmable gate arrays;Design automation;History;Technological innovation;Design methodology;Circuit simulation;Circuit synthesis}, 
doi={10.1109/43.898825}, 
ISSN={0278-0070}, 
month={Dec},}
@ARTICLE{6770564, 
author={P. F. DeDuck and S. R. Johnson}, 
journal={AT T Technical Journal}, 
title={The FT-2000 OC-48 lightwave system}, 
year={1992}, 
volume={71}, 
number={1}, 
pages={14-22}, 
abstract={Next generation terrestrial lightwave terminals must do more than transport digital information from one location to another. FT-2000, AT&T's newest high-capacity lightwave transmission system, is designed to meet the needs of customers into the next century. It combines a flexible hardware platform and a powerful software-based architecture. As an intelligent lightwave system, FT-2000 can operate in sophisticated self-healing networks, and is managed by an advanced control system that simplifies installing, provisioning, monitoring, and maintaining it. It is fully compliant with the American National Standards Institute (ANSI) optical interface standard, the Synchronous Optical Network (SONET). We explore the broad range of applications and customer needs that drove the specification of FT-2000, and present the architectural solution that achieves the flexibility to meet those specifications.}, 
keywords={}, 
doi={10.1002/j.1538-7305.1992.tb00143.x}, 
ISSN={8756-2324}, 
month={Jan},}
@INPROCEEDINGS{7374941, 
author={P. Suthar and M. Stolic}, 
booktitle={2015 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob)}, 
title={Carrier grade Telco-Cloud}, 
year={2015}, 
volume={}, 
number={}, 
pages={101-107}, 
abstract={The Telco service providers business is undergoing a fundamental shift, and operators are transforming their network to meet new business challenges. Biggest focus area for Telco Cloud is rapid time to market (TTM) for new services and reduction in total cost of ownership (TCO). Telco service providers are facing challenges from web and content providers because of agility and convergence of voice and data. Telco Cloud is transformation of traditional wireline, wireless, voice, text, data, and web etc. services to common compute cloud infrastructures. Cloud infrastructures can be set-up on-premise, off-premise or hybrid based upon service level agreements (SLA), security and maturity of services. Key component of Telco Cloud is IP Multimedia System (IMS) which provides convergence of voice, data, video, multimedia messaging etc. Designing and developing Telco Cloud, which meets criteria of “carrier grade”, is very important to gain confidence and comfort level of different stakeholders. This paper discusses design and deployment criteria for building high quality Telco Cloud.}, 
keywords={cloud computing;electronic messaging;IP networks;multimedia communication;carrier grade Telco-cloud infrastructure;time to market;total cost of ownership reduction;Telco service provider;service level agreement;IP multimedia system;service maturity;service security;voice convergence;data convergence;video convergence;multimedia messaging convergence;Cloud computing;Business;Logic gates;Mobile communication;Hardware;Wireless communication}, 
doi={10.1109/APWiMob.2015.7374941}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4559609, 
author={}, 
journal={IEEE Unapproved Draft Std P11073-00101/D5, Jun 2008}, 
title={IEEE Draft Health Informatics - Point-Of-Care Medical Device Communication - Technical Report - Guidelines for the Use of RF Wireless Technology}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={The following Guidance document addresses the use of radio frequency 1 (RF) wireless technology for the transport of medical data both to and from point-of-care (PoC) medical devices. The context of such wireless medical data transport can range from home- or mobile-based healthcare to in hospital ambulatory and stationary situations. The intent of the guidance document is to be global with respect to wireless spectrum and equipment, although working group participation and expertise have favored detail of scenarios from the US. At the time of this Guidance document several applicable RF wireless technologies exist with a range of capabilities and characteristics, and in different stages of maturity, standardization, and adoption in healthcare. It is recognized that RF technologies are rapidly evolving, and new options may become available (or sufficiently established) after the publication of this Guidance document. The recommendations, therefore, avoid being overly prescriptive and instead attempt to assist medical device manufacturers, wireless equipment manufacturers, healthcare providers, government agencies and any other end-user of this document to make reasonable judgments regarding performance and practical implementation of wireless solutions. The Guidance document defines specific use cases to estimate, compare, and contrast performance of known technologies operating on wireless personal area (WPAN), wireless local area (WLAN), wireless metropolitan area (WMAN), and wireless wide area (WWAN) networks. Major considerations are 1) the quality-of-service (QoS) requirements (reliability, latency, priority, bandwidth) associated with the data being transported, 2) the expected performance (power, link range, throughput, link establishment and maintenance) of the wireless technology, and 3) the specific needs and resources of the end user. Related issues include network architecture, EMI/EMC, coexistence with other data streams, security, cost, power consumption, and technology configurability. Performance summaries for specific wireless technologies that support defined use cases are not intended as an endorsement of optimal solution because different needs, resources, sizes, and environments cannot be comprehensively addressed. This overview document is meant to be a foundation and reference for several follow-on IEEE 11073.3.5.x standards that will profile specific classes of off-the-shelf RF wireless technologies for medical data transport. Importantly, this guidance document is not envisioned to be periodically updated, but instead will act as a source of information for the follow-on IEEE 11073-0305.x standards that will supplant it. Periodic updates will be performed on the IEEE 11073-0305.x standards only.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{4138229, 
author={R. Sombrutzki and A. Zubow and M. Kurth and J. Redlich}, 
booktitle={2006 1st Workshop on Operator-Assisted (Wireless Mesh) Community Networks}, 
title={Self-Organization in Community Mesh Networks The Berlin RoofNet}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-11}, 
abstract={A community network must be usable for inexperienced end users; thus self-organization is essential. On the one hand, we propose an approach for self-organization in ad-hoc wireless multi-hop mesh networks, where the client is fully freed from such mundane tasks as IP configuration, etc. On the other hand, the community mesh network itself is fully self-organized thus no operator or provider is required. We present the architecture of the Berlin RoofNet (BRN) and a distributed realization of services like DHCP, ARP and Internet gateway discovery and selection. In addition, results of a detailed simulation and experimental evaluation comparing our distributed hash table based approach to traditional methods are presented. We show that our approach is more reliable, efficient and responsive}, 
keywords={ad hoc networks;Internet;protocols;telecommunication network topology;self-organization;ad-hoc wireless multihop mesh networks;dynamic host configuration protocol;DHCP;address resolution protocol;ARP;Internet gateway;Mesh networks;Computer architecture;Spread spectrum communication;IP networks;Cities and towns;Protocols;Web and internet services;Computer network reliability;Telecommunication network reliability;Wireless mesh networks;Community Networks;Self-Organization;Distributed Hash Table}, 
doi={10.1109/WOACN.2006.337188}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{5254276, 
author={L. Mei and W. K. Chan and T. H. Tse and F. Kuo}, 
booktitle={2009 33rd Annual IEEE International Computer Software and Applications Conference}, 
title={An Empirical Study of the Use of Frankl-Weyuker Data Flow Testing Criteria to Test BPEL Web Services}, 
year={2009}, 
volume={1}, 
number={}, 
pages={81-88}, 
abstract={Programs using service-oriented architecture (SOA) often feature ultra-late binding among components. These components have well-defined interfaces and are known as Web services. Messages between every pair of Web services dually conform to the output interface of a sender and the input interface of a receiver. Unit testing of Web services should not only test the logic of Web services, but also assure the correctness of the Web services during input, manipulation, and output of messages. There is, however, little software testing research in this area. In this paper, we study the unit testing problem to assure components written in orchestration languages, WS-BPEL in particular. We report an empirical study of the effectiveness of the Frankl-Weyuker data flow testing criteria (particularly the all-uses criterion) on WS-BPEL subject programs. Our study shows that conventional data flow testing criteria can be much less effective in revealing faults in interface artifacts (WSDL documents) and message manipulations (XPath queries) than revealing faults in BPEL artifacts.}, 
keywords={business data processing;data flow analysis;program testing;software architecture;software fault tolerance;specification languages;Web services;Frankl-Weyuker data flow testing criteria;BPEL Web service;service-oriented architecture;unit testing;software testing;WS-BPEL;software fault;Web services;Logic testing;XML;Software testing;Application software;Service oriented architecture;Councils;Information retrieval;Computer applications;Data flow computing;WS-BPEL;XPath;data flow testing}, 
doi={10.1109/COMPSAC.2009.21}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{7342393, 
author={S. McGinty and D. Hadad and C. Nappi and B. Caquelin}, 
booktitle={2015 IEEE International Test Conference (ITC)}, 
title={Developing a modern platform for test engineering — Introducing the origen semiconductor developer's kit}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Many of the tools used today in semiconductor test engineering are single-point solutions that are concerned with the mechanics of translating test IP between domains and formats. There is no cohesive standardized framework to bind them all together; and workflow and application architecture choices are largely left up to the individual engineer. Learning from the state-of-the-art in other software engineering domains, we have developed a modern framework for semiconductor engineering that favors a convention-based approach to application architectures. By following conventions, powerful abstractions can be created to enable truly modular test development within a unified environment for the creation of test patterns, test programs, and all other test collateral. The paper reviews the background and some of the main capabilities of the framework and discusses how it is being used in production today to replace many conventional pattern flows. This is also a formal announcement that Freescale Semiconductor is open-sourcing the Origen Semiconductor Developer's Kit (SDK) to enable future development to be done in collaboration with the global semiconductor engineering community.}, 
keywords={circuit analysis computing;integrated circuit testing;software engineering;global semiconductor engineering community;SDK;Origen semiconductor developer kit;Freescale semiconductor;test programs;test patterns;truly modular test development;convention-based approach;software engineering domains;test IP;semiconductor test engineering;IP networks;Companies;Complexity theory;Testing;Industries;Computer architecture;Silicon}, 
doi={10.1109/TEST.2015.7342393}, 
ISSN={}, 
month={Oct},}
@ARTICLE{6768354, 
author={A. R. McGee and S. R. Vasireddy and K. J. Johnson and U. Chandrashekhar and S. H. Richman and M. El-Sayed}, 
journal={Bell Labs Technical Journal}, 
title={Dynamic virtual private networks}, 
year={2002}, 
volume={6}, 
number={2}, 
pages={116-135}, 
abstract={Modifications to a virtual private network's (VPN's) topology, security, service provisioning options, or quality of service (QoS) typically require an end-user request to their service provider, whose personnel currently perform the VPN management. This process incurs more provisioning delay and is more costly than user self-provisioning. This paper presents a new service approach and dynamic virtual private network (D-VPN) technology that marries VPNs with directory enabled networking and Web-based subscriber service selection. It places VPN management into the hands of the user to produce instantaneous results, lowering service-provider operations costs, and subsequently reducing the cost to the end user. The paper also describes the target architecture and framework as well as the initial types of services that could be supported by D-VPN technology.3}, 
keywords={}, 
doi={10.1002/bltj.9}, 
ISSN={1538-7305}, 
month={},}
@INPROCEEDINGS{6595796, 
author={M. Diepenbeck and M. Soeken and D. Grobe and R. Drechsler}, 
booktitle={2013 8th International Workshop on Automation of Software Test (AST)}, 
title={Towards automatic scenario generation from coverage information}, 
year={2013}, 
volume={}, 
number={}, 
pages={82-88}, 
abstract={Nowadays, the design of software systems is pushed towards agile development practices. One of its most fundamental approaches is Test Driven Development (TDD). This procedure is based on test cases which are incrementally written prior to the implementation. Recently, Behavior Driven Development (BDD) has been introduced as an extension of TDD, in which natural language scenarios are the starting point for the test cases. This description offers a ubiquitous communication mean for both the software developers and stakeholders. Following the BDD methodology thoroughly, one would expect 100 % code coverage, since code is only written to make the test cases pass. However, as we show in an empirical study this expectation is not valid in practice. It becomes even worse in the process of development, i.e. the coverage decreases over time. To close the coverage gap, we sketch an algorithm that generates BDD-style scenarios based on uncovered code.}, 
keywords={program testing;software prototyping;scenario generation;coverage information;software system design;agile development practice;test driven development;TDD approach;behavior driven development;natural language scenario;BDD methodology;code coverage;Data structures;Boolean functions;Natural languages;Software;Testing;Unified modeling language;Context}, 
doi={10.1109/IWAST.2013.6595796}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4539573, 
author={W. Hargassner and T. Hofer and C. Klammer and J. Pichler and G. Reisinger}, 
booktitle={2008 1st International Conference on Software Testing, Verification, and Validation}, 
title={A Script-Based Testbed for Mobile Software Frameworks}, 
year={2008}, 
volume={}, 
number={}, 
pages={448-457}, 
abstract={Software testing is essential and takes a large part of resources during software development. This motivates automating software testing as far as possible. Frameworks for automating unit testing are approved and applied for a plethora of programming languages to write tests for small units in the same programming language. Both constraints, unit size and programming language, inhibit automation of software testing in domain of mobile software frameworks. This circumstance has motivated the development of a new testbed for a framework in the domain of mobile systems. In this paper, we describe requirements and challenges in testing mobile software frameworks in general and present a novel testbed for the APOXI framework that addresses these requirements. The main ideas behind this testbed are the usage of a scripting language to specify test cases and to incorporate domain-specific aspects on the language level. The testbed facilitates component and system testing but can be used for unit testing as well.}, 
keywords={mobile computing;program testing;software engineering;script-based testbed;mobile software;software testing;software development;programming languages;Software testing;System testing;Protocols;Application software;Computer languages;Control systems;Automatic testing;Hardware;Mobile handsets;Electronic equipment testing;Software testing}, 
doi={10.1109/ICST.2008.51}, 
ISSN={2159-4848}, 
month={April},}
@ARTICLE{991333, 
author={E. Hieatt and R. Mee}, 
journal={IEEE Software}, 
title={Going faster: testing the Web application}, 
year={2002}, 
volume={19}, 
number={2}, 
pages={60-65}, 
abstract={This article documents the experiences of Evant's Extreme Programming team with testing XP. Testing is fundamental to XP but is a practice that often falls by the wayside in today's fast-paced Web application development culture. From the beginning, Evant adhered to each of XP's principles, and testing was no exception. This article explains how the team found that testing, positioned as the drive behind development, was critical to the success of building Evant's application at speed while maintaining high quality.}, 
keywords={Internet;information resources;program testing;hypermedia markup languages;Web application testing;Internet;HTML;Extreme Programming;XP;application development;Evant;software quality;Application software;Software testing;Writing;Buildings;Software engineering;Internet;Software quality;Software maintenance;Software tools;Java}, 
doi={10.1109/52.991333}, 
ISSN={0740-7459}, 
month={March},}
@INPROCEEDINGS{6201497, 
author={Z. Hui and P. Lei and W. Yifei}, 
booktitle={2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)}, 
title={Design amp;amp; implementation of laboratory information management system based on agile method}, 
year={2012}, 
volume={}, 
number={}, 
pages={2490-2493}, 
abstract={Agile software development is a new methodology of developing high quality software timely when facing significant change; it is convenient for managers to accomplish the collection, disposal, output and other work to the data. There are chart and table functions to the output data and information of laboratories, functions of the output of original and final reports of laboratories, making it affiance to precede the quality control of the data and to help managers arrange analytical plans, staff and other daily work.}, 
keywords={design engineering;information management;laboratories;quality control;software prototyping;software quality;laboratory information management system;agile method;agile software development;high quality software development;chart;table functions;data quality control;Laboratories;Programming;Information management;Software;Quality assurance;Servers;agile Method;management system;LMIS;agile software development}, 
doi={10.1109/CECNet.2012.6201497}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{140801, 
author={F. Godon and D. Al-Khalili and R. Inkol}, 
booktitle={Proceedings of the 33rd Midwest Symposium on Circuits and Systems}, 
title={A memory controller for mapping an array of circular buffers into a RAM}, 
year={1990}, 
volume={}, 
number={}, 
pages={645-648 vol.2}, 
abstract={A 1.5- mu m CMOS ASIC with a total complexity of over 22000 gates has been developed to generate and keep track of the offsets within 32 circular buffers. It offers a fair arbitration of interleaved read/write operations at a maximum data transfer rate of 20 MHz. Although the device is intended for a specialized electronic warfare system application, the design features incorporated make it generic and suitable for other applications such as communications interfaces in multiprocessor systems.<<ETX>>}, 
keywords={application specific integrated circuits;buffer storage;cellular arrays;CMOS integrated circuits;electronic warfare;random-access storage;storage management chips;VLSI;circular buffer memory controller;buffers mapping into RAM;gate arrays;CMOS ASIC;arbitration of interleaved read/write operations;data transfer rate;electronic warfare system;communications interfaces;multiprocessor systems;20 Mbit/s;1.5 micron;Random access memory;Read-write memory;Buffer storage;Radar;Very large scale integration;Military computing;Process control;Computer architecture;Counting circuits;Physics computing}, 
doi={10.1109/MWSCAS.1990.140801}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{1383107, 
author={M. Lil and Y. Wei and D. Desovski and H. Nejad and S. Ghose and B. Cukic and C. Smidts}, 
booktitle={15th International Symposium on Software Reliability Engineering}, 
title={Validation of a methodology for assessing software reliability}, 
year={2004}, 
volume={}, 
number={}, 
pages={66-76}, 
abstract={Software-based digital systems are progressively replacing analog systems in safety-critical applications. However the ability to predict their reliability is not well understood and needs further study. A first step towards systematic resolution of this issue was presented in a recent software engineering measure study. In that study a set of software engineering measures were ranked with respect to their ability in predicting software reliability through an expert opinion elicitation process. This study also proposed a concept of reliability prediction system (RePS) to bridge the gap between software engineering measures and software reliability. The research presented in this paper validates the rankings obtained and the concept of RePS proposed in the previous study.}, 
keywords={software reliability;program verification;software metrics;software validation;software reliability;software-based digital system;safety-critical application;software engineering measure study;opinion elicitation process;reliability prediction system;Software reliability;Software measurement;Software engineering;Reliability engineering;Phase measurement;Application software;Object oriented modeling;Usability;Software testing;Educational institutions}, 
doi={10.1109/ISSRE.2004.47}, 
ISSN={1071-9458}, 
month={Nov},}
@INPROCEEDINGS{7019292, 
author={C. M. Prathibhan and A. Malini and N. Venkatesh and K. Sundarakantham}, 
booktitle={2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies}, 
title={An automated testing framework for testing Android mobile applications in the cloud}, 
year={2014}, 
volume={}, 
number={}, 
pages={1216-1219}, 
abstract={The testing of mobile application faces many issues due to the complexity of testing these applications and the limited resources available in mobile devices. Testing in various mobile devices under varying conditions takes a lot of time when done manually. Also by using emulators it is not possible to generate the same real time network connections and real device characteristics. There is a need for a testing framework that allows automated testing of mobile applications in many mobile devices in limited time. In this paper we propose a mobile testing framework in the cloud environment that aims to provide automated testing of mobile applications in various mobile devices. This testing framework has an automated testing tool, the Mobile Application Testing (MAT) Tool integrated to it that performs functional, performance and compatibility testing of mobile applications.}, 
keywords={cloud computing;mobile computing;program testing;smart phones;automated testing framework;Android mobile application testing tool;mobile computing;time network connections;cloud environment;MAT Tool;Testing;Mobile communication;Performance evaluation;Androids;Humanoid robots;Mobile Testing;Automated Testing;Testing as a Service;Cloud Testing}, 
doi={10.1109/ICACCCT.2014.7019292}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6903575, 
author={A. Kane and T. Fuhrman and P. Koopman}, 
booktitle={2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks}, 
title={Monitor Based Oracles for Cyber-Physical System Testing: Practical Experience Report}, 
year={2014}, 
volume={}, 
number={}, 
pages={148-155}, 
abstract={Testing Cyber-Physical Systems is becoming increasingly challenging as they incorporate advanced autonomy features. We investigate using an external runtime monitor as a partial test oracle to detect violations of critical system behavioral requirements on an automotive development platform. Despite limited source code access and using only existing network messages, we were able to monitor a hardware-in-the-loop vehicle simulator and analyze prototype vehicle log data to detect violations of high-level critical properties. Interface robustness testing was useful to further exercise the monitors. Beyond demonstrating feasibility, the experience emphasized a number of remaining research challenges, including: approximating system intent based on limited system state observability, how to best balance the simplicity and expressiveness of the specification language used to define monitored properties, how to warm up monitoring of system variable state after mode change discontinuities, and managing the differences between simulation and real vehicles when conducting such tests.}, 
keywords={automotive engineering;observability;program testing;safety-critical software;source code (software);specification languages;monitor based oracles;cyber-physical system testing;practical experience report;advanced autonomy feature;external runtime monitor;partial test oracle;critical system behavioral requirements;automotive development platform;limited source code access;hardware-in-the-loop vehicle simulator;prototype vehicle log data;high-level critical property;interface robustness testing;limited system state observability;specification language;monitored property;system variable state;mode change discontinuity;Monitoring;Vehicles;Testing;Runtime;Robustness;Safety;Prototypes;runtime monitoring;testing;cyber-physical systems}, 
doi={10.1109/DSN.2014.28}, 
ISSN={1530-0889}, 
month={June},}
@ARTICLE{5233611, 
author={J. C. Duenas and J. L. Ruiz and F. Cuadrado and B. Garcia and H. A. Parada G.}, 
journal={IEEE Internet Computing}, 
title={System Virtualization Tools for Software Development}, 
year={2009}, 
volume={13}, 
number={5}, 
pages={52-59}, 
abstract={The configuration complexity of preproduction sites coupled with access-control mechanisms often impede the software development life cycle. Virtualization is a cost-effective way to remove such barriers and provide a test environment similar to the production site, reducing the burden in IT administrators. An eclipse-based virtualization tool framework can offer developers a personal runtime environment for launching and testing their applications. The authors have followed a model-driven architecture (MDA) approach that integrates best-of-breed virtualization technologies, such as Xen and VDE.}, 
keywords={authorisation;software engineering;software tools;system virtualization tool;configuration complexity;access-control mechanism;software development life cycle;IT administrator;eclipse-based virtualization tool;model-driven architecture approach;Programming;Testing;Impedance;Production;Application virtualization;Runtime environment;Application software;virtualization;software development;distributed systems;Eclipse;model-driven architecture;MDA}, 
doi={10.1109/MIC.2009.115}, 
ISSN={1089-7801}, 
month={Sept},}
@INPROCEEDINGS{6120072, 
author={Jiao Yu and B. M. Wilamowski}, 
booktitle={IECON 2011 - 37th Annual Conference of the IEEE Industrial Electronics Society}, 
title={Recent advances in in-vehicle embedded systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={4623-4625}, 
abstract={The number of computer based functions embedded in vehicles has increased significantly in the past two decades. An in-vehicle embedded electronic architecture is a complex distributed system; the development of which is a cooperative work involving different manufacturers and suppliers. There are several key demands in the development process, such as safety requirements, real-time assessment, schedulability, composability, etc. Intensive research is being conducted to address these issues. This paper reviews recent technology advances in relevant aspects and covers a range of topics highlighted above.}, 
keywords={automotive electronics;computer architecture;embedded systems;in-vehicle embedded electronic architecture;distributed system;computer based functions;Real time systems;Embedded systems;Automotive engineering;Field programmable gate arrays;Multicore processing;In-vehicle embedded electronic architecture;FPGA;real-time assessment;composability}, 
doi={10.1109/IECON.2011.6120072}, 
ISSN={1553-572X}, 
month={Nov},}
@INPROCEEDINGS{1342755, 
author={J. H. Andrews}, 
booktitle={Proceedings. 19th International Conference on Automated Software Engineering, 2004.}, 
title={A case study of coverage-checked random data structure testing}, 
year={2004}, 
volume={}, 
number={}, 
pages={316-319}, 
abstract={We study coverage-checked random unit testing (CRUT), the practice of repeatedly testing units on sequences of random function calls until given code coverage goals are achieved. Previous research has shown that this practice can be a useful complement to traditional testing methods. However, questions remained as to the breadth of its applicability. In this paper, we report on a case study in which we applied CRUT to the testing of two mature public-domain data structures packages. We show that CRUT helped in identifying faults, in debugging, in extracting and specifying actual behaviour, and in achieving greater assurance of the correctness of the debugged software}, 
keywords={data structures;fault diagnosis;program debugging;program testing;public domain software;software engineering;coverage-checked random data structure testing;coverage-checked random unit testing;random function calls;code coverage goals;public-domain data structures packages;fault identification;behaviour extraction;behaviour specification;debugging correctness;software debugging;Computer aided software engineering;Data structures;Software testing;Packaging;Fault diagnosis;Software engineering;Automatic testing;Documentation;Computer science;Debugging}, 
doi={10.1109/ASE.2004.1342755}, 
ISSN={1938-4300}, 
month={Sept},}
@INPROCEEDINGS{7880429, 
author={N. Jamous and S. Bosse and C. Görling and J. Hintsch and A. Khan and F. Kramer and H. Müller and K. Turowski}, 
booktitle={2016 4th International Conference on Enterprise Systems (ES)}, 
title={Towards an IT Service Lifecycle Management (ITSLM) Concept}, 
year={2016}, 
volume={}, 
number={}, 
pages={29-38}, 
abstract={Information Technology (IT) usage in enterprises has evolved over the last years. This led to today's complex, heterogeneous, and dynamic IT system landscapes that support business processes in enterprises. To manage these landscapes, the IT Service Management (ITSM) concept is gaining more importance in today's business and research. Studies demonstrate that introducing ITSM standards lead to positive effects, such as improved customer-orientation as well as efficiency and transparency of IT support, which justify the costs of implementation. However, companies still face difficulties in deciding which processes to be implement (first), and to which extent. Questions like: "How can the currently applied ITSM be adapted or extended when new business-related or technological challenges appear?" arise. Goods producing companies started early relying on Product Lifecycle Management (PLM). PLM delivers a solid means to define, discuss, analyze, and better standardize value creation processes. With PLM in mind, we propose a concept to adopt and further develop it towards IT Service Lifecycle Management (ITSLM) suitable for the IT services provider environment. After introducing ITSLM, analyzing its processes, and its correlation to PLM, we design ITSLM as a model-driven process support. The selection of appropriate models with different complexity can be used to implement and adapt standard supporting tasks with minimum effort. Two use cases are detailed: fault-tolerance design optimization as well as automation of IT service provisioning. In these areas, suitable model complexity levels, computer-aided task support as well as the knowledge transfer among these models are discussed.}, 
keywords={business data processing;product life cycle management;IT service lifecycle management;ITSLM;information technology;IT usage;enterprises;dynamic IT system;business processes;ITSM standards;companies;product lifecycle management;PLM;value creation processes;model-driven process support;fault-tolerance design optimization;IT service provisioning;model complexity levels;computer-aided task support;knowledge transfer;Business;Biological system modeling;Computational modeling;Adaptation models;Standards;Complexity theory;Information technology;IT Service Management (ITSM);Moddeling;Information Technology Infrastructure Library (ITIL);A Model-Driven IT Service Engineering}, 
doi={10.1109/ES.2016.10}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6080786, 
author={A. R. Yazdanshenas and L. Moonen}, 
booktitle={2011 27th IEEE International Conference on Software Maintenance (ICSM)}, 
title={Crossing the boundaries while analyzing heterogeneous component-based software systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={193-202}, 
abstract={One way to manage the complexity of software systems is to compose them from reusable components, instead of starting from scratch. Components may be implemented in different programming languages and are tied together using configuration files, or glue code, defining instantiation, initialization and interconnections. Although correctly engineering the composition and configuration of components is crucial for the overall behavior, there is surprisingly little support for incorporating this information in the static verification and validation of these systems. Analyzing the properties of programs within closed code boundaries has been studied for some decades and is well-established. This paper contributes a method to support analysis across the components of a component-based system. We build upon the Knowledge Discovery Metamodel to reverse engineer homogeneous models for systems composed of heterogeneous artifacts. Our method is implemented in a prototype tool that has been successfully used to track information flow across the components of a component-based system using program slicing.}, 
keywords={Complexity theory;Software systems;Component architectures;Computer languages;Prototypes;Knowledge engineering}, 
doi={10.1109/ICSM.2011.6080786}, 
ISSN={1063-6773}, 
month={Sept},}
@INPROCEEDINGS{6405446, 
author={G. Carrozza and M. Faella and F. Fucci and R. Pietrantuono and S. Russo}, 
booktitle={2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops}, 
title={Integrating MDT in an Industrial Process in the Air Traffic Control Domain}, 
year={2012}, 
volume={}, 
number={}, 
pages={225-230}, 
abstract={Air Traffic Control (ATC) systems are typical software-intensive mission-critical systems with stringent dependability requirements. The major providers of ATC systems are system integrators that address such requirements at the cost of a very expensive testing effort. They envisage Model Driven Testing (MDT) as a promising approach to reduce this effort while achieving better product quality. Within the context of a public-private partnership for software innovation in the ATC domain, we address the problem of integrating MDT into a software development process based on Model Driven Architecture. Specifically, we propose a solution to the integration of MDT into a V-model, focusing on a parallel MDA-MDT flow in a real industrial software process.}, 
keywords={air traffic control;formal verification;program testing;safety-critical software;software architecture;parallel MDA-MDT flow;industrial process;air traffic control;ATC system;software-intensive mission-critical system;model driven testing;public-private partnership;software innovation;software development process;model driven architecture;V-model;Unified modeling language;Software;Testing;Computer architecture;Adaptation models;Atmospheric modeling;Europe;MDA;MDT;Testing automation}, 
doi={10.1109/ISSREW.2012.87}, 
ISSN={}, 
month={Nov},}
@ARTICLE{6834762, 
author={W. Xia and Y. Wen and C. H. Foh and D. Niyato and H. Xie}, 
journal={IEEE Communications Surveys Tutorials}, 
title={A Survey on Software-Defined Networking}, 
year={2015}, 
volume={17}, 
number={1}, 
pages={27-51}, 
abstract={Emerging mega-trends (e.g., mobile, social, cloud, and big data) in information and communication technologies (ICT) are commanding new challenges to future Internet, for which ubiquitous accessibility, high bandwidth, and dynamic management are crucial. However, traditional approaches based on manual configuration of proprietary devices are cumbersome and error-prone, and they cannot fully utilize the capability of physical network infrastructure. Recently, software-defined networking (SDN) has been touted as one of the most promising solutions for future Internet. SDN is characterized by its two distinguished features, including decoupling the control plane from the data plane and providing programmability for network application development. As a result, SDN is positioned to provide more efficient configuration, better performance, and higher flexibility to accommodate innovative network designs. This paper surveys latest developments in this active research area of SDN. We first present a generally accepted definition for SDN with the aforementioned two characteristic features and potential benefits of SDN. We then dwell on its three-layer architecture, including an infrastructure layer, a control layer, and an application layer, and substantiate each layer with existing research efforts and its related research areas. We follow that with an overview of the de facto SDN implementation (i.e., OpenFlow). Finally, we conclude this survey paper with some suggested open research challenges.}, 
keywords={computer network management;Internet;software defined networking;software-defined networking;mega-trends;information and communication technologies;ICT;future Internet;ubiquitous accessibility;dynamic management;physical network infrastructure;SDN;innovative network designs;three-layer architecture;infrastructure layer;control layer;application layer;Optical switches;Routing;Software;Computer architecture;Complexity theory;Software-defined networking;SDN;network virtualization;OpenFlow}, 
doi={10.1109/COMST.2014.2330903}, 
ISSN={1553-877X}, 
month={Firstquarter},}
@INPROCEEDINGS{6716404, 
author={J. Kolek and Z. Jovanovic and N. Šljivic and D. Narancic}, 
booktitle={2013 21st Telecommunications Forum Telfor (TELFOR)}, 
title={Adding microMIPS backend to the LLVM compiler infrastructure}, 
year={2013}, 
volume={}, 
number={}, 
pages={1015-1018}, 
abstract={This work describes extending of the LLVM Compiler Infrastructure with the new backend support for microMIPS, which is an architecture from MIPS family of architectures. New backend consists of 16- and 32-bit instructions, out of which 180 of 32-bit instructions are recoded MIPS32 instructions, and 14 of 32-bit instructions are new microMIPS instructions. There are the 39 highly optimized 16-bit instructions.}, 
keywords={instruction sets;program compilers;microMIPS backend;LLVM compiler infrastructure;backend support;MIPS family of architectures;16-bit instructions;32-bit instructions;Encoding;Registers;Computer architecture;Libraries;Generators;Switches;Computers;Compilers;LLVM;microMIPS}, 
doi={10.1109/TELFOR.2013.6716404}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{5615100, 
author={C. Torens and L. Ebrecht}, 
booktitle={2010 Fifth International Conference on Software Engineering Advances}, 
title={RemoteTest: A Framework for Testing Distributed Systems}, 
year={2010}, 
volume={}, 
number={}, 
pages={441-446}, 
abstract={This work deals with general difficulties and aims when testing complex distributed systems, especially when heterogeneous interfaces are used. As a solution RemoteTest is proposed, a framework for the test of distributed systems and their interfaces. This is done by integrating individual system components into a virtual environment that emulates the adjacent modules of the system. The interface details are thereby abstracted by the framework and there is no special interface knowledge necessary by the tester. In addition to the decoupling of components and interface abstraction, RemoteTest facilitates the testing of distributed systems with flexible mechanisms to write test scripts and an architecture that can be easily adapted to different systems.}, 
keywords={data structures;distributed processing;program testing;virtual reality;distributed system;RemoteTest;virtual environment;components decoupling;interface abstraction;software testing;Testing;Software;Computer architecture;Virtual environment;Hardware;Programming;Complexity theory;software test;distributed systems;test framework;test tools;test methods}, 
doi={10.1109/ICSEA.2010.75}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{271291, 
author={H. Dai and M. Choo and J. A. Starzyk}, 
booktitle={[1992] Proceedings of the 35th Midwest Symposium on Circuits and Systems}, 
title={Noninvasive voltage measurement through an on-chip test structure (IC testing)}, 
year={1992}, 
volume={}, 
number={}, 
pages={340-343 vol.1}, 
abstract={A method to evaluate internal voltages through a built-in test structure is presented. Multiplexers are used to increase accessibility. The test structure does not affect normal operation of the circuit. Individual subcircuits can be tested selectively based on evaluated internal voltages.<<ETX>>}, 
keywords={application specific integrated circuits;built-in self test;integrated circuit testing;voltage measurement;noninvasive voltage measurement;digital subcircuits;multiplexer test structure;VLSI;IC testing;analogue circuits;mixed signal testing;on-chip test structure;internal voltages;built-in test structure;Integrated circuit testing;Voltage measurement;Circuit testing;Multiplexing;Large-scale systems;MOSFET circuits;MOS capacitors;Built-in self-test;Performance evaluation;Analog circuits}, 
doi={10.1109/MWSCAS.1992.271291}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6984094, 
author={P. Costa and A. C. R. Paiva and M. Nabuco}, 
booktitle={2014 9th International Conference on the Quality of Information and Communications Technology}, 
title={Pattern Based GUI Testing for Mobile Applications}, 
year={2014}, 
volume={}, 
number={}, 
pages={66-74}, 
abstract={This paper presents a study aiming to assess the feasibility of using the Pattern Based GUI Testing approach, PBGT, to test mobile applications. PBGT is a new model based testing approach that aims to increase systematization, reusability and diminish the effort in modelling and testing. It is based on the concept of User Interface Test Patterns (UITP) that contain generic test strategies for testing common recurrent behaviour, the so-called UI Patterns, on GUIs through its possible different implementations after a configuration step. Although PBGT was developed having web applications in mind, it is possible to develop drivers for other platforms in order to test a wide set of applications. However, web and mobile applications are different and only the development of a new driver to execute test cases over mobile applications may not be enough. This paper describes a study aiming to identify the adaptations and updates the PBGT should undergo in order to test mobile applications.}, 
keywords={graphical user interfaces;Internet;mobile computing;pattern recognition;program testing;Web applications;recurrent behaviour testing;UITP;user interface test patterns;model based testing;mobile application testing;PBGT;pattern based GUI testing;Mobile communication;Testing;Graphical user interfaces;Connectors;Androids;Humanoid robots;Optical character recognition software}, 
doi={10.1109/QUATIC.2014.16}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{776011, 
author={Y. Hodge and P. Bajpay and C. -. Chao and G. Grammer and H. Kan and D. Nadle}, 
booktitle={IEEE GLOBECOM 1998 (Cat. NO. 98CH36250)}, 
title={AT amp;amp;T service maintenance platform for next century}, 
year={1998}, 
volume={6}, 
number={}, 
pages={3757-3762 vol.6}, 
abstract={With rapid deployment of new services and increasing competitive pressure have come new challenges in the telecommunications management arena. This paper presents an evolved service maintenance platform intended to streamline, simplify and automate network management operations. A unified Business Maintenance Platform (BMP) for AT&T voice and data services is a key enabler for supporting AT&T continuous commitment to quality of service (QoS). The BMP is critical to the seamless and cost effective integration of voice, data and frame relay services and provides a flexible platform to encompass local, ATM, wireless services and new services in the future.}, 
keywords={maintenance engineering;telecommunication network management;quality of service;asynchronous transfer mode;business communication;integrated voice/data communication;frame relay;telecommunication computing;AT&T service maintenance platform;telecommunications management;telecommunication services;automated network management;Business Maintenance Platform;data services;voice services;quality of service;QoS;frame relay services;local services;wireless services;ATM services;SONET;GUI;Quality of service;Costs;Frame relay;Asynchronous transfer mode;Customer service;SONET;Availability;Chaos;Laboratories;Pressing}, 
doi={10.1109/GLOCOM.1998.776011}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7102608, 
author={S. H. Jensen and S. Thummalapenta and S. Sinha and S. Chandra}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Test Generation from Business Rules}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Enterprise applications are difficult to test because their intended functionality is either not described precisely enough or described in cumbersome business rules. It takes a lot of effort on the part of a test architect to understand all the business rules and design tests that "cover" them, i.e., exercise all their constituent scenarios. Part of the problem is that it takes a complicated set up sequence to drive an application to a state in which a business rule can even fire. In this paper, we present a business rule modeling language that can be used to capture functional specification of an enterprise system. The language makes it possible to build tool support for rule authoring, so that obvious deficiencies in rules can be detected mechanically. Most importantly, we show how to mechanically generate test sequences--i.e., test steps and test data--needed to exercise these business rules. To this end, we translate the rules into logical formulae and use constraint solving to generate test sequences. One of our contributions is to overcome scalability issues in this process, and we do this by using a novel algorithm for organizing search through the space of candidate sequences to discover covering sequences. Our results on three case studies show the promise of our approach.}, 
keywords={business data processing;formal specification;program testing;specification languages;test generation;test architect;design test;business rule modeling language;functional specification;enterprise system;rule authoring;logical formulae;constraint solving;test sequence;candidate sequence;covering sequence;Business;Databases;Testing;Syntactics;Algorithm design and analysis;Systematics;Context}, 
doi={10.1109/ICST.2015.7102608}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{6188784, 
author={}, 
booktitle={2012 International Conference on Devices, Circuits and Systems (ICDCS)}, 
title={2012 International Conference on Devices, Circuits and Systems (ICDCS)}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-748}, 
abstract={Provides the entire conference content.}, 
keywords={CMOS integrated circuits;CMOS technology;MESFETs;MOSFETs;Optical imaging;Random access memory;Field programmable gate arrays}, 
doi={10.1109/ICDCSyst.2012.6188784}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7338253, 
author={R. Rodriguez-Echeverria and F. Macias}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={A statistical analysis approach to assist model transformation evolution}, 
year={2015}, 
volume={}, 
number={}, 
pages={226-235}, 
abstract={Model Driven Engineering (MDE) is essentially based in metamodel definition, model edition and the specification of model transformations (MT) among these. In many cases the development, evolution and adaptation of these transformations is still carried out without the support of proper methods and tools to reduce the effort and related costs to these activities. In this work, a novel model testing approach specifically designed to assist the engineer in model transformation evolution is presented. A statistical analysis of the actual behavior of the transformations is performed by means of the computation of well-known information extraction metrics. In order to assist the MT adaptation, a detailed interpretation of the possible results of those metrics is also presented. And finally, the results of applying this approach on a Model-Driven Reverse Engineering (MDRE) scenario defined in the context of the MIGRARIA project are discussed.}, 
keywords={formal specification;program testing;reverse engineering;software metrics;statistical analysis;statistical analysis approach;metamodel definition;model edition;model transformation specification;cost reduction;model testing approach;information extraction metrics;MT adaptation;model-driven reverse engineering scenario;MDRE scenario;MIGRARIA project;software metrics;Adaptation models;Concrete;Contracts;Testing;Context modeling;Measurement;Unified modeling language;Model Transformation;Model Transformation Evolution;Model Transformation Testing;Testing Oracle}, 
doi={10.1109/MODELS.2015.7338253}, 
ISSN={}, 
month={Sept},}
@ARTICLE{8456508, 
author={Y. Yu and X. Li and X. Leng and L. Song and K. Bu and Y. Chen and J. Yang and L. Zhang and K. Cheng and X. Xiao}, 
journal={IEEE Communications Surveys Tutorials}, 
title={Fault Management in Software-Defined Networking: A Survey}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Software-defined networking (SDN) has emerged as a new network paradigm that promises control/data plane separation and centralized network control. While these features simplify network management and enable innovative networking, they give rise to persistent concerns about reliability. The new paradigm suffers from the disadvantage that various network faults may consistently undermine the reliability of such a network, and such faults are often new and difficult to resolve with existing solutions. To ensure SDN reliability, fault management, which is concerned with detecting, localizing, correcting and preventing faults, has become a key component in SDN networks. Although many SDN fault management solutions have been proposed, we find that they often resolve SDN faults from an incomplete perspective which may result in side effects. More critically, as the SDN paradigm evolves, additional fault types are being exposed. Therefore, comprehensive reviews and constant improvements are required to remain on the leading edge of SDN fault management. In this paper, we present the first comprehensive and systematic survey of SDN faults and related management solutions identified through advancements in both the research community and industry. We apply a systematic classification of SDN faults, compare and analyze existing SDN fault management solutions in the literature, and conduct a gap analysis between solutions developed in an academic research context and practical deployments. The current challenges and emerging trends are also noted as potential future research directions. This paper aims to provide academic researchers and industrial engineers with a comprehensive survey with the hope of advancing SDN and inspiring new solutions.}, 
keywords={Software;Hardware;Industries;Fault tolerance;Fault tolerant systems;Computer architecture;Software-defined networking (SDN);SDN reliability;SDN faults;fault classification;system monitoring;fault diagnosis;fault recovery and repair;fault tolerance.}, 
doi={10.1109/COMST.2018.2868922}, 
ISSN={1553-877X}, 
month={},}
@INPROCEEDINGS{4063813, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={1}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284569}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4064168, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={3}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284942}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4063976, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={2}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284735}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4064349, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={4}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.285102}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7927913, 
author={}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Table of contents}, 
year={2017}, 
volume={}, 
number={}, 
pages={v-x}, 
abstract={The following topics are dealt with: Presents the table of contents/splash page of the proceedings record.}, 
keywords={mobile computing;parallel processing;program testing;security of data;software engineering;software testing;software verification;software validation;fault localization;fault injection;program debugging;complexity analysis;composite faults;security testing;regression testing;Web applications;mobile applications;parallel systems;concurrency;model-based testing;automated testing;run-time testing;search-based testing;model checking;white box testing;DSL-based testing;code analysis;dynamic analysis}, 
doi={10.1109/ICST.2017.4}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5501146, 
author={}, 
booktitle={2009 5th Central and Eastern European Software Engineering Conference in Russia (CEE-SECR)}, 
title={[Title page]}, 
year={2009}, 
volume={}, 
number={}, 
pages={i-ii}, 
abstract={The following topics are dealt with: crisis-time distributed systems development; regression test selection technique; agile project management; software project feasibility study; graphical processing units; industrial C/C++ software; video registration and security systems; reliable software development; industrial Java applications; parallel programs; e-government and outsourcing; program reliability; operation-friendly software development; software product management; SaaS concept; SOA testing stack; complex hardware-software systems; UML-model; Microsoft DSL technology; Microsoft.NET micro framework; WBEM/CIM &amp; WS-MAN technology application; and agile Web development.}, 
keywords={C++ listings;government data processing;Java;outsourcing;regression analysis;software development management;software prototyping;software reliability;Unified Modeling Language;distributed systems development;regression test selection technique;agile project management;software project feasibility;graphical processing units;industrial C++ software;video registration;security systems;reliable software development;industrial Java applications;parallel programs;e-government;outsourcing;program reliability;operation-friendly software development;software product management;SaaS concept;SOA testing stack;hardware-software systems;UML-model;Microsoft.NET;Microsoft DSL technology;WS-MAN technology;agile Web development}, 
doi={10.1109/CEE-SECR.2009.5501146}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8449407, 
author={}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={Table of contents}, 
year={2018}, 
volume={}, 
number={}, 
pages={5-26}, 
abstract={Presents the table of contents/splash page of the proceedings record.}, 
keywords={}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@ARTICLE{8472900, 
author={P. Duan and Y. Zhou and X. Gong and B. Li}, 
journal={IEEE Access}, 
title={A Systematic Mapping Study on the Verification of Cyber-Physical Systems}, 
year={2018}, 
volume={6}, 
number={}, 
pages={59043-59064}, 
abstract={Cyber-physical system (CPS) is a kind of complex real-time hybrid system which involves deep interactions between computation processors, communication network, and physical environments are deemed as the key enablers of next generation computer applications. However, how to verify CPS effectively is always a great challenge. Based on current scientific works about CPS verification, this paper aims at identifying the gap of current studies and suggesting promising areas for the future works. For this purpose, we conduct a systematic mapping study over the topic on verification of cyber-physical system. We carry out a widely search of publications from 2006 to 2018 in 11 electronic databases. After the step of study selection, 80 papers are selected as primary studies for answering proposed research questions, focused questions, and statistical questions. According to these questions and their answers, this paper not only presents a quantitative and comprehensive analysis of verification challenges, abstraction methods, verification techniques, assistance tools, and verification scenarios that represent each step of verification works, but also summarizes CPS systematic natures, main routine of verification and future research directions. We believe that this survey can identify gaps in current research works and reveal new insights for the future works.}, 
keywords={Systematics;Cyber-physical systems;Tools;Databases;Guidelines;Sociology;Statistics;Systematic mapping study;verification of cyber-physical system;verification challenges;abstraction methods;verification techniques;assistance tools;verification scenarios}, 
doi={10.1109/ACCESS.2018.2872015}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8449448, 
author={C. Kröher and S. El-Sharkawy and K. Schmid}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={KernelHaven – An Experimentation Workbench for Analyzing Software Product Lines}, 
year={2018}, 
volume={}, 
number={}, 
pages={73-76}, 
abstract={Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem. KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.}, 
keywords={Data mining;Pipelines;Data models;Feature extraction;Tools;Analytical models;Software engineering;Software product line analysis;variability extraction;static analysis;empirical software engineering}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}