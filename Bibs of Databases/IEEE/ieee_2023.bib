@INPROCEEDINGS{8859410, 
author={Ramler, Rudolf and Klammer, Claus}, 
booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={Enhancing Acceptance Test-Driven Development with Model-Based Test Generation}, 
year={2019}, 
volume={}, 
number={}, 
pages={503-504}, 
abstract={Acceptance test-driven development is widely used in practice. However, writing and maintaining acceptance tests is a costly and time-consuming activity, in particular when a system is tested via the GUI. In model-based testing, the tests are automatically generated from a model of the system. In this paper, we report our experience from applying a combination of acceptance test-driven development and model-based testing in several real-world projects from industry. With the application of model-based testing, we increased test coverage and extend testing to usage scenarios not exercised by the existing acceptance tests. In the industry projects, MBT was used as an enhancement rather than a replacement for ATDD. By creating a layered test automation architecture, we were able to reuse the established automation for model-based testing and to apply both approaches simultaneously. This strategy also helped us to minimize the risks and to reduce the effort involved in introducing MBT in the projects.}, 
keywords={Testing;Graphical user interfaces;Automation;Adaptation models;Tools;Software;Industries;acceptance testing;model based testing;BDD;GUI testing;end-to-end testing;test automation architecture}, 
doi={10.1109/QRS-C.2019.00096}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{9529903, 
author={Xu, Jincheng and Du, Qingfeng and Li, Xiaojun}, 
booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
title={A Requirement-based Regression Test Selection Technique in Behavior-Driven Development}, 
year={2021}, 
volume={}, 
number={}, 
pages={1303-1308}, 
abstract={Regression testing is an essential software maintenance activity before the release of a new version implementing a bug fix or a new feature. A regression test selection (RTS) technique chooses a subset of existing test cases to ensure that the system will not be adversely affected by the latest modifications. With the rise of DevOps, behavior-driven development (BDD) is growing in popularity as it is in close alignment with agile practices, for example, continuous integration. Hence, it is necessary to propose a novel and effective RTS technique for BDD specifically to accelerate the development process while ensuring software quality. Since most existing techniques for RTS are code-based and thus subject to some limitations, we present a requirement-based technique which uses the requirements in BDD to select test cases in both high-level (acceptance testing) and low-level (unit testing). Our technique firstly illustrates the new requirement with a scenario, and subsequently computes the semantic similarity of the new scenario and all existing scenarios with the vector space model. According to the results, the modification-traversing regression test cases can be selected in a semi-automated way. We also conduct an experimental study to evaluate our technique in terms of inclusiveness, precision, efficiency and generality. The study shows that our technique is applicable for BDD and effective in practice.}, 
keywords={Software maintenance;Codes;Computational modeling;Conferences;Semantics;Computer bugs;Software quality;regression test selection;behavior-driven development;requirement-based technique;vector space model}, 
doi={10.1109/COMPSAC51774.2021.00182}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{8449605, 
author={Dos Santos, Ernani César and Vilain, Patrícia and Hiura Longo, Douglas}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={Poster: A Systematic Literature Review to Support the Selection of User Acceptance Testing Techniques}, 
year={2018}, 
volume={}, 
number={}, 
pages={418-419}, 
abstract={User Acceptance Testing (UAT) aims to determine whether or not a software satisfies users acceptance criteria. Although some studies have used acceptance tests as software requirements, no previous study has collected information about available UAT techniques and established a comparison of them, to support an organization in the selection of one over another. This work presents a Systematic Literature Review on UAT to find out available techniques and compare their main features. We selected 80 studies and found out 21 UAT techniques. As result, we created a comparative table summarizing these techniques and their features.}, 
keywords={Testing;Software;Natural languages;Tools;Bibliographies;Software engineering;Systematics;User acceptance testing;techniques;classification;features}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@INPROCEEDINGS{5069055, 
author={Talby, David}, 
booktitle={2009 ICSE Workshop on Automation of Software Test}, 
title={The perceived value of authoring and automating acceptance tests using a model driven development toolset}, 
year={2009}, 
volume={}, 
number={}, 
pages={154-157}, 
abstract={One approach to applying keyword driven testing in a model-driven development environment is by defining a domain specific language for test cases. The toolset then provides test editors, versioning, validation, reporting and hyperlinks across models - in addition to enabling automated test execution. This case study evaluates the effectiveness of such a solution as perceived by two teams of professional testers, who used it to test several products over a two year period. The results suggest that in addition to the expected benefits of automation, the solution reduces the time and effort required to write tests, maintain tests and plan the test authoring and execution efforts - at the expense of requiring longer training and a higher bar for recruiting testers.}, 
keywords={Automatic testing;DSL;Logic testing;Domain specific languages;Application software;Automation;Recruitment;Programming;Metamodeling;Context modeling}, 
doi={10.1109/IWAST.2009.5069055}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7133548, 
author={Rahman, Mazedur and Gao, Jerry}, 
booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
title={A Reusable Automated Acceptance Testing Architecture for Microservices in Behavior-Driven Development}, 
year={2015}, 
volume={}, 
number={}, 
pages={321-325}, 
abstract={Cloud Computing and Mobile Cloud Computing are reshaping the way applications are being developed and deployed due to their unique needs such as massive scalability, guaranteed fault tolerance, near zero downtime, etc. and also daunting challenges such as security, reliability, continuous deployment and update capability. Microservices architecture, where application is composed of a set of independently deployable services, is increasingly becoming popular due to its capability to address most of these needs and challenges. In recent years, the Behavior-Driven Development (BDD) has become one of the most popular agile software development processes, and frequently used in microservices development. The key to success of BDD is the executable acceptance tests that describe the expected behavior of a feature and its acceptance criteria in the form of scenarios using simple and business people readable syntax. The reusability, auditability, and maintainability become some of the major concerns when BDD test framework is applied for each microservice repository and no previous research addresses these concerns. In this paper, we present a reusable automated acceptance testing architecture to address all these concerns.}, 
keywords={Data structures;Boolean functions;Business;Testing;Software;Maintenance engineering;executable automated acceptance testing; Gherkin; functional testing; behavior-driven development; microservice}, 
doi={10.1109/SOSE.2015.55}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8387650, 
author={Pinkevich, Vasiliy and Platunov, Alexey}, 
booktitle={2018 IEEE Industrial Cyber-Physical Systems (ICPS)}, 
title={Model-driven functional testing of cyber-physical systems using deterministic replay techniques}, 
year={2018}, 
volume={}, 
number={}, 
pages={141-146}, 
abstract={Specialized embedded computer systems are one of the core technologies of modern industrial cyber-physical systems. They implement application algorithms and perform data acquisition, processing and transfer. The article presents the original approach to functional testing of embedded computer systems to overcome a number of restrictions imposed by specifics of the process of their design and development.}, 
keywords={cyber-physical systems;embedded systems;record and deterministic replay;computer architecture;high-level modeling;functional testing;verification;debug}, 
doi={10.1109/ICPHYS.2018.8387650}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4464015, 
author={Diaz, Jessica and Yague, Agustin and Alarcon, Pedro P. and Garbajosa, Juan}, 
booktitle={Seventh International Conference on Composition-Based Software Systems (ICCBSS 2008)}, 
title={A Generic Gateway for Testing Heterogeneous Components in Acceptance Testing Tools}, 
year={2008}, 
volume={}, 
number={}, 
pages={110-119}, 
abstract={Acceptance testing tools and Systems Under Test (SUT) require a gateway that will set up the communication link between them. Nevertheless, SUTs are often large systems composed of heterogeneous components that are executed in heterogeneous networks and platforms. Therefore, a non trivial communication problem between testing tools and these SUT heterogeneous components arises. A significant effort is invested in designing and implementing gateways for each specific component interface to cope with heterogeneity. This problem may be addressed through the use of middleware technologies that hide heterogeneity. However, this solution is too specific for each SUT domain. It may require a noteworthy effort to support the wide range of currently available interface standards that are provided by the different platforms and networks. An approach for testing heterogeneous components based on a generic gateway is presented in this paper. The generic gateway implements a service-oriented middleware named OSGi (Open Service Gateway initiative). OSGi helps to solve the heterogeneity problem and reduces the impact of designing a gateway for each specific SUT domain. The solution has been validated using the acceptance testing tool TOPEN (Test and Operation ENvironment) in a home automation scenario.}, 
keywords={System testing;Automatic testing;Middleware;Software systems;Software testing;Home automation;Embedded system;Intelligent sensors;Computer architecture;Software tools;test automation;middleware;complex systems testing;acceptance testing tools;OSGI;TOPEN;gateway;validation}, 
doi={10.1109/ICCBSS.2008.31}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{9793870, 
author={Elsner, Daniel and Wuersching, Roland and Schnappinger, Markus and Pretschner, Alexander and Graber, Maria and Dammer, René and Reimer, Silke}, 
booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
title={Build System Aware Multi-language Regression Test Selection in Continuous Integration}, 
year={2022}, 
volume={}, 
number={}, 
pages={87-96}, 
abstract={At IVU Traffic Technologies, continuous integration (CI) pipelines build, analyze, and test the code for inadvertent effects before pull requests are merged. However, compiling the entire code base and executing all regression tests for each pull request is infeasible due to prohibitively long feedback times. Regression test selection (RTS) aims to reduce the testing effort. Yet, existing safe RTS techniques are not suitable, as they largely rely on language-specific program analysis. The IVU code base consists of more than 13 million lines of code in Java or C/C++ and contains thousands of non-code artifacts. Regression tests commonly operate across languages, using cross-language links, or read from non-code artifacts. In this paper, we describe our build system aware multi-language RTS approach, which selectively compiles and executes affected code modules and regression tests, respectively, for a pull request. We evaluate our RTS technique on 397 pull requests, covering roughly 2,700 commits. The results show that we are able to safely exclude up to 75% of tests on average (no undetected real failures slip into the target branches) and thereby save 72% of testing time, whereas end-to-end CI pipeline time is reduced by up to 63% on average.}, 
keywords={Java;Codes;Runtime;Pipelines;Software;Performance analysis;Testing;Software testing;regression test selection;continuous integration}, 
doi={10.1145/3510457.3513078}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6569751, 
author={Törsel, Arne-Michael}, 
booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
title={A Testing Tool for Web Applications Using a Domain-Specific Modelling Language and the NuSMV Model Checker}, 
year={2013}, 
volume={}, 
number={}, 
pages={383-390}, 
abstract={Test case generation from formal models using model checking software is an established method. This paper presents a model-based testing approach for web applications based on a domain-specific language model. It is shown how the domain-specific language is transformed into the input language of the NuSMV model checker and how the resulting traces are converted into executable test scripts for various test automation tools. The presented approach has been implemented with comprehensive automation in a research tool which architecture is outlined.}, 
keywords={DSL;Automation;Adaptation models;Software;Web pages;Model checking;web applications;model-based testing;model checking;test automation}, 
doi={10.1109/ICST.2013.54}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{9039968, 
author={Nuriddinov, Askhat and Tavernier, Wouter and Colle, Didier and Pickavet, Mario and Peustery, Manuel and Schneidery, Stefan}, 
booktitle={2019 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)}, 
title={Reproducible Functional Tests for Multi-scale Network Services}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Network functions virtualization (NFV) is developed to take advantage of virtualization technologies to separate network functions from the underlying hardware appliances. This approach brings a new level of flexibility in network services deployment for rapid composition, migration and scaling based on changing traffic load and current demand, leading to a higher QoS. Moreover, it allows introducing DevOps approach in the development process resulting in reduced life-cycle and shorter time-to-market. However, in order to realize the anticipated benefits of NFV, developers and network operators need mechanisms to adequately test Virtualized Network Functions (VNF) against different deployment scenarios and in different scales. To this end, we introduce a new library for light-weight automated functional testing of VNFs. It helps the developers to write functional tests in Python and run them on different platforms. Using the library, test developers choose which infrastructure to use, which network services and test VNFs to launch, how to interconnect them, how to trigger the test process and inspect, verify and validate the output against the expected values and conditions. Test developers can use network service packages to deploy services or can compose them in a test code using different parameters to simulate various deployment scenarios. Finally, the library can be used to set automated testing in CI/CD environments or can be used locally during the development phase. In this paper, we describe the architecture of the library, the basic workflow and give an example of testing multiple flavors of a network service using the same code.}, 
keywords={Testing;Libraries;Python;Network function virtualization;Computer architecture;Emulation;Conferences;5GTANGO;NFV;network functions virtualization;VNF;virtualized network functions;functional testing}, 
doi={10.1109/NFV-SDN47374.2019.9039968}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8990263, 
author={Bertolino, Antonia and De Angelis, Guglielmo and Lonetti, Francesca}, 
booktitle={2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={Governing Regression Testing in Systems of Systems}, 
year={2019}, 
volume={}, 
number={}, 
pages={144-148}, 
abstract={Great advances in network technology and software engineering have triggered the development and spread of Systems of Systems (SoSs). The dynamic and evolvable nature of SoSs poses important challenges on the validation of such systems and in particular on their regression testing, aiming at assessing that run-time changes and evolutions do not introduce regression in SoS behavior. This paper outlines issues and challenges of regression testing of SoSs, identifying the main kinds of evolution that can impact on their regression testing activity. Furthermore, it presents a conceptual framework for governing the regression testing of SoSs. The proposed framework leverages the concept of an orchestration graph that describes the flow of test cases and sketches a solution for deriving a regression test plan according to test cases dependencies.}, 
keywords={System of Systems;Regression Testing;Governance;Test Cases Orchestration}, 
doi={10.1109/ISSREW.2019.00064}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7367051, 
author={Kumar, Rajesh and Kumar, Vivek}, 
booktitle={2015 World Congress on Information Technology and Computer Applications (WCITCA)}, 
title={Process optimization for testing of domain specific languages in industrial automation}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Software testing is essential part of software development. The goal of the testing process is not only to enhance the quality and robustness of the software but also verify the correctness and non functional requirements of the software under all working conditions. Large software has their large testing suites to verify the stability of legacy features. Testing processes have huge challenges to maintain effectiveness and efficiency of the legacy test cases. There are many different processes and techniques available all technique or processes have their advantages and limitations. A tailored testing process has been tried to utilize all technique together to improvise the benefits and efficiency of testing in the industrial automation domain. This paper tries to explain a customized approach of utilizing the available testing techniques in such a way that it enhances the effectiveness and efficiency of regression testing, thus improving the time to market of large product-line Industrial automation software.}, 
keywords={Automation;Software testing;Fault detection;Software engineering;Software maintenance;Regression Testing;Test Effectiveness;Test suite optimization;Software Testing process;Test Automation;Industial automation}, 
doi={10.1109/WCITCA.2015.7367051}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{9920124, 
author={von Olberg, Pauline and Strey, Lukas}, 
booktitle={2022 IEEE 30th International Requirements Engineering Conference Workshops (REW)}, 
title={Approach to Generating Functional Test Cases from BPMN Process Diagrams}, 
year={2022}, 
volume={}, 
number={}, 
pages={185-189}, 
abstract={Business Process Model and Notation (BPMN) is a popular and widespread modelling language used to describe business processes. These BPMN business process models can serve as a foundation for functional software testing. Functional software testing is an important part of software development, which ensures that software works as expected and that it includes all the desired functionality, as defined in the process models. This position paper presents an approach and considers two different methods on how to automatically create functional test cases from BPMN business process models. The generated test cases shall be understandable for all stakeholders and abstracting from the technical implementation. To achieve this general understandability of the test cases, Gherkin is used as a test case definition language. The two proposed methods will be developed and evaluated in future work. This planned evaluation includes comparing the automatically created test cases with manually created ones.}, 
keywords={Software testing;Conferences;Software;Stakeholders;Requirements engineering;Business;BPMN;functional software testing;Gherkin;business process modelling}, 
doi={10.1109/REW56159.2022.00042}, 
ISSN={2770-6834}, 
month={Aug},}
@INPROCEEDINGS{5540750, 
author={Vanderbauwhede, W. and Margala, M. and Chalamalasetti, S. R. and Purohit, S.}, 
booktitle={ASAP 2010 - 21st IEEE International Conference on Application-specific Systems, Architectures and Processors}, 
title={A C++-embedded Domain-Specific Language for programming the MORA soft processor array}, 
year={2010}, 
volume={}, 
number={}, 
pages={141-148}, 
abstract={MORA is a novel platform for high-level FPGA programming of streaming vector and matrix operations, aimed at multimedia applications. It consists of soft array of pipelined low-complexity SIMD processors-in-memory (PIM). We present a Domain-Specific Language (DSL) for high-level programming of the MORA soft processor array. The DSL is embedded in C++, providing designers with a familiar language framework and the ability to compile designs using a standard compiler for functional testing before generating the FPGA bitstream using the MORA toolchain. The paper discusses the MORA-C++ DSL and the compilation route into the assembly for the MORA machine and provides examples to illustrate the programming model and performance.}, 
keywords={Domain specific languages;Field programmable gate arrays;Parallel processing;DSL;Streaming media;Application specific integrated circuits;Parallel programming;Concurrent computing;Algorithm design and analysis;Programming profession;Reconfigurable Processor;Soft Processor Array;Multimedia Processing;Domain-Specific Language}, 
doi={10.1109/ASAP.2010.5540750}, 
ISSN={1063-6862}, 
month={July},}
@INPROCEEDINGS{8725698, 
author={Nuriddinov, Askhat and Tavernier, Wouter and Colle, Didier and Pickavet, Mario}, 
booktitle={2018 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)}, 
title={A framework for functional testing of VNFs}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-2}, 
abstract={Network Function Virtualization (NFV) is a promising technology which can significantly boost innovations in the area of telecommunication networks. However, to realize the anticipated benefits of NFV, network operators need to solve several challenges which include performance and functional testing of Virtualized Network Functions (VNF). Furthermore, the development process of VNFs is very complex and error-prone, therefore the developers also need to do functional testing of their VNFs.To this end, we introduce a new open-source framework for functional testing of VNFs. It allows to write tests in Python and use its simplicity to write tests with minimal effort. The framework has integration with virtual infrastructure to make the test process seamless and less time consuming.}, 
keywords={Testing;Conferences;Python;Network function virtualization;Acceleration;Software defined networking}, 
doi={10.1109/NFV-SDN.2018.8725698}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{9799224, 
author={Abdalla, Zeinab and Andrews, Anneliese and Alhaddad, Ahmed}, 
booktitle={2021 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
title={Regression Testing of Mobile Apps}, 
year={2021}, 
volume={}, 
number={}, 
pages={1912-1917}, 
abstract={Because mobile applications, or apps, are becoming essential in our personal lives and at work, and mobile applications update frequently, it is important that developers perform regression testing to ensure their quality. In this paper we adapt the FSMWeb approach for selective regression testing for mobile applications. We apply rules to classify the original set of tests into obsolete, retestable, and reusable tests based on the types of changes to the model. New tests are added to cover portions the App that have not been tested.}, 
keywords={Adaptation models;Scientific computing;Minimization;Mobile applications;Computational intelligence;Testing;Selective Regression Testing;FSM;Mobile Apps}, 
doi={10.1109/CSCI54926.2021.00055}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7570913, 
author={Pandit, Pallavi and Tahiliani, Swati and Sharma, Meena}, 
booktitle={2016 Symposium on Colossal Data Analysis and Networking (CDAN)}, 
title={Distributed agile: Component-based user acceptance testing}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Testing is conducted at multiple levels during the development of software. User Acceptance Testing conforms that the software meets user's criteria. In this paper, User Acceptance Testing is automatically conducted based on acceptance criteria. The acceptance criteria are written in the form of Given-When-Then Template. These acceptance criteria are broken down into steps and numbered. The dependencies among the steps are determined as Given->When->Then. Henceforth, the steps are arranged in a dependency graph. This graph further leads to the creation of a decision table in which the outcome of one step leads to the outcomes of its dependent steps. The decision table forms the basis of generation of a binary weighted dependency tree. This tree becomes the means to form test coverage (number of combinations to test) which forms the basis of generation of acceptance test cases.}, 
keywords={Testing;Online banking;Data analysis;Credit cards;Software;Algorithm design and analysis}, 
doi={10.1109/CDAN.2016.7570913}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{9610682, 
author={Corradini, Davide and Zampieri, Amedeo and Pasqua, Michele and Ceccato, Mariano}, 
booktitle={2021 IEEE 21st International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={Empirical Comparison of Black-box Test Case Generation Tools for RESTful APIs}, 
year={2021}, 
volume={}, 
number={}, 
pages={226-236}, 
abstract={In literature, we can find research tools to automatically generate test cases for RESTful APIs, addressing the specificity of this particular programming domain. However, no direct comparison of these tools is available to guide developers in deciding which tool best fits their REST API project.In this paper, we present the results of an empirical comparison of automated black-box test case generation approaches for REST APIs. We surveyed the available black-box testing tools that have been proposed in recent literature, finding four usable prototypes: RestTestGen, RESTler, bBOXRT and RESTest. We used these tools to generate test cases for 14 real-world REST services. Then, testing results have been analyzed and compared in terms of robustness (i.e., success rate) and test coverage.Among the considered tools, RESTler appears to be the most solid, able to successfully test all case studies (the other tools experienced crashes). Conversely, test cases generated by RestTestGen scored the highest coverage, suggesting that its testing strategy is the most effective in testing REST APIs.}, 
keywords={Codes;Restful API;Tools;Programming;Solids;Robustness;Computer crashes;REST API;Test coverage;Black-box testing;Automated software testing;Experimental comparison}, 
doi={10.1109/SCAM52516.2021.00035}, 
ISSN={2470-6892}, 
month={Sep.},}
@ARTICLE{10015143, 
author={de Oliveira, Lívia Fernanda and Rodrigues, Cássio Leonardo and Bulcão-Neto, Renato de Freitas}, 
journal={IEEE Latin America Transactions}, 
title={Characterizing the Software Acceptance Testing and the Inclusion of People with Disabilities by Means of a Systematic Mapping}, 
year={2023}, 
volume={21}, 
number={1}, 
pages={35-46}, 
abstract={Acceptance testing is a test technique where the final user evaluates the actual use of the software. In this test, software must meet the acceptance criteria defined on the requirement engineering phase to be approved. This paper describes the participation of the person with a disability in acceptance testing in terms of testing approaches, tools and user feedback. By performing a systematic mapping, we analyzed 609 articles, of which 58 were considered to answer four research questions. We identified that acceptance tests have no standard and present a dispersed variety of approaches and tools. We also few acceptance tests studies applied to testers with disabilities, although this approach has been rising in recent years. Considering accessibility for all users when including them in acceptance tests can ensure greater reach of users to systems}, 
keywords={Software;Testing;Systematics;IEEE transactions;Libraries;Statistics;Standards;Software Testing;Acceptance Testing;Disabled User;Systematic Mapping;Secondary Study}, 
doi={10.1109/TLA.2023.10015143}, 
ISSN={1548-0992}, 
month={Jan},}
@INPROCEEDINGS{5463692, 
author={Chatley, Robert and Ayres, John and White, Tom}, 
booktitle={2010 Third International Conference on Software Testing, Verification, and Validation Workshops}, 
title={LiFT: Driving Development Using a Business-Readable DSL for Web Testing}, 
year={2010}, 
volume={}, 
number={}, 
pages={460-468}, 
abstract={This paper describes the development and evolution of LiFT, a framework for writing automated tests in a style that makes them very readable, even for non-programmers. We call this style 'literate testing'. By creating a domain-specific language embedded within Java, we were able to write automated tests that read almost like natural language, allowing business requirements to be expressed very clearly. This allows development to be driven from tests that are created by developers and customers together, helping give all stakeholders confidence that the right things are being tested and hence a correct system being built. We discuss the experiences of a team using these tools and techniques in a large commercial project, and the lessons learned from the experience.}, 
keywords={DSL;Automatic testing;System testing;Writing;Software testing;Domain specific languages;Java;Natural languages;Business communication;Formal specifications;TDD;acceptance testing;DSL}, 
doi={10.1109/ICSTW.2010.12}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{9161781, 
author={Popic, Srdjan and Komadina, Vanja and Arsenovic, Ranka and Stepanovic, Mia}, 
booktitle={2020 Zooming Innovation in Consumer Technologies Conference (ZINC)}, 
title={Implementation of the simple domain-specific language for system testing in V-Model development lifecycle}, 
year={2020}, 
volume={}, 
number={}, 
pages={290-294}, 
abstract={This paper presents easy to use domain-specific language for system testing in V-model development lifecycle. The systematic approach offered by the domain-specific language for system testing eliminates miscommunications between testers and requirement engineers making the testing closer to the requirement engineers. This concept enables automation in the generation of the tests based on given System Requirements in the future. As many would argue on V-Model's difficulty to align system requirements and system tests, this approach enables better mapping between those two parts of the V-diagram. This will make no functional requirement missing its counterpart in testing and vice-versa.}, 
keywords={DSL;System testing;Task analysis;XML;Tools;Software;domain-specific language;V-model;system testing}, 
doi={10.1109/ZINC50678.2020.9161781}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7128889, 
author={Visic, Niksa and Fill, Hans-Georg and Buchmann, Robert Andrei and Karagiannis, Dimitris}, 
booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
title={A domain-specific language for modeling method definition: From requirements to grammar}, 
year={2015}, 
volume={}, 
number={}, 
pages={286-297}, 
abstract={The core process a modeling method engineer needs to accomplish starts with the acquisition of domain knowledge and requirements, and ends with the deployment of a usable modeling tool. In between, a key intermediate deliverable of this process is the modeling method specification which, ideally, should be platform independent. On one hand, it takes input from a structured understanding of the application domain and scenarios; on the other hand, it provides sufficiently structured input to support the implementation of tool support for modeling activities. It is quite common that such modeling methods are domain-specific, in the sense that they provide concepts from the domain as “first-class modeling citizens”. However, for the purposes of this paper, we raise the level of abstraction for “domain specificity” and consider “modeling method engineering” as the application domain. Consequently, we raise several research questions - whether a domain-specific language can support this domain, and what would be its requirements, properties, constructs and grammar. We propose an initial draft of such a language - one that abstracts away from meta-modeling platforms by establishing a meta2 layer of abstraction where a modeling method can be defined in a declarative manner, then the final modeling tool is generated by automated compilation of the method definition for the meta-modeling environment of choice.}, 
keywords={Unified modeling language;Analytical models;DSL;Metamodeling;Semantics;Computational modeling;Domain specific languages;domain-specific language;modeling method;meta-modeling;modeling tool}, 
doi={10.1109/RCIS.2015.7128889}, 
ISSN={2151-1357}, 
month={May},}
@INPROCEEDINGS{6200141, 
author={Borjesson, Emil}, 
booktitle={2012 IEEE Fifth International Conference on Software Testing, Verification and Validation}, 
title={Industrial Applicability of Visual GUI Testing for System and Acceptance Test Automation}, 
year={2012}, 
volume={}, 
number={}, 
pages={475-478}, 
abstract={The software market is becoming more challenging as demands for faster time-to-market and higher software quality continue to grow. These challenges are embedded in all areas of Software Engineering, including Verification and Validation where they are proposed as solvable with automated testing. However, most automated testing techniques focus on low system level testing and are not suitable for high level tests, i.e. System and Acceptance tests, leaving industrial needs for test automation unfulfilled. In this paper we present a research plan to evaluate a novel automated testing technique, called visual GUI testing, based on image recognition algorithms and scripts that interact through the system GUI to automate complex scenario based tests. The technique has been evaluated at the company Saab AB where industrial, safety critical, scenario based, test cases were automated showing the industrial applicability of the technique. However, many factors are still unknown regarding the techniques industrial applicability, i.e. script maintenance costs, usability and learn ability, etc. Our research aims to uncover these unknown factors with the final research goal to show that visual GUI testing is a viable and cost-effective technique that will fill the gap in industry for a cost-effective, simple, robust, high-level test automation technique.}, 
keywords={Testing;Graphical user interfaces;Visualization;Industries;Automation;Robustness;Maintenance engineering;V&V;Automated testing;Visual GUI testing;Image recognition;Scripted testing}, 
doi={10.1109/ICST.2012.129}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{9978261, 
author={Corradini, Davide and Zampieri, Amedeo and Pasqua, Michele and Ceccato, Mariano}, 
booktitle={2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={RestTestGen: An Extensible Framework for Automated Black-box Testing of RESTful APIs}, 
year={2022}, 
volume={}, 
number={}, 
pages={504-508}, 
abstract={Over the past few years, several novel black-box testing approaches targeting RESTful APIs have been proposed. In order to assess their effectiveness, such testing strategies had to be implemented as a prototype tool and validated on empirical data. However, developing a testing tool is a time-consuming task, and reimplementing from scratch the same common basic features represents a waste of resources that causes a remarkable overhead in the "time to market" of research results.In this paper, we present RestTestGen, an extensible framework for implementing new automated black-box testing strategies for RESTful APIs. The framework provides a collection of commonly used components, such as a robust OpenAPI specification parser, dictionaries, input value generators, mutation operators, oracles, and others. Many of the provided components are customizable and extensible, enabling researchers and practitioners to quickly prototype, deploy, and evaluate their novel ideas. Additionally, the framework facilitates the development of novel black-box testing strategies by guiding researchers, by means of abstract components that explicitly identify those parts of the framework requiring a concrete implementation.As an adoption example, we show how we can implement nominal and error black-box testing strategies for RESTful APIs, by reusing primitives and features provided by the framework, and by concretely extending very few abstract components.RestTestGen is open-source, actively maintained, and publicly available on GitHub at https://github.com/SeUniVr/RestTestGen}, 
keywords={Software maintenance;Dictionaries;Closed box;Restful API;Prototypes;Time to market;Generators;REST API;Test case generation;Black-box testing}, 
doi={10.1109/ICSME55016.2022.00068}, 
ISSN={2576-3148}, 
month={Oct},}
@INPROCEEDINGS{8411756, 
author={Elodie, Bernard and Fabrice, Ambert and Bruno, Legeard and Arnaud, Bouzy}, 
booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={Lightweight Model-Based Testing for Enterprise IT}, 
year={2018}, 
volume={}, 
number={}, 
pages={224-230}, 
abstract={Model-Based Testing (MBT) popularity in IT is growing at a very slow pace. A recent survey stated that no more than 14% of respondents use MBT in their projects. Our experience, presented in this paper, demonstrates that the complexity in use of the current MBT approaches for the average tester is the main reason for this low dissemination. Then we introduce a lightweight MBT approach and a tool, called Yest, dedicated to business process-based testing of enterprise information systems. This tool uses a workflow-based graphical representation linked with decision tables to be used by functional testers without requiring any kind of modeling skill (such as UML for example). These approach and tool are dedicated to a particular class of applications (i.e. enterprise IT applications such as ERP and bespoke business applications). This focus strongly helps to simplify the approach and to adapt the tooling to the targeted users (namely IT functional testers). Finally, we discuss the way MBT may support emerging Acceptance Test Driven Development practices in agile.},
keywords={Tools;Testing;Unified modeling language;Task analysis;Password;Electronic mail;Model-based-testing;Lightweight MBT;MBT tool;Test cases generation;Business process-based testing}, 
doi={10.1109/ICSTW.2018.00053}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6228998, 
author={Hallenberg, Niels and Carlsen, Philip Lykke}, 
booktitle={2012 7th International Workshop on Automation of Software Test (AST)}, 
title={Declarative automated test}, 
year={2012}, 
volume={}, 
number={}, 
pages={96-102}, 
abstract={Automated tests at the business level can be expensive to develop and maintain. One common approach is to have a domain expert instruct a QA developer to implement what she would do manually in the application. Though there exist record-replay tools specifically developed for this, these tend to scale poorly for more complicated test scenarios. We present a different solution: An Embedded Domain Specific Language (EDSL) in F#, containing the means to model the user interface, and the various manipulations of it. We hope that this DSL will bridge the gap between the business domain and technical domain of applications to such a degree that domain experts may be able to construct automatic tests without depending on QA developers, and that these tests will prove more maintainable.}, 
keywords={Testing;DSL;Documentation;Phantoms;Engines;Business;User interfaces;Functional Testing;Automated Testing;Domain Specific Language;F#}, 
doi={10.1109/IWAST.2012.6228998}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{5254116, 
author={Miller, Anne and Kumar, Balaji and Singhal, Anukul}, 
booktitle={2009 33rd Annual IEEE International Computer Software and Applications Conference}, 
title={Photon: A Domain-Specific Language for Testing Converged Applications}, 
year={2009}, 
volume={2}, 
number={}, 
pages={269-274}, 
abstract={Automated testing of converged applications can be complex, as it is rare for a single testing tool to provide a single solution for all access points which a given application supports. As such, testing teams often create customized testing frameworks, which integrate several different testing tools, and a myriad of programming languages and scripting tools. When an applicationpsilas unique set of access points changes, or a new testing tool comes to market which offers a competitive advantage over existing test tools, the cost of updating these customized frameworks can be difficult to justify. This paper provides a solution to this problem by introducing ldquoPhotonese,rdquo a domain-specific language which testers can use to compose automation scripts which are independent of the test tool used for automation. In this way, the tester creates reusable testing assets in a framework which is reusable across multiple projects.}, 
keywords={Domain specific languages;Automatic testing;Automation;Software testing;Life testing;Telephony;Books;Application software;Displays;Computer applications;Software quality;Software reusability;Software testing}, 
doi={10.1109/COMPSAC.2009.143}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{6470771, 
author={Mayrhofer, Dieter and Huemer, Christian}, 
booktitle={2012 IEEE 14th International Conference on Commerce and Enterprise Computing}, 
title={REA-DSL: Business Model Driven Data-Engineering}, 
year={2012}, 
volume={}, 
number={}, 
pages={9-16}, 
abstract={An accounting information system (AIS) manages data about a company's financial and economic status. The contribution of this paper is closing the gap between the languages used by business domain experts and IT-experts in analyzing the relevant data. A well accepted approach for an accountability infrastructure is the Resource-Event-Agent (REA) ontology. Although REA has been based on well-established concepts of the accounting theory, its representation has not been intuitive to domain experts. In previous work, we developed the REA-DSL, a dedicated and easy-to-understand graphical domain specific modeling language for the REA ontology. Evidently, a model-driven approach requires to transform the REA-DSL artifacts to code. In this paper we present the transformation of the REA-DSL to a relational database for AIS. This approach offers the advantage that a domain expert verifies the relevant data in an "accounting language", whereas the IT expert is able to work with traditional data base structures.}, 
keywords={Economics;Marine animals;Business;Ontologies;Unified modeling language;Marketing and sales;Analytical models;REA;domain-specific language;business models;relational schema}, 
doi={10.1109/CEC.2012.12}, 
ISSN={2378-1971}, 
month={Sep.},}
@ARTICLE{8440671, 
author={Mirza, Aamir Mehmood and Khan, Muhammad Naeem Ahmed}, 
journal={IEEE Access}, 
title={An Automated Functional Testing Framework for Context-Aware Applications}, 
year={2018}, 
volume={6}, 
number={}, 
pages={46568-46583}, 
abstract={In the modern era of mobile computing, context-aware computing is an emerging paradigm due to its widespread applications. Context-aware applications are gaining increasing popularity in our daily lives since these applications can determine and react according to the situational context and help users to enhance usability experience. However, testing these applications is not straightforward since it poses several challenges, such as generating test data, designing context-coupled test cases, and so on. However, the testing process can be automated to a greater extent by employing model-based testing technique for context-aware applications. To achieve this goal, it is necessary to automate model transformation, test data generation, and test case execution processes. In this paper, we propose an approach for behavior modeling of context-aware application by extending the UML activity diagram. We also propose an automated model transformation approach to transform the development model, i.e., extended UML activity diagram into the testing model in the form of function nets. The objective of this paper is to automate the context-coupled test case generation and execution. We propose a functional testing framework for automated execution of keyword-based test cases. Our functional testing framework can reduce the testing time and cost, thus enabling the test engineers to execute more testing cycles to attain a higher degree of test coverage.}, 
keywords={Unified modeling language;Testing;Petri nets;Context-aware services;Sensors;Context modeling;Context-aware applications;model based testing;function net;petri net;model transformation}, 
doi={10.1109/ACCESS.2018.2865213}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{9004946, 
author={Reinhardt, Oliver and Uhrmacher, Adelinde M. and Hinsch, Martin and Bijak, Jakub}, 
booktitle={2019 Winter Simulation Conference (WSC)}, 
title={Developing Agent-Based Migration Models in Pairs}, 
year={2019}, 
volume={}, 
number={}, 
pages={2713-2724}, 
abstract={Developing a realistic agent-based model of human migration requires particular care. Committing too early to a specific model architecture, design, or language environment can later become costly in terms of the revisions required. To examine specifically the impact of differences in implementation, we have developed two instances of the same model in parallel. One model is realized in the programming language Julia, the underlying execution semantics is of a discrete stepwise stochastic process. The other is realized in an external domain-specific language ML3, based on a continuous-time Markov chain (CTMC) semantics. By developing models in pairs in different approaches, important properties of the target model can be more effectively revealed. In addition, the realization within a programming language and an external domain-specific modeling language respectively, helped identifying crucial features and trade-offs for the future implementation of the model and the design of the domain-specific modeling language.}, 
keywords={Urban areas;Biological system modeling;Adaptation models;Computational modeling;Semantics;Data models;Stochastic processes}, 
doi={10.1109/WSC40007.2019.9004946}, 
ISSN={1558-4305}, 
month={Dec},}
@INPROCEEDINGS{7928002, 
author={Dwarakanath, Anurag and Era, Dipin and Priyadarshi, Aditya and Dubash, Neville and Podder, Sanjay}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Accelerating Test Automation through a Domain Specific Language}, 
year={2017}, 
volume={}, 
number={}, 
pages={460-467}, 
abstract={Test automation involves the automatic execution of test scripts instead of being manually run. This significantly reduces the amount of manual effort needed and thus is of great interest to the software testing industry. There are two key problems in the existing tools & methods for test automation - a) Creating an automation test script is essentially a code development task, which most testers are not trained on, and b) the automation test script is seldom readable, making the task of maintenance an effort intensive process. We present the Accelerating Test Automation Platform (ATAP) which is aimed at making test automation accessible to non-programmers. ATAP allows the creation of an automation test script through a domain specific language based on English. The English-like test scripts are automatically converted to machine executable code using Selenium WebDriver. ATAP's English-like test script makes it easy for non-programmers to author. The functional flow of an ATAP script is easy to understand as well thus making maintenance simpler (you can understand the flow of the test script when you revisit it many months later). ATAP has been built around the Eclipse ecosystem and has been used in a real-life testing project. We present the details of the implementation of ATAP and the results from its usage in practice.}, 
keywords={Automation;DSL;Tools;Selenium;Natural languages;Java;Programming;Test automation;Selenium;Xtext;DSL}, 
doi={10.1109/ICST.2017.52}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7005185, 
author={Peltola, Jukka and Sierla, Seppo and Vyatkin, Valeriy}, 
booktitle={Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)}, 
title={Adapting Keyword driven test automation framework to IEC 61131-3 industrial control applications using PLCopen XML}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Factory Acceptance Testing should involve customer's experts and knowledge in defining, reading and validating tests, while keeping labor costs at moderate level. This involvement requires a testing approach, which hides implementation details and emphasizes domain terminology. Keyword driven testing is seen a viable test automation solution to reduce cost and enable customer involvement in acceptance testing. We propose an approach for adaptation of Keyword driven testing framework to IEC 61131-3 industrial process control applications. It utilizes importing of application elements, presented with PLCopen XML, and transforming them to proxy objects to be used as variables in test code, with domain specific names. Benefits include simplification of test and keyword specifications and hiding of implementation details from testers.}, 
keywords={Testing;Libraries;XML;Automation;Process control;IEC standards;Radio frequency;Industrial Process Control System;IEC 61131-3;PLCopen XML;Factory Acceptance Testing;Test Automation;Keyword Driven Testing;Test Framework}, 
doi={10.1109/ETFA.2014.7005185}, 
ISSN={1946-0759}, 
month={Sep.},}
@INPROCEEDINGS{9626192, 
author={Schneid, Konrad and Stapper, Leon and Thöne, Sebastian and Kuchen, Herbert}, 
booktitle={2021 IEEE 25th International Enterprise Distributed Object Computing Conference (EDOC)}, 
title={Automated Regression Tests: A No-Code Approach for BPMN-based Process-Driven Applications}, 
year={2021}, 
volume={}, 
number={}, 
pages={31-40}, 
abstract={BPMN-based Process-Driven Applications (PDA) require less coding since they are not only based on source code, but also on executable process models. Automated testing of such model-driven applications gains growing relevance, and it becomes a key enabler if we want to found their development on continuous integration (CI) techniques.While process analysts are typically responsible for test case specifications from a business perspective, technically skilled process engineers take the responsibility for implementing the required test code. This is time-consuming and, due to their often different skills and backgrounds, might result in communication problems such as information losses and misunderstandings. This paper presents a new approach which enables an analyst to generate executable tests for PDAs without the need for manual coding. It consists of a sophisticated model analysis, a wizard-based specification of test cases, and a subsequent code generation. The resulting tests can easily be integrated into CI pipelines.The concept is underpinned by a user-friendly tool which has been evaluated in case studies and in real-world implementation projects from different industry sectors. During the evaluation, the prototype proved a more efficient test creation process and a higher test quality.}, 
keywords={Industries;Analytical models;Codes;Handheld computers;Conferences;Prototypes;Manuals;Model-Based Testing;BPMN;No-Code;Process-Driven Application}, 
doi={10.1109/EDOC52215.2021.00014}, 
ISSN={2325-6362}, 
month={Oct},}
@INPROCEEDINGS{8241120, 
author={Jumagaliyev, Assylbek and Whittle, Jon and Elkhatib, Yehia}, 
booktitle={2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)}, 
title={Using DSML for Handling Multi-tenant Evolution in Cloud Applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={272-279}, 
abstract={Multi-tenancy is sharing a single application's resources to serve more than a single group of users (i.e. tenant). Cloud application providers are encouraged to adopt multi-tenancy as it facilitates increased resource utilization and ease of maintenance, translating into lower operational and energy costs. However, introducing multi-tenancy to a single-tenant application requires significant changes in its structure to ensure tenant isolation, configurability and extensibility. In this paper, we analyse and address the different challenges associated with evolving an application's architecture to a multi-tenant cloud deployment. We focus specifically on multi-tenant data architectures, commonly the prime candidate for consolidation and multi-tenancy. We present a Domain-Specific Modeling language (DSML) to model a multi-tenant data architecture, and automatically generate source code that handles the evolution of the application's data layer. We apply the DSML on a representative case study of a single-tenant application evolving to become a multi-tenant cloud application under two resource sharing scenarios. We evaluate the costs associated with using this DSML against the state of the art and against manual evolution, reporting specifically on the gained benefits in terms of development effort and reliability.}, 
keywords={Databases;Data models;Load modeling;Computer architecture;Cloud computing;Software as a service;Business}, 
doi={10.1109/CloudCom.2017.31}, 
ISSN={2330-2186}, 
month={Dec},}
@INPROCEEDINGS{8425200, 
author={Petruzza, Steve and Treichler, Sean and Pascucci, Valerio and Bremer, Peer-Timo}, 
booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
title={BabelFlow: An Embedded Domain Specific Language for Parallel Analysis and Visualization}, 
year={2018}, 
volume={}, 
number={}, 
pages={463-473}, 
abstract={The rapid growth in simulation data requires large-scale parallel implementations of scientific analysis and visualization algorithms, both to produce results within an acceptable timeframe and to enable in situ deployment. However, efficient and scalable implementations, especially of more complex analysis approaches, require not only advanced algorithms, but also an in-depth knowledge of the underlying runtime. Furthermore, different machine configurations and different applications may favor different runtimes, i.e., MPI vs Charm++ vs Legion, etc., and different hardware architectures. This diversity makes developing and maintaining a broadly applicable analysis software infrastructure challenging. We address some of these problems by explicitly separating the implementation of individual tasks of an algorithm from the dataflow connecting these tasks. In particular, we present an embedded domain specific language (EDSL) to describe algorithms using a new task graph abstraction. This task graph is then executed on top of one of several available runtimes (MPI, Charm++, Legion) using a thin layer of library calls. We demonstrate the flexibility and performance of this approach using three different large scale analysis and visualization use cases, i.e., topological analysis, rendering and compositing dataflow, and image registration of large microscopy scans. Despite the unavoidable overheads of a generic solution, our approach demonstrates performance portability at scale, and, in some cases, outperforms hand-optimized implementations.}, 
keywords={Task analysis;Runtime;Software algorithms;Payloads;Software;Libraries;Rendering (computer graphics);Embedded DSL;user productivity;in situ analytics;Simulation runtime systems;programming models}, 
doi={10.1109/IPDPS.2018.00056}, 
ISSN={1530-2075}, 
month={May},}
@INPROCEEDINGS{6984109, 
author={Häser, Florian and Felderer, Michael and Breu, Ruth}, 
booktitle={2014 9th International Conference on the Quality of Information and Communications Technology}, 
title={Test Process Improvement with Documentation Driven Integration Testing}, 
year={2014}, 
volume={}, 
number={}, 
pages={156-161}, 
abstract={Improving the maturity of the test process in an organization, especially but not limited to integration testing, involves obstacles and risks, such as the additional work overhead of the new process. In addition, integration testing descriptions are often too technical not addressing the language needs of the domain. In research cooperations with companies from the insurance and banking domain it turned out that test descriptions and reports are one of the most useful testing artifacts, while doing adhoc testing. This paper presents a bottom up testing approach, which first helps the integration tester in producing a semi-formal test description and report, up to be an enabler for automatic model-based testing in the very end. The presented approach is based on a textual domain specific language that is able to evolve over time. This is done by analyzing the test descriptions and reports automatically with machine learning techniques as well as manually by integration testers. Often recurring test steps or used components are integrated into the test language, making it specially tailored for a specific organization. For each test step implementations can be attached, preparing it for the next iteration. In this paper the methodology and architecture of our integration testing approach are presented together with the underlying language concepts.}, 
keywords={Testing;Documentation;Unified modeling language;DSL;Insurance;Companies;Model-Based Integration Testing;Test Process Improvement;Regression Testing}, 
doi={10.1109/QUATIC.2014.29}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5626417, 
author={Strasser, T and Peters, T and Jägle, H. and Zrenner, E. and Wilke, R.}, 
booktitle={2010 Annual International Conference of the IEEE Engineering in Medicine and Biology}, 
title={An integrated domain specific language for post-processing and visualizing electrophysiological signals in Java}, 
year={2010}, 
volume={}, 
number={}, 
pages={4687-4690}, 
abstract={Electrophysiology of vision - especially the electroretinogram (ERG) - is used as a non-invasive way for functional testing of the visual system. The ERG is a combined electrical response generated by neural and non-neuronal cells in the retina in response to light stimulation. This response can be recorded and used for diagnosis of numerous disorders. For both clinical practice and clinical trials it is important to process those signals in an accurate and fast way and to provide the results as structured, consistent reports. Therefore, we developed a freely available and open-source framework in Java (http://www.eye.uni-tuebingen.de/project/idsI4sigproc). The framework is focused on an easy integration with existing applications. By leveraging well-established software patterns like pipes-and-filters and fluent interfaces as well as by designing the application programming interfaces (API) as an integrated domain specific language (DSL) the overall framework provides a smooth learning curve. Additionally, it already contains several processing methods and visualization features and can be extended easily by implementing the provided interfaces. In this way, not only can new processing methods be added but the framework can also be adopted for other areas of signal processing. This article describes in detail the structure and implementation of the framework and demonstrate its application through the software package used in clinical practice and clinical trials at the University Eye Hospital Tuebingen one of the largest departments in the field of visual electrophysiology in Europe.}, 
keywords={Lead;Java;Artificial neural networks;Hospitals;HTML;Visualization}, 
doi={10.1109/IEMBS.2010.5626417}, 
ISSN={1558-4615}, 
month={Aug},}
@INPROCEEDINGS{7323087, 
author={Iber, Johannes and Kajtazović, Nermin and Höller, Andrea and Rauter, Tobias and Kreiner, Christian}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Ubtl UML testing profile based testing language}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={The continuous increase of software complexity is one of the major problems associated with the development of today's complex technical systems. In particular, for safety-critical systems, which usually require to be thoroughly verified and validated, managing such a complexity is of high importance. To this end, industry is utilizing Model-Driven Development (MDD) in many aspects of systems engineering, including verification and validation activities. Until now many specifications and standards have been released by the MDD community to support those activities by putting models in focus. The general problem is, however, that applying those specifications is often difficult, since they comprise a broader scope than usually required to solve specific problems. In this paper we propose a domain-specific language (DSL) that allows to specify tests from the UML Testing Profile (UTP). The main contribution is that only particular aspects of UTP are captured, thereby allowing the MDD process to be narrowed to specific needs, such as supporting code generation facilities for certain types of tests or even specific statements in tests. In the end we show the application of the DSL using a simple example within a MDD process, and we report on performance of that process.}, 
keywords={Unified modeling language;Testing;Generators;Biological system modeling;DSL;Software;Concrete;UML Testing Profile;UML;Textual Domain-Specific Language;Test Specification Language;Software Testing;Model-Driven Development}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7862407, 
author={Wienke, Johannes and Wrede, Sebastian}, 
booktitle={2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)}, 
title={Continuous regression testing for component resource utilization}, 
year={2016}, 
volume={}, 
number={}, 
pages={273-280}, 
abstract={Unintended changes in the utilization of resources like CPU and memory can lead to severe problems for the operation of robotics and intelligent systems. Still, systematic testing for such performance regressions has largely been ignored in this domain. We present a method to specify and execute performance tests for individual components of component-based robotics systems based on their component interfaces. The method includes an automatic analysis of each component revision against previous ones that reports potential changes to the resource usage characteristics. This informs developers about the impact of their changes. We describe the design of the framework and present evaluation results for the automatic detection of performance changes based on tests for a variety of robotics components.}, 
keywords={Robots;Testing;Middleware;Intelligent systems;Radiation detectors;Systematics}, 
doi={10.1109/SIMPAR.2016.7862407}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8904507, 
author={Losvik, Daneil Steen and Rutle, Adrian}, 
booktitle={2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
title={A Domain-Specific Language for the Development of Heterogeneous Multi-robot Systems}, 
year={2019}, 
volume={}, 
number={}, 
pages={549-558}, 
abstract={In this paper, we explore how model-driven software engineering can be used in the development of heterogeneous multi-robot systems where we have different robots with different capabilities. Multiple robots can achieve more complex tasks that are impossible to achieve for a single robot alone. We propose a framework where simple actions are used as building blocks to define larger tasks that require multiple robots with different capabilities to achieve. We show how task distribution can be performed in such a system and how the robot operating system can be utilized. We also show how a user interface can be used to define multiple different missions for a team of heterogeneous robots without the need for regeneration of code and redeployment on each robot.}, 
keywords={model-driven;mdse;domain-specific;dsl;robotics;multi robot;heterogeneous multi-robot system;heterogeneous multi-robot;task definition robotics}, 
doi={10.1109/MODELS-C.2019.00085}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5974321, 
author={Prasetya, I.S.W.B. and Amorim, J. and Vos, T.E.J. and Baars, A.}, 
booktitle={6th Iberian Conference on Information Systems and Technologies (CISTI 2011)}, 
title={Using Haskell to script combinatoric testing of Web Services}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The Classification Tree Method (CTM) is a popular approach in functional testing as it allows the testers to systematically partition the input domain of an SUT, and specifies the combinations they want. We have implemented the approach as a small domain specific language (DSL) embedded in the functional language Haskell. Such an embedding leads to clean syntax and moreover we can natively access Haskell's full features. This paper will explain the approach, and how it is applied for testing Web Services.}, 
keywords={Cities and towns;Strips;automated testing;combinatoric testing}, 
doi={}, 
ISSN={2166-0735}, 
month={June},}
@INPROCEEDINGS{6058739, 
author={Headrick, William J and Bodkin, Michael A and Fox, Robert R and Davis, Timothy W and Dusch, Kevin and Wolfe, Dan}, 
booktitle={2011 IEEE AUTOTESTCON}, 
title={Signal Based Domain Specific Language (SBDSL) a proposal for a next generation test}, 
year={2011}, 
volume={}, 
number={}, 
pages={240-244}, 
abstract={Signal Based Domain Specific Language (SBDSL) is a domain specific language which combines the use of ATLAS Signal statements with high-level programming language constructs. The goals of this new language are: facilitate the writing of concurrent test programs, provide a language that is easy to extend with new constructs, maintain backwards compatibility with ATLAS Family of languages, enable interoperability between test stations, and enable engineers' fresh out of college to quickly become productive with a test programming language. This paper will cover how the design of the SBDSL language, SBDSL Integrated Development Environment (IDE) and runtime executable will accomplish these goals and present results from the technology demonstration developed.}, 
keywords={Hardware;Instruments;Visualization;Syntactics;Debugging;Programming;Libraries}, 
doi={10.1109/AUTEST.2011.6058739}, 
ISSN={1558-4550}, 
month={Sep.},}
@INPROCEEDINGS{802127, 
author={Reyes, A.A. and Richardson, D.}, 
booktitle={14th IEEE International Conference on Automated Software Engineering}, 
title={Siddhartha: a method for developing domain-specific test driver generators}, 
year={1999}, 
volume={}, 
number={}, 
pages={81-90}, 
abstract={Siddhartha applies the domain-specific language (DSL) paradigm to solve difficult problems in specification-based testing (SBT). Domain-specific test case data specifications (TestSpecs) and difficult-to-test program design styles engender difficult SBT problems, which are the essential phenomena of interest to Siddhartha. Difficult-to-test program design styles are explicitly represented by domain-specific, unit test driver reference designs that accommodate the problematic program design styles. DSLs are developed to represent both TestSpecs and Driver reference designs. A DSL language processing tool (a translator) is developed that maps TestSpecs into Drivers. We developed a prototype implementation of Siddhartha via Reasoning SDK (formerly known as Software Refinery) and developed two domain-specific TestSpec/spl rarr/Driver translators. Each translator generated Drivers that revealed new failures in a real-world digital flight control application program.}, 
keywords={Automatic testing;Application software;DSL;Aerospace control;Computer science;Aerospace electronics;Domain specific languages;Prototypes;Formal specifications;Automatic control}, 
doi={10.1109/ASE.1999.802127}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{9103781, 
author={Liu, Huabin}, 
booktitle={2020 International Conference on Computer Engineering and Application (ICCEA)}, 
title={A Universal Automated Test Solution for Trunking Communication System}, 
year={2020}, 
volume={}, 
number={}, 
pages={47-51}, 
abstract={In order to improve the portability of the automated test of the trunking communication system, this paper proposed a universal automated test solution for the trunking communication system. It's based on the general architecture design of the trunking communication system, providing a replaceable communication protocol codec module. With the DSL-defined test script language describing the test cases, and efficient scheduling schemes for the test task, the automated functional test and performance test of the trunking communication system are realized. Theoretical analysis and experimental results show that the minimum load test task scheduling scheme based on user operation load prediction has lower response time and lower load balancing effect compared with the traditional static task scheduling scheme.}, 
keywords={Task analysis;Testing;Protocols;Codecs;Media;Computer architecture;component;trunking communication;automated test;test task scheduling}, 
doi={10.1109/ICCEA50009.2020.00017}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{713476, 
author={Daou, F.H.}, 
booktitle={1998 IEEE AUTOTESTCON Proceedings. IEEE Systems Readiness Technology Conference. Test Technology for the 21st Century (Cat. No.98CH36179)}, 
title={Overview of ADSL test requirement towards conformance, performance and interoperability}, 
year={1998}, 
volume={}, 
number={}, 
pages={413-420}, 
abstract={The enormous installed base of copper in the access network with the right transmission techniques present a huge potential for delivering broadband services to bandwidth hungry customers. Various Digital Subscriber line technologies (xDSL) employ various transmission methods and efficiently utilize the last available bandwidth on existing copper wires. Asymmetric Digital Subscriber Line (ADSL) delivers up to 6 Mbps to the user. This transmission of 6 Mbps is achieved using sophisticated modulation and compression techniques in a spectrum up to 1.1 MHz, pushing the physical limit on the usable bandwidth in the copper. This technology co-exists with impairments, noise intrusions, bridge taps, and other non-spectrally compatible transmissions. This paper presents an overview of xDSL Technology (which includes ADSL), the test challenges facing ADSL technology, and outlines the three areas of ADSL tests needed to ensure product conformance and cross vendor interoperability.}, 
keywords={Testing;Copper;Bandwidth;Bit rate;OFDM modulation;Quadrature amplitude modulation;DSL;Modulation coding;Working environment noise;Phase modulation}, 
doi={10.1109/AUTEST.1998.713476}, 
ISSN={1088-7725}, 
month={Aug},}
@INPROCEEDINGS{6233406, 
author={Landhäußer, 1Mathias and Genaid, Adrian}, 
booktitle={2012 Third International Workshop on Recommendation Systems for Software Engineering (RSSE)}, 
title={Connecting User Stories and code for test development}, 
year={2012}, 
volume={}, 
number={}, 
pages={33-37}, 
abstract={User Stories are short feature descriptions from the user's point of view. Functional tests ensure that the feature described by a User Story is fully implemented. We present a tool that builds an ontology for code and links completed User Stories in natural language with the related code artifacts. The ontology also contains links to API components that were used to implement the functional tests. Preliminary results show that these links can be used to recommend reusable test steps for new User Stories.}, 
keywords={Ontologies;Natural languages;Software;Data structures;Boolean functions;Compounds;Testing;code mining;functional testing;reasoning;traceability;ontology}, 
doi={10.1109/RSSE.2012.6233406}, 
ISSN={2327-0942}, 
month={June},}
@INPROCEEDINGS{8115712, 
author={Meftah, Lakhdar and Gomez, Maria and Rouvoy, Romain and Chrisment, Isabelle}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={ANDROFLEET: Testing WiFi peer-to-peer mobile apps in the large}, 
year={2017}, 
volume={}, 
number={}, 
pages={961-966}, 
abstract={WiFi P2P allows mobile apps to connect to each other via WiFi without an intermediate access point. This communication mode is widely used by mobile apps to support interactions with one or more devices simultaneously. However, testing such P2P apps remains a challenge for app developers as i) existing testing frameworks lack support for WiFi P2P, and ii) WiFi P2P testing fails to scale when considering a deployment on more than two devices. In this paper, we therefore propose an acceptance testing framework, named Androfleet, to automate testing of WiFi P2P mobile apps at scale. Beyond the capability of testing point-to-point interactions under various conditions, An-drofleet supports the deployment and the emulation of a fleet of mobile devices as part of an alpha testing phase in order to assess the robustness of a WiFi P2P app once deployed in the field. To validate Androfleet, we demonstrate the detection of failing black-box acceptance tests for WiFi P2P apps and we capture the conditions under which such a mobile app can correctly work in the field. The demo video of Androfleet is made available from https://youtu.be/gJ5_Ed7XL04.}, 
keywords={Wireless fidelity;Peer-to-peer computing;Testing;Mobile communication;Androids;Humanoid robots;Mobile handsets}, 
doi={10.1109/ASE.2017.8115712}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4052587, 
author={Yilmaz, Cemal and Porter, Adam and Krishna, Arvind S. and Memon, Atif M. and Schmidt, Douglas C. and Gokhale, Aniruddha S. and Natarajan, Balachandran}, 
journal={IEEE Transactions on Software Engineering}, 
title={Reliable Effects Screening: A Distributed Continuous Quality Assurance Process for Monitoring Performance Degradation in Evolving Software Systems}, 
year={2007}, 
volume={33}, 
number={2}, 
pages={124-141}, 
abstract={Developers of highly configurable performance-intensive software systems often use in-house performance-oriented "regression testing" to ensure that their modifications do not adversely affect their software's performance across its large configuration space. Unfortunately, time and resource constraints can limit in-house testing to a relatively small number of possible configurations, followed by unreliable extrapolation from these results to the entire configuration space. As a result, many performance bottlenecks escape detection until systems are fielded. In our earlier work, we improved the situation outlined above by developing an initial quality assurance process called "main effects screening". This process 1) executes formally designed experiments to identify an appropriate subset of configurations on which to base the performance-oriented regression testing, 2) executes benchmarks on this subset whenever the software changes, and 3) provides tool support for executing these actions on in-the-field and in-house computing resources. Our initial process had several limitations, however, since it was manually configured (which was tedious and error-prone) and relied on strong and untested assumptions for its accuracy (which made its use unacceptably risky in practice). This paper presents a new quality assurance process called "reliable effects screening" that provides three significant improvements to our earlier work. First, it allows developers to economically verify key assumptions during process execution. Second, it integrates several model-driven engineering tools to make process configuration and execution much easier and less error prone. Third, we evaluate this process via several feasibility studies of three large, widely used performance-intensive software frameworks. Our results indicate that reliable effects screening can detect performance degradation in large-scale systems more reliably and with significantly less resources than conventional techniques}, 
keywords={Quality assurance;Monitoring;Degradation;Software systems;Performance evaluation;Software performance;Software testing;System testing;Time factors;Extrapolation;Distributed continuous quality assurance;performance-o-ri-ented regression testing;design-of-experiments theory.}, 
doi={10.1109/TSE.2007.20}, 
ISSN={1939-3520}, 
month={Feb},}
@INPROCEEDINGS{7437001, 
author={Hafidhoh, Nisa'ul and Liem, Inggriani and Azizah, Fazat Nur}, 
booktitle={2015 International Conference on Data and Software Engineering (ICoDSE)}, 
title={Source code generator for automating business rule implementation}, 
year={2015}, 
volume={}, 
number={}, 
pages={219-224}, 
abstract={Business rules can be implemented on business processes, business behavior, people, or software in an organization. Aligned with software development, business rules are captured from requirement elicitation and analysis, then designed and implemented in the software. The changes of business environment may affect business rules. The changes of the business rules may bring impact in the software, so that the software needs to be redeveloped. In this paper, we present a source code generator to automate business rule implementation using business rule approach. We propose a Domain Specific Language (DSL) of business rule to help expressing business rules in a business-friendly language. We also develop a business rule generator to generate source codes based on a DSL script for expressing the business rules. The proposed solution has been tested in two case studies. It is shown that the generator can help the implementation of the business rules in source codes and that the generated code can be used in business applications.}, 
keywords={DSL;Natural languages;Software;Generators;Organizations;Grammar;business rule;implementation;source code generator;DSL}, 
doi={10.1109/ICODSE.2015.7437001}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4600272, 
author={Sadri, S.M.R. and Harandi, Y. N. and Pirhadi, M. and Waskasi, M. Yaghoubi and Tabrizipoor, A. Iravani and Mirzabaghi, M.}, 
booktitle={2007 International Symposium on High Capacity Optical Networks and Enabling Technologies}, 
title={Test strategy for DSL broadband IP access services}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={In this paper the test methodology has been expanded for evaluation of DSL broadband services. This is a new strategy for testing and considering different aspects of test. This strategy can precisely test various features of DSL broadband equipment and services which can be delivered by them in terms of different aspects of testing such as functionality, performance, conformance, etc. It was practically executed over a designed testbed in Iran Telecommunications Research Center (ITRC) where a lab named NGN Pilot exists. The introduced strategy has a hierarchical structure from the topmost level of testing including different categories to the bottommost level including detailed test topics and test cases. This paper elaborates how the strategy was designed and how it can be applied to a desired environment to compare different DSL scenarios with each other. The strategy can then be used by a service provider to help deciding which solutions can achieve the best outcome.}, 
keywords={Modems;Broadband communication;IP networks;Multiprotocol label switching;Throughput;Quality of service;Next generation networking;DSL;Broadband Access;Test Strategy;Testbed;Pilot}, 
doi={10.1109/HONET.2007.4600272}, 
ISSN={1949-4106}, 
month={Nov},}
@INPROCEEDINGS{8877042, 
author={España, Sergio and Bik, Niels and Overbeek, Sietse}, 
booktitle={2019 13th International Conference on Research Challenges in Information Science (RCIS)}, 
title={Model-driven engineering support for social and environmental accounting}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={Social and environmental accounting (SEA) is becoming commonplace in organisations that want to assess and report on their business ethics and sustainability performance. While there is an abundance of methods and standards that define how to perform such practice, tool support is much more limited and does not fully match the needs of organisations. In this paper, we present a model-driven engineering approach to support SEA. We are engineering a domain-specific language that allows creating models of SEA methods and we are implementing a tool that interprets such models at runtime, effectively supporting the accounting, and providing basic model management operations. This paper presents the preliminary results and, as a running example, we use UniSAF, a SEA method to assess the sustainability of universities. The results are promising, according to two SEA experts we have consulted, and have triggered the interest of the Fair Trade Software Foundation.}, 
keywords={Tools;DSL;Sustainable development;Companies;Unified modeling language;ISO Standards;social and environmental accounting;sustainability reporting;model-driven engineering;domain-specific language;method engineering;software for responsibility;ICT for sustainability}, 
doi={10.1109/RCIS.2019.8877042}, 
ISSN={2151-1357}, 
month={May},}
@INPROCEEDINGS{7985680, 
author={Rolim, Reudismam and Soares, Gustavo and D'Antoni, Loris and Polozov, Oleksandr and Gulwani, Sumit and Gheyi, Rohit and Suzuki, Ryo and Hartmann, Björn}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)}, 
title={Learning Syntactic Program Transformations from Examples}, 
year={2017}, 
volume={}, 
number={}, 
pages={404-415}, 
abstract={Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.}, 
keywords={DSL;Programming profession;Tools;C# languages;Pattern matching;Open source software;Program transformation;program synthesis;tutoring systems;refactoring}, 
doi={10.1109/ICSE.2017.44}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{9000047, 
author={Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin}, 
booktitle={2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Achieving Test Automation with Testers without Coding Skills: An Industrial Report}, 
year={2018}, 
volume={}, 
number={}, 
pages={749-756}, 
abstract={We present a process driven test automation solution which enables delegating (part of) automation tasks from test automation engineer (expensive resource) to test analyst (non-developer, less expensive). In our approach, a test automation engineer implements test steps (or actions) which are executed automatically. Such automated test steps represent user actions in the system under test and specified by a natural language which is understandable by a non-technical person. Then, a test analyst with a domain knowledge organizes automated steps combined with test input to create an automated test case. It should be emphasized that the test analyst does not need to possess programming skills to create, modify or execute automated test cases. We refine benchmark test automation architecture to be better suitable for an effective separation and sharing of responsibilities between the test automation engineer (with coding skills) and test analyst (with a domain knowledge). In addition, we propose a metric to empirically estimate cooperation between test automation engineer and test analyst's works. The proposed automation solution has been defined based on our experience in the development and maintenance of Helsenorge, the national electronic health services in Norway which has had over one million of visits per month past year, and we still use it to automate the execution of regression tests.}, 
keywords={Test automation;process-driven test automation;keyword-driven test automation;DSL for test automation;Helsenorge}, 
doi={10.1145/3238147.3240463}, 
ISSN={2643-1572}, 
month={Sep.},}
@INPROCEEDINGS{7102636, 
author={Herbold, Steffen and De Francesco, Alberto and Grabowski, Jens and Harms, Patrick and Hillah, Lom M. and Kordon, Fabrice and Maesano, Ariele-Paolo and Maesano, Libero and Di Napoli, Claudia and De Rosa, Fabio and Schneider, Martin A. and Tonellotto, Nicola and Wendland, Marc-Florian and Wuillemin, Pierre-Henri}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={The MIDAS Cloud Platform for Testing SOA Applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={While Service Oriented Architectures (SOAs) are for many parts deployed online, and today often in a cloud, the testing of the systems still happens mostly locally. In this paper, we want to present the MIDAS Testing as a Service (TaaS), a cloud platform for the testing of SOAs. We focus on the testing of whole SOA orchestrations, a complex task due to the number of potential service interactions and the increasing complexity with each service that joins an orchestration. Since traditional testing does not scale well with such a complex setup, we employ a Model-based Testing (MBT) approach based on the Unified Modeling Language (UML) and the UML Testing Profile (UTP) within MIDAS. Through this, we provide methods for functional testing, security testing, and usage-based testing of service orchestrations. Through harnessing the computational power of the cloud, MIDAS is able to generate and execute complex test scenarios which would be infeasible to run in a local environment.}, 
keywords={Testing;Unified modeling language;DSL;Cloud computing;Service-oriented architecture;Monitoring}, 
doi={10.1109/ICST.2015.7102636}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{5261058, 
author={McMahon, Chris}, 
booktitle={2009 Agile Conference}, 
title={History of a Large Test Automation Project Using Selenium}, 
year={2009}, 
volume={}, 
number={}, 
pages={363-368}, 
abstract={In 2007 I started work as a tester for a company called Socialtext. When I joined the company there was already a Selenium-based test framework in place, but there were only a couple of automated test cases created; we had about 400 test steps, or individual assertions about the behavior of the application. When I left Socialtext two years later, we had just surpassed 10,000 test steps in the main set of regression tests. We also had browser-specific test sets in place, an automated test case for visually checking the application, and a Continuous-Integration-like script that ran all day and all night against the latest version of the code. At about 4000 test steps, regression bugs released to production dropped essentially to zero. The other 6000 test steps covered ongoing new features in the project, and more robust testing of the older application functions. This report discusses how I helped grow this system, and the things we learned along the way that helped it be such a successful ongoing project. The report covers initial conditions and test design; discusses issues in application feature coverage; how and when to grow the system quickly; a couple of test design smells that caused us problems along the way; how we treat Continuous Integration in a system like this; and how we coped when significant parts of the User Interface were completely re-engineered.}, 
keywords={History;Automatic testing;System testing;Quality management;Failure analysis;User interfaces;Engineering profession;Home automation;Radio access networks;Computer bugs;UI automation;Selenium;test design;experience}, 
doi={10.1109/AGILE.2009.9}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6498461, 
author={Duclos, Etienne and Le Digabel, Sébastien and Guéhéneuc, Yann-Gaël and Adams, Bram}, 
booktitle={2013 17th European Conference on Software Maintenance and Reengineering}, 
title={ACRE: An Automated Aspect Creator for Testing C++ Applications}, 
year={2013}, 
volume={}, 
number={}, 
pages={121-130}, 
abstract={We present ACRE, an Automated aspect creator, to use aspect-oriented programming (AOP) to perform memory, invariant and interferences testing for software programs written in C++. ACRE allows developers without knowledge in AOP to use aspects to test their programs without modifying the behavior of their source code. ACRE uses a domain-specific language (DSL), which statements testers insert into the source code like comments to describe the aspects to be used. The presence of DSL statements in the code does not modify the program's compilation and behavior. ACRE parses the DSL statements and automatically generates appropriate aspects that are then weaved into the source code to identify bugs due to memory leaks, incorrect algorithm implementation, or interference among threads. Thanks to the use of aspects and ACRE, testers can add or remove tests easily. Using an aspect generated by ACRE, we find a memory leak in a complex C++ software program, NOMAD, used in both industry and research. We also verify a crucial mathematical point of the algorithm behind NOMAD and collect data to find possible interference bugs, in NOMAD.}, 
keywords={Testing;DSL;Interference;Computer bugs;Radiation detectors;Java;Weaving;AOP;C++;NOMAD;interference bug pattern;memory testing;invariant testing}, 
doi={10.1109/CSMR.2013.22}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7095983, 
author={Kayama, Mizue and Ogata, Shinpei and Nagai, Takashi and Yokoka, Hiroaki and Masumoto, Kento and Hashimoto, Masami}, 
booktitle={2015 IEEE Global Engineering Education Conference (EDUCON)}, 
title={Effectiveness of Model-Driven Development in conceptual modeling education for university freshmen}, 
year={2015}, 
volume={}, 
number={}, 
pages={274-282}, 
abstract={The purpose of this study is to explore educational methods for conceptual modeling for novices. In this research, the subjects are mainly freshmen in university. Model driven development (MDD) and a domain specific language (DSL) are key factors in this study. By using MDD, learners are expected to be able to evaluate their own model by observing the target device's behavior. By using a DSL, teachers can control the difficulty of the problems given to their learners. In this paper, we describe our research approach using MDD and a DSL, then, show our experiment design and results. We also discuss the effectiveness of MDD in university freshmen courses with proposed educational methodology.}, 
keywords={Unified modeling language;Object oriented modeling;DSL;Computational modeling;Robots;Conferences;Analytical models;model driven development;university freshmen;state machine diagram;model quality;achievment level}, 
doi={10.1109/EDUCON.2015.7095983}, 
ISSN={2165-9567}, 
month={March},}
@INPROCEEDINGS{7323106, 
author={Beckmann, Kai}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Integrating existing proprietary system models into a model-driven test process for an industrial automation scenario}, 
year={2015}, 
volume={}, 
number={}, 
pages={255-262}, 
abstract={The introduction of modern model-driven software development methodologies into the industrial practise still proves to be a challenge. Especially small or medium-sized enterprises (SMEs) need an incremental and continuous modernisation process, which incorporates existing projects, is customised and cost-effective. Particularly, suitable solutions for model-based or -driven testing with test automation to increase the efficiency are in demand. This paper presents an approach for integrating existing proprietary system models of an SME partner for describing industrial automation processes into a model-driven test process, utilising a domain-specific language for the test specification. The test objectives focuses on the correct implementation of the communication and synchronisation of distributed state machines. The presented approach is integrated into a test framework, which is based on the Eclipse Modelling Framework (EMF) and the Eclipse Test and Performance Tools Platform Project (TPTP) framework. To separate the possibly changeable system and DSL-specific models from the implementation of the test framework, a stable and more generic test meta model was defined.}, 
keywords={Unified modeling language;Adaptation models;DSL;Object oriented modeling;Biological system modeling;Software;Automation;MDSD;DSL;Metamodelling;Testing;MDT;Model-driven Testing}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8990249, 
author={Janes, Andrea and Russo, Barbara}, 
booktitle={2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={Automatic Performance Monitoring and Regression Testing During the Transition from Monolith to Microservices}, 
year={2019}, 
volume={}, 
number={}, 
pages={163-168}, 
abstract={The transition from monolith to microservices poses several challenges, like how to redistribute the features of system over different microservices. During the transition, developers may also redesign or rethink system services significantly, which can have a strong impact on various quality aspects of the resulting system. Thus, the new system may be more or less performing depending on the ability of the developers to design microservices and the capability of the microservice architecture to represent the system. Overall, a transition to microservices may or may not end up with the same or a better performing system. One way to control the migration to microservices is to continuously monitor a system by continuously collecting performance data and feeding the resulting data analysis back in the transition process. In DevOps, such continuous feedback can be exploited to re-tune the development and deployment of system's builds. In this paper, we present PPTAM+, a tool to continuously assess the degradation of a system during a transition to microservices. In an in-production system, the tool can continuously monitor each microservice and provide indications of lost performance and overall degradation. The system is designed to be integrated in a DevOps process. The tool automates the whole process from collecting data for building the reference operational profile to streamline performance data and automatically adapt and regress performance tests on each build based the analysis' feedback obtained from tests of the previous build.}, 
keywords={Microservices;DevOps;Testing}, 
doi={10.1109/ISSREW.2019.00067}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{5069041, 
author={Clark, Tony}, 
booktitle={2009 ICSE Workshop on Automation of Software Test}, 
title={Model based functional testing using pattern directed filmstrips}, 
year={2009}, 
volume={}, 
number={}, 
pages={53-61}, 
abstract={Model driven functional system testing generates test scenarios from behavioural and structural models. In order to autmatically generate tests, conditions such as invariants and pre-/post-conditions must be precisely defined. UML provides the Object Constraint Language (OCL) for this purpose; however OCL expressions can become very complex. This paper describes an approach that allows many commonly found OCL patterns to be expressed as snapshot patterns that correspond directly to the information model diagrams. Behaviour is constructed as chains of snapshots, or filmstrips. Snapshots and filmstrips are as expressive as UML behaviour models and OCL but it is argued that they are more accessible and more modular.}, 
keywords={System testing;Unified modeling language;Logic testing;Software testing;Context modeling;Software engineering;Test pattern generators;Process design;Concrete;Graphical user interfaces}, 
doi={10.1109/IWAST.2009.5069041}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7102598, 
author={Alegroth, Emil and Bache, Geoffrey and Bache, Emily}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={On the Industrial Applicability of TextTest: An Empirical Case Study}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Software systems are becoming more complex, not least in their Graphical User Interfaces (GUIs), which presents challenges for existing testing practices. Pressure to reduce time to market leaves less time for manual testing and increases the importance of test automation. Previous research has identified several generations of automated GUI-based test approaches with different cost-benefit tradeoffs. Whilst test automation provides fast quality feedback it can be associated with high costs and inability to identify defects not explicitly anticipated by the test designer. TextTest is a capture-replay tool for GUI-based testing with a novel approach that overcomes several of the challenges experienced with previous approaches. Firstly the tool supports Approval Testing, an approach where ASCII-art representations of the GUI's visual state are used to verify correct application behavior at the system level. Secondly it records and replays test scripts in a user defined domain specific language (DSL) that is readable by all stakeholders. In this paper we present a three phase industrial case study that aims to identify TextTest's applicability in industrial practice. The paper reports that the tool is associated with (1) low script development costs due to recording functionality, (2) low maintenance costs, on average 7 minutes per test case, (3) better defect finding ability than manual system testing, (4) high test case execution performance (In this case 500 test cases in 20 minutes), (5) high script readability due to DSL defined scripts, and (6) test suites that are robust to change (In this case 93 percent per iteration). However, the tool requires a higher degree of technical skill for customization work, test maintainers need skills in designing regular expressions and the tool's applicability is currently restricted to Java and Python based applications.}, 
keywords={Graphical user interfaces;Testing;Maintenance engineering;DSL;Companies;Manuals;Data collection}, 
doi={10.1109/ICST.2015.7102598}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{4547343, 
author={Syed, Tariq and Das, Sunil R. and Biswas, Satyendra N. and Petriu, Emil M.}, 
booktitle={2008 IEEE Instrumentation and Measurement Technology Conference}, 
title={Developing Automated Test System for ADSL Equipment}, 
year={2008}, 
volume={}, 
number={}, 
pages={1833-1838}, 
abstract={The requirement for an automated test system has immensely increased due to the realization that manual testing is associated with additional resources and staffing constraints. In order to achieve a competitive edge, reduced development cost, timely product delivery, and product quality are mandatory in today's organization. Manual testing requires skilled operators that increase cost, time, and product delivery. The low cost computer based automated system helps to get an edge by fulfilling these organizational demands. In this paper, an automated testing system has been developed to support functional testing of Nortel Network's modem system (1-Meg SUT). The modem is an inherently complex asymmetric digital subscriber line (ADSL) product and its testing is far more complex than just verification of process faults. The complexity of ADSL system renders automated test system an important and imperative part of ADSL testing. The subject paper demonstrates the indispensable need of automated test system for ADSL testing and its advantages in providing a competitive edge for the organization.}, 
keywords={Automatic testing;System testing;Automation;Modems;Costs;DSL;Information technology;Internet;Graphical user interfaces;Software testing;Asymmetric digital subscriber line (ADSL);automated testing;hardware platform;software platform}, 
doi={10.1109/IMTC.2008.4547343}, 
ISSN={1091-5281}, 
month={May},}
@INPROCEEDINGS{9155827, 
author={Arrieta, Aitor and Agirre, Joseba A. and Sagardui, Goiuria}, 
booktitle={2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={A Tool for the Automatic Generation of Test Cases and Oracles for Simulation Models Based on Functional Requirements}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Simulation models are frequently used to model, simulate and test complex systems (e.g., Cyber-Physical Systems (CPSs)). To allow full test automation, test cases and test oracles are required. Safety standards (e.g., the ISO 26262) highly recommend that the test cases of systems like CPSs are associated to requirements. As a result, typically, test cases that need to cover specific requirements are manually generated in the context of simulation models. This is, of course, a time-consuming and non-systematic process. However, the current practice lacks tools that generate test cases by considering functional requirements for simulation-based testing. In this short paper we propose a Domain-Specific Language (DSL) for specifying requirements for simulation-based testing in an easy manner. These files are later parsed by an automatic test generation algorithm, which generates test cases that follow the ASAM-XiL standard. The tool was integrated with two professional tools: (1) SYNECT from dSPACE and (2) xMOD from FEV. An initial validation was also performed with an industrial simulation model from YASA motors.}, 
keywords={Tools;DSL;Test pattern generators;Generators;Standards;Subspace constraints;Simulation-based Testing;Functional Requirements;Test Case Generation}, 
doi={10.1109/ICSTW50294.2020.00018}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{9143910, 
author={Wolde, Behailu Getachew and Boltana, Abiot Sinamo}, 
booktitle={2020 Seventh International Conference on Software Defined Systems (SDS)}, 
title={Behavior-Driven Re-engineering for Testing the Cloud}, 
year={2020}, 
volume={}, 
number={}, 
pages={75-82}, 
abstract={Cloud is an emerging technology in software development by its capability using orchestration to provide composite services through implementations somewhere on the Internet. Once it is composed the services can be easily usable by diversified clients. In practice, however, each client relies on the presentation of high level features to utilize the required Service Level Agreements. On top of this, test engineer knows only these features while the implementation and the infrastructure remain hidden. Testing becomes challenging the classical and legacy testing procedure. The paper describes the importance of cloud challenges and elaborates the appropriate way like a behavior- driven model-based re-engineering to design and validate test instances with a sample scenario for testing the cloud. It also illustrates this scenario with GraphWalker model which generates test paths through a Finite State Machine through which it enables instances of domain specific language. The generated paths include forwarding inputs and assertions to validate quality of test instances. As evaluation, the tester uses an Open Source Web Application implemented in Eclipse environment with REST assured libraries. The web application setup is configured on the cloud before a testing operation is done. Then, the implemented service is deployed on Azure platform as-a Service using Microsoft Azure plugin toolkit for Eclipse to execute the test plans on specified service.}, 
keywords={Testing;Cloud computing;Software;Object oriented modeling;Data models;Java;Measurement;REST Service;Behavior-Driven Model;Test Path Generation;Domain Specific Language}, 
doi={10.1109/SDS49854.2020.9143910}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7321521, 
author={Schuts, Mathijs and Hooman, Jozef}, 
booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
title={Using Domain Specific Languages to improve the development of a power control unit}, 
year={2015}, 
volume={}, 
number={}, 
pages={781-788}, 
abstract={To improve the design of a power control unit at Philips, two Domain Specific Languages (DSLs) have been used. The first DSL provides a concise and readable notation for the essential state transitions. It is used to generate both configuration files and analysis models. In addition, we also generate instances of a second DSL which represents test traces. This second DSL is used to generate test cases for the power control unit. The use of DSLs not only improved productivity, but also the quality of the configuration files and the test set.}, 
keywords={DSL;Power control;Hardware;Generators;X-ray imaging;Software;Voltage control}, 
doi={10.15439/2015F46}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{8306153, 
author={Bures, Miroslav and Frajtak, Karel and Ahmed, Bestoun S.}, 
journal={IEEE Transactions on Reliability}, 
title={Tapir: Automation Support of Exploratory Testing Using Model Reconstruction of the System Under Test}, 
year={2018}, 
volume={67}, 
number={2}, 
pages={557-580}, 
abstract={For a considerable number of software projects, the creation of effective test cases is hindered by design documentation that is either lacking, incomplete, or obsolete. The exploratory testing approach can serve as a sound method in such situations. However, the efficiency of this testing approach strongly depends on the method, the documentation of explored parts of a system, the organization and distribution of work among individual testers on a team, and the minimization of potential (very probable) duplicities in performed tests. In this paper, we present a framework for replacing and automating a portion of these tasks. A screen-flow-based model of the tested system is incrementally reconstructed during the exploratory testing process by tracking testers' activities. With additional metadata, the model serves for an automated navigation process for a tester. Compared with the exploratory testing approach, which is manually performed in two case studies, the proposed framework allows the testers to explore a greater extent of the tested system and enables greater detection of the defects present in the system. The results show that the time efficiency of the testing process improved with the framework support. This efficiency can be increased by team-based navigational strategies that are implemented within the proposed framework, which is documented by another case study presented in this paper.}, 
keywords={Testing;Navigation;Lead;Documentation;Browsers;Web pages;Automation;Functional testing;generation of test cases from model;model reengineering;model-based testing (MBT);system under test (SUT) model;web applications testing}, 
doi={10.1109/TR.2018.2799957}, 
ISSN={1558-1721}, 
month={June},}
@INPROCEEDINGS{7809817, 
author={Ratiu, Daniel and Voelter, Markus}, 
booktitle={2016 IEEE/ACM 11th International Workshop in Automation of Software Test (AST)}, 
title={Automated Testing of DSL Implementations - Experiences from Building mbeddr}, 
year={2016}, 
volume={}, 
number={}, 
pages={15-21}, 
abstract={Domain specific languages promise to improve productivity and quality of software by providing problem-adequate abstractions to developers. Projectional language workbenches like JetBrains MPS allow the definition of modular and extensible domain specific languages, generators and development environments. While recent advances in language engineering have enabled the definition of DSLs and tooling in a modular and cost-effective manner, the quality assurance of their implementation is still challenging. In this paper we present our work on testing the implementation of domain specific languages and associated tools, and discuss different approaches to increase the automation of language testing. We illustrate this based on MPS and our experience with testing mbeddr, a set of domain specific languages and tools on top of C tailored to embedded software development.}, 
keywords={Testing;DSL;Generators;C languages;Software;Semantics;Automation;domain specific languages;quality assurance;automated testing}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1226344, 
author={Caouras, N. and Freda, M. and Monfet, F. and Aldea, V.S. and Naeem, O. and Tho Le-Ngoc and Champagne, B.}, 
booktitle={CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436)}, 
title={Performance evaluation platform for xDSL deployment in a complex multi-segment environment}, 
year={2003}, 
volume={1}, 
number={}, 
pages={61-64 vol.1}, 
abstract={This paper presents a highly flexible simulation platform catering to the easy and rapid evaluation of existing and future digital subscriber line (DSL) deployments as well as DSL modem performance prediction using practical modem implementations in a complex multi-segment environment. The paper outlines the methodology employed to architect and develop the core software, followed by a description of the performance prediction hooks for a variety of current and future DSL modem technologies. The graphical user interface (GUI) abstracting the core software for the user is described in terms of the various configuration options and the quick and easy graphical design of typical and complex deployment scenarios. The proposed simulator's calculations, notably theoretical SNR margin, maximum theoretical capacity and reach, plus performance evaluation using user-designed modem models, are also outlined. To support the accuracy of the new simulator, results for some example scenarios are presented and compared against other available simulators.}, 
keywords={DSL;Crosstalk;Modems;Communication cables;Background noise;Electromagnetic coupling;Copper;Telephony;Frequency;Colored noise}, 
doi={10.1109/CCECE.2003.1226344}, 
ISSN={0840-7789}, 
month={May},}
@INPROCEEDINGS{8369563, 
author={Glock, Thomas and Sillman, Björn and Kobold, Max and Rebmann, Sebatian and Sax, Eric}, 
booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
title={Model-based validation and testing of industry 4.0 plants}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={One of the major technical aspects of Industry 4.0 (I4.0) is the decentralized task and function distribution based on a service-oriented approach. This leads to new challenges by using the existing technologies and planning methods of I4.0 industrial plants. The validation of such dynamic service­oriented architectures requires new validation methods to ensure the correct functionality of the I4.0 plant. The interactions of distributed functions and the integration of functions on several devices play an important role in planning, configuration, and validation of such plants. This work presents a new approach of a continuous model-based validation and testing method of I4.0 plant service-oriented architectures. The method allows to reuse existing functional tests of services and to adjust them with the help of the model-based information of a plant architecture model.}, 
keywords={Tools;Unified modeling language;Testing;Computer architecture;Sensors;Industries;Planning;Functional Testing of plants;industry 4.0;service-oriented;distributed systems;reuse of tests;validation of I4.0 plant systems}, 
doi={10.1109/SYSCON.2018.8369563}, 
ISSN={2472-9647}, 
month={April},}
@INPROCEEDINGS{8754465, 
author={Tuglular, Tugkan and Şensülün, Sercan}, 
booktitle={2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)}, 
title={SPL-AT Gherkin: A Gherkin Extension for Feature Oriented Testing of Software Product Lines}, 
year={2019}, 
volume={2}, 
number={}, 
pages={344-349}, 
abstract={As cloud platforms turn into software product lines (SPLs), testing products composed of customer selected features becomes more and more important. In this paper, we propose a feature-oriented testing approach for platform-based SPLs through a novel extension to Gherkin called SPL-AT Gherkin and a novel automatic test method generation technique, which utilizes TestNG framework. We demonstrate the applicability of the proposed approach by a case study.}, 
keywords={Testing;Mobile applications;Software product lines;Web pages;Software;Feature extraction;Gold;software product lines;feature-oriented testing;acceptance testing;Gherkin;automatic test generation}, 
doi={10.1109/COMPSAC.2019.10230}, 
ISSN={0730-3157}, 
month={Jul},}
@INPROCEEDINGS{4839232, 
author={Díaz, Jessica and Yagüe, Agustín and Garbajosa, Juan}, 
booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems}, 
title={A Systematic Process for Implementing Gateways for Test Tools}, 
year={2009}, 
volume={}, 
number={}, 
pages={58-66}, 
abstract={Test automation is facing a new challenge because tools, as well as having to provide conventional test functionalities, must be capable to interact with ever more heterogeneous complex systems under test (SUT). The number of existing software interfaces to access these systems is also a growing number. The problem cannot be analyzed only from a technical or engineering perspective; the economic perspective is as important. This paper presents a process to systematically implement gateways which support the communication between test tools and SUTs with a reduced cost. The proposed solution does not preclude any  interface protocol at the SUT side. This process is supported using a generic architecture of a  gateway defined on top of  OSGi. Any test tool can communicate with the gateway through a unique defined interface. To communicate the gateway and the SUT, basically, the driver corresponding to the SUT software interface has to be loaded.}, 
keywords={System testing;Automatic testing;Costs;Service oriented architecture;Home automation;Internet;Radiofrequency identification;Intelligent sensors;Software testing;Web services;test automation;gateway;complex systems testing;acceptance testing tools;OSGI;TOPEN}, 
doi={10.1109/ECBS.2009.40}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{9922557, 
author={Su, Haoxiang and Chai, Ming and Liu, Hongjie and Chai, Jinchuan and Yue, Chaopeng}, 
booktitle={2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)}, 
title={A Model-Based Testing System for Safety of Railway Interlocking}, 
year={2022}, 
volume={}, 
number={}, 
pages={335-340}, 
abstract={Testing is an important safety assurance technique for railway interlocking systems. Model-based testing (MBT) allows for designing and maintaining tests with high-level models and generating test suites from these models automatically. Although MBT has the potential to improve testing efficiency and quality, it is not clear whether this technique is applicable for testing complex variant-rich interlocking software. In this paper, we report our experience in introducing MBT in inter-locking testing. We develop feature models of the interlocking route control process to improve the reusability of the models. The experimental results show that our approach is able to improve the quality and efficiency of real-world interlocking acceptance testing.}, 
keywords={Computational modeling;Natural languages;Process control;Organizations;Manuals;Rail transportation;Software}, 
doi={10.1109/ITSC55140.2022.9922557}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{9796462, 
author={Elsner, Daniel and Wuersching, Roland and Schnappinger, Markus and Pretschner, Alexander}, 
booktitle={2022 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
title={Probe-based Syscall Tracing for Efficient and Practical File-level Test Traces}, 
year={2022}, 
volume={}, 
number={}, 
pages={126-137}, 
abstract={Efficiently collecting per-test execution traces is a common prerequisite of dynamic regression test optimization techniques. However, as these test traces are typically recorded through language-specific code instrumentation, non-code artifacts and multi-language source code are usually not included. In contrast, more complete test traces can be obtained by instrumenting operating system calls and thereby tracing all accessed files during a test’s execution. Yet, existing test optimization techniques that use syscall tracing are impractical as they either modify the Linux kernel or operate in user space, thus raising transferability, performance, and security concerns. Recent advances in operating system development provide versatile, lightweight, and safe kernel instrumentation frameworks: They allow to trace syscalls by instrumenting probes in the operating system kernel. Probe-based Syscall Tracing (ProST), our novel technique, harnesses this potential to collect file-level test traces that go beyond language boundaries and consider non-code artifacts. To evaluate ProST’s efficiency and the completeness of obtained test traces, we perform an empirical study on 25 multi-language open-source software projects and compare our approach to existing language-specific instrumentation techniques. Our results show that most studied projects use source files from multiple languages (22/25) or non-code artifacts during testing (22/25) that are missed by language-specific techniques. With the low execution time overhead of 4.6% compared to non-instrumented test execution, ProST is more efficient than language-specific instrumentation. Furthermore, it collects on average 89% more files on top of those collected by language-specific techniques. Consequently, ProST paves the way for efficiently extracting valuable information through dynamic analysis to better understand and optimize testing in multi-language software systems. CCS CONCEPTS • Software and its engineering → Software testing and debugging.}, 
keywords={Software testing;Codes;Runtime;Instruments;Linux;Software systems;Security;software testing;dynamic program analysis;multi-language software;non-code artifacts}, 
doi={10.1145/3524481.3527239}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4626839, 
author={Magro, Bélen and Garbajosa, Juan and Pérez, Jennifer}, 
booktitle={2008 12th International Software Product Line Conference}, 
title={A Software Product Line Definition for Validation Environments}, 
year={2008}, 
volume={}, 
number={}, 
pages={45-54}, 
abstract={Functional requirements must be tested to check if the system executes as the end user expects. Validation environments must be able to test multiple kinds of applications that belong to different domains and technologies. Since this wide validation spectrum is very difficult to cope with, validation environments are usually specialized in domains, programming languages, technologies, etc. However, it is possible to identify that the validation processes for different systems share a set of commonalities and variability points. This is a perfect framework to apply the software product line approach to develop domain specific validation environments for testing specific products. In this paper we present our experience of applying software product lines to support the variability of validation environments. We illustrate our product-line experience of developing two domain-specific validation environments for two different case studies: digital TV and slots machines.}, 
keywords={Graphical user interfaces;Software;Engines;Testing;Computer architecture;Computer languages;Databases;tools;CASE;acceptance testing;software product lines;validation environments;validation tools;testing tools;software engineering environments;software development environments}, 
doi={10.1109/SPLC.2008.35}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8101621, 
author={Olajubu, Oyindamola and Ajit, Suraj and Johnson, Mark and Thomson, Scott and Edwards, Mark and Turner, Scott}, 
booktitle={2017 9th Computer Science and Electronic Engineering (CEEC)}, 
title={Automated test case generation from high-level logic requirements using model transformation techniques}, 
year={2017}, 
volume={}, 
number={}, 
pages={178-182}, 
abstract={It is not uncommon for industries to use natural language to represent high-level software requirement specifications. It is also not uncommon for these requirement specifications to be translated into design and used further for implementation and generation of test cases in the software engineering life-cycle. These requirements are often ambiguous, incorrect, and incomplete. Finding them late in the development lifecycle proves very expensive and lowers the productivity. This paper reports on the experience of applying model-based technologies from academia to a real-world problem domain in the aviation industry to improve the productivity. The paper focuses on the application of a model-based technique to automatically generate test cases to satisfy Modified Condition/Decision Coverage (MC/DC) from high-level logic requirements expressed in a Domain Specific Language (DSL).}, 
keywords={Legged locomotion;DSL;Software;Testing;Industries;Productivity;Natural languages}, 
doi={10.1109/CEEC.2017.8101621}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7237259, 
author={Lavrishcheva, Ekaterina}, 
booktitle={2015 Science and Information Conference (SAI)}, 
title={Ontological approach to the formal specification of the standard life cycle}, 
year={2015}, 
volume={}, 
number={}, 
pages={965-972}, 
abstract={Approach is offered to the formal specification of Standard Life Cycle (LC) of the program systems (PS) by the ontology facilities with purpose automation and generation of the variants LC for making the appropriate kinds process for development different PS. Ontological approach to presentation LC model of the standard ISO/IEC 12207-2007 is included the specification of general, organizational and support processes. These processes are presented in the subject-oriented DSL, which than transformed to XML for realization. One of the processes, the testing process is given in terms of Protégé systems. An eventual result of this system Protégé got generally at accepted to the XML, suitable for implementation tasks testing PS on computer.}, 
keywords={DSL;Ontologies;XML;ISO Standards;IEC Standards;Testing;ontology;the life cycle standard;model of life cycle;processes;actions;task;testing;DSL;description;Protégé;XML}, 
doi={10.1109/SAI.2015.7237259}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{786779, 
author={Nelson, G.}, 
booktitle={IEEE ATM Workshop '99 Proceedings (Cat. No. 99TH8462)}, 
title={Testing techniques for next-generation IP networks}, 
year={1999}, 
volume={}, 
number={}, 
pages={63-68}, 
abstract={In this paper, we look at some examples of how "IP meets ATM" and then discuss some of the recent advances in IP standards. For the remainder of the paper, we examine testing techniques used in three different scenarios: functional testing of a layer 2/layer 3 switching device; class of service (CoS) contract verification in an IP network; interworking testing of an IP/ATM access device.}, 
keywords={Testing;Next generation networking;IP networks;Asynchronous transfer mode;Multiprotocol label switching;Routing;Telecommunication traffic;Protocols;Packet switching;Proposals}, 
doi={10.1109/ATM.1999.786779}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9287702, 
author={Chu, Minh-Hue and Dang, Duc-Hanh}, 
booktitle={2020 12th International Conference on Knowledge and Systems Engineering (KSE)}, 
title={Automatic Extraction of Analysis Class Diagrams from Use Cases}, 
year={2020}, 
volume={}, 
number={}, 
pages={109-114}, 
abstract={At the early phase of software development, functional requirements of the software often need to be represented in the developer's language, resulting in a so-called analysis model. Current works in literature aim to increase automation in software development by either generating automatically the analysis model from a use case specification or transforming the analysis model to a design model. However, up to now, to precisely specify use cases is still a challenge, preventing us from realizing this aim. This paper proposes a method to extract analysis classes from a use case specification. Within our method, use cases are represented using our domain-specific modeling language named USL. We then define algorithms with transformation rules as a representation of analysis patterns in order to extract analysis classes from the USL use case model. We develop a support tool for our method in which transformation rules are realized using the ATL model-to-model transformation technique.}, 
keywords={Knowledge engineering;Analytical models;Buildings;Transforms;Tools;Software;Generators;Use Case Specification;Model Transformation;Analysis Model;UML/OCL}, 
doi={10.1109/KSE50997.2020.9287702}, 
ISSN={2164-2508}, 
month={Nov},}
@INPROCEEDINGS{7999652, 
author={de Moura, Jéssica Lasch and Charão, Andrea Schwertner and Lima, João Carlos Damasceno and de Oliveira Stein, Benhur}, 
booktitle={2017 17th International Conference on Computational Science and Its Applications (ICCSA)}, 
title={Test case generation from BPMN models for automated testing of Web-based BPM applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={This article proposes an approach to generate test cases from BPMN models, for automated testing of Web applications implemented with the support of BPM suites. The work is primarily focused on functional testing and has the following objectives: (i) identify execution paths from the flow analysis in the BPMN model and (ii) generate the initial code of test scripts to be run on a given Web application testing tool. Throughout the article, we describe the design and implementation of a solution to achieve these goals, targeting automated tests using Selenium and Cucumber as tools. The approach was applied to processes from a public repository and was able to generate test scenarios from different BPMN models.}, 
keywords={Testing;Tools;Logic gates;Selenium;Process control;XML;Monitoring;Business Process Management;BPMN;automatic software testing;test case generation}, 
doi={10.1109/ICCSA.2017.7999652}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{9649736, 
author={Wanninger, Constantin and Rossi, Sebastian and Schörner, Martin and Hoffmann, Alwin and Poeppel, Alexander and Eymueller, Christian and Reif, Wolfgang}, 
booktitle={2021 21st International Conference on Control, Automation and Systems (ICCAS)}, 
title={ROSSi A Graphical Programming Interface for ROS 2}, 
year={2021}, 
volume={}, 
number={}, 
pages={255-262}, 
abstract={The Robot Operating System (ROS) offers developers a large number of ready-made packages for developing robot programs. The multitude of packages and the different interfaces or adapters is also the reason why ROS projects often tend to become confusing. Concepts of model-driven software development using a domain-specific modeling language could counteract this and at the same time speed up the development process of such projects. This is investigated in this paper by transferring the core concepts from ROS 2 into a graphical programming interface. Elements of established graphical programming tools are compared and approaches from modeling languages such as UML are used to create a novel approach for graphical development of ROS projects. The resulting interface is evaluated through the development of a project built on ROS, and the approach shows promise towards facilitating work with the Robot Operating System.}, 
keywords={Adaptation models;Visualization;Runtime;Operating systems;Unified modeling language;Programming;Software;robot operating system;ros;unmaned aerial vehicle;uav;model driven development;semantic plug and play}, 
doi={10.23919/ICCAS52745.2021.9649736}, 
ISSN={2642-3901}, 
month={Oct},}
@INPROCEEDINGS{6569742, 
author={Di Nardo, Daniel and Alshahwan, Nadia and Briand, Lionel and Labiche, Yvan}, 
booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
title={Coverage-Based Test Case Prioritisation: An Industrial Case Study}, 
year={2013}, 
volume={}, 
number={}, 
pages={302-311}, 
abstract={This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.}, 
keywords={Testing;Fault detection;Measurement;Software;Computer aided software engineering;Minimization;Data collection;regression testing;industrial case study;test case prioritisation}, 
doi={10.1109/ICST.2013.27}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{9463040, 
author={Arrieta, Aitor and Ayerdi, Jon and Illarramendi, Miren and Agirre, Aitor and Sagardui, Goiuria and Arratibel, Maite}, 
booktitle={2021 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
title={Using Machine Learning to Build Test Oracles: an Industrial Case Study on Elevators Dispatching Algorithms}, 
year={2021}, 
volume={}, 
number={}, 
pages={30-39}, 
abstract={The software of elevators requires maintenance over several years to deal with new functionality, correction of bugs or legislation changes. To automatically validate this software, test oracles are necessary. A typical approach in industry is to use regression oracles. These oracles have to execute the test input both, in the software version under test and in a previous software version. This practice has several issues when using simulation to test elevators dispatching algorithms at system level. These issues include a long test execution time and the impossibility of re-using test oracles both at different test levels and in operation. To deal with these issues, we propose DARIO, a test oracle that relies on regression learning algorithms to predict the Qualify of Service of the system. The regression learning algorithms of this oracle are trained by using data from previously tested versions. An empirical evaluation with an industrial case study demonstrates the feasibility of using our approach in practice. A total of five regression learning algorithms were validated, showing that the regression tree algorithm performed best. For the regression tree algorithm, the accuracy when predicting verdicts by DARIO ranged between 79 to 87%.}, 
keywords={Machine learning algorithms;Software algorithms;Legislation;Machine learning;Maintenance engineering;Prediction algorithms;Software;Test Oracle;Regression Testing;Machine-Learning}, 
doi={10.1109/AST52587.2021.00012}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5954390, 
author={Efkemann, Christof and Peleska, Jan}, 
booktitle={2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops}, 
title={Model-Based Testing for the Second Generation of Integrated Modular Avionics}, 
year={2011}, 
volume={}, 
number={}, 
pages={55-62}, 
abstract={In this paper the authors present the current research and development activities regarding automated testing of Integrated Modular Avionics controllers in the European research project SCARLETT. The authors describe the goals of the SCARLETT project and explain its background of Integrated Modular Avionics. Furthermore, they explain different levels of testing of components required for certification. A domain-specific modelling language designed for the IMA platform is presented. This language is used to create models from which tests of different levels can be generated automatically. The authors expect significant improvements in terms of effort to create and maintain test procedures compared to conventional test creation.}, 
keywords={Aerospace electronics;Testing;Generators;Aircraft;Europe;Random access memory;Concrete;IMA;SCARLETT;TTCN-3;avionics;domain-specific modelling;model-based testing}, 
doi={10.1109/ICSTW.2011.72}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7000030, 
author={Albuquerque, Marco Túlio C. F. and Ramalho, Geber Lisboa and Corruble, Vincent and Santos, André Luís Medeiros and Freitas, Fred}, 
booktitle={2014 Brazilian Symposium on Computer Games and Digital Entertainment}, 
title={Helping Developers to Look Deeper inside Game Sessions}, 
year={2014}, 
volume={}, 
number={}, 
pages={31-40}, 
abstract={Game design and development activities are increasingly relying on the analysis of gamer's behavior and preferences data. Various tools are available to the developers to track and analyze general data concerning acquisition, retention and monetization aspects of game commercialization. This is good enough to give hints on where problems are, but not to enable a precise diagnosis, which demands fine-grained data. For this kind of data, there is not enough support or guidance to decide which data to capture, to write the code to capture it, to choose the best representation of it and to allow an adequate retrieval and presentation of it. This paper introduces GameGuts (GG), a framework devoted to give further assistance to developers in choosing, representing, accessing and presenting game sessions fine-grained data. As a case study, GG recorded sessions of a game platform with over a hundred thousand users. The logs were analyzed using a Visual Domain Specific Language (as a query language) and an ensemble of rules (as a compliance test). The results are encouraging, since we could - among other results - find bugs and catch cheaters, as well as spot design flaws.}, 
keywords={Games;Ontologies;Visualization;Servers;DSL;Database languages;Measurement;game analytics;knowledge representation;game data mining}, 
doi={10.1109/SBGAMES.2014.28}, 
ISSN={2159-6662}, 
month={Nov},}
@INPROCEEDINGS{5967120, 
author={Piho, Gunnar and Tepandi, Jaak and Roost, Mart and Parman, Marko and Puusep, Viljam}, 
booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
title={From archetypes based domain model via requirements to software: Exemplified by LIMS Software Factory}, 
year={2011}, 
volume={}, 
number={}, 
pages={570-575}, 
abstract={The Archetypes Based Development (ABD) proceeds from archetypes based domain model via requirements to software. We give an overview of ABD and exemplify its application on Laboratory Information Management Systems (LIMS) Software Factory development. ABD is guided by Zachman Framework and utilizes software engineering triptych together with archetypes and archetype patterns. For modelling of domains the Test Driven Modelling (TDM) techniques are used. TDM utilizes test driven development techniques in domain engineering. The resultant domain models serve as the Domain Specific Language for prescribing requirements. Implementation and testing of the LIMS Software Factory proves feasibility of archetypes based techniques in real life systems. ABD helps developers to better understand business requirements, to design cost effective enterprise applications through systematic reuse of archetypal components, as well as to validate and verify requirements resulting in higher quality software.}, 
keywords={Software;Production facilities;Laboratories;Capability maturity model;DSL;Organizations;Archetypes;archetype patterns;domain analysis;domain model;domain modelling;software engineering;software factory;laboratory information management system (LIMS);laboratory domain model}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9755390, 
author={Tang, Weijian and Wang, Keming}, 
booktitle={2021 16th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)}, 
title={Automatic Generation of Test Cases of Multi-Agent Systems Based on Model Checking}, 
year={2021}, 
volume={}, 
number={}, 
pages={24-29}, 
abstract={The reliability of a train control system depends on the consistency between the system under the test cases and the design specification. It is vital to ensure consistency by verifying that the functions of the train control system meet the design specifications. This paper introduces a novel and formal approach of test cases generation that guarantees the reliability of a train control system. The proposed methodology is effectively applied to a reactive, concurrent and complex case study of the train control system, namely the level transition, and uses multi-agent systems to describe each system component as an intelligent agent. We use the model check tool, NuSMV, that supports formal specifications, modelling the communication among agents. Furthermore, this paper combines computation tree logic with modified condition/decision coverage to design the trap properties. Based on the model checking technology, the test sequence is automatically generated by verifying the trap properties. Lastly, a set of functional test cases that satisfy the transition coverage are obtained by revising the test sequence. The research results show that the method in this paper can be used to generate test cases that are executed to verify the consistency between the system under testing and the design specification.}, 
keywords={Knowledge engineering;Computational modeling;Model checking;Control systems;Reliability engineering;Intelligent agents;Formal specifications;model checking;automatic generation;test cases;level transition;multi-agent;NuSMV}, 
doi={10.1109/ISKE54062.2021.9755390}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7962331, 
author={Chiw, Charisee and Kindlmann, Gordon and Reppy, John}, 
booktitle={2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST)}, 
title={DATm: Diderot's Automated Testing Model}, 
year={2017}, 
volume={}, 
number={}, 
pages={45-51}, 
abstract={Diderot is a parallel domain-specific language forthe analysis and visualization of multidimensional scientific images, such as those produced by CT and MRI scanners. Diderot is designed to support algorithms that are based on differential tensor calculus and produces a higher-order mathematical model which allows direct manipulation of tensor fields. One of the main challenges of the Diderot implementation is bridging this semantic gap by effectively translating high-level mathematical notation of tensor calculus into efficient low-level code in the target language. A key question for a high-level language, such as Diderot, is how do we know that the implementation is correct. We have previously presented and defended a core set of rewriting rules, but the full translation from source to executable requires much more work. In this paper, we present DATm, Diderot's automated testing model to check the correctness of the core operations in the programming language. DATm can automatically create test programs, and predict what the outcome should be. We measure the accuracy of the computations written in the Diderot language, based on how accurately the output of the program represents the mathematical equivalent of the computations. This paper describes a model for testing a high-level language based on correctness. It introduces the pipeline for DATm, a tool that can automatically create and test tens of thousands of Diderot test programs and that has found numerous bugs. We make a case for the necessity of extensive testing by describing bugs that are deep in the compiler, and only could be found with a unique application of operations. Lastly, we demonstrate that the model can be used to create other types of tests by visual verification.}, 
keywords={Testing;Tensile stress;Kernel;Shape;Computational modeling;Generators;Calculus;domain specific testing;Diderot;DSL;tensor calc;visual verificaiton}, 
doi={10.1109/AST.2017.5}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8456399, 
author={Quenum, José G. and Aknine, Samir}, 
booktitle={2018 IEEE International Conference on Services Computing (SCC)}, 
title={Towards Executable Specifications for Microservices}, 
year={2018}, 
volume={}, 
number={}, 
pages={41-48}, 
abstract={This paper presents an empirical approach for microservice automated testing. With the rise of the agile methodology, automated testing has gained momentum in software development, including using microservices as an architectural style. However, the tests are not always related to the core specifications of the system being developed. In this paper, we discuss an approach to derive the tests, especially the acceptance tests, from the specifications of the systems. To avoid any ambiguity in the specifications, we focus on the formal specifications of the system. To this end, we introduce intelligent agents as a conceptual unit to encapsulate the formal specifications of services. Indeed, a comparison of microservice tenets and the general characterization of agents reveals that both can be thought of as autonomous software entities, driven by goals and evolving within a distributed environment and communicating with one another. Using a real-world application we show how agent formal specifications can be linked to microservice automated testing.}, 
keywords={Testing;Service-oriented architecture;Intelligent agents;Business;Computer architecture;Curriculum development;Services;Testing;Intelligent Agents;Formal Specifications}, 
doi={10.1109/SCC.2018.00013}, 
ISSN={2474-2473}, 
month={July},}
@INPROCEEDINGS{7980397, 
author={Contan, Andrei and Dehelean, Catalin and Miclea, Liviu}, 
booktitle={2017 14th International Conference on Engineering of Modern Electric Systems (EMES)}, 
title={Applying coding systems in the process of testing software applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={127-131}, 
abstract={The challenges met during the software projects fall into any number of categories. The development and the technical solutions bring about technical challenges, but the situations one is confronted with, may also be sociological, psychological or managerial in nature. Without any knowledge in the field of social sciences, the programmers, testers and managers might interpret the social aspects of the project improperly, and such interpretations lead to the inability to fully understand the problem and, ultimately, to inefficiency in the decision-making process. Furthermore, solid knowledge of theories in the area of the social sciences is required for a better understanding of both the context in which the application runs and of the final users who will use the developed project. The understanding of and the involvement in a software acceptance testing (SAT) project, requires the combination of multiple theories and principles from different disciplines.}, 
keywords={Encoding;Testing;Software quality;Software engineering;Computational modeling;Psychology;coding models;software testing;software testing;social science}, 
doi={10.1109/EMES.2017.7980397}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{9643783, 
author={Patkar, Nitish and Chiş, Andrei and Stulova, Nataliia and Nierstrasz, Oscar}, 
booktitle={2021 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
title={Interactive Behavior-driven Development: a Low-code Perspective}, 
year={2021}, 
volume={}, 
number={}, 
pages={128-137}, 
abstract={Within behavior-driven development (BDD), different types of stakeholders collaborate in creating scenarios that specify application behavior. The current workflow for BDD expects non-technical stakeholders to use an integrated development environment (IDE) to write textual scenarios in the Gherkin language and verify application behavior using test passed/failed reports. Research to date shows that this approach leads non-technical stakeholders to perceive BDD as an overhead in addition to the testing. In this vision paper, we propose an alternative approach to specify and verify application behavior visually, interactively, and collaboratively within an IDE. Instead of writing textual scenarios, non-technical stakeholders compose, edit, and save scenarios by using tailored graphical interfaces that allow them to manipulate involved domain objects. Upon executing such interactively composed scenarios, all stakeholders verify the application behavior by inspecting domain-specific representations of run-time domain objects instead of a test run report. Such a low code approach to BDD has the potential to enable nontechnical stakeholders to engage more harmoniously in behavior specification and validation together with technical stakeholders within an IDE. There are two main contributions of this work: (i) we present an analysis of the features of 13 BDD tools, (ii) we describe a prototype implementation of our approach, and (iii) we outline our plan to conduct a large-scale developer survey to evaluate our approach to highlight the perceived benefits over the existing approach.}, 
keywords={Codes;Prototypes;Writing;Programming;Model driven engineering;Stakeholders;Testing;bdd;behavior-driven development;collaborative development;acceptance testing;visual programming;end-user programming}, 
doi={10.1109/MODELS-C53483.2021.00024}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{5630332, 
author={Agaram, Mukundan K. and Laird, Brenda}, 
booktitle={2010 14th IEEE International Enterprise Distributed Object Computing Conference}, 
title={A Componentized Architecture for Externalized Business Rules}, 
year={2010}, 
volume={}, 
number={}, 
pages={175-183}, 
abstract={Delta Dental is the leading provider of Dental benefits in the Midwest, serving nearly 5.1 million members. Delta Dental of Michigan uses Business Rules to articulate complex claims adjudication rules, from legacy COBOL programs. These rules are entirely authored, managed, tested and governed by Subject Matter Experts with no programming background. The Business Rules Architecture incorporates a componentized approach to authoring and organizing Business Rules which promotes extensive rule reuse at several levels through layered components. To mitigate complexity, the framework provides features for sandbox testing and regression testing of the rules by the Business Users. It also provides a rich reporting and trace ability back to the Policy charter. As a result of this framework a legacy adjudication COBOL program of 50,000+ lines were reduced to 30+ reusable rules. Nearly 94 % of the claims processed are automatically adjudicated by the rules engine. The purpose of this paper is to describe in detail the insights gained from the architecture and the measurable productivity gains accomplished.}, 
keywords={Business;Dentistry;Vocabulary;Testing;Production;Runtime;Humans;Architecture;Business Rules;BRIDE;Component}, 
doi={10.1109/EDOC.2010.26}, 
ISSN={1541-7719}, 
month={Oct},}
@INPROCEEDINGS{7101658, 
author={McCormick, Patrick and Sweeney, Christine and Moss, Nick and Prichard, Dean and Gutierrez, Samuel K. and Davis, Kei and Mohd-Yusof, Jamaludin}, 
booktitle={2014 Fourth International Workshop on Domain-Specific Languages and High-Level Frameworks for High Performance Computing}, 
title={Exploring the Construction of a Domain-Aware Toolchain for High-Performance Computing}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={The push towards exascale computing has sparked a new set of explorations for providing new productive programming environments. While many efforts are focusing on the design and development of domain-specific languages (DSLs), few have addressed the need for providing a fully domain-aware toolchain. Without such domain awareness critical features for achieving acceptance and adoption, such as debugger support, pose a long-term risk to the overall success of the DSL approach. In this paper we explore the use of language extensions to design and implement the Scout DSL and a supporting toolchain infrastructure. We highlight how language features and the software design methodologies used within the toolchain play a significant role in providing a suitable environment for DSL development.}, 
keywords={DSL;Runtime;Computer architecture;Syntactics;Graphics processing units;Image color analysis}, 
doi={10.1109/WOLFHPC.2014.9}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7102627, 
author={Krenn, Willibald and Schlick, Rupert and Tiran, Stefan and Aichernig, Bernhard and Jobstl, Elisabeth and Brandl, Harald}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={MoMut::UML Model-Based Mutation Testing for UML}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Model-based mutation testing (MBMT) is a promising testing methodology that relies on a model of the system under test (SUT) to create test cases. Hence, MBMT is a so-called black-box testing approach. It also is fault based, as it creates test cases that are guaranteed to reveal certain faults: after inserting a fault into the model of the SUT, it looks for a test case revealing this fault. This turns MBMT into one of the most powerful and versatile test case generation approaches available as its tests are able to demonstrate the absence of certain faults, can achieve both, control-flow and data-flow coverage of model elements, and also may include information about the behaviour in the failure case. The latter becomes handy whenever the test execution framework is bound in the number of observations it can make and - as a consequence - has to restrict them. However, this versatility comes at a price: MBMT is computationally expensive. The tool MoMuT::UML (https://www.momut.org) is the result of a multi-year research effort to bring MBMT from the academic drawing board to industrial use. In this paper we present the current stable version, share the lessons learnt when applying two generations of MoMuT::UML in an industrial setting, and give an outlook on the upcoming, third,generation.}, 
keywords={Unified modeling language;Testing;Computational modeling;Integrated circuit modeling;Circuit faults;Object oriented modeling;Semantics}, 
doi={10.1109/ICST.2015.7102627}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{5464138, 
author={Noubissi, Agnes C. and Iguchi-Cartigny, Julien and Lanet, Jean-Louis}, 
booktitle={2010 Fifth International Conference on Systems}, 
title={Incremental Dynamic Update for Java-Based Smart Cards}, 
year={2010}, 
volume={}, 
number={}, 
pages={110-113}, 
abstract={One of the most appealing feature for multi-application smart cards is their ability to dynamically download or delete applications once the card has been issued. Applications can be updated by deleting old versions and loading the new ones. Nevertheless, for system components, the update is sligthly more complex because the systems never stop. Indeed, for smart cards based on Java called JavaCard, the virtual machine has a life cycle similar to the card because persistent objects are preserved after the communication sessions with the reader have expired. We present in this paper, our research in dynamic system components updating of JavaCard. Our technique requires a lot of off-card and on-card mechanisms. Our approach uses control flow graph to determine change between versions, a domain specific language to represent the change for minimization of the download overhead throughout the communication link with the card.}, 
keywords={Java;Smart cards;Virtual machining;Runtime;Application software;Domain specific languages;Aerodynamics;Embedded software;DSL;Operating systems;Smart Card;HotSwUp;Java Card;Dynamic update;e-passport}, 
doi={10.1109/ICONS.2010.27}, 
ISSN={}, 
month={April},}
@ARTICLE{5441292, 
author={Amatriain, Xavier and Arumi, Pau}, 
journal={IEEE Transactions on Software Engineering}, 
title={Frameworks Generate Domain-Specific Languages: A Case Study in the Multimedia Domain}, 
year={2011}, 
volume={37}, 
number={4}, 
pages={544-558}, 
abstract={We present an approach to software framework development that includes the generation of domain-specific languages (DSLs) and pattern languages as goals for the process. Our model is made of three workflows-framework, metamodel, and patterns-and three phases-inception, construction, and formalization. The main conclusion is that when developing a framework, we can produce with minimal overhead-almost as a side effect-a metamodel with an associated DSL and a pattern language. Both outputs will not only help the framework evolve in the right direction, but will also be valuable in themselves. In order to illustrate these ideas, we present a case study in the multimedia domain. For several years, we have been developing a multimedia framework. The process has produced a full-fledged domain-specific metamodel for the multimedia domain, with an associated DSL and a pattern language.}, 
keywords={Domain specific languages;DSL;Unified modeling language;Vocabulary;Concrete;Software engineering;Computer aided software engineering;Natural languages;Metamodeling;Best practices;Domain-specific architectures;visual programming;life cycle;CASE.}, 
doi={10.1109/TSE.2010.48}, 
ISSN={1939-3520}, 
month={July},}
@INPROCEEDINGS{7880820, 
author={Guo, Xiaolong and Dutta, Raj Gautam and Mishra, Prabhat and Jin, Yier}, 
booktitle={2016 17th International Workshop on Microprocessor and SOC Test and Verification (MTV)}, 
title={Automatic RTL-to-Formal Code Converter for IP Security Formal Verification}, 
year={2016}, 
volume={}, 
number={}, 
pages={35-38}, 
abstract={The wide usage of hardware intellectual property (IP) cores from untrusted vendors has raised security concerns in the integrated circuit (IC) industry. Existing testing methods are designed to validate the functionality of the hardware IP cores. These methods often fall short in detecting unspecified (often malicious) logic. Formal methods like Proof-Carrying Hardware (PCH), on the other hand, can help eliminate hardware Trojans and/or design backdoors by formally proving security properties on soft IP cores despite the high proof development cost. One of the causes to the high cost is the manual conversion of the hardware design from RTL code to a domain-specific language prior to verification. To mitigate this issue and to lower the overall cost of PCH framework, we propose an automatic code converter for translating VHDL to Formal-HDL, a domain specific language for representing hardware designs in Coq language. Our code converter provides support to wide variety of hardware designs. Towards the goal of speeding up the verification procedure in our PCH framework, the code converter is the important first step. The applicability of the tool is demonstrated by converting soft IP cores of AES to its Coq equivalent code.}, 
keywords={Security;Hardware;IP networks;Hardware design languages;Trojan horses;Syntactics;Hardware Security;Hardware IP Protection;Formal Verification}, 
doi={10.1109/MTV.2016.23}, 
ISSN={2332-5674}, 
month={Dec},}
@INPROCEEDINGS{7958601, 
author={Petsios, Theofilos and Tang, Adrian and Stolfo, Salvatore and Keromytis, Angelos D. and Jana, Suman}, 
booktitle={2017 IEEE Symposium on Security and Privacy (SP)}, 
title={NEZHA: Efficient Domain-Independent Differential Testing}, 
year={2017}, 
volume={}, 
number={}, 
pages={615-632}, 
abstract={Differential testing uses similar programs as cross-referencing oracles to find semantic bugs that do not exhibit explicit erroneous behaviors like crashes or assertion failures. Unfortunately, existing differential testing tools are domain-specific and inefficient, requiring large numbers of test inputs to find a single bug. In this paper, we address these issues by designing and implementing NEZHA, an efficient input-format-agnostic differential testing framework. The key insight behind NEZHA's design is that current tools generate inputs by simply borrowing techniques designed for finding crash or memory corruption bugs in individual programs (e.g., maximizing code coverage). By contrast, NEZHA exploits the behavioral asymmetries between multiple test programs to focus on inputs that are more likely to trigger semantic bugs. We introduce the notion of δ-diversity, which summarizes the observed asymmetries between the behaviors of multiple test applications. Based on δ-diversity, we design two efficient domain-independent input generation mechanisms for differential testing, one gray-box and one black-box. We demonstrate that both of these input generation schemes are significantly more efficient than existing tools at finding semantic bugs in real-world, complex software. NEZHA's average rate of finding differences is 52 times and 27 times higher than that of Frankencerts and Mucerts, two popular domain-specific differential testing tools that check SSL/TLS certificate validation implementations, respectively. Moreover, performing differential testing with NEZHA results in 6 times more semantic bugs per tested input, compared to adapting state-of-the-art general-purpose fuzzers like American Fuzzy Lop (AFL) to differential testing by running them on individual test programs for input generation. NEZHA discovered 778 unique, previously unknown discrepancies across a wide variety of applications (ELF and XZ parsers, PDF viewers and SSL/TLS libraries), many of which constitute previously unknown critical security vulnerabilities. In particular, we found two critical evasion attacks against ClamAV, allowing arbitrary malicious ELF/XZ files to evade detection. The discrepancies NEZHA found in the X.509 certificate validation implementations of the tested SSL/TLS libraries range from mishandling certain types of KeyUsage extensions, to incorrect acceptance of specially crafted expired certificates, enabling man-in-the-middle attacks. All of our reported vulnerabilities have been confirmed and fixed within a week from the date of reporting.}, 
keywords={Testing;Computer bugs;Semantics;Tools;Software;Libraries;Security;nezha;differential testing;fuzzing}, 
doi={10.1109/SP.2017.27}, 
ISSN={2375-1207}, 
month={May},}
@INPROCEEDINGS{1609829, 
author={Prashant Gandhi and Haugen, N.C. and Hill, M. and Watt, R.}, 
booktitle={Agile Development Conference (ADC'05)}, 
title={Creating a living specification using FIT documents}, 
year={2005}, 
volume={}, 
number={}, 
pages={253-258}, 
abstract={Using FIT for automated acceptance testing supports a process in which developers and customers collaborate on a single executable specification for each story, i.e. the FIT documents. By collaborating closely on the FIT documents, the developers and customers reach a shared understanding of the domain and develop the ubiquitous language of the application. Our experience with this process was ultimately successful but not completely pain free. In this experience report we highlight the benefits and pitfalls and share techniques for achieving successful developer and customer collaboration in specifying executable FIT documents.}, 
keywords={Collaboration;Automatic testing;Writing;Pain;Fixtures;Collaborative tools;Encoding;System testing;Automation}, 
doi={10.1109/ADC.2005.19}, 
ISSN={}, 
month={July},}
@ARTICLE{6312844, 
author={Baker, F. Terry}, 
journal={IEEE Transactions on Software Engineering}, 
title={Structured programming in a production programming environment}, 
year={1975}, 
volume={SE-1}, 
number={2}, 
pages={241-252}, 
abstract={Discusses how structured programming methodology has been introduced into a large production programming organization using an integrated but flexible approach. It next analyzes the advantages and disadvantages of each component of the methodology and presents some quantitative results on its use. It concludes with recommendations based on this generally successful experience, which could be useful to other organizations interested in improving reliability and productivity.}, 
keywords={Programming;DSL;Libraries;Organizations;Encoding;Standards;Guidelines;Chief programmer teams (CPT's);development support libraries (DSL's);structured coding;structured programming;top-down development;top-down programming}, 
doi={10.1109/TSE.1975.6312844}, 
ISSN={1939-3520}, 
month={June},}
@INPROCEEDINGS{1158072, 
author={Saha, A. and Mishra, V. and Saxena, P. and Mundhada, R. and Katiyar, K. and Kumar, S.}, 
booktitle={15th Annual IEEE International ASIC/SOC Conference}, 
title={Design of broadband controller for residential gateway applications}, 
year={2002}, 
volume={}, 
number={}, 
pages={283-287}, 
abstract={As Internet technology becomes more pervasive, homes are getting connected to cable or DSL. The increasing user demands for "always on" service along with multiple connectivity for voice and data is gradually making the presence of a residential gateway in every household a reality. A residential gateway should have routing and bridging capabilities along with seamless connectivity to contemporary premise networking technologies. Integration of all these features on a single device essentially requires a rich architecture, smart design techniques and thorough verification. This paper describes the architecture and design of a broadband network controller which is a critical component of TI's voice and data centric residential gateway solution.}, 
keywords={Internet;DSL;Broadband communication;Ethernet networks;Universal Serial Bus;Control systems;Instruments;Routing;Home automation;Transceivers}, 
doi={10.1109/ASIC.2002.1158072}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8673038, 
author={Liu, Xiao and Jiang, Yufei and Wu, Dinghao}, 
booktitle={2019 IEEE 19th International Symposium on High Assurance Systems Engineering (HASE)}, 
title={A Lightweight Framework for Regular Expression Verification}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Regular expressions and finite state automata have been widely used in programs for pattern searching and string matching. Unfortunately, despite the popularity, regular expressions are difficult to understand and verify even for experienced programmers. Conventional testing techniques remain a challenge as large regular expressions are constantly used for security purposes such as input validation and network intrusion detection. In this paper, we present a lightweight verification framework for regular expressions. In this framework, instead of a large number of test cases, it takes in requirements in natural language descriptions to automatically synthesize formal specifications. By checking the equivalence between the synthesized specifications and target regular expressions, errors will be detected and counterexamples will be reported. We have built a web application prototype and demonstrated its usability with two case studies.}, 
keywords={Automata;Testing;Software;Tools;Security;Natural language processing;regular expression;verification;natural language;formal specification;domain-specific language}, 
doi={10.1109/HASE.2019.00011}, 
ISSN={2640-7507}, 
month={Jan},}
@INPROCEEDINGS{9568383, 
author={Deantoni, Julien and Cambeiro, João and Bateni, Soroush and Lin, Shaokai and Lohstroh, Marten}, 
booktitle={2021 Forum on specification & Design Languages (FDL)}, 
title={Debugging and Verification Tools for Lingua Franca in Gemoc Studio}, 
year={2021}, 
volume={}, 
number={}, 
pages={01-08}, 
abstract={LINGUA Franca (lf) is a polyglot coordination language designed for the composition of concurrent, time-sensitive, and potentially distributed reactive components called reactors. The LF coordination layer facilitates the use of target languages (e.g., C, C++, Python, TypeScript) to realize the program logic, where each target language requires a separate runtime implementation that must correctly implement the reactor semantics. Verifying the correctness of runtime implementations is not a trivial task, and is currently done on the basis of regression testing. To provide a more formal verification tool for existing and future target runtimes, as well as to help verify properties of LF programs, we recruit the use of GemocStudio-an Eclipse-based workbench for the development, integration, and use of heterogeneous executable modeling languages. We present an operational model for LF, realized in GEmocStudio, that is primed to interact with a rich set of analysis and verification tools. Our instrumentation provides the ability to navigate the execution of LF programs using an omniscient debugger with graphical model animation; to check assertions in particular execution runs, or exhaustively, using a model checker; and to validate or debug traces obtained from arbitrary LF runtime environments.}, 
keywords={Runtime environment;Graphical models;Navigation;Instruments;Semantics;Tools;Inductors}, 
doi={10.1109/FDL53530.2021.9568383}, 
ISSN={1636-9874}, 
month={Sep.},}
@INPROCEEDINGS{7552103, 
author={Poon, Chung Keung and Wong, Tak-Lam and Yu, Y. T. and Lee, Victor C. S. and Tang, Chung Man}, 
booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)}, 
title={Toward More Robust Automatic Analysis of Student Program Outputs for Assessment and Learning}, 
year={2016}, 
volume={1}, 
number={}, 
pages={780-785}, 
abstract={Automated analysis and assessment of students' programs, typically implemented in automated program assessment systems (APASs), are very helpful to both students and instructors in modern day computer programming classes. The mainstream of APASs employs a black-box testing approach which compares students' program outputs with instructor-prepared outputs. A common weakness of existing APASs is their inflexibility and limited capability to deal with admissible output variants, that is, outputs produced by acceptable correct programs that differ from the instructor's. This paper proposes a more robust framework for automatically modelling and analysing student program output variations based on a novel hierarchical program output structure called HiPOS. Our framework assesses student programs by means of a set of matching rules tagged to the HiPOS, which produces a better verdict of correctness. We also demonstrate the capability of our framework by means of a pilot case study using real student programs.}, 
keywords={Robustness;Programming profession;Education;Computers;Testing;Natural language processing;automated assessment technology;computer science education;learning computer programming;program output variant;student program analysis}, 
doi={10.1109/COMPSAC.2016.208}, 
ISSN={0730-3157}, 
month={June},}
@ARTICLE{6784505, 
author={Vara, Juan Manuel and Bollati, Verónica A. and Jiménez, Álvaro and Marcos, Esperanza}, 
journal={IEEE Transactions on Software Engineering}, 
title={Dealing with Traceability in the MDDof Model Transformations}, 
year={2014}, 
volume={40}, 
number={6}, 
pages={555-583}, 
abstract={Traceability has always been acknowledged as a relevant topic in Software Engineering. However, keeping track of the relationships between the different assets involved in a development process is a complex and tedious task. The fact that the main assets handled in any model-driven engineering project are models and model transformations eases the task. In order to take advantage of this scenario, which has not been appropriately capitalized on by the most widely adopted model transformation languages before, this work presents MeTAGeM-Trace, a methodological and technical proposal with which to support the model-driven development of model transformations that include trace generation. The underlying idea is to start from a high-level specification of the transformation which is subsequently refined into lower-level transformation models in terms of a set of DSLs until the source code that implements the transformation can be generated. Running this transformation produces not only the corresponding target models, but also a trace model between the elements of the source and target models. As part of the proposal, an EMF-based toolkit has been developed to support the development of ATL and ETL model transformations. This toolkit has been empirically validated by conducting a set of case studies following a systematic research methodology.}, 
keywords={Proposals;Object oriented modeling;Software;DSL;Complexity theory;Data models;Software engineering;Model-driven engineering;model transformations;traceability}, 
doi={10.1109/TSE.2014.2316132}, 
ISSN={1939-3520}, 
month={June},}
@INPROCEEDINGS{7577380, 
author={Kapre, Nachiket and Bayliss, Samuel}, 
booktitle={2016 26th International Conference on Field Programmable Logic and Applications (FPL)}, 
title={Survey of domain-specific languages for FPGA computing}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={High-performance FPGA programming has typically been the exclusive domain of a small band of specialized hardware developers. They are capable of reasoning about implementation concerns at the register-transfer level (RTL) which is analogous to assembly-level programming in software. Sometimes these developers are required to push further down to manage even lower levels of abstraction closer to physical aspects of the design such as detailed layout to meet critical design constraints. In contrast, software programmers have long since moved away from textual assembly-level programming towards relying on graphical integrated development environments (IDEs), high-level compilers, smart static analysis tools and runtime systems that optimize, manage and assist the program development tasks. Domain-specific languages (DSLs) can bridge this productivity gap by providing higher levels of abstraction in environments close to the domain of application expert. DSLs carefully limit the set of programming constructs to minimize programmer mistakes while also enabling a rich set of domain-specific optimizations and program transformations. With a large number of DSLs to choose from, an inexperienced FPGA user may be confused about how to select an appropriate one for the intended domain. In this paper, we review a combination of legacy and state-of-the-art DSLs available for FPGA development and provide a taxonomy and classification to guide selection and correct use of the framework.}, 
keywords={Field programmable gate arrays;Hardware;DSL;Hardware design languages;Software;Programming;Productivity}, 
doi={10.1109/FPL.2016.7577380}, 
ISSN={1946-1488}, 
month={Aug},}
@INPROCEEDINGS{6890825, 
author={Wanderley, Fernando and Silva, António and Araujo, João and Silveira, Denis S.}, 
booktitle={2014 IEEE 4th International Model-Driven Requirements Engineering Workshop (MoDRE)}, 
title={SnapMind: A framework to support consistency and validation of model-based requirements in agile development}, 
year={2014}, 
volume={}, 
number={}, 
pages={47-56}, 
abstract={Two fundamental principles and values of agile methods are customer satisfaction by rapid delivery of useful software and the improvement of the communication process by continuous stakeholders' involvement. But, how to deal with customers' satisfaction and find a better visualization model at the requirements level (which stakeholders can understand and be involved) in an agile development context? Also, how this visualization model enhancement can guarantee consistency between agile requirements artefacts (e.g., user stories and domain models)? Thus, to answer these questions, this paper presents the SnapMind framework. This framework aims to make the requirements modelling process more user-centered, through the definition of a visual requirements language, based on mind maps, model-driven and domain specific language techniques. Moreover, through these techniques, the SnapMind framework focuses on support for consistency between user stories and the domain models using a model animation technique called snapshots. The framework was applied to an industrial case study to investigate its feasibility.}, 
keywords={Visualization;Software;Syntactics;Unified modeling language;Adaptation models;Business;Semantics;Agile Software Requirements;Model-Driven Engineering;Domain-Specific Languages;Mind Map;Snapshots}, 
doi={10.1109/MoDRE.2014.6890825}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6062146, 
author={Groce, Alex and Havelund, Klaus and Smith, Margaret}, 
booktitle={2010 ACM/IEEE 32nd International Conference on Software Engineering}, 
title={From scripts to specifications: the evolution of a flight software testing effort}, 
year={2010}, 
volume={2}, 
number={}, 
pages={129-138}, 
abstract={This paper describes the evolution of a software testing effort during a critical period for the flagship Mars Science Laboratory rover project at the Jet Propulsion Laboratory. Formal specification for post-run analysis of log files, using a domain-specific language, LogScope, replaced scripted real-time analysis. Log analysis addresses the key problems of on-the-fly approaches and cleanly separates specification and execution. Mining the test repository suggested the inadequacy of the scripted approach, and encouraged a partly engineer-driven development. LogScope development should hold insights for others facing the tight deadlines and reactionary nature of testing for critical projects. LogScope received a JPL Mariner Award for "improving productivity and quality of the MSL Flight Software" and has been discussed as an approach for other flight missions. We note LogScope features that most contributed to ease of adoption and effectiveness. LogScope is general and can be applied to any software producing logs.}, 
keywords={Software;Telemetry;Space vehicles;Laboratories;Libraries;Semantics;Python;development practices;logs;runtime verification;space flight software;temporal logic;test infrastructure;testing}, 
doi={10.1145/1810295.1810314}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{5967121, 
author={Piho, Gunnar and Tepandi, Jaak and Parman, Marko and Puusep, Viljam and Roost, Mart}, 
booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
title={Test Driven domain modelling}, 
year={2011}, 
volume={}, 
number={}, 
pages={576-581}, 
abstract={To write software we have to know requirements; to know requirements we have to know domain; to know the domain we have to analyze and model one. We propose a methodology for applying Test Driven Modelling in engineering of domains, requirements and software. We will restrict ourselves here to enterprise information systems and therefore to business domains. As common for Software Factories, domain models (as well as all other models) are software artefacts, not only documentation artefacts. In our approach Test Driven Modelling utilizes Test Driven Development for domain modelling. Domain models engineered in this way are used as Domain Specific Language for specifying software requirements. The hypothesis is that such domain models can be used for validation of requirements and verification of software, lead developments towards Software Factories, and increase dependability of software.}, 
keywords={Software;Measurement units;Time division multiplexing;Analytical models;Business;Silicon;Production facilities;domain analysis and engineering;domain model and domain modelling;software engineering;software factory;software testing;test driven development;test driven modelling;verification and validation}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7302512, 
author={Vinogradov, Sergey and Ozhigin, Artem and Ratiu, Daniel}, 
booktitle={2015 IEEE International Symposium on Systems Engineering (ISSE)}, 
title={Modern model-based development approach for embedded systems practical experience}, 
year={2015}, 
volume={}, 
number={}, 
pages={56-59}, 
abstract={Control functionality of modern rail vehicles is getting more and more complex. It contains several modules such as the traction control unit or the central control unit, as well as input and output stations, such as driver's cab terminals and process I/Os. A plethora of devices are connected to the vehicle and train bus and are able to communicate. The functions of the vehicle control and traction systems are configured by using function blocks from which loadable programs are generated. The languages used to program the control units are well established in the field. However, one-size-fits-all approach cannot adequately address the increased complexity of the software in modern trains. In this paper we describe our preliminary experience with using the multi-paradigm modeling tool “mbeddr” in the railway domain. The following aspects have been in focus during the work: (a) matching the application requirements and domain specific language used for implementation; (b) integration of model-based approach into traditional product lifecycle; (c) reengineering existing functionality using modeling and code generation capabilities of mbeddr. The system example we chose was the application logic of automated train driving system implemented in development environment of Siemens process automation framework.}, 
keywords={Software;Mathematical model;Complexity theory;Control systems;Domain specific languages;Formal verification;Rail transportation;model based development;language engineering}, 
doi={10.1109/SysEng.2015.7302512}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{9270318, 
author={Mai, Phu X. and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.}, 
booktitle={2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)}, 
title={SMRL: A Metamorphic Security Testing Tool for Web Systems}, 
year={2020}, 
volume={}, 
number={}, 
pages={9-12}, 
abstract={We present a metamorphic testing tool that alleviates the oracle problem in security testing. The tool enables engineers to specify metamorphic relations that capture security properties of Web systems. It automatically tests Web systems to detect vulnerabilities based on those relations. We provide a domain-specific language accompanied by an Eclipse editor to facilitate the specification of metamorphic relations. The tool automatically collects the input data and transforms the metamorphic relations into executable Java code in order to automatically perform security testing based on the collected data. The tool has been successfully evaluated on a commercial system and a leading open source system (Jenkins). Demo video: https://youtu.be/9kx6u9LsGxs.}, 
keywords={Tools;Testing;Security;Software engineering;Java;Europe;Manuals;Software and application security;Software verification and validation;Metamorphic Security Testing}, 
doi={}, 
ISSN={2574-1926}, 
month={Oct},}
@INPROCEEDINGS{9954383, 
author={Krook, Robert and Hui, John and Svensson, Bo Joel and Edwards, Stephen A. and Claessen, Koen}, 
booktitle={2022 20th ACM-IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE)}, 
title={Creating a Language for Writing Real-Time Applications for the Internet of Things}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-20}, 
abstract={We describe the development of a new programming language Scoria and its compiler. Scoria is a high-level reactive real-time language based on the sparse synchronous model (SSM), designed to produce time- and power-efficient low-level C code that can run on small IoT devices. While the compiler is not yet in a state where it is meaningful to measure power usage, we carefully profile the timing behaviour and identify bottlenecks that can improve performance. The language and compiler are implemented as an Embedded Domain-Specific Language (EDSL) on top of Haskell.}, 
keywords={Codes;Program processors;Sensitivity;Computer bugs;Writing;Real-time systems;Internet of Things;Real-time;IoT;Compilers;Embedded Domain-Specific Languages}, 
doi={10.1109/MEMOCODE57689.2022.9954383}, 
ISSN={2832-6520}, 
month={Oct},}
@INPROCEEDINGS{5381641, 
author={Hartmann, Tobias}, 
booktitle={2009 Testing: Academic and Industrial Conference - Practice and Research Techniques}, 
title={Model Based Testing of End-to-End Chains Using Domain Specific Languages}, 
year={2009}, 
volume={}, 
number={}, 
pages={82-91}, 
abstract={In this paper, the author explains a new approach of model based end-to-end chain testing using scenarios with original and simulated equipment. The first goal is to automatically derive test data and test cases from the model, which is defined by a domain specific language. Several solvers can be attached to the conversion to quickly create a wide variety of stimuli for the system(s) under test. Furthermore, the system under test can be stimulated by either original equipment - which is connected to the test bench - or the test bench can simulate equipment and create inputs for the tested systems. Any mixture of simulated and original equipment is possible and can be changed on the fly. In the end, the results from the system under test are collected. These results can then be displayed back in the model. This method is currently used and improved in the project "E-Cab" in which the author is involved. Passengers travelling by plane are in the focus of this project. Complete services and service chains - from the booking at home up to leaving the destination airport - are created and used by many systems communicating with each other. The author expects advantages from testing these end-to-end chains with this approach.}, 
keywords={Domain specific languages;System testing;Automatic testing;Logic testing;Constraint theory;Logistics;Operating systems;Airports;Complex networks;Robustness;model based;end-to-end chain;testing;automatic test case generation;automatic test data generation}, 
doi={10.1109/TAICPART.2009.25}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8729118, 
author={Tyugashev, A.}, 
booktitle={2018 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)}, 
title={On Use of Adaptive Schedules in Semantic Modeling of Real- Time Control Algorithms}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The paper presents a semantic model for time-driven real-time control algorithms based on adaptive schedule. In contrast to known models of semantics of programs and reactive systems based on the notion of state, use of adaptive schedules allows avoiding surplus detalization which leads to a `state explosion' problem. On the other hand, adaptive schedules correspond well to the nature of real-time control algorithms. The use of this model allows automated checking of the control algorithms' synchronization properties. The paper describes two case studies of application of a presented model for automation of design and verification of spacecraft's onboard flight control programs. The use of adaptive schedules semantics together with domain specific language, and automation toolset allow formal specification and verification of the important properties of realtime systems.}, 
keywords={Semantics;Schedules;Adaptation models;Software;Real-time systems;Software algorithms;Task analysis;program semantics;control algorithm;real-time mode;flight control software;formal verification;control algorithm's logic;CASE toolset}, 
doi={10.1109/ICIEAM.2018.8729118}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9153386, 
author={Cotroneo, Domenico and De Simone, Luigi and Liguori, Pietro and Natella, Roberto}, 
booktitle={2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)}, 
title={ProFIPy: Programmable Software Fault Injection as-a-Service}, 
year={2020}, 
volume={}, 
number={}, 
pages={364-372}, 
abstract={In this paper, we present a new fault injection tool (ProFIPy) for Python software. The tool is designed to be programmable, in order to enable users to specify their software fault model, using a domain-specific language (DSL) for fault injection. Moreover, to achieve better usability, ProFIPy is provided as software-as-a-service and supports the user through the configuration of the faultload and workload, failure data analysis, and full automation of the experiments using container- based virtualization and parallelization.}, 
keywords={Tools;Computer bugs;Python;DSL;Pattern matching;Open source software;Software Fault Injection;Python;Software-as-a-Service;Bug Pattern}, 
doi={10.1109/DSN48063.2020.00052}, 
ISSN={1530-0889}, 
month={June},}
@INPROCEEDINGS{6462664, 
author={Combemale, Benoît and Crégut, Xavier and Pantel, Marc}, 
booktitle={2012 19th Asia-Pacific Software Engineering Conference}, 
title={A Design Pattern to Build Executable DSMLs and Associated V&V Tools}, 
year={2012}, 
volume={1}, 
number={}, 
pages={282-287}, 
abstract={Model executability is now a key concern in model-driven engineering, mainly to support early validation and verification (V&V). Some approaches allow to weave executability into metamodels, defining executable domain-specific modeling languages (DSMLs). Model validation can then be achieved by simulation and graphical animation through direct interpretation of the conforming models. Other approaches address model executability by model compilation, allowing to reuse the virtual machines or V&V tools existing in the target domain. Nevertheless, systematic methods are currently not available to help the language designer in the definition of such an execution semantics and related tools. For instance, simulators are mostly hand-crafted in a tool specific manner for each DSML. In this paper, we propose to reify the elements commonly used to support state-based execution in a DSML. We infer a design pattern (called Executable DSML pattern) providing a general reusable solution for the expression of the executability concerns in DSMLs. It favors flexibility and improves reusability in the definition of semantics-based tools for DSMLs. We illustrate how this pattern can be applied to ease the development of V&V tools.}, 
keywords={Semantics;Unified modeling language;Runtime;Computational modeling;Abstracts;Animation;Concrete;Model Driven Engineering;Software Language Engineering;Validation & Verification}, 
doi={10.1109/APSEC.2012.79}, 
ISSN={1530-1362}, 
month={Dec},}
@INPROCEEDINGS{8804458, 
author={Giorgi, Fabio and Paulisch, Frances}, 
booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
title={Transition Towards Continuous Delivery in the Healthcare Domain}, 
year={2019}, 
volume={}, 
number={}, 
pages={253-254}, 
abstract={Continuous Delivery is meanwhile well-established in many parts of the software industry. In a transition towards continuous delivery in the healthcare domain, there are a number of additional challenges that should be addressed. We present how we have addressed some of these challenges and highlight some potential research topics that could be addressed in this space to make further progress in this important area. Although our focus is on the healthcare domain, the approach and the research topics are applicable also to a broad range of other application domains.}, 
keywords={Software;Medical services;Automation;Industries;Documentation;Organizations;DSL;continuous delivery, agile, test-driven development, behavior-driven development, domain-driven design, test automation, pair-programming, deployment pipeline}, 
doi={10.1109/ICSE-SEIP.2019.00035}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{766632, 
author={Blome, F. and Hamich, A.}, 
booktitle={International Symposium on Switching}, 
title={24 months of commercial isdn experience}, 
year={1990}, 
volume={2}, 
number={}, 
pages={159-164}, 
abstract={}, 
keywords={ISDN;Switches;Telephony;Packet switching;DSL;Telecommunications;Circuits;Roads;Testing;Cities and towns}, 
doi={10.1109/ISS.1990.766632}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6104710, 
author={Liu, Jianqing and Liu, Guangyong}, 
booktitle={2011 4th International Conference on Intelligent Networks and Intelligent Systems}, 
title={Research and Implementation of SNMP-Based Network Management System}, 
year={2011}, 
volume={}, 
number={}, 
pages={129-132}, 
abstract={After the brief introduction of the current situation of matured system framework based on SNMP, the basic problems of network management framework design are presented in detail. This paper focuses on the construction of SNMP-based platform in terms of software framework. Finally, by the use of the medium-sized data exchange network, it achieves the function of discovering the topology.}, 
keywords={Topology;Software;Programming;Testing;Synchronization;DSL;Scalability;Network management framework;Auto-Topology control;Scalability;Generality}, 
doi={10.1109/ICINIS.2011.39}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{410732, 
author={Kelly, D. and Hartmann, Q. and Gude, W.}, 
booktitle={Sixth Annual IEEE International ASIC Conference and Exhibit}, 
title={A multi-FPGA prototype of a DS1/HDSL synchronizer and desynchronizer prior to ASIC fabrication}, 
year={1993}, 
volume={}, 
number={}, 
pages={332-335}, 
abstract={As the speed and complexity of today's ASICs continues to grow, conventional prototyping techniques for algorithm verification begin to break down. A novel implementation of DS1/HDSL synchronizer and desynchronizer utilizing an array of four FPGA devices to verify algorithm performances prior to ASIC fabrication is described. The utilization of FPGA devices for ASIC prototyping can significantly reduce the risk, cost, and time-to-market involved with complex ASIC devices.<>}, 
keywords={Prototypes;Application specific integrated circuits;Fabrication;Timing;Field programmable gate arrays;Jitter;Frequency synchronization;Costs;DSL;Testing}, 
doi={10.1109/ASIC.1993.410732}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{916745, 
author={Lightfoot, R.S.}, 
booktitle={Proceedings of the 2000 IEEE International Conference on Management of Innovation and Technology. ICMIT 2000. 'Management in the 21st Century' (Cat. No.00EX457)}, 
title={Establishing long-term viability in the information economy: the strength of the integrated systems engineering process}, 
year={2000}, 
volume={2}, 
number={}, 
pages={526-529 vol.2}, 
abstract={Information technology companies are under increasing pressure to develop high technology products and services at an accelerated pace. They are often rewarded for such efforts through increases in their stock prices only to see these gains drop due to poor quality, failed technology, and poor customer support. In considering these events the question that comes to mind is, "What is the formula for providing long term viability in the high-tech information economy?" This paper presents recommendations that can be applied to the integrated systems/software engineering process which, when implemented, will provide companies with the formula for long-term viability in this rapidly changing fast-paced environment.}, 
keywords={Information technology;Companies;Systems engineering and theory;Stock markets;IEEE news;Educational institutions;Acceleration;Software engineering;DSL;Modems}, 
doi={10.1109/ICMIT.2000.916745}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{739853, 
author={Pingree, P.J.}, 
booktitle={17th DASC. AIAA/IEEE/SAE. Digital Avionics Systems Conference. Proceedings (Cat. No.98CH36267)}, 
title={Deep space one integration and test challenges: getting to the launch pad in the faster, better, cheaper world}, 
year={1998}, 
volume={2}, 
number={}, 
pages={H21/1-H21/8 vol.2}, 
abstract={This paper describes the integration and test challenges of verifying and validating the avionics hardware and flight software which have been experienced in meeting the New Millennium Program Deep Space One (DS1) project's faster, better, cheaper requirements. This paper gives a high level overview of the development and application of the two flight system testbeds (DSI Hotbench), the testbed activities supported the DSI Spacecraft Assembly, Test and Launch Operations (ATLO), and the testing performed to prepare for and support post-launch mission operations. In the "Faster, Better, Cheaper" environment of the New Millennium Program, DSI Integration and Test (I&T) has defined new methods and decision criteria to meet our requirements and goals while assessing and minimizing the risk in the paths we have taken. Our successes and failures are largely yet to be seen as we approach our July 1 launch date. This paper describes the challenges that have been faced and some that have been overcome during the DSI I&T phase. It presents the risks that have been accepted in our attempts to test completely the DSI Avionics system in preparation for launch and mission operations.}, 
keywords={Atherosclerosis;Aerospace electronics;Space technology;Electronic equipment testing;Software testing;System testing;Hardware;Space vehicles;DSL;Space missions}, 
doi={10.1109/DASC.1998.739853}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{4055046, 
author={Kim, Dae-woo and Lim, Hyun-min and Lee, Sang-kon}, 
booktitle={2006 Canadian Conference on Electrical and Computer Engineering}, 
title={Testing Activities for KT-OSS Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={2397-2400}, 
abstract={This paper describes the testing activities for the development of the KTOSS (Korea Telecom Operations Support System). In this paper, we show the test phases for performing the verification and validation activities for the development and maintenance of KT-OSS. They are based on the general software development lifecycle, with an operational test added to it as an additional phase. To ensure the successful development of the KT-OSS, we performed various tests related to functionality, efficiency and others. Also, the tests were performed for maintenance after the field release. We also show the criteria for them and deal with the test organizations and the test-bed for managing and controlling the quality of the KT-OSS in this paper. Through these testing activities, we were able to successfully develop and release the KT-OSS}, 
keywords={Life testing;Software testing;Programming;System testing;Laboratories;Performance evaluation;Quality management;DSL;Telecommunications;ISO standards;Test;Operations Support System;Verification;Validation;Software Development Lifecycle}, 
doi={10.1109/CCECE.2006.277822}, 
ISSN={0840-7789}, 
month={May},}
@INPROCEEDINGS{5479285, 
author={Guo-Ming Sung and Yen-Tang Chang and Wen-Huei Chen and Hsiang-Yuan Hsieh}, 
booktitle={2010 International Conference on Networking and Digital Society}, 
title={A new architecture of broadband network system suitable for asymmetric digital subscriber line application}, 
year={2010}, 
volume={1}, 
number={}, 
pages={624-628}, 
abstract={This paper presents the design and implementation on a new architecture of broadband network system which suitable for asymmetric digital subscriber line (ADSL) application. The main design skill is based on the cell-based digital IC design process, and is implemented in 0.18 μm 1P6M CMOS process. The main function of this chip is to build a bridge between Ethernet and ATM which is used to substitute for RISC processor, leading to enhance the broadband network switching ability and stability. Furthermore, the clock management system is adopted to manage the packages. By this technique, a small size and low cost chip will be obtained.},
keywords={Broadband communication;DSL;CMOS integrated circuits;CMOS digital integrated circuits;Process design;CMOS process;Bridges;Ethernet networks;Asynchronous transfer mode;Reduced instruction set computing;component;Asynchronous transfer mode (ATM);asymmetric digital subscriber line (ADSL);Broadband Network}, 
doi={10.1109/ICNDS.2010.5479285}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9163526, 
author={Ammon, Lorenz and Deutschmann, Jörg and Hielscher, Kai-Steffen and German, Reinhard}, 
booktitle={2020 43rd International Conference on Telecommunications and Signal Processing (TSP)}, 
title={One-Way Delay and Goodput Measurements with a VDSL, DOCSIS, and MPTCP Internet Access}, 
year={2020}, 
volume={}, 
number={}, 
pages={563-568}, 
abstract={Different Internet access technologies have both advantages and disadvantages. Two common wire-based standards are Very high speed Digital Subscriber Line (VDSL) and Data Over Cable Service Interface Specification (DOCSIS). The simultaneous use of multiple Internet access links can be achieved with Multipath TCP (MPTCP). This paper presents one-way delay and bulk data transfer measurements of a typical end-user DSL as well as DOCSIS Internet access. In addition, MPTCP is used for bandwidth aggregation of both Internet access links. The results show that the DSL Internet access has a more stable performance than the DOCSIS Internet access. MPTCP can provide bandwidth aggregation unless the link rates are too different. The observed fluctuations indicate that Internet access link models with simple impairment assumptions need to be extended. However, some effects cannot be explained without having access to the operator and backbone infrastructure.}, 
keywords={DSL;Internet;Delays;Downlink;Uplink;Data transfer;Bandwidth;DOCSIS;Goodput;Hybrid Access;Multipath TCP;Network Measurements;One-Way Delay;Throughput;VDSL}, 
doi={10.1109/TSP49548.2020.9163526}, 
ISSN={}, 
month={July},}
@ARTICLE{182763, 
author={}, 
journal={IEEE Std 610}, 
title={IEEE Standard Computer Dictionary: A Compilation of IEEE Standard Computer Glossaries}, 
year={1991}, 
volume={}, 
number={}, 
pages={1-217}, 
abstract={Identifies terms currently in use in the computer field. Standard definitions for thoseterms are established. Compilation of IEEE Stds IEEE Std 1084, IEEE Std 610.2, IEEE Std 610.3, IEEE Std 610.4, IEEE Std 610.5 and IEEE Std 610.12}, 
keywords={Terminology;terminology;computer;applications;glossary;definitions;dictionary;610}, 
doi={10.1109/IEEESTD.1991.106963}, 
ISSN={}, 
month={Jan},}
@ARTICLE{9801672, 
author={Yaraghi, Ahmadreza Saboor and Bagherzadeh, Mojtaba and Kahani, Nafiseh and Briand, Lionel}, 
journal={IEEE Transactions on Software Engineering}, 
title={Scalable and Accurate Test Case Prioritization in Continuous Integration Contexts}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-24}, 
abstract={Continuous Integration (CI) requires efficient regression testing to ensure software quality without significantly delaying its CI builds. This warrants the need for techniques to reduce regression testing time, such as Test Case Prioritization (TCP) techniques that prioritize the execution of test cases to detect faults as early as possible. Many recent TCP studies employ various Machine Learning (ML) techniques to deal with the dynamic and complex nature of CI. However, most of them use a limited number of features for training ML models and evaluate the models on subjects for which the application of TCP makes little practical sense, due to their small regression testing time and low number of failed builds. In this work, we first define, at a conceptual level, a data model that captures data sources and their relations in a typical CI environment. Second, based on this data model, we define a comprehensive set of features that covers all features previously used by related studies. Third, we develop methods and tools to collect the defined features for 25 open-source software systems with enough failed builds and whose regression testing takes at least five minutes. Fourth, relying on the collected dataset containing a comprehensive feature set, we answer four research questions concerning data collection time, the effectiveness of ML-based TCP, the impact of the features on effectiveness, the decay of ML-based TCP models over time, and the trade-off between data collection time and the effectiveness of ML-based TCP techniques.}, 
keywords={Feature extraction;Codes;Testing;History;Training;Data collection;Computational modeling;Continuous integration;machine learning;software testing;test case prioritization;test case selection}, 
doi={10.1109/TSE.2022.3184842}, 
ISSN={1939-3520}, 
month={},}
@INPROCEEDINGS{6605922, 
author={Sobernig, Stefan and Hoisl, Bernhard and Strembeck, Mark}, 
booktitle={2013 13th International Conference on Quality Software}, 
title={Requirements-Driven Testing of Domain-Specific Core Language Models Using Scenarios}, 
year={2013}, 
volume={}, 
number={}, 
pages={163-172}, 
abstract={In this paper, we present an approach for the scenario-based testing of the core language models of domain-specific modeling languages (DSML). The core language model is a crucial artifact in DSML development, because it captures all relevant domain abstractions and specifies the relations between these abstractions. In software engineering, scenarios are used to explore and to define (actual or intended) system behavior as well as to specify user requirements. The different steps in a requirements-level scenario can then be refined through detailed scenarios. In our approach, we use scenarios as a primary design artifact. Non-executable, human-understandable scenario descriptions can be refined into executable test scenarios. To demonstrate the applicability of our approach, we implemented a scenario-based testing framework based on the Eclipse Modeling Framework (EMF) and the Epsilon model-management toolkit.}, 
keywords={Biological system modeling;Testing;Unified modeling language;Software;Prototypes;Metamodeling;domain-specific modeling;scenario-based testing;language engineering;metamodel testing}, 
doi={10.1109/QSIC.2013.56}, 
ISSN={2332-662X}, 
month={July},}
@ARTICLE{1094910, 
author={Ahamed, S. and Bohn, P. and Gottfried, N.}, 
journal={IEEE Transactions on Communications}, 
title={A Tutorial on Two-Wire Digital Transmission in the Loop Plant}, 
year={1981}, 
volume={29}, 
number={11}, 
pages={1554-1564}, 
abstract={This paper explores the constraints on the design of twowire repeaterless digital subscriber loop (DSL) systems. Broadly categorized, the design depends on the technical feasibility of the approach used to achieve two-wire transmission, constraints related to compatibility with other systems sharing the same cable, and immunity to central office noise. Each of these varies With the choice of system parameters including the transmission rate, transmit power, choice of line codes, etc. Technical feasibility is evaluated by computer simulation studies. Compatibility with other systems is explored by crosstalk calculations. Noise immunity considerations, as they translate into digital line power levels, are also explored.}, 
keywords={Tutorial;Crosstalk;DSL;Echo cancellers;Wire;Repeaters;Central office;Noise cancellation;Computer simulation;Noise level}, 
doi={10.1109/TCOM.1981.1094910}, 
ISSN={1558-0857}, 
month={November},}
@INPROCEEDINGS{9172680, 
author={Lawler, Christopher R. and Ridenhour, Forrest L. and Khan, Shaheer A. and Rossomando, Nicholas M. and Rothstein-Dowden, Ansel}, 
booktitle={2020 IEEE Aerospace Conference}, 
title={Blackbird: Object-Oriented Planning, Simulation, and Sequencing Framework Used by Multiple Missions}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-20}, 
abstract={Every JPL flight mission relies on activity planning and sequence generation software to perform operations. Most such tools in use at JPL and elsewhere use attribute-based schemas or domain-specific languages (DSLs) to define activities. This reliance poses user training, software maintenance, performance, and other challenges. To solve this problem for future missions, a new software called Blackbird was developed which allows engineers to specify behavior in standard Java. The new code base has over an order of magnitude fewer lines of code than other JPL planning software, since no DSL or schema interpreter is needed. The use of Java for defining activities also allows mission adapters to debug their code in an integrated development environment, seamlessly call external libraries, and set up truly multi-mission models. These efficiency gains have significantly reduced the amount of development effort required to support the software. This paper discusses Blackbird's design, principles, and use cases. Within a year of its completion, six projects have begun using Blackbird. The Mars 2020 mission is using Blackbird to generate command sequences for cruise and Mars approach. By using multi-mission models, the Mars 2020 cruise adaptation was created in fewer than three months by three engineers at less than half time each. Work has begun to use Blackbird for communications planning during Mars 2020 surface operations. The Psyche mission uses Blackbird to generate its reference mission plans in development. Full simulations with 123,000 activities and 4.7 million resource value changes complete in about one minute. Psyche is also working towards using Blackbird in operations to support integrated activity planning and generate sequences. The InSight project is using Blackbird for mission planning in operations, replacing error-prone manual processes. For the NISAR mission, Blackbird evaluates threats to the commissioning phase timeline. The Europa Lander pre-project used Blackbird to perform a trade study. The ASTERIA mission is automating sequence generation in Blackbird. Going forward, more interested projects are likely to begin using Blackbird, and the capabilities of the core and multi-mission models will keep growing.}, 
keywords={Training;Adaptation models;Java;Codes;Object oriented modeling;Planning;DSL}, 
doi={10.1109/AERO47225.2020.9172680}, 
ISSN={1095-323X}, 
month={March},}
@INPROCEEDINGS{4696109, 
author={Khoshbakhtian, Masoumeh and HezareMoghadam, Nasrin and Mazoochi, Mojtaba}, 
booktitle={2008 Third International Conference on Broadband Communications, Information Technology & Biomedical Applications}, 
title={Identification of Various Problems in an Environment of Multi Vendor Equipments for PSTN Services in NGN}, 
year={2008}, 
volume={}, 
number={}, 
pages={194-201}, 
abstract={This paper focuses on Iran next generation network (NGN) Pilot, the purpose of which is to achieve appropriate knowledge for a desired move through current telecommunication network of Iran to NGN. On the base of this purpose, obtaining a proper knowledge about capabilities and weaknesses of vendors' NGN equipments is necessary as well as verifying the functionalities of these equipments in a multi-vendor environment. Accordingly, a test bed has been designed with the capability of executing various tests scenarios related to NGN and processing the results. At these stage two types of tests, i.e. Functional tests and Interoperability tests have been done. Since Functional tests are performed in a single-vendor environment; the proper interoperation among multi-vendor equipments can not be guaranteed. Consequently, other types of tests called Interoperability tests have been designed. These tests are performed in a multi-vendor environment and classified into two groups which process interoperation of two Call Servers or a Call Server and Gateway from different vendors, respectively. The following paper represents the results obtained from the mentioned tests as well as the precise assessment of them.}, 
keywords={Next generation networking;Testing;Cities and towns;Network servers;File servers;Databases;Performance evaluation;Modems;Access protocols;Switching circuits;interoperability;Megaco;NGN;SIP}, 
doi={10.1109/BROADCOM.2008.14}, 
ISSN={}, 
month={Nov},}
@ARTICLE{1519613, 
author={Young-Jae Cho and Seung-Hoon Lee}, 
journal={IEEE Transactions on Circuits and Systems I: Regular Papers}, 
title={An 11b 70-MHz 1.2-mm/sup 2/ 49-mW 0.18-/spl mu/m CMOS ADC with on-chip current/voltage references}, 
year={2005}, 
volume={52}, 
number={10}, 
pages={1989-1995}, 
abstract={This work proposes an 11b 70-MHz CMOS pipelined analog-digital converter (ADC) as one of core circuit blocks for very high speed digital subscriber line system applications. The proposed ADC for the internal use has the strictly limited number of externally connected I/O pins while the ADC employs on-chip CMOS current/voltage references and a merged-capacitor switching technique to improve ADC performances. The ADC implemented in a 0.18-/spl mu/m 1P4M CMOS technology shows the maximum signal-to-noise distortion ratio (SNDR) of 60 dB at 70 MSample/s. The ADC maintains the SNDR of 58 dB and the spurious-free dynamic resistance of 68 dB for input frequencies up to the Nyquist rate at 60 MSample/s. The measured differential and integral nonlinearities of the ADC are within /spl plusmn/0.63 and /spl plusmn/1.21 LSB, respectively. The active chip area is 1.2 mm/sup 2/ and the ADC consumes 49 mW at 70 MSample/s at 1.8 V.}, 
keywords={Analog-digital conversion;CMOS technology;CMOS digital integrated circuits;CMOS analog integrated circuits;DSL;Pins;Voltage;Frequency;Distortion measurement;Electrical resistance measurement;Analog–digital converter (ADC);CMOS;low power;on-chip references}, 
doi={10.1109/TCSI.2005.853251}, 
ISSN={1558-0806}, 
month={Oct},}
@ARTICLE{7270333, 
author={Mellegård, Niklas and Ferwerda, Adry and Lind, Kenneth and Heldal, Rogardt and Chaudron, Michel R. V.}, 
journal={IEEE Transactions on Software Engineering}, 
title={Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study}, 
year={2016}, 
volume={42}, 
number={3}, 
pages={245-260}, 
abstract={Domain-specific modelling (DSM) is a modern software development technology that aims at enhancing productivity. One of the claimed advantages of DSM is increased maintainability of software. However, current empirical evidence supporting this claim is lacking. In this paper, we contribute evidence from a case study conducted at a software development company. We study how the introduction of DSM affected the maintenance of a legacy system. We collected data about the maintenance phase of a system that was initially developed using manual programming, but which was gradually replaced by DSM development. We performed statistical analyses of the relation between the use of DSM and the time needed to resolve defects, the defect density, and the phase in which defects were detected. The results show that after introducing DSM the defect density is lower, that defects are found earlier, but resolving defects takes longer. Other observed benefits are that the number of developers and the number of person-hours needed for maintaining the system decreased, and the portability to new platforms increased. Our findings are useful for organizations that consider introducing DSM and would like to know which benefits can be realized in software maintenance.}, 
keywords={DSL;Maintenance engineering;Unified modeling language;Business;Software maintenance;Productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity}, 
doi={10.1109/TSE.2015.2479221}, 
ISSN={1939-3520}, 
month={March},}
@INPROCEEDINGS{8080500, 
author={Mueller, Peter and Belschner, Tim and Reichel, Reinhard}, 
booktitle={2017 IEEE AUTOTESTCON}, 
title={Automated test artifact generation for a distributed avionics platform utilizing abstract state machines}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The development of complex and highly safety-critical avionics systems, such as fly-by-wire, is typically linked with high efforts, risks and thus costs. Especially with regard to certification the testing activities during verification are playing a major role. This paper introduces the automatization complex of the testing artifact generation by use of Abstract State Machines (ASM), which allows a unified approach for system and software testing. The baseline is the Flexible Platform technology (a platform based development approach) currently under development by the Institute of Aircraft Systems (ILS) of the University of Stuttgart. The remaining automatization complex is the automated generation of certification relevant documentation, i.e. the requirements. These three complexes establish the AAA-Process which lays the foundation for an effective total system capability for complex avionics systems while simultaneously mitigating risks and costs. The actual test artifact generation is strictly aligned to development standards used in the aviation industry. Requirements exist as classes in a textual representation as well as in a specification model, represented by ASMs. The functional behavior, as described by the models, serves as a test oracle for test case generation. For this the model is translated into a graph system, instrumented by selectable testing methods and executed. The resulting trace data is used to automatically derive test procedures under consideration of the corresponding test environment as scripts, which are directly executable within our testing infrastructure consisting of a HiL simulation. Furthermore this includes the automatic generation of the associated traceability data and test specification documentation. An initial framework has been defined to support exchangeability of individual tasks in the generation tool-chain. The feasibility of the approach has been demonstrated by testing the complete heterogeneous signal communication of an exemplary avionics system, resp. platform instance, at system level as well as at software high-level.}, 
keywords={Aerospace electronics;Software;Hardware;Tools;Aircraft;DSL;Testing}, 
doi={10.1109/AUTEST.2017.8080500}, 
ISSN={1558-4550}, 
month={Sep.},}
@INPROCEEDINGS{7203010, 
author={Abreu, Rui and Erdogmus, Hakan and Perez, Alexandre}, 
booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
title={CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts}, 
year={2015}, 
volume={2}, 
number={}, 
pages={551-554}, 
abstract={Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plug in mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, Code Aware, for distributed and fine-grained artifact analysis. Code Aware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. Code ware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems, (b) the ability to perform both static and dynamic analyses on these artifacts, and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of Code Aware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges.}, 
keywords={Probes;Software;Ecosystems;Software engineering;Monitoring;Electronic mail;DSL}, 
doi={10.1109/ICSE.2015.192}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{1253823, 
author={Brini, S. and Benjelloun, D. and Castanier, F.}, 
booktitle={2003 Design, Automation and Test in Europe Conference and Exhibition}, 
title={A flexible virtual platform for computational and communication architecture exploration of DMT VDSL modems}, 
year={2003}, 
volume={}, 
number={}, 
pages={164-169}, 
abstract={}, 
keywords={Computer architecture;OFDM modulation;Modems;Transceivers;DSL;System-level design;Digital signal processing;Quality of service;Testing;Libraries}, 
doi={10.1109/DATE.2003.1253823}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{9583706, 
author={Marksteiner, Stefan and Bronfman, Slava and Wolf, Markus and Lazebnik, Eddie}, 
booktitle={2021 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)}, 
title={Using Cyber Digital Twins for Automated Automotive Cybersecurity Testing}, 
year={2021}, 
volume={}, 
number={}, 
pages={123-128}, 
abstract={Cybersecurity testing of automotive systems has become a practical necessity, with the wide adoption of advanced driving assistance functions and vehicular communications. These functionalities require the integration of information and communication technologies that not only allow for a plethora of on-the-fly configuration abilities, but also provide a huge surface for attacks. Theses circumstances have also been recognized by standardization and regulation bodies, making the need for not only proper cybersecurity engineering but also proving the effectiveness of security measures by verification and validation through testing also a formal necessity. In order to keep pace with the rapidly growing demand of neutral-party security testing of vehicular systems, novel approaches are needed. This paper therefore presents a methodology to create and execute cybersecurity test cases on the fly in a black box setting by using pattern matching-based binary analysis and translation mechanisms to formal attack descriptions as well as model-checking techniques. The approach is intended to generate meaningful attack vectors on a system with next-to-zero a priori knowledge.}, 
keywords={Analytical models;Digital twin;Tools;Regulation;DSL;Computer security;Vehicle dynamics;automotive;cybersecurity;testing;digital twin;model-based testing}, 
doi={10.1109/EuroSPW54576.2021.00020}, 
ISSN={2768-0657}, 
month={Sep.},}
@ARTICLE{1341380, 
author={Varsamou, M. and Antonakopoulos, T. and Papandreou, N.}, 
journal={IEEE Design & Test of Computers}, 
title={From protocol models to their implementation: a versatile testing methodology}, 
year={2004}, 
volume={21}, 
number={5}, 
pages={416-428}, 
abstract={The design and test of communication protocols relies extensively on formal description languages. In this protocol design and verification scheme, high-level models serve in generating simulation sequences for low-level models, and all simulation is based on directed testing. The methodology is versatile and flexible, and difficult to set up the first time.}, 
keywords={Protocols;Object oriented modeling;System testing;Mathematical model;Design methodology;Computational modeling;Appropriate technology;Process design;DSL;Automata}, 
doi={10.1109/MDT.2004.61}, 
ISSN={1558-1918}, 
month={Sep.},}
@INPROCEEDINGS{8327149, 
author={Binamungu, Leonard Peter and Embury, Suzanne M. and Konstantinou, Nikolaos}, 
booktitle={2018 IEEE Workshop on Validation, Analysis and Evolution of Software Tests (VST)}, 
title={Detecting duplicate examples in behaviour driven development specifications}, 
year={2018}, 
volume={}, 
number={}, 
pages={6-10}, 
abstract={In Behaviour-Driven Development (BDD), the behaviour of the software to be built is specified as a set of example interactions with the system, expressed using a “Given-When-Then” structure. The examples are written using customer language, and are readable by end-users. They are also executable, and act as tests that determine whether the implementation matches the desired behaviour or not. This approach can be effective in building a common understanding of the requirements, but it can also face problems. When the suites of examples grow large, they can be difficult and expensive to change. Duplication can creep in, and can be challenging to detect manually. Current tools for detecting duplication in code are also not effective for BDD examples. Moreover, human concerns of readability and clarity can rise. We present an approach for detecting duplication in BDD suites that is based around dynamic tracing, and describe an evaluation based on three open source systems.}, 
keywords={Tools;Production;Software;Syntactics;Semantics;Cloning;DSL;behaviour-driven development;duplication detection;dynamic tracing}, 
doi={10.1109/VST.2018.8327149}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6976605, 
author={Jiménez, Miguel A. and Gómez, Ángela Villota and Villegas, Norha M. and Tamura, Gabriel and Duchien, Laurence}, 
booktitle={2014 IEEE 8th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems}, 
title={A Framework for Automated and Composable Testing of Component-Based Services}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={The vision of service-oriented computing has been largely developed on the fundamental principle of building systems by composing and orchestrating services in their control flow. Nowadays, software development is notably influenced by service-oriented architectures (SOAs), in which the quality of software systems is determined by the quality of the involved services and their actual composition. Despite the efforts on improving their individual quality, adding or replacing services in an evolving system can introduce failures, thus compromising the satisfaction of the system's functional and extra-functional requirements. These failures erode the trust in the SOA vision. Thus, a key issue for the industrial adoption of SOA is providing service providers, integrators, and consumers the means to build confidence that services behave according to the contracted quality conditions. In this paper we present a first version of PA SCA NI, a framework for specifying and executing test specifications for service-oriented systems. From a test specification, PA SCA NI generates a configuration of testing services compliant with the Service Component Architecture (SCA) specification, which can be composed to integrate different testing strategies, being these tests traceable in an automated way. Our evaluation results show the applicability of the framework and a substantial gain in the tester's effort for developing tests.}, 
keywords={Testing;Service-oriented architecture;Computer architecture;DSL;Software systems;Software service testing;SOA testing;composable tests;SCA testing}, 
doi={10.1109/MESOCA.2014.9}, 
ISSN={2326-6937}, 
month={Sep.},}
@INPROCEEDINGS{8227296, 
author={Tse, Kit Sum and Johnson, Peter C.}, 
booktitle={2017 IEEE Security and Privacy Workshops (SPW)}, 
title={A Framework for Validating Session Protocols}, 
year={2017}, 
volume={}, 
number={}, 
pages={110-119}, 
abstract={Communication protocols are complex, their implementations are difficult, causing many unintended (and severe) vulnerabilities in protocol parsing. While the problem of packet parsing is solved, session parsing remains challenging. Building on existing systems that reliably parse individual messages, we present our four-component framework for implementing protocol session parsers with the goal to improve security of protocol parsing: specification of a protocol message, description of a protocol state machine, testing routines to validate implementations against fake and real data, and graph generation to visualize implementations. This framework enables the creation of a session parser, which validates individual protocol messages in the context of other messages in the same conversation. This is helpful because more secure parsers lead to more secure communication.}, 
keywords={Protocols;Data structures;Security;Testing;DSL;Semantics;Language-theoretic security;protocol state machine;protocol parsing;session parsing}, 
doi={10.1109/SPW.2017.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1689533, 
author={Dae-Woo Kim and Hyun-Min Lim and Sang-Kon Lee}, 
booktitle={2006 IEEE International Symposium on Consumer Electronics}, 
title={A Case Study on Testing and Evaluation in the KT-OSS Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper describes the test and evaluation activities for the development of the KT-OSS (Korea Telecom Operations Support System). In this paper, we show the test and evaluation phases for the development and maintenance of the KT-OSS. To ensure the successful development of the KT-OSS, we performed various tests related to functionality, efficiency and others. We also show the criteria for them and deal with the test organizations and the test-bed for managing and controlling the quality of the KT-OSS. And we describe our experiences in performing these tests. Through these test and evaluation activities, we were able to successfully develop and release the KT-OSS}, 
keywords={Computer aided software engineering;Life testing;Software testing;Programming;System testing;Performance evaluation;Quality management;Telecommunications;Information management;DSL;Test;Evaluation;Operations Support System;Software Development Lifecycle}, 
doi={10.1109/ISCE.2006.1689533}, 
ISSN={2159-1423}, 
month={June},}
@INPROCEEDINGS{1186689, 
author={Brini, S. and Benjelloun, D. and Castanier, F.}, 
booktitle={2003 Design, Automation and Test in Europe Conference and Exhibition}, 
title={A flexible virtual platform for computational and communication architecture exploration of DMT VDSL modems}, 
year={2003}, 
volume={}, 
number={}, 
pages={164-169 suppl.}, 
abstract={In this paper a high-level SoC architecture exploration of DMT (Discrete Multitone) VDSL transceivers (Very high speed Digital Subscriber Line) is presented. A flexible and complete virtual platform was developed for the purpose, exploiting the paradigm of "orthogonalization of concerns" (functionality independent from architecture) in the framework of Cadence VCC system level design tool. An accurate processor model, obtained through the back-annotation of profiling results on a target DSP core, allowed the exploration of different HW/SW partitioning and the study of the computational units required. A transaction-accurate VCC bus model was developed for the investigation of the on-chip bus architecture and its relevant parameters dimensioning.}, 
keywords={Computer architecture;OFDM modulation;Modems;Transceivers;DSL;System-level design;Digital signal processing;Quality of service;Testing;Libraries}, 
doi={10.1109/DATE.2003.1186689}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{8428788, 
author={Bussenot, Robin and Leblanc, Hervé and Percebois, Christian}, 
booktitle={2018 13th Annual Conference on System of Systems Engineering (SoSE)}, 
title={Orchestration of Domain Specific Test Languages with a Behavior Driven Development approach}, 
year={2018}, 
volume={}, 
number={}, 
pages={431-437}, 
abstract={An airplane is composed by many complexes and embedded systems. During the integration testing phase, the design office produces requirements of the targeted system, and the test center produces concrete test procedures to be executed on a test bench. In this context, integration tests are mostly written in natural language and manually executed step by step by a tester. In order to formalize integration tests procedures dedicated to each system with domain specific languages approved by testers, and in order to automatize integration tests, we have introduced agile practices in the integration testing phase. We have chosen a Behavior Driven Development (BDD) approach to orchestrate Domain Specific Test Languages produced for the ACOVAS FUI project.}, 
keywords={Testing;Software;DSL;Natural languages;Hardware;Aerospace electronics;Aircraft}, 
doi={10.1109/SYSOSE.2018.8428788}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4299957, 
author={Kim, Dae-Woo and Lim, Hyun-Min and Lee, Sang-Kon}, 
booktitle={International Conference on Software Engineering Advances (ICSEA 2007)}, 
title={A Case Study on Testing Activites for KT-OSS Maintenance}, 
year={2007}, 
volume={}, 
number={}, 
pages={77-77}, 
abstract={This paper describes the testing activities for the maintenance of the KT-OSS (Korea Telecom Operations Support System). Since the KT-OSS is a large software, it is essential to continuously perform maintenance activities such as the addition of new services from business departments and new functions requested by users and operators, performance improvement of existing functions, correction of the errors found during operation of the system, and so on. To ensure the successful maintenance of the KT-OSS without any effect on the existing functions and performance, we performed various tests related to functionality, efficiency and others before the added and modified parts were applied to the KT-OSS. In this paper, we show the maintenance process, the various tests related to it, the test organization, and the test environment for controlling the quality of the KT-OSS maintenance. Through these testing activities, we were able to successfully maintain the KT-OSS.}, 
keywords={Software maintenance;Error correction;Telecommunications;Preventive maintenance;System testing;Information management;Quality management;DSL;Paper technology;Research and development}, 
doi={10.1109/ICSEA.2007.1}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6210038, 
author={Bianchin, Carlos Gabriel and Demonti, Rogers and de Almeida, André Rubens and da Silva Filho, Matheus T. and da Silva Pinto, Cleverson L.}, 
booktitle={2012 IEEE International Conference on Industrial Technology}, 
title={Development of static switch with high speed algorithm to fault detection}, 
year={2012}, 
volume={}, 
number={}, 
pages={808-814}, 
abstract={This paper presents the updated results on research for development of a static switch for operation in medium voltage. It shows the results of the algorithm that detects an outage and enables the digital processing to control the static switch. This algorithm was based on the Clarke and Park transforms. The operation of the switch consists of transferring the power supply from the main source, where an outage occurred, to an alternative power source (backup). The load stays off for a time of milliseconds. A low voltage prototype was built, to allow the evaluation of the algorithm. Then a prototype of the static switch was built in voltage of 13,8kV. The first tests - feeding resistive load - are presented in voltage of 13.8 kV.}, 
keywords={DSL;Switches;algorithm;digital processing;series-connected thyristors;medium voltage}, 
doi={10.1109/ICIT.2012.6210038}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{9576969, 
author={Ngwenya, Sikhumbuzo and Shebeshi, Zelalem and Terzoli, Alfredo}, 
booktitle={2021 IST-Africa Conference (IST-Africa)}, 
title={Implementing a Content-Based Routing Framework for Application Integration on to Teleweaver Application Server}, 
year={2021}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper presents an architectural overview of content-based dynamic routing for integrating applications on to an application server named TeleWeaver, a middleware platform developed within Siyakhula Living Lab (SLL). SLL is an ICT4D project in the Eastern Cape Province of South Africa. TeleWeaver was created as a mediation layer between software systems developed for use by beneficiaries of the Siyakhula Living Lab. The main challenge with these disparate systems was that they had unnecessary, redundant components; TeleWeaver acts as a common platform that suits the development of many services such as eGovernment, eHealth, and eJudiciary.}, 
keywords={Java;XML;Routing;Software systems;Routing protocols;Electronic healthcare;DSL;Application Integration;Dynamic Routing;Teleweaver.}, 
doi={}, 
ISSN={2576-8581}, 
month={May},}
@INPROCEEDINGS{7073229, 
author={Mahmoudi, Charif and Mourlin, Fabrice}, 
booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
title={Business Process Management with mobile routes}, 
year={2014}, 
volume={}, 
number={}, 
pages={420-427}, 
abstract={Business processes are milestone of the information system of any companies. Their availability is a crucial aspect. We provide a solution for the high level of availability of business processes by the use of cluster of enterprise service buses (ESB). Our approach is based on the dynamic creation of the route between the business services and the migration of a runtime context from one ESB to another one. So, we insure the management of business processes over a cluster and measure the impact of such incident. Through the use of log, we also report these events which allow the administrator for preparing updates of the information system. With the use of open source software, we guarantee the reuse of our case study with other kinds of enterprise service bus, which respect open standard exchanges like XML language and REST API.}, 
keywords={Business;Containers;Context;DSL;XML;Servers;Routing;SOA architecture;orchestration;cluster of bus;message routing;web service}, 
doi={10.1109/AICCSA.2014.7073229}, 
ISSN={2161-5330}, 
month={Nov},}
@INPROCEEDINGS{5959781, 
author={Miksovic, Christoph and Zimmermann, Olaf}, 
booktitle={2011 Ninth Working IEEE/IFIP Conference on Software Architecture}, 
title={Architecturally Significant Requirements, Reference Architecture, and Metamodel for Knowledge Management in Information Technology Services}, 
year={2011}, 
volume={}, 
number={}, 
pages={270-279}, 
abstract={Capturing and sharing design knowledge such as architectural decisions is becoming increasingly important in professional Information Technology (IT) services firms. Methods, models, and tools supporting explicit knowledge management strategies have been proposed in recent years. In this paper, we extend previous work in the architectural knowledge management community to satisfy the requirements of an additional user group: the designers of IT infrastructure solutions that are outsourced from one company to another. Such strategic outsourcing solutions require complex, contractually relevant design decisions concerning many different resources such as IT infrastructures, people, and real estate. In this paper, we present a reference architecture and a decision process-oriented knowledge metamodel that we synthesized from the domain-specific functional requirements and quality attributes. We also present a tool implementation of these decision modeling concepts and discuss user feedback.}, 
keywords={Proposals;Knowledge engineering;Knowledge based systems;Computer architecture;Communities;Engines;DSL;knowledge management;outsourcing;workflow}, 
doi={10.1109/WICSA.2011.43}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7102616, 
author={Haser, Florian and Breu, Ruth}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Non-Intrusive Documentation-Driven Integration Testing}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-2}, 
abstract={Powerful development frameworks and adoption of agile development methods are continuously increasing release frequency, thus compress test cycles. Test automation, often relying on model based approaches, helps to reduce test time, however the introduction of related heavy weight processes is often quite challenging. In order to tackle this problem, we propose a bottom up testing approach, which in a nutshell, in the initial phase supports the integration tester in creating a semi-formal test case description and report. The approach, a textual domain specific framework, will guide the test expert in evolving the base language, in order to be tailored for a domain language of an organization. The evolved language can be linked to executable code, which enables in the long run (semi-)automated model based regression testing.}, 
keywords={Testing;Automation;Context;Unified modeling language;Business;Writing;Measurement}, 
doi={10.1109/ICST.2015.7102616}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{9105761, 
author={Iqbal, Hena}, 
booktitle={2019 International Conference on Digitization (ICD)}, 
title={Notice of Violation of IEEE Publication Principles: Mobile Application Development: Automated Test Input Generation Via Model Inference based on User Story and Acceptance Criteria}, 
year={2019}, 
volume={}, 
number={}, 
pages={92-103}, 
abstract={In the past few years, there has been observed explosive growth in the development of Mobile Applications across Android and iOS operating system which has led to the direct impact towards mobile app development. In order to design and propose quality-oriented apps, it is the primary responsibility of the developers to devote time and sufficient efforts towards testing to make the Apps bug free and operational in the hands of end users without any hiccup. In order to test the mobile apps, manual testing procedures takes prolonged amount of time in writing test cases and even the full testing requirements are not met. In addition to this, lack of sufficient knowledge by the tester also impacts overall quality and assurance that app is bug free. To overcome all the issues of testing, and to assure that apps designed by developers are almost bug free, we propose a new testing methodology cum tool “AgileUATM” which works primarily towards white-box and black-box testing. With this tool, all the test cases are generated automatically based on user stories and acceptance criteria by using formal specification and Z3 SMT solvers. To test the validity of the proposed tool, we applied the tool in real-time operational environment with regard to test Mobile apps. Using this tool, all the acceptance criteria is determined via user stories. The testers/developers specify requirements with formal specifications based on programs properties, predicates, invariants, and constraints. From the results, it is observed that the proposed tool i.e. AgileUATM generated effective and accurate test cases, test input, and expected output was generated in a unified fashion from the user stories to meet acceptance criteria. In addition to this, the tool also reduced the development time to identify test data as compared to manual Behavior Driven Development (BDD) methodologies. With this tool, the developers got better idea with regard to required tests and able to translate the customers natural languages to the computer language as well.}, 
keywords={}, 
doi={10.1109/ICD47981.2019.9105761}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{9282667, 
author={Makedonski, Philip and Gheorghe-Pop, Ilie-Daniel and Rennoch, Axel and Kristoffersen, Finn and Pintar, Boštjan and Ulrich, Andreas}, 
booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={Using TDL for Standardised Test Purpose Definitions}, 
year={2020}, 
volume={}, 
number={}, 
pages={514-521}, 
abstract={This article reports on experiences from the use of the ETSI Test Description Language (TDL) and its extension for structured test objective specification (TDL-TO) for the definition of functional and non-functional test purposes in the Internet of Things (IoT) domain. The experiences are based on results from different working groups at ETSI TC MTS and the ETSI Specialist Task Force (STF) 574, focusing on the definition of test purposes for functional, security, and performance testing of the CoAP and MQTT protocols as well as VxLTeinteroperability testing.}, 
keywords={Protocols;Software quality;Software reliability;Security;Internet of Things;Task analysis;Testing;Test description;test purposes;security;performance;interoperability;CoAP;MQTT;IoT;VxLTE}, 
doi={10.1109/QRS-C51114.2020.00091}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6737898, 
author={Parizad, A.}, 
booktitle={2013 13th International Conference on Environment and Electrical Engineering (EEEIC)}, 
title={Dynamic stability Analysis for damavand power plant considering PMS functions by DIGSILENT software}, 
year={2013}, 
volume={}, 
number={}, 
pages={145-155}, 
abstract={For a number of years Power Management System (PMS) have been used to control islanded power systems. It can control system frequency and voltage as well as generated real and reactive power. The PMS would often have a load shedding facility to prevent the cascade failure of the generation system. The investigation is based on dynamics time domain simulations by DigSilent software and aims to define the main functions for the power management system (PMS) that will govern the operation of the power plant. In this paper, DigSilent Simulation Language (DSL) is used for simulation of Turbine-Governor and excitation system. Also DigSilent Program Language (DPL) is used to simulate action of PMS when it is required. Also, different dynamic contingencies related to Damavand power plant such as Trip of grid connection, Trip of One GT, Trip of load, Trip of Two GTs, Trip of Grid connection (According to governor logic, Automatic change mode acts, Solving frequency deviation by PMS) are considered. In each scenario, power plant stability is investigated and where required, PMS functions (such as load shedding and sending appropriate set points) are applied.}, 
keywords={Power system stability;Turbines;Generators;Load modeling;Velocity control;Power system dynamics;power system stability;power management system (PMS);Digsilent Software;Turbine-Governor model}, 
doi={10.1109/EEEIC-2.2013.6737898}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8821229, 
author={Purushothaman, Sarath Kumar and Kashyap, Nishant and HA, Divya and Agarwal, Sneha and Iqbal, Sandal and Behere, Vaishali}, 
booktitle={2018 International Conference on Circuits and Systems in Digital Enterprise Technology (ICCSDET)}, 
title={Unified Approach Towards Automation of any Desktop Web, Mobile Web, Android, iOS, REST and SOAP API Use Cases}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In today's world, testing cannot be efficient without automation. We need automation for avoiding repetitive work, making sure that the time from development to deployment is reduced with good quality. We just don't need automation tests, rather tests which are reliable, robust, easy to code, debug, scale and can run in parallel on distributed environment.The paper explores the challenges that can be faced while automating web, mobile web, android, ios, api tests via a common framework such as, a common way of interaction with web and mobile apps, automatically identifying connected mobile devices and run the parallel test instances, writing hybrid tests; a combination of web and api use cases, not tightly coupled with a specific application rather can be used for automating any web, mobile based application.This paper further discusses the approach to bring efficient, generic and re-usable solution for these challenges while ensuring that the users write reliable, robust and effective tests. The approach followed here spans across following areas:- Webdriver management and parallel run. DSL layer on top of the existing api libraries for easy to use interface. Reporting and logging mechanism. Utilities and integration with third parties (like Slack, Jira, Bitbucket, Jenkins, Device Farms, etc )The benefit of this framework is, it is application agnostic capability, which helps to use the same framework against any web applications (web, mobile) for test automation.}, 
keywords={Testing;Libraries;Automation;Browsers;XML;Java;Simple object access protocol;api;rest;soap;webservice;automation;mobile;web;ios;android}, 
doi={10.1109/ICCSDET.2018.8821229}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7323104, 
author={Sanz, Concepción and Salas, Alejandro and de Miguel, Miguel and Alonso, Alejandro and de la Puente, Juan Antonio and Benac, Clara}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Automated model-based testing based on an agnostic-platform modeling language}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Currently multiple Domain Specific Languages (DSLs) are used for model-driven software development, in some specific domains. Software development methods, such as agile development, are test-centered, and their application in model-based frameworks requires model support for test development. We introduce a specific language to define generic test models, which can be automatically transformed into executable tests for particular testing platforms. The resulting test models represent the test plan for applications also built according to a model-based approach. The approach presented here includes some customisations for the application of the developed languages and transformation tools for some specific testing platforms. These languages and tools have been integrated with some specific DSL designed for software development.}, 
keywords={Testing;Unified modeling language;Software;Architecture;Computer architecture;Complexity theory;Engines;Model-based Testing;Automated Testing;Agile Development}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8004372, 
author={Ahmad, Amro Al-Said and Brereton, Pearl and Andras, Peter}, 
booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={A Systematic Mapping Study of Empirical Studies on Software Cloud Testing Methods}, 
year={2017}, 
volume={}, 
number={}, 
pages={555-562}, 
abstract={Context: Software has become more complicated, dynamic, and asynchronous than ever, making testing more challenging. With the increasing interest in the development of cloud computing, and increasing demand for cloud-based services, it has become essential to systematically review the research in the area of software testing in the context of cloud environments. Objective: The purpose of this systematic mapping study is to provide an overview of the empirical research in the area of software cloud-based testing, in order to build a classification scheme. We investigate functional and non-functional testing methods, the application of these methods, and the purpose of testing using these methods. Method: We searched for electronically available papers in order to find relevant literature and to extract and analyze data about the methods used. Result: We identified 69 primary studies reported in 75 research papers published in academic journals, conferences, and edited books. Conclusion: We found that only a minority of the studies combine rigorous statistical analysis with quantitative results. The majority of the considered studies present early results, using a single experiment to evaluate their proposed solution.}, 
keywords={Cloud computing;Security;Software testing;Reliability;Systematics;systematic mapping study;cloud software testing methods;software testing;empirical studies}, 
doi={10.1109/QRS-C.2017.94}, 
ISSN={}, 
month={July},}
@ARTICLE{4749742, 
author={Scully, Padraig and Skehill, Ronan and McGrath, Sean}, 
journal={IEEE Wireless Communications}, 
title={Mobility in an RF-isolated test platform}, 
year={2008}, 
volume={15}, 
number={6}, 
pages={8-15}, 
abstract={Understanding the practical impact of mobility in a wireless network is essential for the wireless networks of tomorrow. Mobility influences the network performance, behavior, and ability to provide seamless service across a wide area. Testing and evaluating the real impact of mobility is difficult in the field with so many variables such as interference, fading, and so on. Experimental wireless evaluation in a test environment must correspond to an actual deployment. Furthermore, it is important to achieve repeatability without sacrificing realism. This study presents a functional test platform that uses real IEEE 802.11 equipment, providing repeatability and reliability. The platform is used to test the practical impact of device movement in a WLAN cell while voice and data applications are running. The mobility characteristics of wireless devices are based on individual models used by researchers with the addition of real aspects of mobility from empirical studies to improve realism.}, 
keywords={Testing;Fading;Wireless networks;Wireless LAN;Radio transmitters;Interference;GSM;3G mobile communication;Receivers;Wireless communication}, 
doi={10.1109/MWC.2008.4749742}, 
ISSN={1558-0687}, 
month={December},}
@INPROCEEDINGS{7785748, 
author={Carter, J. and Gardner, W. B.}, 
booktitle={2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)}, 
title={BHive: Towards Behaviour-Driven Development Supported by B-Method}, 
year={2016}, 
volume={}, 
number={}, 
pages={249-256}, 
abstract={Behaviour-Driven Development (BDD) is an "outside-in" approach to software development built upon semi-formal mediums for specifying the behaviour of a system as it would be observed externally. Through the representation of a system as a collection of user stories and scenarios using BDD's notation, practitioners automate acceptance tests using examples of desired behaviour for the envisioned system. A formal model created in concert with BDD tests would provide valuable insight into test validity and enhance the visibility of the problem domain. This work called BHive builds upon the formal underpinnings of BDD scenarios by mapping their "Given," "When," and "Then" statements to "Precondition," "Command," and "Postcondition" constructs as introduced by Floyd-Hoare logic. We posit that this mapping allows for a B-Method representation to be created and that such a model is useful for exploring system behaviour and exposing gaps in requirements. We also outline extensions to BDD tooling required for the described integration and present benefits of the BHive approach to integrating formalism within a BDD project.}, 
keywords={Software;Testing;Stakeholders;Shape;Documentation;Conferences;BDD;Behaviour-Driven Development;B-Method;Agile}, 
doi={10.1109/IRI.2016.39}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7140371, 
author={Gebert, Steffen and Schwartz, Christian and Zinner, Thomas and Tran-Gia, Phuoc}, 
booktitle={2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
title={Continuously delivering your network}, 
year={2015}, 
volume={}, 
number={}, 
pages={766-769}, 
abstract={Softwarization and cloudification of networks through software defined networking and network functions virtualisation promise a new degree of flexibility and agility. By moving logic from device firmware into software applications and applying software development mechanisms, innovation can be introduced with less effort. Concrete ways how to operate and orchestrate such systems are not yet defined. The process of making changes to a controller software or a virtualized network function in a production network without the risk of network disruption is not covered by literature. Complexity of systems brings the risk of unexpected side-effects and has so long been a show-stopper for administrators applying changes to networking devices. This paper suggests the adaption of the successful concept of continuous delivery into the software defined networking world. Test-driven development and automatic acceptance tests demonstrate that the software engineering community already found ways to ensure that changes do not break. Applied to network engineering, the adaption of continuous delivery can be seen as an enabler for risk-free and frequent changes in production infrastructure through push button deployments.}, 
keywords={Software;Pipelines;Production;Servers;Measurement;Technological innovation}, 
doi={10.1109/INM.2015.7140371}, 
ISSN={1573-0077}, 
month={May},}
@INPROCEEDINGS{7018484, 
author={Hois, Bernhard and Sobernig, Stefan and Strembeck, Mark}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Natural-language scenario descriptions for testing core language models of domain-specific languages}, 
year={2014}, 
volume={}, 
number={}, 
pages={356-367}, 
abstract={The core language model is a central artifact of domain-specific modeling languages (DSMLs) as it captures all relevant domain abstractions and their relations. Natural-language scenarios are a means to capture requirements in a way that can be understood by technical as well as non-technical stakeholders. In this paper, we use scenarios for the testing of structural properties of DSML core language models. In our approach, domain experts and DSML engineers specify requirements via structured natural-language scenarios. These scenario descriptions are then automatically transformed into executable test scenarios providing forward and backward traceability of domain requirements. To demonstrate the feasibility of our approach, we used Eclipse Xtext to implement a requirements language for the definition of semi-structured scenarios. Transformation specifications generate executable test scenarios that run in our test platform which is built on the Eclipse Modeling Framework and the Epsilon language family.}, 
keywords={Testing;Unified modeling language;Vocabulary;Natural languages;Collaboration;Abstracts;Syntactics;Domain-Specific Modeling;Natural-Language Requirement;Scenario-based Testing;Metamodel Testing;Eclipse Modeling Framework;Xtext;Epsilon;EUnit}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{8632429, 
author={Warnke, Tom and Uhrmacher, Adelinde M.}, 
booktitle={2018 Winter Simulation Conference (WSC)}, 
title={COMPLEX SIMULATION EXPERIMENTS MADE EASY}, 
year={2018}, 
volume={}, 
number={}, 
pages={410-424}, 
abstract={Diverse methods for complex simulation experiments can contribute to developing and gaining insights into simulation models, for example simulation-based optimization, sensitivity analysis, or statistical model-checking. An effective tool for conducting simulation experiments must be highly flexible to support such a broad range of experimental methods. Furthermore, to facilitate reproducibility and communication of simulation experiments, an effective tool for simulation experimentation must yield experiment descriptions that are easily portable, executable, and human-readable. In this tutorial we introduce SESSL, a domain-specific language for setting up simulation experiments. SESSL is flexible and extensible, and experiment descriptions are executable, often succinct, and can be executed reproducibly across machines and operating systems. Based on a few examples, we demonstrate how SESSL can be leveraged to easily conduct complex simulation experiments while reusing existing software and methods, and how SESSL's capabilities can be extended and combined with arbitrary simulation software via bindings.}, 
keywords={Analytical models;Software;Tools;Object oriented modeling;Computational modeling;Optimization;Sensitivity analysis}, 
doi={10.1109/WSC.2018.8632429}, 
ISSN={1558-4305}, 
month={Dec},}
@INPROCEEDINGS{7928008, 
author={Ukai, Hiroshi and Qu, Xiao}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Test Design as Code: JCUnit}, 
year={2017}, 
volume={}, 
number={}, 
pages={508-515}, 
abstract={In a development process where testing is highly automated, there is a major challenge to cope with issues such as huge test size and test stability. In this paper, we propose a model-based testing (MBT) tool called JCUnit, which generates a test suite from a model given as a Java class. Unlike other tools, it is designed to generate small and stable test suites and supports various popular models. With this tool, developers can apply MBT approach to their products without learning domain-specific language of proprietary MBT tools. Moreover, features such as portability and pluggability make it useful in a wide range of phases from unit testing to system testing. As a result, the efforts required in practical software testing will be reduced.}, 
keywords={Tools;Java;Software;Graphical user interfaces;Pipelines;Software testing;automated testing;FSM spec;model-based testing;combinatorial interaction testing}, 
doi={10.1109/ICST.2017.58}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{9609160, 
author={Kirinuki, Hiroyuki and Matsumoto, Shinsuke and Higo, Yoshiki and Kusumoto, Shinji}, 
booktitle={2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={NLP-assisted Web Element Identification Toward Script-free Testing}, 
year={2021}, 
volume={}, 
number={}, 
pages={639-643}, 
abstract={End-to-end test automation is important in modern web application development. However, existing test automation techniques have challenges in implementing and maintaining test scripts. It is difficult to keep correct locators, which test scripts require to identify web elements on web pages. The reason is that locators depend on the metadata in web elements or the structure of each web page. One efficient way to solve the problem of locators is to make test cases written in natural language executable without test scripts. As the first step of script-free testing, we propose a technique to identify web elements to be operated and to determine test procedures by interpreting test cases. The test cases are written in a domain-specific language without relying on the metadata of web elements or the structural information of web pages. We leverage natural language processing techniques to understand the semantics of web elements. We also create heuristic search algorithms to find promising test procedures. To evaluate our proposed technique, we applied it to two open-source web applications. The experimental results show that our technique successfully identified 94% of web elements to be operated in the test cases.}, 
keywords={Software maintenance;Automation;Heuristic algorithms;Semantics;Web pages;Metadata;Natural language processing;Script-free Testing;Web Testing;Locator}, 
doi={10.1109/ICSME52107.2021.00072}, 
ISSN={2576-3148}, 
month={Sep.},}
@INPROCEEDINGS{9825814, 
author={Kirinuki, Hiroyuki and Matsumoto, Shinsuke and Higo, Yoshiki and Kusumoto, Shinji}, 
booktitle={2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Web Element Identification by Combining NLP and Heuristic Search for Web Testing}, 
year={2022}, 
volume={}, 
number={}, 
pages={1055-1065}, 
abstract={End-to-end test automation is critical in modern web application development. However, test automation techniques used in industry face challenges in implementing and maintaining test scripts. It is difficult to determine and maintain the locators needed by test scripts to identify web elements on web pages. The reason is that locators depend on the metadata of web elements and the structure of each web page. One effective way to solve such a problem of locators is to allow test cases written in natural language to be executed without test scripts. In this study, we propose a technique to identify web elements that should be operated on a web page by interpreting natural-language-like test cases. The test cases are written in a domain-specific language that independents on the metadata of web elements and the structural information of web pages. We leverage natural language processing techniques to understand the semantics of web elements. We also create heuristic search algorithms to explore web pages and find promising test procedures. To evaluate the proposed technique, we applied it to test cases for two open-source web applications. The experimental results show that our technique was able to successfully identify about 94% of web elements to be operated in the test cases. Our approach also succeeded in identifying all the web elements that were operated in 68% of the test cases.}, 
keywords={Industries;Automation;Heuristic algorithms;Semantics;Web pages;Metadata;Natural language processing;test automation;test script;NLP;web testing}, 
doi={10.1109/SANER53432.2022.00123}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7886903, 
author={Menendez, David and Nagarakatte, Santosh}, 
booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)}, 
title={Termination-Checking for LLVM Peephole Optimizations}, 
year={2016}, 
volume={}, 
number={}, 
pages={191-202}, 
abstract={Mainstream compilers contain a large number of peephole optimizations, which perform algebraic simplification of the input program with local rewriting of the code. These optimizations are a persistent source of bugs. Our recent research on Alive, a domain-specific language for expressing peephole optimizations in LLVM, addresses a part of the problem by automatically verifying the correctness of these optimizations and generating C++ code for use with LLVM. This paper identifies a class of non-termination bugs that arise when a suite of peephole optimizations is executed until a fixed point. An optimization can undo the effect of another optimization in the suite, which results in non-terminating compilation. This paper (1) proposes a methodology to detect non-termination bugs with a suite of peephole optimizations, (2) identifies the necessary condition to ensure termination while composing peephole optimizations, and (3) provides debugging support by generating concrete input programs that cause non-terminating compilation. We have discovered 184 optimization sequences, involving 38 optimizations, that cause non-terminating compilation in LLVM with Alive-generated C++ code.}, 
keywords={Optimization;Computer bugs;C++ languages;Semantics;Concrete;Toxicology;Pattern matching;Compiler Verification;Peephole Optimization;Alive;Termination}, 
doi={10.1145/2884781.2884809}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{8648600, 
author={Koehler, Wolfgang and Jing, Yanguo}, 
booktitle={2018 11th International Conference on Developments in eSystems Engineering (DeSE)}, 
title={A Novel Block-Based Programming Framework for Non-programmers to Validate PLC Based Machine Tools for Automotive Manufacturing Facilities}, 
year={2018}, 
volume={}, 
number={}, 
pages={202-207}, 
abstract={An automotive manufacturing cell typically consists of multiple stations, controlled by a single industrial programmable controller. Design flaws or assembly mistakes are normally discovered during the highly time-constrained integration phase, which leads to time loss and inefficiency. This paper presents a novel domain-specific 'language' to eliminate the PLC experts from the testing process, to minimize input from operators and to reduce cost significantly. The proposed 'language' was inspired by widely available educational robotic toys, built on a block based programming environment, which allows for intuitive interaction with novice users. A comparison and evaluation study has been carried out to compare the new framework to the traditional process of building equipment for an automotive manufacturing cell. The study has shown that the proposed 'language' not only eliminates the need for PLC experts, in the testing process, but also reduces the time needed for setup and testing by 90%. In addition, the high level of abstraction decreased the potential for programming errors by 95%.}, 
keywords={Programming;Testing;Solenoids;XML;Machine tools;Automotive engineering;Sensors}, 
doi={10.1109/DeSE.2018.00046}, 
ISSN={2161-1351}, 
month={Sep.},}
@INPROCEEDINGS{5190273, 
author={Wang, Xinchun and Xu, Peijie}, 
booktitle={2009 International Conference on Information Technology and Computer Science}, 
title={Build an Auto Testing Framework Based on Selenium and FitNesse}, 
year={2009}, 
volume={2}, 
number={}, 
pages={436-439}, 
abstract={Writing auto testing is a required engineering technique that can save time and money, and help businesses better respond to changes. But if we use testing framework improperly, more problems would possibly be caused. An auto testing framework based on Selenium and FitNesse is discussed in this article which can help with those problems. The framework use Selenium APIs to get page value, DbFit to init database, FitNesse to manage the test fixture, and a special DSL to write test fixture. It could greatly reduce the line numbers of testing code and the project developing period, lower the random error rate, facilitate writing fixture table, improve the coding productivity, and the quality of final product.}, 
keywords={Software testing;Fixtures;Java;Automatic testing;Information security;Writing;Open source software;Programming;Information technology;Computer science;Auto Testing Framework;Selenium;FitNesse}, 
doi={10.1109/ITCS.2009.228}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{5525697, 
author={Blondel, Enrico and Monney, Claude}, 
booktitle={Intelec 2010}, 
title={Efficient powering of communication and IT equipments using rotating UPS}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Today, the demand for uninterrupted power supply for Telecommunication and Internet services increases drastically. Following the same trend, cooling demand explodes. Supplying sufficient power with extremely high reliability becomes even more challenging. The era of telephone exchanges using 48V power supply and 8 hours of battery backup is past. Telecom Operators are currently migrating from POTS to all IP. In addition, broadband Internet access for everyone using DSL or fibre is already reality. More and more applications like TV on demand, streaming services, online gaming or entertainment are very power hungry. For such large systems, static UPS systems are no more efficient. A better alternative is offered by rotating UPS, also known as "No-break". In addition to requiring less space and no batteries, these systems have a better power efficiency too.}, 
keywords={Uninterruptible power systems;Batteries;Alternators;Energy storage;Power quality;Power supplies;Cooling;Ventilation;Inductors;Acoustic testing}, 
doi={10.1109/INTLEC.2010.5525697}, 
ISSN={2158-5210}, 
month={June},}
@INPROCEEDINGS{5758967, 
author={Blondel, E. and Monney, C.}, 
booktitle={4th International Telecommunication - Energy special conference}, 
title={Efficient powering of communication and IT equipments using rotating UPS}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Today, the demand for uninterrupted power supply for Telecommunication and Internet services increases drastically. Following the same trend, cooling demand explodes. Supplying sufficient power with extremely high reliability becomes even more challenging. The era of telephone exchanges using 48V power supply and 8 hours of battery backup is past. Telecom Operators are currently migrating from POTS to all IP. In addition, broadband Internet access for everyone using DSL or fibre is already reality. More and more applications like TV on demand, streaming services, online gaming or entertainment are very power hungry. For such large systems, static UPS systems are no more efficient. A better alternative is offered by rotating UPS, also known as "No-break". In addition to requiring less space and no batteries, these systems have a better power efficiency too.}, 
keywords={Uninterruptible power systems;Electromagnetic compatibility;Batteries;Generators;Inductors;Copper}, 
doi={}, 
ISSN={}, 
month={May},}
@ARTICLE{6834762, 
author={Xia, Wenfeng and Wen, Yonggang and Foh, Chuan Heng and Niyato, Dusit and Xie, Haiyong}, 
journal={IEEE Communications Surveys & Tutorials}, 
title={A Survey on Software-Defined Networking}, 
year={2015}, 
volume={17}, 
number={1}, 
pages={27-51}, 
abstract={Emerging mega-trends (e.g., mobile, social, cloud, and big data) in information and communication technologies (ICT) are commanding new challenges to future Internet, for which ubiquitous accessibility, high bandwidth, and dynamic management are crucial. However, traditional approaches based on manual configuration of proprietary devices are cumbersome and error-prone, and they cannot fully utilize the capability of physical network infrastructure. Recently, software-defined networking (SDN) has been touted as one of the most promising solutions for future Internet. SDN is characterized by its two distinguished features, including decoupling the control plane from the data plane and providing programmability for network application development. As a result, SDN is positioned to provide more efficient configuration, better performance, and higher flexibility to accommodate innovative network designs. This paper surveys latest developments in this active research area of SDN. We first present a generally accepted definition for SDN with the aforementioned two characteristic features and potential benefits of SDN. We then dwell on its three-layer architecture, including an infrastructure layer, a control layer, and an application layer, and substantiate each layer with existing research efforts and its related research areas. We follow that with an overview of the de facto SDN implementation (i.e., OpenFlow). Finally, we conclude this survey paper with some suggested open research challenges.}, 
keywords={Optical switches;Routing;Software;Computer architecture;Complexity theory;Software-defined networking;SDN;network virtualization;OpenFlow}, 
doi={10.1109/COMST.2014.2330903}, 
ISSN={1553-877X}, 
month={Firstquarter},}
@INPROCEEDINGS{4221614, 
author={Bertolino, Antonia}, 
booktitle={Future of Software Engineering (FOSE '07)}, 
title={Software Testing Research: Achievements, Challenges, Dreams}, 
year={2007}, 
volume={}, 
number={}, 
pages={85-103}, 
abstract={Software engineering comprehends several disciplines devoted to prevent and remedy malfunctions and to warrant adequate behaviour. Testing, the subject of this paper, is a widespread validation approach in industry, but it is still largely ad hoc, expensive, and unpredictably effective. Indeed, software testing is a broad term encompassing a variety of activities along the development cycle and beyond, aimed at different goals. Hence, software testing research faces a collection of challenges. A consistent roadmap of the most relevant challenges to be addressed is here proposed. In it, the starting point is constituted by some important past achievements, while the destination consists of four identified goals to which research ultimately tends, but which remain as unreachable as dreams. The routes from the achievements to the dreams are paved by the outstanding research challenges, which are discussed in the paper along with interesting ongoing work.}, 
keywords={Software testing;Software engineering;Laboratories;Software systems;Software quality;Councils;Computer industry;Quality assurance;Feedback;State estimation}, 
doi={10.1109/FOSE.2007.25}, 
ISSN={}, 
month={May},}
@ARTICLE{5388069, 
author={Walston, C. E. and Felix, C. P.}, 
journal={IBM Systems Journal}, 
title={A method of programming measurement and estimation}, 
year={1977}, 
volume={16}, 
number={1}, 
pages={54-73}, 
abstract={The present approach to productivity estimation, although useful, is far from being optimized. Based on the results of the variable analysis described in this paper, and supplemented by the results of the continued investigation of additional variables related to productivity, an experimental regression model has been developed. Preliminary results indicate that the model reduces the scatter. Further work is being done to determine the potential of regression as an estimating tool, as well as to extend the analyses of the areas of computer usage, documentation volume, duration, and staffing.}, 
keywords={}, 
doi={10.1147/sj.161.0054}, 
ISSN={0018-8670}, 
month={},}
@ARTICLE{6805151, 
author={Jarraya, Yosr and Madi, Taous and Debbabi, Mourad}, 
journal={IEEE Communications Surveys & Tutorials}, 
title={A Survey and a Layered Taxonomy of Software-Defined Networking}, 
year={2014}, 
volume={16}, 
number={4}, 
pages={1955-1980}, 
abstract={Software-defined networking (SDN) has recently gained unprecedented attention from industry and research communities, and it seems unlikely that this will be attenuated in the near future. The ideas brought by SDN, although often described as a “revolutionary paradigm shift” in networking, are not completely new since they have their foundations in programmable networks and control-data plane separation projects. SDN promises simplified network management by enabling network automation, fostering innovation through programmability, and decreasing CAPEX and OPEX by reducing costs and power consumption. In this paper, we aim at analyzing and categorizing a number of relevant research works toward realizing SDN promises. We first provide an overview on SDN roots and then describe the architecture underlying SDN and its main components. Thereafter, we present existing SDN-related taxonomies and propose a taxonomy that classifies the reviewed research works and brings relevant research directions into focus. We dedicate the second part of this paper to studying and comparing the current SDN-related research initiatives and describe the main issues that may arise due to the adoption of SDN. Furthermore, we review several domains where the use of SDN shows promising results. We also summarize some foreseeable future research challenges.}, 
keywords={Control systems;Taxonomy;Software defined networking;Ports (Computers);Network operating systems;Software-defined networking;OpenFlow;programmable networks;controller;management;virtualization;flow}, 
doi={10.1109/COMST.2014.2320094}, 
ISSN={1553-877X}, 
month={Fourthquarter},}
@ARTICLE{6179576, 
author={Katsigiannis, Yiannis A. and Georgilakis, Pavlos S. and Karapidakis, Emmanuel S.}, 
journal={IEEE Transactions on Sustainable Energy}, 
title={Hybrid Simulated Annealing–Tabu Search Method for Optimal Sizing of Autonomous Power Systems With Renewables}, 
year={2012}, 
volume={3}, 
number={3}, 
pages={330-338}, 
abstract={Small autonomous power systems (SAPS) that include renewable energy sources are a promising option for isolated power generation at remote locations. The optimal sizing problem of SAPS is a challenging combinatorial optimization problem, and its solution may prove a very time-consuming process. This paper initially investigates the performance of two popular metaheuristic methods, namely, simulated annealing (SA) and tabu search (TS), for the solution of SAPS optimal sizing problem. Moreover, this paper proposes a hybrid SA-TS method that combines the advantages of each one of the above-mentioned metaheuristic methods. The proposed method has been successfully applied to design an SAPS in Chania region, Greece. In the study, the objective function is the minimization of SAPS cost of energy (€/kWh), and the design variables are: 1) wind turbines size, 2) photovoltaics size, 3) diesel generator size, 4) biodiesel generator size, 5) fuel cells size, 6) batteries size, 7) converter size, and 8) dispatch strategy. The performance of the proposed hybrid optimization methodology is studied for a large number of alternative scenarios via sensitivity analysis, and the conclusion is that the proposed hybrid SA-TS improves the obtained solutions, in terms of quality and convergence, compared to the solutions provided by individual SA or individual TS methods.}, 
keywords={Generators;Batteries;Fuels;Power systems;Simulated annealing;Renewable energy resources;Hybrid power systems;optimal sizing;optimization methods;power generation dispatch;renewable energy sources;simulated annealing (SA);small autonomous power systems (SAPS);solar energy;tabu search (TS);wind energy}, 
doi={10.1109/TSTE.2012.2184840}, 
ISSN={1949-3037}, 
month={July},}
@INPROCEEDINGS{1498464, 
author={Ott, J. and Kutscher, D.}, 
booktitle={Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Communications Societies.}, 
title={A disconnection-tolerant transport for drive-thru Internet environments}, 
year={2005}, 
volume={3}, 
number={}, 
pages={1849-1862 vol. 3}, 
abstract={Today's mobile, wireless, and ad-hoc communications often exhibit extreme characteristics challenging assumptions underlying the traditional way of end-to-end communication protocol design in the Internet. One specific scenario is Internet access from moving vehicles on the road as we are researching in the drive-thru Internet project. Using wireless LAN as a broadly available access technology leads to intermittent - largely unpredictable and usually short-lived - connectivity, yet providing high performance while available. To allow Internet applications to deal reasonably well with such intermittent connectivity patterns, we have introduced a supportive drive-thru architecture. A key component is a "session" protocol offering persistent end-to-end communications even in the presence of interruptions. In this paper, we present the design of the persistent connectivity management protocol (PCMP) and report on findings from our implementation.}, 
keywords={Internet;Access protocols;Mobile communication;Transport protocols;Road vehicles;Wireless LAN;Wireless networks;Performance loss;Wireless application protocol;Vehicle driving}, 
doi={10.1109/INFCOM.2005.1498464}, 
ISSN={0743-166X}, 
month={March},}
@ARTICLE{4814954, 
author={Liggesmeyer, Peter and Trapp, Mario}, 
journal={IEEE Software}, 
title={Trends in Embedded Software Engineering}, 
year={2009}, 
volume={26}, 
number={3}, 
pages={19-25}, 
abstract={Software's importance in the development of embedded systems has been growing rapidly over the last 20 years. Because of current embedded systems' complexity, they require sophisticated engineering methods for systematically developing high-quality software. Embedded software development differs from IT system development in several ways. For example, IT systems developers can use standard hardware and software platforms and don't face the resource requirements that embedded systems developers must take into account. To meet embedded software's extrafunctional requirements, embedded systems development is shifting from programming to model-driven development. Another important trend is the emphasis on the quality assurance of safety-related systems.}, 
keywords={Embedded software;Object oriented modeling;Mathematical model;Embedded system;Hardware;IEC standards;Costs;Automotive engineering;Computer languages;Operating systems;embedded systems development;model-driven development;embedded software;quality assurance;safety-critical systems}, 
doi={10.1109/MS.2009.80}, 
ISSN={1937-4194}, 
month={May},}
@INPROCEEDINGS{6263963, 
author={Maji, Amiya K. and Arshad, Fahad A. and Bagchi, Saurabh and Rellermeyer, Jan S.}, 
booktitle={IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012)}, 
title={An empirical study of the robustness of Inter-component Communication in Android}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={Over the last three years, Android has established itself as the largest-selling operating system for smartphones. It boasts of a Linux-based robust kernel, a modular framework with multiple components in each application, and a security-conscious design where each application is isolated in its own virtual machine. However, all of these desirable properties would be rendered ineffectual if an application were to deliver erroneous messages to targeted applications and thus cause the target to behave incorrectly. In this paper, we present an empirical evaluation of the robustness of Inter-component Communication (ICC) in Android through fuzz testing methodology, whereby, parameters of the inter-component communication are changed to various incorrect values. We show that not only exception handling is a rarity in Android applications, but also it is possible to crash the Android runtime from unprivileged user processes. Based on our observations, we highlight some of the critical design issues in Android ICC and suggest solutions to alleviate these problems.}, 
keywords={Androids;Humanoid robots;Smart phones;Robustness;Testing;Runtime;Receivers;android;fuzz;security;smartphone;robustness;exception}, 
doi={10.1109/DSN.2012.6263963}, 
ISSN={2158-3927}, 
month={June},}
@INPROCEEDINGS{493448, 
author={Kieburtz, R.B. and McKinney, L. and Bell, J.M. and Hook, J. and Kotov, A. and Lewis, J. and Oliva, D.P. and Sheard, T. and Smith, I. and Walton, L.}, 
booktitle={Proceedings of IEEE 18th International Conference on Software Engineering}, 
title={A software engineering experiment in software component generation}, 
year={1996}, 
volume={}, 
number={}, 
pages={542-552}, 
abstract={The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.}, 
keywords={Software engineering;Paper technology;Automatic programming;System testing;Specification languages;Military communication;Communication system control;Control systems;Information systems;Personnel}, 
doi={10.1109/ICSE.1996.493448}, 
ISSN={0270-5257}, 
month={March},}
@INPROCEEDINGS{1317496, 
author={Batory, D.}, 
booktitle={Proceedings. 26th International Conference on Software Engineering}, 
title={Feature-oriented programming and the AHEAD tool suite}, 
year={2004}, 
volume={}, 
number={}, 
pages={702-703}, 
abstract={Feature oriented programming (FOP) is an emerging paradigm for application synthesis, analysis, and optimization. A target application is specified declaratively as a set of features, like many consumer products (e.g., personal computers, automobiles). FOP technology translates such declarative specifications into efficient programs.}, 
keywords={Automatic programming;Algebra;Application software;Java;Software engineering;Design optimization;Query processing;Large-scale systems;Domain specific languages;Prototypes}, 
doi={10.1109/ICSE.2004.1317496}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{687914, 
author={Bernard, D.E. and Dorais, G.A. and Fry, C. and Gamble, E.B. and Kanefsky, B. and Kurien, J. and Millar, W. and Muscettola, N. and Nayak, P.P. and Pell, B. and Rajan, K. and Rouquette, N. and Smith, B. and Williams, B.C.}, 
booktitle={1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)}, 
title={Design of the Remote Agent experiment for spacecraft autonomy}, 
year={1998}, 
volume={2}, 
number={}, 
pages={259-281 vol.2}, 
abstract={This paper describes the Remote Agent flight experiment for spacecraft commanding and control. In the Remote Agent approach, the operational rules and constraints are encoded in the flight software. The software may be considered to be an autonomous "remote agent" of the spacecraft operators in the sense that the operators rely on the agent to achieve particular goals. The experiment will be executed during the flight of NASA's Deep Space One technology validation mission. During the experiment, the spacecraft will not be given the usual detailed sequence of commands to execute. Instead, the spacecraft will be given a list of goals to achieve during the experiment. In flight, the Remote Agent flight software will generate a plan to accomplish the goals and then execute the plan in a robust manner while keeping track of how well the plan is being accomplished. During plan execution, the Remote Agent stays on the lookout for any hardware faults that might require recovery actions or replanning. In addition to describing the design of the remote agent, this paper discusses technology-insertion challenges and the approach used in the Remote Agent approach to address these challenges. The experiment integrates several spacecraft autonomy technologies developed at NASA Ames and the Jet Propulsion Laboratory: on-board planning, a robust multi threaded executive, and model-based failure diagnosis and recovery.}, 
keywords={Space vehicles;Space technology;Robustness;NASA;Humans;Propulsion;Laboratories;Space missions;Orbital robotics;Paper technology}, 
doi={10.1109/AERO.1998.687914}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{898825, 
author={MacMillen, D. and Camposano, R. and Hill, D. and Williams, T.W.}, 
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
title={An industrial view of electronic design automation}, 
year={2000}, 
volume={19}, 
number={12}, 
pages={1428-1448}, 
abstract={The automation of the design of electronic systems and circuits [electronic design automation (EDA)] has a history of strong innovation. The EDA business has profoundly influenced the integrated circuit (IC) business and vice-versa. This paper reviews the technologies, algorithms, and methodologies that have been used in EDA tools and the business impact of these technologies. In particular, we focus on four areas that have been key in defining the design methodologies over time: physical design, simulation/verification, synthesis, and test. We then look briefly into the future. Design will evolve toward more software programmability or some other kind of field configurability like field programmable gate arrays (FPGAs). We discuss the kinds of tool sets needed to support design in this environment.}, 
keywords={Electronics industry;Electronic design automation and methodology;Integrated circuit technology;Field programmable gate arrays;Design automation;History;Technological innovation;Design methodology;Circuit simulation;Circuit synthesis}, 
doi={10.1109/43.898825}, 
ISSN={1937-4151}, 
month={Dec},}
@ARTICLE{8456508, 
author={Yu, Yinbo and Li, Xing and Leng, Xue and Song, Libin and Bu, Kai and Chen, Yan and Yang, Jianfeng and Zhang, Liang and Cheng, Kang and Xiao, Xin}, 
journal={IEEE Communications Surveys & Tutorials}, 
title={Fault Management in Software-Defined Networking: A Survey}, 
year={2019}, 
volume={21}, 
number={1}, 
pages={349-392}, 
abstract={Software-defined networking (SDN) has emerged as a new network paradigm that promises control/data plane separation and centralized network control. While these features simplify network management and enable innovative networking, they give rise to persistent concerns about reliability. The new paradigm suffers from the disadvantage that various network faults may consistently undermine the reliability of such a network, and such faults are often new and difficult to resolve with existing solutions. To ensure SDN reliability, fault management, which is concerned with detecting, localizing, correcting and preventing faults, has become a key component in SDN networks. Although many SDN fault management solutions have been proposed, we find that they often resolve SDN faults from an incomplete perspective which may result in side effects. More critically, as the SDN paradigm evolves, additional fault types are being exposed. Therefore, comprehensive reviews and constant improvements are required to remain on the leading edge of SDN fault management. In this paper, we present the first comprehensive and systematic survey of SDN faults and related management solutions identified through advancements in both the research community and industry. We apply a systematic classification of SDN faults, compare and analyze existing SDN fault management solutions in the literature, and conduct a gap analysis between solutions developed in an academic research context and practical deployments. The current challenges and emerging trends are also noted as potential future research directions. This paper aims to provide academic researchers and industrial engineers with a comprehensive survey with the hope of advancing SDN and inspiring new solutions.}, 
keywords={Software;Hardware;Industries;Fault tolerance;Fault tolerant systems;Computer architecture;Software-defined networking (SDN);SDN reliability;SDN faults;fault classification;system monitoring;fault diagnosis;fault recovery and repair;fault tolerance}, 
doi={10.1109/COMST.2018.2868922}, 
ISSN={1553-877X}, 
month={Firstquarter},}
@ARTICLE{9189773, 
author={Anand, Pooja and Singh, Yashwant and Selwal, Arvind and Alazab, Mamoun and Tanwar, Sudeep and Kumar, Neeraj}, 
journal={IEEE Access}, 
title={IoT Vulnerability Assessment for Sustainable Computing: Threats, Current Solutions, and Open Challenges}, 
year={2020}, 
volume={8}, 
number={}, 
pages={168825-168853}, 
abstract={Over the last few decades, sustainable computing has been widely used in areas like social computing, artificial intelligence-based agent systems, mobile computing, and Internet of Things (IoT). There are social, economic, and commercial impacts of IoT on human lives. However, IoT nodes are generally power-constrained with data transmission using an open channel, i.e., Internet which opens the gates for various types of attacks on them. In this context, several efforts are initiated to deal with the evolving security issues in IoT systems and make them self-sufficient to harvest energy for smooth functioning. Motivated by these facts, in this paper, we explore the evolving vulnerabilities in IoT devices. We provide a state-of-the-art survey that addresses multiple dimensions of the IoT realm. Moreover, we provide a general overview of IoT, Sustainable IoT, its architecture, and the Internet Engineering Task Force (IETF) protocol suite. Subsequently, we explore the open-source tools and datasets for the proliferation in research and growth of IoT. A detailed taxonomy of attacks associated with various vulnerabilities is also presented in the text. Then we have specifically focused on the IoT Vulnerability Assessment techniques followed by a case study on sustainability of Smart Agriculture. Finally, this paper outlines the emerging challenges related to IoT and its sustainability, and opening the doors for the beginners to start research in this promising area.}, 
keywords={Security;Protocols;Internet of Things;Computer architecture;Temperature sensors;IoT;machine learning;sustainability;cyberattacks;vulnerabilities;security;privacy}, 
doi={10.1109/ACCESS.2020.3022842}, 
ISSN={2169-3536}, 
month={},}
@ARTICLE{7820199, 
author={Fleck, Martin and Troya, Javier and Kessentini, Marouane and Wimmer, Manuel and Alkhazi, Bader}, 
journal={IEEE Transactions on Software Engineering}, 
title={Model Transformation Modularization as a Many-Objective Optimization Problem}, 
year={2017}, 
volume={43}, 
number={11}, 
pages={1009-1032}, 
abstract={Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.}, 
keywords={Unified modeling language;Object oriented modeling;Adaptation models;Measurement;Algorithm design and analysis;Software engineering;Computer bugs;Model transformation;modularization;ATL;NSGA-III;MDE;SBSE}, 
doi={10.1109/TSE.2017.2654255}, 
ISSN={1939-3520}, 
month={Nov},}
@INPROCEEDINGS{6227131, 
author={Thummalapenta, Suresh and Sinha, Saurabh and Singhania, Nimit and Chandra, Satish}, 
booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
title={Automating test automation}, 
year={2012}, 
volume={}, 
number={}, 
pages={881-891}, 
abstract={Mention “test case”, and it conjures up the image of a script or a program that exercises a system under test. In industrial practice, however, test cases often start out as steps described in natural language. These are essentially directions a human tester needs to follow to interact with an application, exercising a given scenario. Since tests need to be executed repeatedly, such manual tests then have to go through test automation to create scripts or programs out of them. Test automation can be expensive in programmer time. We describe a technique to automate test automation. The input to our technique is a sequence of steps written in natural language, and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention. The technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets, except that there can be ambiguity in identifying segments, in identifying the action in a segment, as well as in the specification of the target of the action. The technique resolves this ambiguity by backtracking, until it can synthesize a successful sequence of calls. We present an evaluation of our technique on professionally created manual test cases for two open-source web applications as well as a proprietary enterprise application. Our technique could automate over 82% of the steps contained in these test cases with no human intervention, indicating that the technique can reduce the cost of test automation quite effectively.}, 
keywords={Manuals;Automation;Optimization;Humans;Natural languages;Programming profession}, 
doi={10.1109/ICSE.2012.6227131}, 
ISSN={1558-1225}, 
month={June},}
@ARTICLE{7997723, 
author={Raibulet, Claudia and Arcelli Fontana, Francesca and Zanoni, Marco}, 
journal={IEEE Access}, 
title={Model-Driven Reverse Engineering Approaches: A Systematic Literature Review}, 
year={2017}, 
volume={5}, 
number={}, 
pages={14516-14542}, 
abstract={This paper explores and describes the state of the art for what concerns the model-driven approaches proposed in the literature to support reverse engineering. We conducted a systematic literature review on this topic with the aim to answer three research questions. We focus on various solutions developed for model-driven reverse engineering, outlining in particular the models they use and the transformations applied to the models. We also consider the tools used for model definition, extraction, and transformation and the level of automation reached by the available tools. The model-driven reverse engineering approaches are also analyzed based on various features such as genericity, extensibility, automation of the reverse engineering process, and coverage of the full or partial source artifacts. We describe in detail and compare fifteen approaches applying model-driven reverse engineering. Based on this analysis, we identify and indicate some hints on choosing a model-driven reverse engineering approach from the available ones, and we outline open issues concerning the model-driven reverse engineering approaches.}, 
keywords={Reverse engineering;Object oriented modeling;Tools;Systematics;Analytical models;Bibliographies;Search engines;Models;reverse engineering;model-driven reverse engineering;model transformation;legacy system}, 
doi={10.1109/ACCESS.2017.2733518}, 
ISSN={2169-3536}, 
month={},}
@ARTICLE{991333, 
author={Hieatt, E. and Mee, R.}, 
journal={IEEE Software}, 
title={Going faster: testing the Web application}, 
year={2002}, 
volume={19}, 
number={2}, 
pages={60-65}, 
abstract={This article documents the experiences of Evant's Extreme Programming team with testing XP. Testing is fundamental to XP but is a practice that often falls by the wayside in today's fast-paced Web application development culture. From the beginning, Evant adhered to each of XP's principles, and testing was no exception. This article explains how the team found that testing, positioned as the drive behind development, was critical to the success of building Evant's application at speed while maintaining high quality.}, 
keywords={Application software;Software testing;Writing;Buildings;Software engineering;Internet;Software quality;Software maintenance;Software tools;Java}, 
doi={10.1109/52.991333}, 
ISSN={1937-4194}, 
month={March},}
@INPROCEEDINGS{879426, 
author={Yu-Wen Tung and Aldiwan, W.S.}, 
booktitle={2000 IEEE Aerospace Conference. Proceedings (Cat. No.00TH8484)}, 
title={Automating test case generation for the new generation mission software system}, 
year={2000}, 
volume={1}, 
number={}, 
pages={431-437 vol.1}, 
abstract={The significant expansion of autonomous control and information processing capabilities in the coming generation of mission software systems results in a qualitatively larger space of behaviors that needs to be "covered" during testing, not only at the system level but also at subsystem and unit levels. A major challenge in this area is to automatically generate a relatively small set of test cases that, collectively, guarantees a selected degree of coverage of the behavior space. This paper describes an algorithm for a parametric test case generation tool that applies a combinatorial design approach to the selection of candidate test cases. Evaluation of this algorithm on test parameters from the Deep Space One mission reveals a valuable reduction in the number of test cases, when compared to an earlier home-brewed generator.}, 
keywords={Automatic testing;Space missions;Automatic generation control;Process control;Control systems;Information processing;Software systems;Software testing;System testing;Algorithm design and analysis}, 
doi={10.1109/AERO.2000.879426}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{4346539, 
author={Chen, Yan and Bindel, David and Song, Han Hee and Katz, Randy H.}, 
journal={IEEE/ACM Transactions on Networking}, 
title={Algebra-Based Scalable Overlay Network Monitoring: Algorithms, Evaluation, and Applications}, 
year={2007}, 
volume={15}, 
number={5}, 
pages={1084-1097}, 
abstract={Overlay network monitoring enables distributed Internet applications to detect and recover from path outages and periods of degraded performance within seconds. For an overlay network with end hosts, existing systems either require measurements, and thus lack scalability, or can only estimate the latency but not congestion or failures. Our earlier extended abstract [Y. Chen, D. Bindel, and R. H. Katz, ldquoTomography-based overlay network monitoring,rdquo Proceedings of the ACM SIGCOMM Internet Measurement Conference (IMC), 2003] briefly proposes an algebraic approach that selectively monitors linearly independent paths that can fully describe all the paths. The loss rates and latency of these paths can be used to estimate the loss rates and latency of all other paths. Our scheme only assumes knowledge of the underlying IP topology, with links dynamically varying between lossy and normal. In this paper, we improve, implement, and extensively evaluate such a monitoring system. We further make the following contributions: i) scalability analysis indicating that for reasonably large n (e.g., 100), the growth of k is bounded as O(n log n), ii) efficient adaptation algorithms for topology changes, such as the addition or removal of end hosts and routing changes, iii) measurement load balancing schemes, iv) topology measurement error handling, and v) design and implementation of an adaptive streaming media system as a representative application. Both simulation and Internet experiments demonstrate we obtain highly accurate path loss rate estimation while adapting to topology changes within seconds and handling topology errors.}, 
keywords={Topology;Delay;Condition monitoring;IP networks;Scalability;Algorithm design and analysis;Degradation;Routing;Load management;Measurement errors;Dynamics;load balancing;network measurement and monitoring;numerical linear algebra;overlay;scalability}, 
doi={10.1109/TNET.2007.896251}, 
ISSN={1558-2566}, 
month={Oct},}
@ARTICLE{8317991, 
author={Johanson, Arne and Hasselbring, Wilhelm}, 
journal={Computing in Science & Engineering}, 
title={Software Engineering for Computational Science: Past, Present, Future}, 
year={2018}, 
volume={20}, 
number={2}, 
pages={90-109}, 
abstract={Despite the increasing importance of in silico experiments to the scientific discovery process, state-of-the-art software engineering practices are rarely adopted in computational science. To understand the underlying causes for this situation and to identify ways to improve it, the authors conducted a literature survey on software engineering practices in computational science. They identified 13 recurring key characteristics of scientific software development that are the result of the nature of scientific challenges, the limitations of computers, and the cultural environment of scientific software development. Their findings allow them to point out shortcomings of existing approaches for bridging the gap between software engineering and computational science and to provide an outlook on promising research directions that could contribute to improving the current situation.}, 
keywords={Scientific computing;Software engineering;Computational modeling;Software development management;survey;software engineering;computational science;software development;history of computing}, 
doi={10.1109/MCSE.2018.021651343}, 
ISSN={1558-366X}, 
month={Mar},}
@INPROCEEDINGS{1387329, 
author={Kundu, S. and Mak, T.M. and Galivanche, R.}, 
booktitle={2004 International Conferce on Test}, 
title={Trends in manufacturing test methods and their implications}, 
year={2004}, 
volume={}, 
number={}, 
pages={679-687}, 
abstract={Driven by market applications in the areas of computing, networking, storage, optical, wireless, portable, and consumer electronics, semiconductor chips today are as diverse as ever. Confluence of multiple applications and rapid integration has also driven the heterogeneity of chips. Test methods have evolved with the products. However, the basic goals in testing remain the same: quality of product, recurring and non-recurring costs and time to market. In this paper we try to catalog some commonly used test methods, identify their associated DFT requirements and trends in terms of tester requirements. Given the diversity of semiconductors chips today such as various PLDs, volatile and non-volatile memories, analog, mixed signal, FPGA, ASIC, SOC, MEMs and processors, it is impossible for a paper of this nature to be fully comprehensive. So we limit our focus on processor, ASIC and SOCs.}, 
keywords={Manufacturing;Testing;Optical computing;Application specific integrated circuits;Semiconductor device manufacture;Computer networks;Portable computers;Optical fiber networks;Consumer electronics;Costs}, 
doi={10.1109/TEST.2004.1387329}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6005491, 
author={Hellmann, Theodore D. and Maurer, Frank}, 
booktitle={2011 Agile Conference}, 
title={Rule-Based Exploratory Testing of Graphical User Interfaces}, 
year={2011}, 
volume={}, 
number={}, 
pages={107-116}, 
abstract={This paper introduces rule-based exploratory testing, an approach to GUI testing that combines aspects of manual exploratory testing with rule-based test automation. This approach uses short, automated rules to increase the bug-detection capability of recorded exploratory test sessions. A preliminary evaluation found that this approach can be used to detect both general and application-specific bugs, but that rules for general bugs are easier to transfer between applications. Also, despite the advantages of keyword-based testing, it interferes with the transfer of rules between applications.}, 
keywords={Testing;Graphical user interfaces;Computer bugs;Humans;Manuals;Security;Fires;GUI testing;rule-based testing;exploratory testing}, 
doi={10.1109/AGILE.2011.23}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6903575, 
author={Kane, Aaron and Fuhrman, Thomas and Koopman, Philip}, 
booktitle={2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks}, 
title={Monitor Based Oracles for Cyber-Physical System Testing: Practical Experience Report}, 
year={2014}, 
volume={}, 
number={}, 
pages={148-155}, 
abstract={Testing Cyber-Physical Systems is becoming increasingly challenging as they incorporate advanced autonomy features. We investigate using an external runtime monitor as a partial test oracle to detect violations of critical system behavioral requirements on an automotive development platform. Despite limited source code access and using only existing network messages, we were able to monitor a hardware-in-the-loop vehicle simulator and analyze prototype vehicle log data to detect violations of high-level critical properties. Interface robustness testing was useful to further exercise the monitors. Beyond demonstrating feasibility, the experience emphasized a number of remaining research challenges, including: approximating system intent based on limited system state observability, how to best balance the simplicity and expressiveness of the specification language used to define monitored properties, how to warm up monitoring of system variable state after mode change discontinuities, and managing the differences between simulation and real vehicles when conducting such tests.}, 
keywords={Monitoring;Vehicles;Testing;Runtime;Robustness;Safety;Prototypes;runtime monitoring;testing;cyber-physical systems}, 
doi={10.1109/DSN.2014.28}, 
ISSN={2158-3927}, 
month={June},}
@ARTICLE{8016712, 
author={}, 
journal={ISO/IEC/IEEE 24765:2017(E)}, 
title={ISO/IEC/IEEE International Standard - Systems and software engineering--Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-541}, 
abstract={This document provides a common vocabulary applicable to all systems and software engineering work. It was prepared to collect and standardize terminology. This document is intended to serve as a useful reference for those in the information technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute. This document includes references to the active source standards for definitions so that systems and software engineering concepts and requirements can be further explored.}, 
keywords={IEEE Standards;ISO Standards;IEC Standards;Informatino technology;Software engineering;Systems engineering and theoryt;Terminology;computer;dictionary;information technology;software engineering;systems engineering;24765}, 
doi={10.1109/IEEESTD.2017.8016712}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4657364, 
author={Mattsson, Anders and Lundell, Björn and Lings, Brian and Fitzgerald, Brian}, 
journal={IEEE Transactions on Software Engineering}, 
title={Linking Model-Driven Development and Software Architecture: A Case Study}, 
year={2009}, 
volume={35}, 
number={1}, 
pages={83-93}, 
abstract={A basic premise of model driven development (MDD) is to capture all important design information in a set of formal or semi-formal models which are then automatically kept consistent by tools. The concept however is still relatively immature and there is little by way of empirically validated guidelines. In this paper we report on the use of MDD on a significant real-world project over several years. Our research found the MDD approach to be deficient in terms of modelling architectural design rules. Furthermore, the current body of literature does not offer a satisfactory solution as to how architectural design rules should be modelled. As a result developers have to rely on time-consuming and error-prone manual practices to keep a system consistent with its architecture. To realise the full benefits of MDD it is important to find ways of formalizing architectural design rules which then allow automatic enforcement of the architecture on the system model. Without this, architectural enforcement will remain a bottleneck in large MDD projects.}, 
keywords={Joining processes;Software architecture;Computer architecture;Guidelines;Context modeling;Computer industry;Computer errors;Programming;Keyword search;Portals;Software Architecture;Model-Driven Development;Case Study Research;Software Architecture;Model-Driven Development;Case Study Research}, 
doi={10.1109/TSE.2008.87}, 
ISSN={1939-3520}, 
month={Jan},}
@ARTICLE{1166666, 
author={Baines, R. and Pulley, D.}, 
journal={IEEE Communications Magazine}, 
title={A total cost approach to evaluating different reconfigurable architectures for baseband processing in wireless receivers}, 
year={2003}, 
volume={41}, 
number={1}, 
pages={105-113}, 
abstract={There is growing interest in the use of flexible digital signal processors for wireless systems, driven by the demands of time to market, cost pressure, the requirement for flexibility to cope with evolving standards, and rapidly increasing processing needs. Much of the discussion of these techniques involves terms like "efficient" or "cost-effective" without necessarily quantifying the terms. This article considers the various architectures applicable to a wideband CDMA node-B base station (ASIC, FPGA, traditional DSP, and two varieties of flexible DSP) and builds a quantitative total cost approach to evaluating them, including benchmarked performance data.}, 
keywords={Costs;Reconfigurable architectures;Baseband;Digital signal processing;Digital signal processors;Time to market;Wideband;Multiaccess communication;Base stations;Application specific integrated circuits}, 
doi={10.1109/MCOM.2003.1166666}, 
ISSN={1558-1896}, 
month={Jan},}
@ARTICLE{7992926, 
author={Garcia, Boni and Gortazar, Francisco and Lopez-Fernandez, Luis and Gallego, Micael and Paris, Miguel}, 
journal={IEEE Communications Standards Magazine}, 
title={WebRTC Testing: Challenges and Practical Solutions}, 
year={2017}, 
volume={1}, 
number={2}, 
pages={36-42}, 
abstract={WebRTC comprises a set of novel technologies and standards that provide Real-Time Communication on Web browsers. WebRTC makes simple the embedding of voice and video communications in all types of applications. However, releasing those applications to production is still very challenging due to the complexity of their testing. Validating a WebRTC service requires assessing many functional (e.g. signaling logic, media connectivity, etc.) and non-functional (e.g. quality of experience, interoperability, scalability, etc.) properties on large, complex, distributed and heterogeneous systems that spawn across client devices, networks and cloud infrastructures. In this article, we present a novel methodology and an associated tool for doing it at scale and in an automated way. Our strategy is based on a blackbox end-to-end approach through which we use an automated containerized cloud environment for instrumenting Web browser clients, which benchmark the SUT (system under test), and fake clients, that load it. Through these benchmarks, we obtain, in a reliable and statistically significant way, both network-dependent QoS (Quality of Service) metrics and media-dependent QoE (Quality of Experience) indicators. These are fed, at a second stage, to a number of testing assertions that validate the appropriateness of the functional and non-functional properties of the SUT under controlled and configurable load and fail conditions. To finish, we illustrate our experiences using such tool and methodology in the context of the Kurento open source software project and conclude that they are suitable for validating large and complex WebRTC systems at scale.}, 
keywords={WebRTC;Browsers;Telecommunication traffic;Media;Real-time systems;Quality of service;Internet}, 
doi={10.1109/MCOMSTD.2017.1700005}, 
ISSN={2471-2833}, 
month={},}
@ARTICLE{4042539, 
author={Beckert, Bernhard and Hoare, Tony and Hahnle, Reiner and Smith, Douglas R. and Green, Cordell and Ranise, Silvio and Tinelli, Cesare and Ball, Thomas and Rajamani, Sriram K.}, 
journal={IEEE Intelligent Systems}, 
title={Intelligent Systems and Formal Methods in Software Engineering}, 
year={2006}, 
volume={21}, 
number={6}, 
pages={71-81}, 
abstract={Over the last few years, technologies for the formal description, construction, analysis, and validation of software - based mostly on logics and formal reasoning - have matured. We can expect them to complement and partly replace traditional software engineering methods in the future. Formal methods in software engineering are an increasingly important application area for intelligent systems. The field has outgrown the area of academic case studies, and industry is showing serious interest. We convincingly argue that we've reached the point where we can solve the problem of how to formally verify industrial-scale software. We propose program verification as a computer science Grand Challenge. Deductive software verification is a core technology of formal methods. We describe recent dramatic changes in the way it's perceived and used. Another important base technique of formal methods, besides software verification, is synthesizing software that's correct by construction because it's formally derived from its specification. We discuss recent developments and trends in this area. Surprisingly efficient decision procedures for the satisfiability modulo theories problem have recently emerged. We explain these techniques and why they're important for all formal-methods tools. We look at formal methods from an industry perspective. We explain the success of Microsoft Research's SLAM project, which has developed a verification tool for device drivers}, 
keywords={Intelligent systems;Software engineering;Costs;Application software;Programming profession;Computer science;Computer errors;Humans;Energy management;Financial management;formal methods;software engineering;program verification;deductive software verification;satisfiability modulo theories;software synthesis}, 
doi={10.1109/MIS.2006.117}, 
ISSN={1941-1294}, 
month={Nov},}
@INPROCEEDINGS{8667986, 
author={Brito, Gleison and Mombach, Thais and Valente, Marco Tulio}, 
booktitle={2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Migrating to GraphQL: A Practical Assessment}, 
year={2019}, 
volume={}, 
number={}, 
pages={140-150}, 
abstract={GraphQL is a novel query language proposed by Facebook to implement Web-based APIs. In this paper, we present a practical study on migrating API clients to this new technology. First, we conduct a grey literature review to gain an in-depth understanding on the benefits and key characteristics normally associated to GraphQL by practitioners. After that, we assess such benefits in practice, by migrating seven systems to use GraphQL, instead of standard REST-based APIs. As our key result, we show that GraphQL can reduce the size of the JSON documents returned by REST APIs in 94% (in number of fields) and in 99% (in number of bytes), both median results.}, 
keywords={Servers;Bibliographies;Computer hacking;Databases;Database languages;Uniform resource locators;Blogs;GraphQL;REST;APIs;Migration Study}, 
doi={10.1109/SANER.2019.8667986}, 
ISSN={1534-5351}, 
month={Feb},}
@INPROCEEDINGS{5431981, 
author={Esquivel, Holly and Akella, Aditya and Mori, Tatsuya}, 
booktitle={2010 Second International Conference on COMmunication Systems and NETworks (COMSNETS 2010)}, 
title={On the effectiveness of IP reputation for spam filtering}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Modern SMTP servers apply a variety of mechanisms to stem the volume of spam delivered to users. These techniques can be broadly classified into two categories: pre-acceptance approaches, which apply prior to a message being accepted (e.g. IP reputation), and post-acceptance techniques which apply after a message has been accepted (e.g. content based signatures). We argue that the effectiveness of these measures varies based on the SMTP sender type. This paper focuses on the most light-weight pre-acceptance filtering mechanism-IP reputation. We first classify SMTP senders into three main categories: legitimate servers, end-hosts, and spam gangs, and empirically study the limits of effectiveness regarding IP reputation filtering for each category. Next, we develop new techniques that build custom IP reputation lists, which significantly improve the performance of existing IP reputation lists. In compiling these lists, we leverage a somewhat surprising fact that both legitimate domains and spam domains often use the DNS Sender Policy Framework (SPF) in an attempt to pass simple authentication checks. That is, good/bad IP addresses can be systematically compiled by collecting good/bad domains and looking up their SPF resource records. We also evaluate the effectiveness of these lists over time. Finally, we aim to understand the characteristics of the three categories of email senders in depth. Overall, we find that it is possible to construct IP reputation lists that can identify roughly 90% of all spam and legitimate mail, but some of the lists, i.e. the lists for spam gangs, must be updated on a constant basis to maintain this high level of accuracy.}, 
keywords={Filtering;Testing;Optical filters;Authentication;Protocols;Optical character recognition software;Unsolicited electronic mail;Laboratories;Postal services;Optical recording}, 
doi={10.1109/COMSNETS.2010.5431981}, 
ISSN={2155-2509}, 
month={Jan},}
@INPROCEEDINGS{8257807, 
author={Perera, Pulasthi and Silva, Roshali and Perera, Indika}, 
booktitle={2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
title={Improve software quality through practicing DevOps}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={DevOps is extended from certain agile practices with a mix of patterns intended to improve collaboration between development and operation teams. The main purpose of this paper is to conduct a study on how DevOps practice has impacted to software quality. The secondary objective is to find how to improve quality efficiently. A literature survey has carried out to explore about current DevOps practices in industry. According to the literature survey, the conceptual research model was developed and five hypotheses were derived. Research objectives were accomplished by testing hypotheses using Pearson correlation. A linear model is derived based on the linear regression analysis. An online questionnaire was used to collect quantitative data whereas interviews with experts on DevOps and Quality assurance have been used to identify how to improve the quality of software by practicing DevOps. Recommendations are given based on interview feedback, hypotheses testing with regression analysis. According to the quantitative study, researchers have identified that quality of the software gets improved when practice DevOps by following CAMS (Culture, Automation, Measurement, Sharing) framework. Automation is the most critical factor to improve the software quality. As per the results of multiple regression analysis, it has proved culture, automation, measurement and sharing are important factors to consider to improve quality of the software. In conclusion it can be recommended to use DevOps to achieve high quality software.}, 
keywords={Companies;Automation;Software quality;Testing;Software measurement;DevOps;CAMS Framework;Quality;ISO 9126;Automation}, 
doi={10.1109/ICTER.2017.8257807}, 
ISSN={2472-7598}, 
month={Sep.},}
@INPROCEEDINGS{7546497, 
author={Argyros, George and Stais, Ioannis and Kiayias, Aggelos and Keromytis, Angelos D.}, 
booktitle={2016 IEEE Symposium on Security and Privacy (SP)}, 
title={Back in Black: Towards Formal, Black Box Analysis of Sanitizers and Filters}, 
year={2016}, 
volume={}, 
number={}, 
pages={91-109}, 
abstract={We tackle the problem of analyzing filter and sanitizer programs remotely, i.e. given only the ability to query the targeted program and observe the output. We focus on two important and widely used program classes: regular expression (RE) filters and string sanitizers. We demonstrate that existing tools from machine learning that are available for analyzing RE filters, namely automata learning algorithms, require a very large number of queries in order to infer real life RE filters. Motivated by this, we develop the first algorithm that infers symbolic representations of automata in the standard membership/equivalence query model. We show that our algorithm provides an improvement of x15 times in the number of queries required to learn real life XSS and SQL filters of popular web application firewall systems such as mod-security and PHPIDS. % Active learning algorithms require the usage of an equivalence oracle, i.e. an oracle that tests the equivalence of a hypothesis with the target machine. We show that when the goal is to audit a target filter with respect to a set of attack strings from a context free grammar, i.e. find an attack or infer that none exists, we can use the attack grammar to implement the equivalence oracle with a single query to the filter. Our construction finds on average 90% of the target filter states when no attack exists and is very effective in finding attacks when they are present. For the case of string sanitizers, we show that existing algorithms for inferring sanitizers modelled as Mealy Machines are not only inefficient, but lack the expressive power to be able to infer real life sanitizers. We design two novel extensions to existing algorithms that allow one to infer sanitizers represented as single-valued transducers. Our algorithms are able to infer many common sanitizer functions such as HTML encoders and decoders. Furthermore, we design an algorithm to convert the inferred models into BEK programs, which allows for further applications such as cross checking different sanitizer implementations and cross compiling sanitizers into different languages supported by the BEK backend. We showcase the power of our techniques by utilizing our black-box inference algorithms to perform an equivalence checking between different HTML encoders including the encoders from Twitter, Facebook and Microsoft Outlook email, for which no implementation is publicly available.}, 
keywords={Algorithm design and analysis;Machine learning algorithms;Learning automata;Transducers;Grammar;HTML;Inference algorithms;sanitizers;filters;automata;learning;web security}, 
doi={10.1109/SP.2016.14}, 
ISSN={2375-1207}, 
month={May},}
@INPROCEEDINGS{7927983, 
author={Artho, Cyrille and Gros, Quentin and Rousset, Guillaume and Banzai, Kazuaki and Ma, Lei and Kitamura, Takashi and Hagiya, Masami and Tanabe, Yoshinori and Yamamoto, Mitsuharu}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Model-Based API Testing of Apache ZooKeeper}, 
year={2017}, 
volume={}, 
number={}, 
pages={288-298}, 
abstract={Apache ZooKeeper is a distributed data storage that is highly concurrent and asynchronous due to network communication, testing such a system is very challenging. Our solution using the tool "Modbat" generates test cases for concurrent client sessions, and processes results from synchronous and asynchronous callbacks. We use an embedded model checker to compute the test oracle for non-deterministic outcomes, the oracle model evolves dynamically with each new test step. Our work has detected multiple previously unknown defects in ZooKeeper. Finally, a thorough coverage evaluation of the core classes show how code and branch coverage strongly relate to feature coverage in the model, and hence modeling effort.}, 
keywords={Servers;Testing;Electronic mail;Computational modeling;Computer science;Tools;Complexity theory;Model-based testing;concurrency;asynchronous systems;networked systems;test oracle;Apache ZooKeeper}, 
doi={10.1109/ICST.2017.33}, 
ISSN={}, 
month={March},}
@ARTICLE{9427143, 
author={Zheng, Miaomiao and Zhang, Shanshan and Zhang, Yidan and Hu, Baozhong}, 
journal={IEEE Access}, 
title={Construct Food Safety Traceability System for People’s Health Under the Internet of Things and Big Data}, 
year={2021}, 
volume={9}, 
number={}, 
pages={70571-70583}, 
abstract={In the context of epidemic prevention and control, food safety monitoring, data analysis and food safety traceability have become more important. At the same time, the most important reason for food safety issues is incomplete, opaque, and asymmetric information. The most fundamental way to solve these problems is to do a good job of traceability, and establish a reasonable and reliable food safety traceability system. The traceability system is currently an important means to ensure food quality and safety and solve the crisis of trust between consumers and the market. Research on food safety traceability systems based on big data, artificial intelligence and the Internet of Things provides ideas and methods to solve the problems of low credibility and difficult data storage in the application of traditional traceability systems. Therefore, this research takes rice as an example and proposes a food safety traceability system based on RFID two-dimensional code technology and big data storage technology in the Internet of Things. This article applies RFID technology to the entire system by analyzing the requirements of the system, designing the system database and database tables, encoding the two-dimensional code and generating the design for information entry. Using RFID radio frequency technology and the data storage function in big data to obtain information in the food production process. Finally, the whole process of food production information can be traced through the design of dynamic query platform and mobile terminal. In this research, the food safety traceability system based on big data and the Internet of Things guarantees the integrity, reliability and safety of traceability information from a technical level. This is an effective solution for enhancing the credibility of traceability information, ensuring the integrity of information, and optimizing the data storage structure.}, 
keywords={Safety;Big Data;Internet of Things;Production;Radiofrequency identification;Python;Epidemics;Two-dimensional code technology;Internet of Things;big data;artificial intelligence;food safety traceability system}, 
doi={10.1109/ACCESS.2021.3078536}, 
ISSN={2169-3536}, 
month={},}
@ARTICLE{8970242, 
author={Zhang, Jingjing and Yang, Lin and Cao, Weipeng and Wang, Qiang}, 
journal={IEEE Access}, 
title={Formal Analysis of 5G EAP-TLS Authentication Protocol Using Proverif}, 
year={2020}, 
volume={8}, 
number={}, 
pages={23674-23688}, 
abstract={As a critical component of the security architecture of 5G network, the authentication protocol plays a role of the first safeguard in ensuring the communication security, such as the confidentiality of user data. EAP-TLS is one of such protocols being defined in the 5G standards to provide key services in the specific IoT circumstances. This protocol is currently under the process of standardization, and it is vital to guarantee that the standardized protocol is free from any design flaws, which may result in severe vulnerabilities and serious consequences when implemented in real systems. However, it is still unclear whether the proposed 5G EAP-TLS authentication protocol provides the claimed security guarantees. To fill this gap, we present in this work a comprehensive formal analysis of the security related properties of the 5G EAP-TLS authentication protocol based on the symbolic model checking approach. Specifically, we build the first formal model of the 5G EAP-TLS authentication protocol in the applied pi calculus, and perform an automated security analysis of the formal protocol model by using the ProVerif model checker. Our analysis results show that there are some subtle flaws in the current protocol design that may compromise the claimed security objectives. To this end, we also propose and verify a possible fix that is able to mitigate these flaws. To the best of our knowledge, this is the first thorough formal analysis of the 5G EAP-TLS authentication protocol.}, 
keywords={Protocols;5G mobile communication;Authentication;Analytical models;Cryptography;Mathematical model;Authentication protocol;5G network;formal verification;model checking;applied pi calculus;ProVerif;EAP-TLS}, 
doi={10.1109/ACCESS.2020.2969474}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{6080786, 
author={Yazdanshenas, Amir Reza and Moonen, Leon}, 
booktitle={2011 27th IEEE International Conference on Software Maintenance (ICSM)}, 
title={Crossing the boundaries while analyzing heterogeneous component-based software systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={193-202}, 
abstract={One way to manage the complexity of software systems is to compose them from reusable components, instead of starting from scratch. Components may be implemented in different programming languages and are tied together using configuration files, or glue code, defining instantiation, initialization and interconnections. Although correctly engineering the composition and configuration of components is crucial for the overall behavior, there is surprisingly little support for incorporating this information in the static verification and validation of these systems. Analyzing the properties of programs within closed code boundaries has been studied for some decades and is well-established. This paper contributes a method to support analysis across the components of a component-based system. We build upon the Knowledge Discovery Metamodel to reverse engineer homogeneous models for systems composed of heterogeneous artifacts. Our method is implemented in a prototype tool that has been successfully used to track information flow across the components of a component-based system using program slicing.}, 
keywords={Complexity theory;Software systems;Component architectures;Computer languages;Prototypes;Knowledge engineering}, 
doi={10.1109/ICSM.2011.6080786}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{6984094, 
author={Costa, Pedro and Paiva, Ana C.R. and Nabuco, Miguel}, 
booktitle={2014 9th International Conference on the Quality of Information and Communications Technology}, 
title={Pattern Based GUI Testing for Mobile Applications}, 
year={2014}, 
volume={}, 
number={}, 
pages={66-74}, 
abstract={This paper presents a study aiming to assess the feasibility of using the Pattern Based GUI Testing approach, PBGT, to test mobile applications. PBGT is a new model based testing approach that aims to increase systematization, reusability and diminish the effort in modelling and testing. It is based on the concept of User Interface Test Patterns (UITP) that contain generic test strategies for testing common recurrent behaviour, the so-called UI Patterns, on GUIs through its possible different implementations after a configuration step. Although PBGT was developed having web applications in mind, it is possible to develop drivers for other platforms in order to test a wide set of applications. However, web and mobile applications are different and only the development of a new driver to execute test cases over mobile applications may not be enough. This paper describes a study aiming to identify the adaptations and updates the PBGT should undergo in order to test mobile applications.}, 
keywords={Mobile communication;Testing;Graphical user interfaces;Connectors;Androids;Humanoid robots;Optical character recognition software}, 
doi={10.1109/QUATIC.2014.16}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{794352, 
author={Smith, B. and Millar, W. and Dunphy, J. and Yu-Wen Tung and Nayak, P. and Gamble, E. and Clark, M.}, 
booktitle={1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403)}, 
title={Validation and verification of the remote agent for spacecraft autonomy}, 
year={1999}, 
volume={1}, 
number={}, 
pages={449-468 vol.1}, 
abstract={The six-day Remote Agent Experiment (RAX) on the Deep Space 1 mission will be the first time that an artificially intelligent agent will control a NASA spacecraft. Successful completion of this experiment will open the way for AI-based autonomy technology on future missions. An important validation objective for RAX is implementation of a credible validation and verification strategy for RAX that also "scales up" to missions that make full use of spacecraft autonomy. Autonomous flight software presents novel and difficult testing challenges that traditional flight software (FSW) does not face. Since autonomous software must respond robustly in an immense number of situations, the all-paths testing approaches used for traditional FSW is not feasible. Instead, we advocate a combination of scenario-based testing and model-based validation. This paper describes the testing challenges faced by autonomous spacecraft commanding software, discusses the testing strategies and model-validation methods that we found effective for RAX, and argues that these methods will "scale up" to missions that make full use of spacecraft autonomy. Among the key challenges for validating autonomous systems such as the RAX are ensuring adequate coverage for scenario-based tests, developing methods for specifying the expected behavior, and developing automated tools for verifying the observed behavior against those specifications. Another challenge, also faced by traditional FSW, is the scarcity of high-fidelity test-beds. The test plan must be designed to take advantage of lower-fidelity test-beds without compromising test effectiveness.}, 
keywords={Space vehicles;Space technology;Software testing;Automatic testing;Space missions;Propulsion;Laboratories;Intelligent agent;NASA;Robustness}, 
doi={10.1109/AERO.1999.794352}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{386480, 
author={Rao, S.R. and Bi-Yu Pan and Armstrong, J.R.}, 
booktitle={1993 European Conference on Design Automation with the European Event in ASIC Design}, 
title={Hierarchical test generation for VHDL behavioral models}, 
year={1993}, 
volume={}, 
number={}, 
pages={175-182}, 
abstract={In this method, the VHDL model to be tested is represented by its process model graph (PMG). Test sets for individual processes of the model are precomputed and stored in the design library. The Hierarchical Behavioral Test Generator (HBTG) algorithm accepts the PMG and the precomputed tests as inputs, from which it hierarchically constructs a test sequence that tests the functionality of the model. Such an automatic test generation process relieves the modeler of the time-consuming task of developing test-benches. The test sequence generated by HBTG is then used for simulation of the model. Experimental results indicate that the tests generated exercise the model thoroughly.<>}, 
keywords={Circuit testing;Automatic testing;Circuit faults;Libraries;Computational modeling;Circuit simulation;Computer simulation;Computer industry;Hardware;Design engineering}, 
doi={10.1109/EDAC.1993.386480}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7332467, 
author={Kim, Jongwook and Batory, Don and Dig, Danny}, 
booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Scripting parametric refactorings in Java to retrofit design patterns}, 
year={2015}, 
volume={}, 
number={}, 
pages={211-220}, 
abstract={Retrofitting design patterns into a program by hand is tedious and error-prone. A programmer must distinguish refactorings that are provided by an Integrated Development Environment (IDE) from those that must be realized manually, determine a precise sequence of refactorings to apply, and perform this sequence repetitively to a laborious degree. We designed, implemented, and evaluated Reflective Refactoring (R2), a Java package to automate the creation of classical design patterns (Visitor, Abstract Factory, etc.), their inverses, and variants. We encoded 18 out of 23 Gang-of-Four design patterns as R2 scripts and explain why the remaining are inappropriate for refactoring engines. We evaluate the productivity and scalability of R2 with a case study of 6 real-world applications. In one case, R2 automatically created a Visitor with 276 visit methods by invoking 554 Eclipse refactorings in 10 minutes - an achievement that could not be done manually. R2 also sheds light on why refactoring correctness, expressiveness, and speed are critical issues for scripting in next-generation refactoring engines.}, 
keywords={Manuals;Graphics;DVD}, 
doi={10.1109/ICSM.2015.7332467}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7019292, 
author={Prathibhan, C.Mano and Malini, A. and Venkatesh, N. and Sundarakantham, K.}, 
booktitle={2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies}, 
title={An automated testing framework for testing Android mobile applications in the cloud}, 
year={2014}, 
volume={}, 
number={}, 
pages={1216-1219}, 
abstract={The testing of mobile application faces many issues due to the complexity of testing these applications and the limited resources available in mobile devices. Testing in various mobile devices under varying conditions takes a lot of time when done manually. Also by using emulators it is not possible to generate the same real time network connections and real device characteristics. There is a need for a testing framework that allows automated testing of mobile applications in many mobile devices in limited time. In this paper we propose a mobile testing framework in the cloud environment that aims to provide automated testing of mobile applications in various mobile devices. This testing framework has an automated testing tool, the Mobile Application Testing (MAT) Tool integrated to it that performs functional, performance and compatibility testing of mobile applications.}, 
keywords={Testing;Mobile communication;Performance evaluation;Androids;Humanoid robots;Mobile Testing;Automated Testing;Testing as a Service;Cloud Testing}, 
doi={10.1109/ICACCCT.2014.7019292}, 
ISSN={}, 
month={May},}
@ARTICLE{9310181, 
author={Rajabli, Nijat and Flammini, Francesco and Nardone, Roberto and Vittorini, Valeria}, 
journal={IEEE Access}, 
title={Software Verification and Validation of Safe Autonomous Cars: A Systematic Literature Review}, 
year={2021}, 
volume={9}, 
number={}, 
pages={4797-4819}, 
abstract={Autonomous, or self-driving, cars are emerging as the solution to several problems primarily caused by humans on roads, such as accidents and traffic congestion. However, those benefits come with great challenges in the verification and validation (V&V) for safety assessment. In fact, due to the possibly unpredictable nature of Artificial Intelligence (AI), its use in autonomous cars creates concerns that need to be addressed using appropriate V&V processes that can address trustworthy AI and safe autonomy. In this study, the relevant research literature in recent years has been systematically reviewed and classified in order to investigate the state-of-the-art in the software V&V of autonomous cars. By appropriate criteria, a subset of primary studies has been selected for more in-depth analysis. The first part of the review addresses certification issues against reference standards, challenges in assessing machine learning, as well as general V&V methodologies. The second part investigates more specific approaches, including simulation environments and mutation testing, corner cases and adversarial examples, fault injection, software safety cages, techniques for cyber-physical systems, and formal methods. Relevant approaches and related tools have been discussed and compared in order to highlight open issues and opportunities.}, 
keywords={Vehicles;Autonomous automobiles;Safety;Software;Accidents;Roads;Systematics;Advanced driver assistance systems;automotive engineering;autonomous vehicles;cyber-physical systems;formal verification;intelligent vehicles;machine learning;system testing;system validation;vehicle safety}, 
doi={10.1109/ACCESS.2020.3048047}, 
ISSN={2169-3536}, 
month={},}
@ARTICLE{50774, 
author={Ehrlich, W.K. and Lee, S.K. and Molisani, R.H.}, 
journal={IEEE Software}, 
title={Applying reliability measurement: a case study}, 
year={1990}, 
volume={7}, 
number={2}, 
pages={56-64}, 
abstract={The problem of knowing when to stop testing software is considered, focusing on the strategy of stopping when a reliability level or rate of failure occurrence acceptable to the customer is reached. The system's reliability is monitored throughout the system test, and the system is released to the field only when the measured reliability is at or above this objective. This approach was applied to test-failure data collected on Remote Measurement System-Digital 1, a large telecommunications testing system that had already gone through system test and been released to the field. The RMS-D1 failure data, which consisted of command-response errors versus commands executed, had been routinely collected by the system-test organization during testing. The testing phase analyzed, the load test, was an operational-profile-driven test in which a controlled load was imposed on the system reflective of the system's busy-hour usage pattern. It was found to be feasible to apply the reliability-measurement approach in real time, to systems actually undergoing system test, given a controlled load-test environment.<>}, 
keywords={System testing;Control systems;Software testing;Reliability;Condition monitoring;Remote monitoring;Pattern analysis;System buses;Real time systems}, 
doi={10.1109/52.50774}, 
ISSN={1937-4194}, 
month={March},}
@ARTICLE{8985259, 
author={Aziz, Omer and Farooq, Muhammad Shoaib and Abid, Adnan and Saher, Rubab and Aslam, Naeem}, 
journal={IEEE Access}, 
title={Research Trends in Enterprise Service Bus (ESB) Applications: A Systematic Mapping Study}, 
year={2020}, 
volume={8}, 
number={}, 
pages={31180-31197}, 
abstract={In recent years, enterprise service bus (ESB) has become a favorable adoption as a technology category in the IT industry as it provides secure and guaranteed delivery of services. The elasticity of Enterprise Service Bus (ESB) enables numerous applications to exchange information makes it a significant middleware layer responsible for transferring information in a Service-Oriented Architecture (SOA). ESB is presently the utmost promising tactic for the integration of business applications in distributed and diverse environments. It also offers essential infrastructure support for transforming messages or data, intelligent routing, and protocol transformation. The idea of ESBs emerged from the requirements to move out from traditional integration patterns, that becomes difficult to manage with the passage of time. Our study aim is to understand and provide ongoing research topics, challenges and future directions concerning ESB applications. A systematic mapping study (SMS) is therefore implemented to categorize the selected papers into the following classification: contribution type, ESB applications, research type and their approaches. We have extracted a total of twenty-two papers for this systematic study and they are classified according to defined criteria. The findings of this SMS are discussed and researchers were provided with suggestions on possible directions for future research.}, 
keywords={Service-oriented architecture;Systematics;Market research;Standards;Routing;Enterprise service bus (ESB);applications;classification;service oriented architecture (SOA);systematic mapping study (SMS);criteria}, 
doi={10.1109/ACCESS.2020.2972195}, 
ISSN={2169-3536}, 
month={},}
@ARTICLE{7548916, 
author={Lübke, Daniel and van Lessen, Tammo}, 
journal={IEEE Software}, 
title={Modeling Test Cases in BPMN for Behavior-Driven Development}, 
year={2016}, 
volume={33}, 
number={5}, 
pages={15-21}, 
abstract={Testing large-scale process integration solutions is complex and cumbersome. To tackle this problem, researchers employed behavior-driven development. They used the Business Process Model and Notation language to model domain-specific test cases. These test cases can be understood by both developers and business stakeholders and can be executed automatically.}, 
keywords={Simple object access protocol;Business process management;Modeling;Testing;Software engineering;Behaviorial sciences;business processes;Business Process Model and Notation;BPMN;behavior-driven development;BDD;test-driven development;TDD;software testing;software development;software engineering}, 
doi={10.1109/MS.2016.117}, 
ISSN={1937-4194}, 
month={Sep.},}
@INPROCEEDINGS{8836763, 
author={Sippl, Christoph and Bock, Florian and Lauer, Christoph and Heinz, Aaron and Neumayer, Thomas and German, Reinhard}, 
booktitle={2019 IEEE International Systems Conference (SysCon)}, 
title={Scenario-Based Systems Engineering: An Approach Towards Automated Driving Function Development}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In this contribution, we propose an approach for scenario-based systems engineering for automated driving functions. The growing complexity of future driving systems presents an enormous challenge for the development and testing processes, the validation and release procedure, as well as the organization of work. Accompanying, the end of development and the concerning effort is difficult to estimate. This often leads to postponements and rise of development costs. Furthermore, recent research activities show that established test methods such as statistical testing are not well suited for the validation/verification and the release process of automated driving functions. Thus, we propose a method for continuous usage of scenarios, embedded in the systems engineering process, to divide complex and intangible development goals in smaller solvable tasks. Furthermore, we suggest how scenarios can be used for validation and verification of the target system. In this approach, various artifacts are generated, which help to plan, organize, and trace the development and validation process. As a result, temporal expenditure and development costs get estimable. Moreover, the technical progress of the system-to-develop becomes apparent at an earlier point of time.}, 
keywords={Tools;Testing;Complexity theory;Task analysis;Requirements engineering;Hazards;scenario;systems engineering;automated driving;development and validation process}, 
doi={10.1109/SYSCON.2019.8836763}, 
ISSN={2472-9647}, 
month={April},}
@ARTICLE{8451922, 
author={SAYAGH, Mohammed and Kerzazi, Noureddine and Adams, Bram and Petrillo, Fabio}, 
journal={IEEE Transactions on Software Engineering}, 
title={Software Configuration Engineering in Practice Interviews, Survey, and Systematic Literature Review}, 
year={2020}, 
volume={46}, 
number={6}, 
pages={646-673}, 
abstract={Modern software applications are adapted to different situations (e.g., memory limits, enabling/disabling features, database credentials) by changing the values of configuration options, without any source code modifications. According to several studies, this flexibility is expensive as configuration failures represent one of the most common types of software failures. They are also hard to debug and resolve as they require a lot of effort to detect which options are misconfigured among a large number of configuration options and values, while comprehension of the code also is hampered by sprinkling conditional checks of the values of configuration options. Although researchers have proposed various approaches to help debug or prevent configuration failures, especially from the end users' perspective, this paper takes a step back to understand the process required by practitioners to engineer the run-time configuration options in their source code, the challenges they experience as well as best practices that they have or could adopt. By interviewing 14 software engineering experts, followed by a large survey on 229 Java software engineers, we identified 9 major activities related to configuration engineering, 22 challenges faced by developers, and 24 expert recommendations to improve software configuration quality. We complemented this study by a systematic literature review to enrich the experts' recommendations, and to identify possible solutions discussed and evaluated by the research community for the developers' problems and challenges. We find that developers face a variety of challenges for all nine configuration engineering activities, starting from the creation of options, which generally is not planned beforehand and increases the complexity of a software system, to the non-trivial comprehension and debugging of configurations, and ending with the risky maintenance of configuration options, since developers avoid touching and changing configuration options in a mature system. We also find that researchers thus far focus primarily on testing and debugging configuration failures, leaving a large range of opportunities for future work.}, 
keywords={Software systems;Interviews;Systematics;Facebook;Bibliographies;Software algorithms;Empirical study;configuration;configuration engineering;Systematic literature review;interviews;survey}, 
doi={10.1109/TSE.2018.2867847}, 
ISSN={1939-3520}, 
month={June},}
@INPROCEEDINGS{7140503, 
author={de Bayser, Maximilien and Azevedo, Leonardo G. and Cerqueira, Renato}, 
booktitle={2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
title={ResearchOps: The case for DevOps in scientific applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={1398-1404}, 
abstract={DevOps (a portmanteau of “development” and “operations”) is a software development method that extends the agile philosophy to rapidly produce software products and services and to improve operations performance and quality assurance. It was born to accelerate the delivery of Web-based systems and quickly bring new value to users. Many Web-based systems evolve according to usage trends without a clear long-term goal. Before the widespread use of Web services, most software with a clear goal were delivered as packages that users installed on their own system. New versions were delivered with a much lower frequency, with periods in between versions ranging from months to years. Development cycles were divided into large design, coding and testing phases culminating in the release of a new stable version. In software development in the context of applied science, even when the goal is clear, the process to attain it is not. Hence, working releases that capture the current software state must be released frequently in order to reduce the risks for all stakeholders and to make it possible to assess the current state of a project and steer it in the right direction. This paper explores the usefulness of DevOps concepts to improve the development of software that supports scientific projects. We establish the similarities and differences between scientific projects and Web applications development, and discuss where the related methodologies need to be extended. Unique challenges are discussed herewith developed solutions, and still open questions. Lessons learned are highlighted as best practices to be followed in research projects. This discussion is rooted in our experience in real-life projects at the IBM Research Brazil Lab, which just as well apply to other research institutions.}, 
keywords={Software;Testing;Prototypes;Conferences;Libraries;Servers;Production}, 
doi={10.1109/INM.2015.7140503}, 
ISSN={1573-0077}, 
month={May},}
@INPROCEEDINGS{6693140, 
author={Gambi, Alessio and Hummer, Waldemar and Dustdar, Schahram}, 
booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Automated testing of cloud-based elastic systems with AUToCLES}, 
year={2013}, 
volume={}, 
number={}, 
pages={714-717}, 
abstract={Cloud-based elastic computing systems dynamically change their resources allocation to provide consistent quality of service and minimal usage of resources in the face of workload fluctuations. As elastic systems are increasingly adopted to implement business critical functions in a cost-efficient way, their reliability is becoming a key concern for developers. Without proper testing, cloud-based systems might fail to provide the required functionalities with the expected service level and costs. Using system testing techniques, developers can expose problems that escaped the previous quality assurance activities and have a last chance to fix bugs before releasing the system in production. System testing of cloud-based systems accounts for a series of complex and time demanding activities, from the deployment and configuration of the elastic system, to the execution of synthetic clients, and the collection and persistence of execution data. Furthermore, clouds enable parallel executions of the same elastic system that can reduce the overall test execution time. However, manually managing the concurrent testing of multiple system instances might quickly overwhelm developers' capabilities, and automatic support for test generation, system test execution, and management of execution data is needed. In this demo we showcase AUToCLES, our tool for automatic testing of cloud-based elastic systems. Given specifications of the test suite and the system under test, AUToCLES implements testing as a service (TaaS): It automatically instantiates the SUT, configures the testing scaffoldings, and automatically executes test suites. If required, AUToCLES can generate new test inputs. Designers can inspect executions both during and after the tests.}, 
keywords={Elasticity;Monitoring;Standards;System testing;Cloud computing}, 
doi={10.1109/ASE.2013.6693140}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{878373, 
author={Polk, J.E. and Kakuda, R.Y. and Anderson, J.R. and Brophy, J.R. and Rawlin, V.K. and Sovey, J. and Hamley, J.}, 
booktitle={2000 IEEE Aerospace Conference. Proceedings (Cat. No.00TH8484)}, 
title={In-flight performance of the NSTAR ion propulsion system on the Deep Space One mission}, 
year={2000}, 
volume={4}, 
number={}, 
pages={123-148 vol.4}, 
abstract={Deep Space 1 is the first interplanetary spacecraft to use an ion propulsion system for the primary delta-v maneuvers. The purpose of the mission is to validate a number of technologies, including ion propulsion and a high degree of spacecraft autonomy, on a flyby of an asteroid and two comets. The ion propulsion system has operated for a total of 3500 hours at engine power levels ranging from 0.48 to 1.94 kW and has completed the encounter with the asteroid 1992KD and the first set of deterministic burns required for a 2001 encounter with comet Wilson-Harrington. The system has worked extremely well after an initial grid short was cleared after launch. Operation during this primary mission phase has demonstrated all ion propulsion system and autonomous navigation functions. All propulsion system operating parameters are very close to the expected values with the exception of the thrust at higher power levels, which is about 2 percent lower than that calculated from the electrical parameters. This paper provides an overview of the system and presents the first flight validation data on an ion propulsion system in interplanetary space.}, 
keywords={Propulsion;Space missions;Space technology;Space vehicles;Engines;NASA;Xenon;Instruments;Control systems;Feeds}, 
doi={10.1109/AERO.2000.878373}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{9127413, 
author={Górski, Tomasz and Bednarski, Jakub}, 
journal={IEEE Access}, 
title={Applying Model-Driven Engineering to Distributed Ledger Deployment}, 
year={2020}, 
volume={8}, 
number={}, 
pages={118245-118261}, 
abstract={Distributed Ledger Technology (DLT) enables data storage in a decentralized manner among collaborating parties. The software architecture of such solutions encompasses models placed in the relevant architectural views. A lot of research is devoted to smart contracts and consensus algorithms, which are realized by distributed applications and can be positioned within the Logical view. However, we see the need to provide modeling support for the Deployment view of distributed ledger solutions. Especially since the chosen DLT framework has a significant impact on implementation and deployment. Besides, consistency between models and configuration deployment scripts should be ensured. So, we have applied Model-Driven Engineering (MDE) that allows on the transformation of models into more detailed models, source code, or tests. We have proposed Unified Modeling Language (UML) stereotypes and tagged values for distributed ledger deployment modeling and placed them in the UML Profile for Distributed Ledger Deployment. We have also designed the UML2Deployment model-to-code transformation for the R3 Corda DLT framework. A UML Deployment model is the source whereas a Gradle Groovy deployment script is the target of the transformation. We have provided the complete solution by incorporating the transformation into the Visual Paradigm modeling tool. Furthermore, we have designed a dedicated plug-in to validate generated deployment scripts. In the paper, we have shown how to design transformation for generating deployment scripts for the R3 Corda DLT framework with the ability to switch to another one.}, 
keywords={Unified modeling language;Distributed ledger;Object oriented modeling;Blockchain;Distributed databases;Model driven engineering;Consensus algorithm;Distributed ledger;model-driven engineering;architectural views model 1+5;deployment view;unified modeling language extensibility mechanisms}, 
doi={10.1109/ACCESS.2020.3005519}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{7886987, 
author={Kim, Jongwook and Batory, Don and Dig, Danny and Azanza, Maider}, 
booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)}, 
title={Improving Refactoring Speed by 10X}, 
year={2016}, 
volume={}, 
number={}, 
pages={1145-1156}, 
abstract={Refactoring engines are standard tools in today's Integrated Development Environments (IDEs). They allow programmers to perform one refactoring at a time, but programmers need more. Most design patterns in the Gang-of-Four text can be written as a refactoring script - a programmatic sequence of refactorings. In this paper, we present R3, a new Java refactoring engine that supports refactoring scripts. It builds a main-memory, non-persistent database to encode Java entity declarations (e.g., packages, classes, methods), their containment relationships, and language features such as inheritance and modifiers. Unlike classical refactoring engines that modify Abstract Syntax Trees (ASTs), R3 refactorings modify only the database; refactored code is produced only when pretty-printing ASTs that reference database changes. R3 performs comparable precondition checks to those of the Eclipse Java Development Tools (JDT) but R3's codebase is about half the size of the JDT refactoring engine and runs an order of magnitude faster. Further, a user study shows that R3 improved the success rate of retrofitting design patterns by 25% up to 50%.}, 
keywords={Java;Engines;Databases;Graphics;Computer bugs;Graphical user interfaces;Maintenance engineering}, 
doi={10.1145/2884781.2884802}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{5771265, 
author={Madhavapeddy, Anil and Singh, Satnam}, 
booktitle={2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines}, 
title={Reconfigurable Data Processing for Clouds}, 
year={2011}, 
volume={}, 
number={}, 
pages={141-145}, 
abstract={Reconfigurable computing in the cloud helps to solve many practical problems relating to scaling out data-centers where computation is limited by energy consumption or latency. However, for reconfigurable computing in the cloud to become practical several research challenges have to be addressed. This paper identifies some of the perquisites for reconfigurable computing systems in the cloud and picks out several scenarios made possible with immense cloud-based computing capability.}, 
keywords={Field programmable gate arrays;Cloud computing;USA Councils;Hardware;Computational modeling;Programming;Operating systems;reconfigurable computing;cloud computing}, 
doi={10.1109/FCCM.2011.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5070969, 
author={Jaaskelainen, Antti and Katara, Mika and Kervinen, Antti and Maunumaa, Mika and Paakkonen, Tuula and Takala, Tommi and Virtanen, Heikki}, 
booktitle={2009 31st International Conference on Software Engineering - Companion Volume}, 
title={Automatic GUI test generation for smartphone applications - an evaluation}, 
year={2009}, 
volume={}, 
number={}, 
pages={112-122}, 
abstract={We present the results of an evaluation where we studied the effectiveness of automatic test generation for graphical user interface (GUI) testing of smartphone applications. To describe the context of our evaluation, the tools and the test model library we have developed for the evaluation are also presented. The library contains test models for basic S60 applications, such as camera, contacts, etc. The tools include an on-line test generator that produces sequences of so called keywords to be executed on the test targets. In our evaluation, we managed to find over 20 defects from applications that had been on the market for several months. We also describe the problems we faced during the evaluation.}, 
keywords={Graphical user interfaces;Automatic testing;Software testing;System testing;Application software;Software systems;Libraries;Cameras;Optical character recognition software;Context modeling}, 
doi={10.1109/ICSE-COMPANION.2009.5070969}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8730198, 
author={Koo, Jinkyu and Saumya, Charitha and Kulkarni, Milind and Bagchi, Saurabh}, 
booktitle={2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)}, 
title={PySE: Automatic Worst-Case Test Generation by Reinforcement Learning}, 
year={2019}, 
volume={}, 
number={}, 
pages={136-147}, 
abstract={Stress testing is an important task in software testing, which examines the behavior of a program under a heavy load. Symbolic execution is a useful tool to find out the worst-case input values for the stress testing. However, symbolic execution does not scale to a large program, since the number of paths to search grows exponentially with an input size. So far, such a scalability issue has been mostly managed by pruning out unpromising paths in the middle of searching based on heuristics, but this kind of work easily eliminates the true worst case as well, providing sub-optimal one only. Another way to achieve scalability is to learn a branching policy of worst-case complexity from small scale tests and apply it to a large scale. However, use cases of such a method are restricted to programs whose worst-case branching policy has a simple pattern. To address such limitations, we propose PySE that uses symbolic execution to collect the behaviors of a given branching policy, and updates the policy using a reinforcement learning approach through multiple executions. PySE's branching policy keeps evolving in a way that the length of an execution path increases in the long term, and ultimately reaches the worst-case complexity. PySE can also learn the worst-case branching policy of a complex or irregular pattern, using an artificial neural network in a fully automatic way. Experiment results demonstrate that PySE can effectively find a path of worst-case complexity for various Python benchmark programs and scales.}, 
keywords={Complexity theory;Testing;Explosions;Stress;Python;History;Genetic algorithms;Machine learning;Q-learning;Symbolic execution;Worst-case complexity;Stress testing}, 
doi={10.1109/ICST.2019.00023}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{6135869, 
author={Ferguson, Donald F. and Hadar, Ethan}, 
booktitle={2011 8th International Conference & Expo on Emerging Technologies for a Smarter World}, 
title={Optimizing the IT business supply chain utilizing cloud computing}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Information technology applications and systems are essential to businesses and enterprises as they implement business components of the enterprise. In some cases, IT is the business, such as with financial services. Optimizing Return-on-Investment (ROI) in the IT area is essential to the business performance. Reducing cost is one component of ROI, however the predominant value is increasing revenue. IT is essential to the enterprise agilely to exploit new business opportunities. Cloud computing is emerging as a technology for optimizing IT costs and supporting agility. Enterprises are incrementally moving to cloud computing in an exploratory, ad hoc manner. Since, enterprises think in terms of IT Services that IT provides to the business, and an IT service is interconnecting hardware and software resources, the management aspects are conceptually similar to a manufacture or retail supply chain. As a result, exploiting cloud computing is a supply chain management problem for IT services using cloud computing. This paper describes the architecture requirements and implementation of a set of components for optimizing the IT supply chain using cloud computing.}, 
keywords={Business;Optimization;Servers;Monitoring;Cloud computing;Hardware;Automation;enterprise IT;cloud archtiecture;supply chain management;composite IT system}, 
doi={10.1109/CEWIT.2011.6135869}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8539069, 
author={Mai, Phu X. and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel C.}, 
booktitle={2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)}, 
title={A Natural Language Programming Approach for Requirements-Based Security Testing}, 
year={2018}, 
volume={}, 
number={}, 
pages={58-69}, 
abstract={To facilitate communication among stakeholders, software security requirements are typically written in natural language and capture both positive requirements (i.e., what the system is supposed to do to ensure security) and negative requirements (i.e., undesirable behavior undermining security). In this paper, we tackle the problem of automatically generating executable security test cases from security requirements in natural language (NL). More precisely, since existing approaches for the generation of test cases from NL requirements verify only positive requirements, we focus on the problem of generating test cases from negative requirements. We propose, apply and assess Misuse Case Programming (MCP), an approach that automatically generates security test cases from misuse case specifications (i.e., use case specifications capturing the behavior of malicious users). MCP relies on natural language processing techniques to extract the concepts (e.g., inputs and activities) appearing in requirements specifications and generates executable test cases by matching the extracted concepts to the members of a provided test driver API. MCP has been evaluated in an industrial case study, which provides initial evidence of the feasibility and benefits of the approach.}, 
keywords={Ontologies;Software reliability;Natural languages;Test pattern generators;Authorization;Computer languages;Password;System Security Testing;Natural Language Requirements;Natural Language Processing;Natural Language Programming}, 
doi={10.1109/ISSRE.2018.00017}, 
ISSN={2332-6549}, 
month={Oct},}
@ARTICLE{8352077, 
author={Rubinov, Konstantin and Baresi, Luciano}, 
journal={Computer}, 
title={What Are We Missing When Testing Our Android Apps?}, 
year={2018}, 
volume={51}, 
number={4}, 
pages={60-68}, 
abstract={Android’s broad adoption drives the development of millions of new apps. Apps on this OS are not just trivial games; many of them handle sensitive information, exhibit complex structure, and require high reliability and trustworthiness. The authors discuss the problem of testing Android apps—the results achieved with current approaches, and what is still missing and requires fresh solutions.}, 
keywords={Software testing;Androids;Humanoid robots;Software development;Graphical user interfaces;Analytical models;Runtime;Computer applications;Android;mobile;software testing;mobile computing;mobile applications;debugging}, 
doi={10.1109/MC.2018.2141024}, 
ISSN={1558-0814}, 
month={April},}
@ARTICLE{9118898, 
author={Mukhiya, Suresh Kumar and Wake, Jo Dugstad and Inal, Yavuz and Lamo, Yngve}, 
journal={IEEE Access}, 
title={Adaptive Systems for Internet-Delivered Psychological Treatments}, 
year={2020}, 
volume={8}, 
number={}, 
pages={112220-112236}, 
abstract={Internet-Delivered Psychological Treatments (IDPT) are based on evidence-based psychological treatment models adjusted for interaction through the Internet. The use of Internet technologies has the potential to increase the availability of evidence-based mental health services for a far-reaching population with the use of fewer resources. Despite evidence that Internet Interventions can be effective means in mental health morbidities, most current IDPT systems are tunnel-based, inflexible, and non-interoperable. Hence it becomes essential to understand which elements of an Internet intervention contribute to effectiveness and treatment outcomes. By analogy, adaptation is a central aspect of successful face-to-face mental health therapy. Adaptability to patient needs can be regarded as an essential outcome factor in online systems for mental health interventions as well. While some aspects of rule-based and machine-learning-based adaptation have attracted attention in recent IDPT development, systematic reporting of core components, dimensions of adaptiveness, information architecture, and strategies for adaptation in the IDPT system are still lacking. To bridge this gap, we propose a model that shows how adaptive systems are represented in classical control theory and discuss how the model can be used to specify adaptive IDPT systems. Concerning the reference model, we outline the core components of adaptive IDPT systems, the main adaptive elements, dimensions of adaptiveness, information architecture applied to adaptive systems, and strategies used in the adaptation process. We also provide comprehensive guidelines on how to develop an adaptive IDPT system based on the Person-Based Approach.}, 
keywords={Adaptive systems;Mental health;Internet;Medical treatment;Adaptation models;Information architecture;Adaptive systems;Internet delivered psychological treatments;person based approach;information architecture;personalized Internet interventions;tailored Internet interventions;ICBT}, 
doi={10.1109/ACCESS.2020.3002793}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8718229, 
author={King, Tariq M. and Arbon, Jason and Santiago, Dionny and Adamo, David and Chin, Wendy and Shanmugam, Ram}, 
booktitle={2019 IEEE International Conference On Artificial Intelligence Testing (AITest)}, 
title={AI for Testing Today and Tomorrow: Industry Perspectives}, 
year={2019}, 
volume={}, 
number={}, 
pages={81-88}, 
abstract={With modern advances in artificial intelligence (AI) and machine learning and their applications to software testing, the intersection of AI and testing is receiving close attention. The 2018 Annual Western Conference on Software Testing Analysis and Review featured a two-session panel on AI for Software Testing (AIST). The panel brought together six industry experts with experience developing AIST products, services, and research prototypes. Questions sourced from the industrial testing community were used to provoke thought, stimulate conversation, and guide panel discussions. This paper provides a review of the industry panel, which includes discussions on the visions, ideas, thoughts, strategies, directions, and lessons learned developing systems that use AI to test software, applying methods to test AI systems, and designing self-testing systems. Both the testing community survey and the expert panel yielded insightful perspectives on AIST in practice.}, 
keywords={Artificial intelligence;Software;Manuals;Software testing;Industries;Built-in self-test;Artificial Intelligence;Machine Learning;Software Testing;Industry;Panel;Survey}, 
doi={10.1109/AITest.2019.000-3}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6823865, 
author={Lackner, Hartmut and Thomas, Martin and Wartenberg, Florian and Weißleder, Stephan}, 
booktitle={2014 IEEE Seventh International Conference on Software Testing, Verification and Validation}, 
title={Model-Based Test Design of Product Lines: Raising Test Design to the Product Line Level}, 
year={2014}, 
volume={}, 
number={}, 
pages={51-60}, 
abstract={System quality assurance techniques like testing are important for high-quality products and processes. The effort for applying them is usually high, but can be reduced using automation. Automated test design is possible by using models to specify test-relevant aspects and by generating tests on this basis. Testing multiple variants of a system like, e.g., a product line of a German car manufacturer, results in a significant, additional effort. In this paper, we deal with model-based testing of product lines. We combine feature models that are used to describe product lines and models that are used for automated model-based test design. Our main contribution is the definition of a test generation approach on the product line level, i.e., that does not depend on resolving single product variants. Furthermore, we compare our approach to other test generation approaches and evaluate it using our tool chain SPLTestbench for some product line examples.}, 
keywords={Unified modeling language;Testing;Biological system modeling;Standards;Security;Quality assurance;Credit cards;Software Product Lines;Quality Assurance;Software Testing;Model-Based Testing;Software Reuse;Domain Level Testing}, 
doi={10.1109/ICST.2014.16}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{7338242, 
author={Kusel, Angelika and Etzlstorfer, Jürgen and Kapsammer, Elisabeth and Retschitzegger, Werner and Schwinger, Wieland and Schönböck, Johannes}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={Consistent co-evolution of models and transformations}, 
year={2015}, 
volume={}, 
number={}, 
pages={116-125}, 
abstract={Evolving metamodels are in the center of Model-Driven Engineering, necessitating the co-evolution of dependent artifacts like models and transformations. While model co-evolution has been extensively studied, transformation co-evolution has received less attention up to now. Current approaches for transformation co-evolution provide a fixed, restricted set of metamodel (MM) changes, only. Furthermore, composite changes are treated as monolithic units, which may lead to inconsistent co-evolution for overlapping atomic changes and prohibits extensibility. Finally, transformation co-evolution is considered in isolation, possibly inducing inconsistencies between model and transformation co-evolution. To overcome these limitations, we propose a complete set of atomic MM changes being able to describe arbitrary MM evolutions. Reusability and extensibility are supported by means of change composition, ensuring an intra-artifact consistent co-evolution. Furthermore, each change provides resolution actions for both, models and transformations, ensuring an inter-artifact consistent co-evolution. Based on our conceptual approach, a prototypical implementation is presented.}, 
keywords={Biological system modeling;Semantics;Systematics;Feature extraction;Software;Syntactics;Companies}, 
doi={10.1109/MODELS.2015.7338242}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8539067, 
author={Camilli, Matteo and Bellettini, Carlo and Gargantini, Angelo and Scandurra, Patrizia}, 
booktitle={2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)}, 
title={Online Model-Based Testing under Uncertainty}, 
year={2018}, 
volume={}, 
number={}, 
pages={36-46}, 
abstract={Modern software systems are required to operate in a highly uncertain and changing environment. They have to control the satisfaction of their requirements at run-time, and possibly adapt and cope with situations that have not been completely addressed at design-time. Software engineering methods and techniques are, more than ever, forced to deal with change and uncertainty (lack of knowledge) explicitly. For tackling the challenge posed by uncertainty in delivering more reliable systems, this paper proposes a novel online Model-based Testing technique that complements classic test case generation based on pseudo-random sampling strategies with an uncertainty-aware sampling strategy. To deal with system uncertainty during testing, the proposed strategy builds on an Inverse Uncertainty Quantification approach that is related to the discrepancy between the measured data at run-time (while the system executes) and a Markov Decision Process model describing the behavior of the system under test. To this purpose, a conformance game approach is adopted in which tests feed a Bayesian inference calibrator that continuously learns from test data to tune the system model and the system itself. A comparative evaluation between the proposed uncertainty-aware sampling policy and classical pseudo-random sampling policies is also presented using the Tele Assistance System running example, showing the differences in achieved accuracy and efficiency.}, 
keywords={Uncertainty;Bayes methods;Probabilistic logic;Testing;Markov processes;Software reliability;Uncertainty quantification;Reliability under Uncertainty;Bayesian Calibration;Online Model-based Testing}, 
doi={10.1109/ISSRE.2018.00015}, 
ISSN={2332-6549}, 
month={Oct},}
@INPROCEEDINGS{8534549, 
author={Schneider, Michael and Hippchen, Benjamin and Abeck, Sebastian and Jacoby, Michael and Herzog, Reinhard}, 
booktitle={2018 Global Internet of Things Summit (GIoTS)}, 
title={Enabling IoT Platform Interoperability Using a Systematic Development Approach by Example}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Today, the IoT landscape consists of a large number of vertical IoT platforms that are rarely interconnected. To enable creation of applications across platforms and domain boundaries interoperability needs to be established between IoT platforms. In this paper we present how this task can be simplified by utilizing a systematic software development process based on behavior- and domain-driven development. This process is illustrated using an example that uses the open source IoT interoperability framework symbIoTe to connect two indoor navigation platforms. We show that developers can actually profit from this approach but existing IoT interoperability frameworks are still cumbersome to use.}, 
keywords={Interoperability;Semantics;Internet of Things;Software;Syntactics;Standards;Registers;Internet of Things;IoT;interoperability;semantic interoperability;behavior-driven development;domain-driven design}, 
doi={10.1109/GIOTS.2018.8534549}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1383107, 
author={Lil, M. and Wei, Y. and Desovski, D. and Nejad, H. and Ghose, S. and Cukic, B. and Smidts, C.}, 
booktitle={15th International Symposium on Software Reliability Engineering}, 
title={Validation of a methodology for assessing software reliability}, 
year={2004}, 
volume={}, 
number={}, 
pages={66-76}, 
abstract={Software-based digital systems are progressively replacing analog systems in safety-critical applications. However the ability to predict their reliability is not well understood and needs further study. A first step towards systematic resolution of this issue was presented in a recent software engineering measure study. In that study a set of software engineering measures were ranked with respect to their ability in predicting software reliability through an expert opinion elicitation process. This study also proposed a concept of reliability prediction system (RePS) to bridge the gap between software engineering measures and software reliability. The research presented in this paper validates the rankings obtained and the concept of RePS proposed in the previous study.}, 
keywords={Software reliability;Software measurement;Software engineering;Reliability engineering;Phase measurement;Application software;Object oriented modeling;Usability;Software testing;Educational institutions}, 
doi={10.1109/ISSRE.2004.47}, 
ISSN={1071-9458}, 
month={Nov},}
@INPROCEEDINGS{843835, 
author={Bommireddy, A. and Khare, J. and Shaikh, S. and Su, S.-T.}, 
booktitle={Proceedings 18th IEEE VLSI Test Symposium}, 
title={Test and debug of networking SoCs-a case study}, 
year={2000}, 
volume={}, 
number={}, 
pages={121-126}, 
abstract={This paper describes the test challenges faced and testability features implemented on Level One's networking System on Chip (SoC), IXE2000. The IXE2000 SoC is a 20+ million transistor Layer 2/3/4 Switch with 24 10/100 Mbps and 2 1000 Mbps Ethernet ports, and a predominantly IP-based design. The chip had constraints in terms of both design time and total system costs, which added an extra burden on test. The paper discusses how these constraints led to the current testability solutions and debug features on the chip.}, 
keywords={Computer aided software engineering;Switches;Clocks;System testing;Ethernet networks;System-on-a-chip;Costs;Business communication;Design for manufacture;IP networks}, 
doi={10.1109/VTEST.2000.843835}, 
ISSN={1093-0167}, 
month={April},}
@INPROCEEDINGS{9012666, 
author={Kundel, Ralf and Nobach, Leonhard and Blendin, Jeremias and Kolbe, Hans-Joerg and Schyguda, Georg and Gurevich, Vladimir and Koldehofe, Boris and Steinmetz, Ralf}, 
booktitle={2019 15th International Conference on Network and Service Management (CNSM)}, 
title={P4-BNG: Central Office Network Functions on Programmable Packet Pipelines}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Large-scale telecommunications providers have to continuously challenge and evolve their network infrastructure to efficiently serve growing markets demands. They must increase performance, lower time-to-market, provide new services, and lower the cost of the infrastructure and its operation. Network Functions Virtualization (NFV) on commodity hardware offers an attractive, low-cost platform to establish innovations much faster than with purpose-built hardware products. Unfortunately, implementing NFV on commodity processors does not match the performance requirements of the high-throughput data plane components in large carrier access networks. In this article, we propose a way to offer residential network access with programmable packet processing architectures. Based on the highly flexible P4 programming language, we present a design and open source implementation of a BNG data plane that meets the challenging demands of Broadband Network Gateways in carrier-grade environments. The proposed evaluation results show the desired performance characteristics and our proposed design together with upcoming P4 hardware can offer a giant leap towards highest performance NFV network access.}, 
keywords={Hardware;Central office;Logic gates;Protocols;Pipelines;Network function virtualization;NFV;P4;Access Networks;Network Functions;Hardware Acceleration;Computer Networks}, 
doi={10.23919/CNSM46954.2019.9012666}, 
ISSN={2165-963X}, 
month={Oct},}
@ARTICLE{7206603, 
author={Jamro, Marcin}, 
journal={IEEE Transactions on Industrial Informatics}, 
title={POU-Oriented Unit Testing of IEC 61131-3 Control Software}, 
year={2015}, 
volume={11}, 
number={5}, 
pages={1119-1129}, 
abstract={Software testing is an important part of project development. Depending on system type and size, it is performed variously. Unit testing is one of the available approaches that is used to ensure that behavior of small software parts is consistent with requirements. It allows to improve software quality and decrease overall costs. Despite the fact that such an approach is commonly judged as a vital concept, it is not usual in control software. In this paper, the comprehensive approach to test the IEC 61131-3 software using unit tests is presented. It supports to create tests in two ways-either in textual and graphical IEC 61131-3 languages or in the CPTest+ dedicated test definition language. The latter is equipped with many advanced features, such as test fixtures and inclusions, parameterized and analog signal extensions, mock objects, as well as a few kinds of suites. The overall solution runs on the developer and testing station; hence, it does not have significant impact on performance of the control program and tests are more reliable and repeatable. To explain the concept, the simple running example is presented in this paper. The described solution has been introduced in the CPDev engineering environment for programming controllers.}, 
keywords={Testing;Software;IEC Standards;Informatics;Automation;Control systems;control software;IEC 61131-3;testing;unit test;Control software;IEC 61131-3;testing;unit test}, 
doi={10.1109/TII.2015.2469257}, 
ISSN={1941-0050}, 
month={Oct},}
@INPROCEEDINGS{6908678, 
author={Granda, Maria Fernanda and Condori-Fernández, Nelly and Vos, Tanja E.J. and Pastor, Oscar}, 
booktitle={2014 IEEE 1st International Workshop on Requirements Engineering and Testing (RET)}, 
title={Towards the automated generation of abstract test cases from requirements models}, 
year={2014}, 
volume={}, 
number={}, 
pages={39-46}, 
abstract={In a testing process, the design, selection, creation and execution of test cases is a very time-consuming and error-prone task when done manually, since suitable and effective test cases must be obtained from the requirements. This paper presents a model-driven testing approach for conceptual schemas that automatically generates a set of abstract test cases, from requirements models. In this way, tests and requirements are linked together to find defects as soon as possible, which can considerably reduce the risk of defects and project reworking. The authors propose a generation strategy which consists of: two meta-models, a set of transformations rules which are used to generate a Test Model, and the Abstract Test Cases from an existing approach to communication-oriented Requirements Engineering; and an algorithm based on Breadth-First Search. A practical application of our approach is included.}, 
keywords={Unified modeling language;Testing;Abstracts;Object oriented modeling;Analytical models;Concrete;Business;Requirements-based testing;Communication Analysis;Model-driven testing;Conceptual Schema Testing;Test Model Generation;Test Case Generation}, 
doi={10.1109/RET.2014.6908678}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{4730478, 
author={Wieczorek, Sebastian and Roth, Andreas and Stefanescu, Alin and Charfi, Anis}, 
booktitle={2008 IEEE International Symposium on Service-Oriented System Engineering}, 
title={Precise Steps for Choreography Modeling for SOA Validation and Verification}, 
year={2008}, 
volume={}, 
number={}, 
pages={148-153}, 
abstract={Service-oriented architecture (SOA) enables organizations to transform their existing IT infrastructure into a more flexible business process platform. In this architecture, decoupled components that provide standard services can be composed to form individually configured and highly flexible applications. When building such applications it is important to have a formal specification of the interaction protocols between the composed services not only because such a specification provides an accurate and unambiguous description of the interactions and their ordering but also to enable automated verification and validation. In this paper, we present a case study from the SAP context showing the interactions between two SAP service components and use that case study to derive a set of modeling requirements. This motivates a discussion about applicable techniques for service choreography modeling and whether existing choreography languages cover the identified needs.}, 
keywords={Semiconductor optical amplifiers;Service oriented architecture;Enterprise resource planning;Marketing and sales;Software systems;Systems engineering and theory;Buildings;Formal specifications;Protocols;Context-aware services;SOA;Choreography;Service;Modeling}, 
doi={10.1109/SOSE.2008.43}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{4138229, 
author={Sombrutzki, Robert and Zubow, Anatolij and Kurth, Mathias and Redlich, Jens-Peter}, 
booktitle={2006 1st Workshop on Operator-Assisted (Wireless Mesh) Community Networks}, 
title={Self-Organization in Community Mesh Networks The Berlin RoofNet}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-11}, 
abstract={A community network must be usable for inexperienced end users; thus self-organization is essential. On the one hand, we propose an approach for self-organization in ad-hoc wireless multi-hop mesh networks, where the client is fully freed from such mundane tasks as IP configuration, etc. On the other hand, the community mesh network itself is fully self-organized thus no operator or provider is required. We present the architecture of the Berlin RoofNet (BRN) and a distributed realization of services like DHCP, ARP and Internet gateway discovery and selection. In addition, results of a detailed simulation and experimental evaluation comparing our distributed hash table based approach to traditional methods are presented. We show that our approach is more reliable, efficient and responsive}, 
keywords={Mesh networks;Computer architecture;Spread spectrum communication;IP networks;Cities and towns;Protocols;Web and internet services;Computer network reliability;Telecommunication network reliability;Wireless mesh networks;Community Networks;Self-Organization;Distributed Hash Table}, 
doi={10.1109/WOACN.2006.337188}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5615100, 
author={Torens, Christoph and Ebrecht, Lars}, 
booktitle={2010 Fifth International Conference on Software Engineering Advances}, 
title={RemoteTest: A Framework for Testing Distributed Systems}, 
year={2010}, 
volume={}, 
number={}, 
pages={441-446}, 
abstract={This work deals with general difficulties and aims when testing complex distributed systems, especially when heterogeneous interfaces are used. As a solution RemoteTest is proposed, a framework for the test of distributed systems and their interfaces. This is done by integrating individual system components into a virtual environment that emulates the adjacent modules of the system. The interface details are thereby abstracted by the framework and there is no special interface knowledge necessary by the tester. In addition to the decoupling of components and interface abstraction, RemoteTest facilitates the testing of distributed systems with flexible mechanisms to write test scripts and an architecture that can be easily adapted to different systems.}, 
keywords={Testing;Software;Computer architecture;Virtual environment;Hardware;Programming;Complexity theory;software test;distributed systems;test framework;test tools;test methods}, 
doi={10.1109/ICSEA.2010.75}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7107423, 
author={Gmeiner, Johannes and Ramler, Rudolf and Haslinger, Julian}, 
booktitle={2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={Automated testing in the continuous delivery pipeline: A case study of an online company}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Companies running an online business need to be able to frequently push new features and bug fixes from development into production. Successful high-performance online companies deliver code changes often several times per day. Their continuous delivery model supports the business needs of the online world. At the same time, however, such practices increase the risk of introducing quality issues and unwanted side effects. Rigorous test automation is therefore a key success factor for continuous delivery. In this paper we describe how automated testing is used in the continuous delivery pipeline of an Austrian online business company. The paper illustrates the complex technical and organizational challenges involved and summarizes the lessons from more than six years of practical experience in establishing and maintaining an effective continuous delivery pipeline.}, 
keywords={Pipelines;Testing;Databases;Companies;Production;Software;automated testing;continuous integration;continuous delivery;continusous deployment;dev ops}, 
doi={10.1109/ICSTW.2015.7107423}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6958413, 
author={Kracht, Jeshua S. and Petrovic, Jacob Z. and Walcott-Justice, Kristen R.}, 
booktitle={2014 14th International Conference on Quality Software}, 
title={Empirically Evaluating the Quality of Automatically Generated and Manually Written Test Suites}, 
year={2014}, 
volume={}, 
number={}, 
pages={256-265}, 
abstract={The creation, execution, and maintenance of tests are some of the most expensive tasks in software development. To help reduce the cost, automated test generation tools can be used to assist and guide developers in creating test cases. Yet, the tests that automated tools produce range from simple skeletons to fully executable test suites, hence their complexity and quality vary. This paper compares the complexity and quality of test suites created by sophisticated automated test generation tools to that of developer-written test suites. The empirical study in this paper examines ten real-world programs with existing test suites and applies two state-of-the-art automated test generation tools. The study measures the resulting test suite quality in terms of code coverage and fault-finding capability. On average, manual tests covered 31.5% of the branches while the automated tools covered 31.8% of the branches. In terms of mutation score, the tests generated by automated tools had an average mutation score of 39.8% compared to the average mutation score of 42.1% for manually written tests. Even though automatically created tests often contain more lines of source code than those written by developers, this paper's empirical results reveal that test generation tools can provide value by creating high quality test suites while reducing the cost and effort needed for testing.}, 
keywords={Manuals;Complexity theory;Software;Testing;Writing;Standards;Java}, 
doi={10.1109/QSIC.2014.33}, 
ISSN={2332-662X}, 
month={Oct},}
@INPROCEEDINGS{6759193, 
author={Clark, Tony and Kulkarni, Vinay and Barn, Balbir and France, Robert and Frank, Ulrich and Turk, Dan}, 
booktitle={2014 47th Hawaii International Conference on System Sciences}, 
title={Towards the Model Driven Organization}, 
year={2014}, 
volume={}, 
number={}, 
pages={4817-4826}, 
abstract={Modern organizations are faced with the need to rapidly respond to frequent changes arising from external business pressures. The effect of such continuous evolution eventually leads to organizational misalignment, that is, situations in which sub-optimal configurations of underlying systems significantly reduce an organization's ability to meet its strategic goals. Ensuring alignment of an organization's systems and its goals has been a concern of researchers and practitioners in the enterprise architecture (EA) domain. Unfortunately, current approaches do not adequately address alignment problems that modern organizations face. In this paper we propose that alignment concerns can be better addressed by making models the primary entities that stakeholders within and outside of an organization use to interact with the organization. We call an organization that maintains and uses an integrated set of models to manage alignment concerns a Model Driven Organization (MDO). In this paper we characterize the alignment problem, discuss the shortcomings of current alignment management approaches and present our MDO vision.}, 
keywords={Organizations;Analytical models;Context modeling;Adaptation models;Electronic mail;Context;Enterprise Architecture;Enterprise Modelling;Simulation}, 
doi={10.1109/HICSS.2014.591}, 
ISSN={1530-1605}, 
month={Jan},}
@ARTICLE{9373305, 
author={Alnafessah, Ahmad and Gias, Alim Ul and Wang, Runan and Zhu, Lulai and Casale, Giuliano and Filieri, Antonio}, 
journal={IEEE Access}, 
title={Quality-Aware DevOps Research: Where Do We Stand?}, 
year={2021}, 
volume={9}, 
number={}, 
pages={44476-44489}, 
abstract={DevOps is an emerging paradigm that reduces the barriers between developers and operations teams to offer continuous fast delivery and enable quick responses to changing requirements within the software life cycle. A significant volume of activity has been carried out in recent years with the aim of coupling DevOps stages with tools and methods to improve the quality of the produced software and the underpinning delivery methodology. While the research community has produced a sustained effort by conducting numerous studies and innovative development tools to support quality analyses within DevOps, there is still a limited cohesion between the research themes in this domain and a shortage of surveys that holistically examine quality engineering work within DevOps. In this paper, we address the gap by comprehensively surveying existing efforts in this area, categorizing them according to the stage of the DevOps lifecycle to which they primarily contribute. The survey holistically spans across all the DevOps stages, identify research efforts to improve architectural design, modeling and infrastructure-as-code, continuous-integration/continuous-delivery (CI/CD), testing and verification, and runtime management. Our analysis also outlines possible directions for future work in quality-aware DevOps, looking in particular at AI for DevOps and DevOps for AI software.}, 
keywords={Software;Testing;Artificial intelligence;Computer architecture;Tools;Production;Software architecture;DevOps;CI/CD;infrastructure as code;testing;artificial intelligence;verification}, 
doi={10.1109/ACCESS.2021.3064867}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{1559629, 
author={Noll, J. and Steel, R.}, 
booktitle={2005 IEEE Aerospace Conference}, 
title={EKLOPS: An Adaptive Approach to a Mission Planning System}, 
year={2005}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={EKLOPS is the Enhanced Kernel Library for Operational Planning and Scheduling. This paper will discuss the area of Mission Planning, and present EKLOPS as a generic mission planning solution proposed by the Mission Planning Group of Anite Systems GmbH3. EKLOPS has evolved from Mission planning systems that were developed under contracts of the European Space Agency. It implements an Adaptive Object Model architecture to integrate the common elements of mission planning systems. The model of a specific satellite mission is expressed as metadata, which configure the MPS. Rules implement functions of the planning process for which a number of specific roles can be identified. The paper will present a language that has so far been utilized to express constraint-checking rules. The experience made with EKLOPS is shown using the examples of the ENVISAT and Mars Express missions. The generic nature of EKLOPS facilitates an extension of its usage outside the field of spacecraft operations planning.—}, 
keywords={Satellites;Space missions;Process planning;Programmable control;Testing;Hardware;Adaptive systems;Steel;Kernel;Libraries}, 
doi={10.1109/AERO.2005.1559629}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{8405633, 
author={Garousi, Vahid and Felderer, Michael and Karapıçak, Çağrı Murat and Yılmaz, Uğur}, 
journal={IEEE Software}, 
title={What We Know about Testing Embedded Software}, 
year={2018}, 
volume={35}, 
number={4}, 
pages={62-69}, 
abstract={To cost-effectively test embedded software, practitioners and researchers have proposed many test techniques, approaches, tools, and frameworks. However, obtaining an overview of the state of the art and state of the practice in this area is challenging for practitioners or new researchers. In addition, owing to an inadequate overview of what already exists in this area, some companies often reinvent the wheel by designing a test approach that’s new to them but already exists. To address these problems, the authors conducted a systematic literature review of this area that covered the testing topics, testing activities, test artifacts, and industries on which the studies focused. The results can benefit both practitioners and researchers by serving as an index to the vast body of knowledge in this important, fast-growing area.}, 
keywords={Testing;Unified modeling language;Automation;Automotive engineering;Embedded software;software testing;embedded systems;embedded software;systematic literature mapping;systematic literature review;software engineering;software development}, 
doi={10.1109/MS.2018.2801541}, 
ISSN={1937-4194}, 
month={July},}
@INPROCEEDINGS{658216, 
author={Murphy, D.M. and Allen, D.M.}, 
booktitle={IECEC-97 Proceedings of the Thirty-Second Intersociety Energy Conversion Engineering Conference (Cat. No.97CH6203)}, 
title={SCARLET development, fabrication, and testing for the Deep Space 1 spacecraft}, 
year={1997}, 
volume={4}, 
number={}, 
pages={2237-2245 vol.4}, 
abstract={An advanced version of "Solar Concentrator Arrays with Refractive Linear Element Technology" (SCARLET) is being assembled for use on the first NASA/JPL New Millennium spacecraft: Deep Space One (DS1). The array is scaled up from the first SCARLET array that was built for the METEOR satellite in 1995 and incorporates advanced technologies such as dual-junction solar cells and an improved structural design. Due to the failure of the Conestoga launch vehicle, this will be the first flight of a modular concentrator array. SCARLET will provide 2.6 kW to the DS1 spacecraft to be launched in July 1998 for a mission that includes fly-bys of the asteroid McAuliffe, Mars and the comet West-Kohoutek-Ikemura. This paper describes the SCARLET design, fabrication/assembly and testing program for the flight system.}, 
keywords={Fabrication;Testing;Space technology;Space vehicles;Building integrated photovoltaics;Assembly;NASA;Satellites;Photovoltaic cells;Mars}, 
doi={10.1109/IECEC.1997.658216}, 
ISSN={}, 
month={July},}
@ARTICLE{1021118, 
author={Chou, E. and Sheu, B.}, 
journal={IEEE Circuits and Devices Magazine}, 
title={Nanometer mixed-signal system-on-a-chip design}, 
year={2002}, 
volume={18}, 
number={4}, 
pages={7-17}, 
abstract={A mixed-signal system-on-a-chip (SoC) design methodology and the supporting CAD tools are presented. A known tools set is identified for illustration purposes and some alternative tools can equally accomplish the task.}, 
keywords={System-on-a-chip;Integrated circuit modeling;Design methodology;Integrated circuit testing;Circuit testing;Design automation;System testing;Research and development;Design engineering;Prototypes}, 
doi={10.1109/MCD.2002.1021118}, 
ISSN={1558-1888}, 
month={July},}
@INPROCEEDINGS{8854494, 
author={Deutschmann, Jörg and Hielscher, Kai-Steffen and German, Reinhard}, 
booktitle={2019 International Conference on Networked Systems (NetSys)}, 
title={Satellite Internet Performance Measurements}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Satellite Internet with geostationary satellites is one way to provide Internet access all over the world. In Europe, there are three major satellite operators. In this paper, their performance is compared with respect to one-way delays, bulk data transfers, and website download times. When one-way delays are measured with UDP packets, the forward link generally shows lower delays than the return link. The bulk data transfers reveal how close actual data rates get to the advertised link rates. Regarding website download times, the web protocols HTTP/1.1, HTTP/2, and QUIC are considered. The results of our TCP measurements show that satellite Internet heavily relies on Performance Enhancement Proxies (PEPs). TCP connections over VPN tunnels cannot benefit from PEPs and therefore result in poor performance. QUIC can perform better than TCP tunneled in VPNs, but performs worse than TCP connections optimized by PEPs. One operator generally shows more stable performance regarding delays, data rates and page load times.}, 
keywords={Satellites;Internet;Delays;Data transfer;Virtual private networks;Throughput;Servers;Satellite Internet;Performance Measurements;UDP;TCP;HTTP;QUIC}, 
doi={10.1109/NetSys.2019.8854494}, 
ISSN={}, 
month={March},}
@ARTICLE{5389507, 
author={Bose, P. and Surya, S.}, 
journal={IBM Journal of Research and Development}, 
title={Architectural timing verification of CMOS RISC processors}, 
year={1995}, 
volume={39}, 
number={1.2}, 
pages={113-129}, 
abstract={We consider the problem of verification and testing of architectural timing models ("timers") coded to predict cycles-per-instruction (CPI) performance of advanced CMOS superscalar (RISC) processors. Such timers are used for pre-hardware performance analysis and prediction. As such, these software models play a vital role in processor performance tuning as well as application-based competitive analysis, years before actual product availability. One of the key problems facing a designer, modeler, or application analyst who uses such a tool is to understand how accurate the model is, in terms of the actual design. In contrast to functional simulators, there is no direct way of testing timers in the classical sense, since the “correct” execution time (in cycles) of a program on the machine model under test is not directly known or computable from equations, truth tables, or other formal specifications. Ultimate validation (or invalidation) of such models can be achieved under actual hardware availability, by direct comparisons against measured performance. However, deferring validation solely to that stage would do little to achieve the overall purpose of accurate pre-hardware analysis, tuning, and projection. We describe a multilevel validation method which has been used successfully to transform evolving timers into highly accurate pre-hardware models. In this paper, we focus primarily on the following aspects of the methodology: a) establishment of cause-effect relationships in terms of model defects and the associated fault signatures; b) derivation of application-based test loop kernels to verify steady-state (periodic) behavior of pipeline flow, against analytically predicted signatures; and c) derivation of synthetic test cases to verify the “core” parameters characterizing the pipeline-level machine organization as implemented in the timer model. The basic tenets of the theory and its application are described in the context of an example processor, comparable in complexity to an advanced member of the PowerPC™ 6XX processor family.}, 
keywords={}, 
doi={10.1147/rd.391.0113}, 
ISSN={0018-8646}, 
month={Jan},}
@INPROCEEDINGS{7964337, 
author={Ringert, Jan Oliver and Rumpe, Bernhard and Schulze, Christoph and Wortmann, Andreas}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET)}, 
title={Teaching agile model-driven engineering for cyber-physical systems}, 
year={2017}, 
volume={}, 
number={}, 
pages={127-136}, 
abstract={Agile development methods, model-driven engineering, and cyber-physical systems are important topics in software engineering education. It is not obvious how to teach their combination while respecting individual challenges posed to students and educators. We have devised a software project class for teaching the agile MDE for CPS. The project class was held in three different semesters. In this paper, we report on the setup of our exploratory study and its goals for teaching. We base our evaluation and insights on interviews and questionnaires. Our results show the feasibility of combination of agile MDE for CPS but also the challenges this combination poses to students and educators.}, 
keywords={Education;Unified modeling language;Educational robots;Software engineering;Model driven engineering;teaching;model-driven engineering;cyber-physical systems;case study}, 
doi={10.1109/ICSE-SEET.2017.16}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7733581, 
author={Awad, Ramez and Heppner, Georg and Roennau, Arne and Bordignon, Mirko}, 
booktitle={2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={ROS engineering workbench based on semantically enriched app models for improved reusability}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={In this work, the ReApp Engineering Workbench and its underlying semantically enriched app models are presented. The usage of a model, which describes the apps functionality, interfaces and other attributes, allows the utilization of engineering tools for code generation and automated testing. Further, it ensures the compatibility of the generated interfaces, which in turn enhances the reusability of the developed apps in larger applications.}, 
keywords={Biological system modeling;Unified modeling language;Hardware;Model driven engineering;Robot kinematics}, 
doi={10.1109/ETFA.2016.7733581}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8369576, 
author={Hains, Gaétan and Jakobsson, Arvid and Khmelevsky, Youry}, 
booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
title={Towards formal methods and software engineering for deep learning: Security, safety and productivity for dl systems development}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Deep Learning (DL) techniques are now widespread and being integrated into many important systems. Their classification and recognition abilities ensure their relevance for multiple application domains far beyond pure signal processing. As a machine-learning technique that relies on training instead of explicit algorithm programming they offer a high degree of productivity. But recent research has shown that they can be vulnerable to attacks and the verification of their correctness is only just emerging as a scientific and engineering possibility. Moreover DL tools are not integrated into classical software engineering so software tools to specify, modify and verify them would make them even more mainstream as software-hardware systems. This paper surveys recent work and proposes research directions and methodologies for this purpose.}, 
keywords={Artificial neural networks;Safety;Testing;Machine learning;Tools;Security;Training;deep-learning systems;neural networks;vulnerability of deep learning;security;verification;software engineering}, 
doi={10.1109/SYSCON.2018.8369576}, 
ISSN={2472-9647}, 
month={April},}
@INPROCEEDINGS{4031222, 
author={Almeida, Joao Paulo and van Eck, Pascal and Iacob, Maria-eugenia}, 
booktitle={2006 10th IEEE International Enterprise Distributed Object Computing Conference (EDOC'06)}, 
title={Requirements Traceability and Transformation Conformance in Model-Driven Development}, 
year={2006}, 
volume={}, 
number={}, 
pages={355-366}, 
abstract={The variety of design artefacts (models) produced in a model-driven design process results in an intricate relationship between requirements and the various models. This paper proposes a methodological framework that simplifies management of this relationship. This framework is a basis for tracing requirements, assessing the quality of model transformation specifications, metamodels, models and realizations. We propose a notion of conformance between application models which reduces the effort needed for assessment activities. We discuss how this notion of conformance can be integrated with model transformations}, 
keywords={Process design;Application software;Testing;Design engineering;Telematics;Information technology;Buildings;Distributed computing;requirements traceability;assessment;con-formance;model transformation;model-driven design}, 
doi={10.1109/EDOC.2006.45}, 
ISSN={1541-7719}, 
month={Oct},}
@ARTICLE{8642838, 
author={Ruscheinski, Andreas and Warnke, Tom and Uhrmacher, Adelinde M.}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Artifact-Based Workflows for Supporting Simulation Studies}, 
year={2020}, 
volume={32}, 
number={6}, 
pages={1064-1078}, 
abstract={Valid models are central for credible simulation studies. If those models do not exist, they need to be developed. In fact, entire simulation studies are often aimed at developing valid models. Thereby, successive model refinement and execution of diverse simulation experiments are closely intertwined. Whereas software-based support for individual simulation experiments exists, the intricate interdependencies and the diversity of tasks that govern simulation studies have prevented a more comprehensive support. To achieve the required flexibility while adhering to the constraints that apply between individual tasks, we adopt a declarative, artifact-based workflow approach. Therefore, central products of these simulation studies are identified and specified as artifacts: the conceptual model (with a focus on formally defined requirements), the simulation model, and the experiment. Each artifact is characterized by stages the artifact moves through to reach certain milestones and which are guarded by conditions. Thereby, the relations and constraints between artifacts become explicit. This is instrumental to check and ensure the consistency between conceptual model and simulation model, to automatically execute simulation experiments to probe the specified requirements, and to develop plans to provide goal-directed guidance to the user. We demonstrate the approach by using it to repeat an existing simulation study.}, 
keywords={Data models;Analytical models;Computational modeling;Mathematical model;Adaptation models;Biological system modeling;Context modeling;Modeling and simulation life cycle;simulation study;artifact-based workflows;planning;user support}, 
doi={10.1109/TKDE.2019.2899840}, 
ISSN={1558-2191}, 
month={June},}
@INPROCEEDINGS{6511809, 
author={Wanderley, Fernando and da Silveria, Denis Silva}, 
booktitle={2012 Eighth International Conference on the Quality of Information and Communications Technology}, 
title={A Framework to Diminish the Gap between the Business Specialist and the Software Designer}, 
year={2012}, 
volume={}, 
number={}, 
pages={199-204}, 
abstract={Requirements Engineering establishes the process for defining requirements as one in which elicitation, modeling and analysis are tasks which must be carried out. This process should involve different stakeholders and their different viewpoints. Among these stakeholders, there is the software designer, responsible for creating models based on the information gathered by business specialists. However, this communication channel may create some "noise" that leads to information being lost. This loss produces a semantic gap between what is desired and what will be developed. The semantic gap is characterized by inconsistencies in the requirements represented by scenarios -- user stories in a behavior-driven context -- and by the conceptual model. This paper presents an interactive approach to the agile requirements modeling, thus fostering greater consistency between the artifacts of the scenarios and the conceptual model. This consistency is ensured by using a mind model specification which will serve as a basis for transforming the definitions of the scenario and generating a conceptual model represented by a UML class diagram. The mind model represents the main role of this approach, and functions as a bond that represents the business entities, thus enabling the requirements to be more consistent with the reality of the business.}, 
keywords={Agile Modeling Requirements;Behaviour Driven Development;UML;Mind Map Modeling;Domain Model}, 
doi={10.1109/QUATIC.2012.9}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{738516, 
author={Bennett, K.H.}, 
booktitle={Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272)}, 
title={Do program transformations help reverse engineering?}, 
year={1998}, 
volume={}, 
number={}, 
pages={247-254}, 
abstract={Program transformations have been advocated as a method for accomplishing reverse engineering. The hypothesis is that the original source code can be progressively transformed into alternative forms, but with the same semantics. At the end of the process, an equivalent program is acquired, but one which is much easier to understand and more maintainable. We have been undertaking an extensive programme of research over twelve years into the design and development of transformations for the support of software maintenance. The paper very briefly explains the theory, practice and tool support for transformational systems, but does not present new theoretical results. The main results are on an analysis of the strengths and weaknesses of the approach, based on experience with case studies and industrial applications. The evaluation framework used (called DERE) is that presented in Bennett and Munro (1998). It is hoped that the results will be of benefit to industry, who might be considering using the technology; and to other researchers, interested in addressing the open problems. The overall conclusion is that transformations can help in the bottom-up analysis and manipulation of source code at approximately the 3GL level, and have proved successful in code migration, but need to be complemented by other top-down techniques to be useful at higher levels of abstraction or in more ambitious re-engineering projects.}, 
keywords={Reverse engineering;Libraries;Computer science;Electronic mail;Electrical capacitance tomography;Computer languages;Read only memory}, 
doi={10.1109/ICSM.1998.738516}, 
ISSN={1063-6773}, 
month={Nov},}
@INPROCEEDINGS{8802773, 
author={Semeráth, Oszkár and Babikian, Aren A. and Pilarski, Sebastian and Varró, Dániel}, 
booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)}, 
title={VIATRA Solver: A Framework for the Automated Generation of Consistent Domain-Specific Models}, 
year={2019}, 
volume={}, 
number={}, 
pages={43-46}, 
abstract={Viatra Solver [1] is a novel open source software tool to automatically synthesize consistent and diverse domain-specific graph models to be used as a test suite for the systematic testing of CPS modelling tools. Taking a metamodel, and a set of well-formedness constraints of a domain as input, the solver derives a diverse set of consistent graph models where each graph is compliant with the metamodel, satisfies consistency constraints, and structurally different from each other. The tool is integrated into the Eclipse IDE or it is executable from the command line.}, 
keywords={Biological system modeling;Tools;Object oriented modeling;Testing;Adaptation models;Metals;Generators;Tool testing;Logic solver;Graph generation;Test generation;Model based system engineering}, 
doi={10.1109/ICSE-Companion.2019.00034}, 
ISSN={2574-1934}, 
month={May},}
@INPROCEEDINGS{6912254, 
author={Pruski, Piotr and Lohar, Sugandha and Aquanette, Rundale and Ott, Greg and Amornborvornwong, Sorawit and Rasin, Alexander and Cleland-Huang, Jane}, 
booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)}, 
title={TiQi: Towards natural language trace queries}, 
year={2014}, 
volume={}, 
number={}, 
pages={123-132}, 
abstract={One of the surprising observations of traceability in practice is the under-utilization of existing trace links. Organizations often create links in order to meet compliance requirements, but then fail to capitalize on the potential benefits of those links to provide support for activities such as impact analysis, test regression selection, and coverage analysis. One of the major adoption barriers is caused by the lack of accessibility to the underlying trace data and the lack of skills many project stakeholders have for formulating complex trace queries. To address these challenges we introduce TiQi, a natural language approach, which allows users to write or speak trace queries in their own words. TiQi includes a vocabulary and associated grammar learned from analyzing NL queries collected from trace practitioners. It is evaluated against trace queries gathered from trace practitioners for two different project environments.}, 
keywords={Unified modeling language;Databases;Vocabulary;Natural languages;Software;Speech;Hazards;Traceability;Queries;Speech Recognition;Natural Language Processing}, 
doi={10.1109/RE.2014.6912254}, 
ISSN={2332-6441}, 
month={Aug},}
@ARTICLE{9115830, 
author={Chen, Yanjiao and Ou, Runmin and Li, Zhiyang and Wu, Kaishun}, 
journal={IEEE Transactions on Mobile Computing}, 
title={WiFace: Facial Expression Recognition Using Wi-Fi Signals}, 
year={2022}, 
volume={21}, 
number={1}, 
pages={378-391}, 
abstract={Facial expressions are an essential form of human nonverbal communication. Recognition of this nonverbal sign may enable developers to understand the feedbacks on smart device functionality and advertising. Existing approaches for facial expression recognition are mainly based on cameras or on-body sensors, which are either sensitive to lighting conditions or cumbersome for users to wear devices on their faces. In this paper, we propose a new facial expression recognition system based on Wi-Fi signals, named WiFace. Our fundamental intuition is that facial muscle movements in different expressions will induce distinctive waveform patterns in the time-series of channel state information (CSI) in Wi-Fi signals. We develop a series of algorithms to process the CSI signals and extract the most representative waveform patterns for facial expression classification. We build a fully-functional prototype of WiFace using commercial off-the-shelf devices, which can recognize six typical facial expressions. We conduct extensive experiments to evaluate the performance of WiFace, and the experimental results show that the average recognition accuracy is 94.80 percent.}, 
keywords={Wireless fidelity;Face recognition;Feature extraction;Sensors;Lighting;Transmitting antennas;Smart devices;Facial expression recognition;Wi-Fi based sensing;channel state information}, 
doi={10.1109/TMC.2020.3001989}, 
ISSN={1558-0660}, 
month={Jan},}
@INPROCEEDINGS{7066751, 
author={Malini, A. and Venkatesh, N. and Sundarakantham, K. and Mercyshalinie, S.}, 
booktitle={International Conference on Computing and Communication Technologies}, 
title={Mobile application testing on smart devices using MTAAS framework in cloud}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Testing of mobile applications which run on smart devices is more complex due to diversity of mobile devices and computational resources. Mobile testing using emulators which doesn't include real network traffic and testing mobile applications in more than one portable device in single system was also not possible in normal testing. In order to overcome the draw backs of normal testing, in this paper we deployed a Mobile Testing as a Service (MTAAS) framework in cloud environment. By using MTAAS framework many mobile applications can be tested in different portable devices and different mobile platforms. Testing of mobile applications using MTAAS provides most realistic results since it includes real network speed. Finally, we conducted an experiment in MTAAS framework and testing results shows that MTAAS can effectively reduce the complexity of mobile testing on different smart devices.}, 
keywords={Testing;Mobile communication;Mobile computing;Performance evaluation;Smart phones;Load modeling;Cloud Testing;Mobile Application Testing;Testing as a service}, 
doi={10.1109/ICCCT2.2014.7066751}, 
ISSN={}, 
month={Dec},}
@ARTICLE{5567088, 
author={Afek, Yehuda and Drepper, Ulrich and Felber, Pascal and Fetzer, Christof and Gramoli, Vincent and Hohmuth, Michael and Rivière, Etienne and Stenström, Per and Unsal, Osman and Moreira, Walther Maldonado and Harmanci, Derin and Marlier, Patrick and Diestelhorst, Stephan and Pohlack, Martin and Cristal, Adrian and Hur, Ibrahim and Dragojevic, Aleksandar and Guerraoui, Rachid and Kapalka, Michal and Tomić, Sasa and Korland, Guy and Shavit, Nir and Nowack, Martin and Riegel, Torvald}, 
journal={IEEE Micro}, 
title={The Velox Transactional Memory Stack}, 
year={2010}, 
volume={30}, 
number={5}, 
pages={76-87}, 
abstract={The adoption of multi- and many-core architectures for mainstream computing undoubtedly brings profound changes in the way software is developed. In particular, the use of fine grained locking as the multi-core programmer's coordination methodology is considered by more and more experts as a dead-end. The transactional memory (TM) programming paradigm is a strong contender to become the approach of choice for replacing locks and implementing atomic operations in concurrent programming. Combining sequences of concurrent operations into atomic transactions allows a great reduction in the complexity of both programming and verification, by making parts of the code appear to execute sequentially without the need to program using fine-grained locking. Transactions remove from the programmer the burden of figuring out the interaction among concurrent operations that happen to conflict when accessing the same locations in memory. The EU-funded FP7 VELOX project designs, implements and evaluates an integrated TM stack, spanning from programming language to the hardware support, and including runtime and libraries, compilers, and application environments. This paper presents an overview of the VELOX TM stack and its associated challenges and contributions.}, 
keywords={Libraries;Runtime;Hardware;Java;Programming;Program processors;concurrent programming;software transactional memory;hardware transactional memory;compilers;language extensions}, 
doi={10.1109/MM.2010.80}, 
ISSN={1937-4143}, 
month={Sep.},}
@INPROCEEDINGS{577990, 
author={Kransner, S.M. and Bernard, D.E.}, 
booktitle={1997 IEEE Aerospace Conference}, 
title={Integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium}, 
year={1997}, 
volume={2}, 
number={}, 
pages={409-420 vol.2}, 
abstract={Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process.}, 
keywords={Space technology;Space vehicles;Hardware;Scheduling;Trajectory;Space missions;Image resolution;Timing;Engines;Radio navigation}, 
doi={10.1109/AERO.1997.577990}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8453176, 
author={Semeráth, Oszkár and Nagy, András Szabolcs and Varró, Dániel}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)}, 
title={A Graph Solver for the Automated Generation of Consistent Domain-Specific Models}, 
year={2018}, 
volume={}, 
number={}, 
pages={969-980}, 
abstract={Many testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy.}, 
keywords={Analytical models;Object oriented modeling;Tools;Biological system modeling;IP networks;Testing;Graph generation;Test generation;Domain Specific Modeling Languages;Logic Solver;Graph Solver}, 
doi={10.1145/3180155.3180186}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{8748595, 
author={Lenka, Rakesh Kumar and Kumar, Srikant and Mamgain, Sunakshi}, 
booktitle={2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)}, 
title={Behavior Driven Development: Tools and Challenges}, 
year={2018}, 
volume={}, 
number={}, 
pages={1032-1037}, 
abstract={Nowadays testing usually applies Test Driven Development (TDD) which is an approach to software development in which developers write tests first which initially fail and by adding more application codes tests pass. However, the latest development in this field is an extension to Test Driven Development (TDD) which usually referred as Behavior Driven Development (BDD). As being a modified version of TDD, both the technologies have various similarities. Nevertheless, the differences are also not unnoticeable. Where BDD is more about communication and collaboration TDD is more about coders and coding. This paper focuses on the advantages and glitches of TDD which led to the development of along with the method of working of BDD and several tools along with their features and a comparison of their functionalities.}, 
keywords={Testing;Tools;Cloud computing;Writing;Software;Business;Java;Manual Testing;Test Driven Development (TDD);Behavior Driven Development (BDD);Collaboration}, 
doi={10.1109/ICACCCN.2018.8748595}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6139107, 
author={Belli, Fevzi and Endo, Andre Takeshi and Linschulte, Michael and Simao, Adenilso}, 
booktitle={Proceedings of 2011 IEEE 6th International Symposium on Service Oriented System (SOSE)}, 
title={Model-based testing of web service compositions}, 
year={2011}, 
volume={}, 
number={}, 
pages={181-192}, 
abstract={The use of web services integrated in different applications, especially the composition of services, brings challenges for testing due to their complex interactions. In this paper, we propose an event-based approach to test web service compositions. The approach is based on event sequence graphs which we extend by facilities to consider the specific features of web service compositions. An enterprise service bus component supports the test case execution. A case study, based on a commercial web application, demonstrates the feasibility of the approach and analyzes its characteristics. The results of empirical work suggest that the approach is a promising candidate to reach a high level of confidence and reliability.}, 
keywords={Testing;Service oriented architecture;Business;Data models;Monitoring;enterprise service bus;event sequence graphs;model-based testing;service composition testing}, 
doi={10.1109/SOSE.2011.6139107}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7813841, 
author={Kumar, Satendra and Rajkumar}, 
booktitle={2016 International Conference on Computing, Communication and Automation (ICCCA)}, 
title={Test case prioritization techniques for software product line: A survey}, 
year={2016}, 
volume={}, 
number={}, 
pages={884-889}, 
abstract={Software product line (SPL) testing is a tougher work than testing of single systems. Still testing of each individual SPL product would be perfect but it is too costly in practice. In fact, when the number of features increases then the number of possible products also increases exponentially usually derived from a feature model. Number of features is leading to thousands of different products. Due to cost and time constraints, it is infeasible or large number of effort to run all the test cases in an existing test suite. To decrease the cost of testing, various techniques have been proposed. One of them is test case prioritization (TCP) techniques. Here we presented a survey for TCP techniques for software SPL.}, 
keywords={Software;Software product lines;Testing;Frequency modulation;Fault detection;Automation;Libraries;Software product lines;Test Case Prioritization;Variability;Commonality;Feature Model}, 
doi={10.1109/CCAA.2016.7813841}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{4539573, 
author={Hargassner, Walter and Hofer, Thomas and Klammer, Claus and Pichler, Josef and Reisinger, Gernot}, 
booktitle={2008 1st International Conference on Software Testing, Verification, and Validation}, 
title={A Script-Based Testbed for Mobile Software Frameworks}, 
year={2008}, 
volume={}, 
number={}, 
pages={448-457}, 
abstract={Software testing is essential and takes a large part of resources during software development. This motivates automating software testing as far as possible. Frameworks for automating unit testing are approved and applied for a plethora of programming languages to write tests for small units in the same programming language. Both constraints, unit size and programming language, inhibit automation of software testing in domain of mobile software frameworks. This circumstance has motivated the development of a new testbed for a framework in the domain of mobile systems. In this paper, we describe requirements and challenges in testing mobile software frameworks in general and present a novel testbed for the APOXI framework that addresses these requirements. The main ideas behind this testbed are the usage of a scripting language to specify test cases and to incorporate domain-specific aspects on the language level. The testbed facilitates component and system testing but can be used for unit testing as well.}, 
keywords={Software testing;System testing;Protocols;Application software;Computer languages;Control systems;Automatic testing;Hardware;Mobile handsets;Electronic equipment testing;Software testing}, 
doi={10.1109/ICST.2008.51}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{8116418, 
author={Jimenez, Ivo and Arpaci-Dusseau, Andrea and Arpaci-Dusseau, Remzi and Lofstead, Jay and Maltzahn, Carlos and Mohror, Kathryn and Ricci, Robert}, 
booktitle={2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
title={PopperCI: Automated reproducibility validation}, 
year={2017}, 
volume={}, 
number={}, 
pages={450-455}, 
abstract={This paper introduces PopperCI, a continous integration (CI) service hosted at UC Santa Cruz that allows researchers to automate the end-to-end execution and validation of experiments. PopperCI assumes that experiments follow Popper, a convention for implementing experiments and writing articles following a DevOps approach that has been proposed recently. PopperCI runs experiments on public, private or government-fundend cloud infrastructures in a fully automated way. We describe how PopperCI executes experiments and present a use case that illustrates the usefulness of the service.}, 
keywords={Tools;Runtime;Measurement;Conferences;Manuals;Writing}, 
doi={10.1109/INFCOMW.2017.8116418}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{217601, 
author={Lyu, M.R. and Chen, J.-H. and Avizienis, A.}, 
booktitle={[1992] Proceedings. The Sixteenth Annual International Computer Software and Applications Conference}, 
title={Software diversity metrics and measurements}, 
year={1992}, 
volume={}, 
number={}, 
pages={69-78}, 
abstract={The authors define and formalize the concept of software diversity which characterizes N-Version software (NVS) from four different points of view that are designated as structural diversity, fault diversity, tough-spot diversity, and failure diversity. The goals are to find a way to quantify software diversity and to investigate the measurements which can be applied during the life cycle of NVS to gain confidence that operation will be dependable when NVS is actually used. The versions from a six-language N-Version programming project for fault-tolerant flight control software were used in the software diversity measurement.<>}, 
keywords={Software measurement;Fault tolerant systems;Aerospace control;Fault tolerance;Gain measurement;Functional programming;Error correction;Delay;Software algorithms;Quality control}, 
doi={10.1109/CMPSAC.1992.217601}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8668006, 
author={Włodarski, Leszek and Pereira, Boris and Povazan, Ivan and Fabry, Johan and Zaytsev, Vadim}, 
booktitle={2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Qualify First! A Large Scale Modernisation Report}, 
year={2019}, 
volume={}, 
number={}, 
pages={569-573}, 
abstract={Typically in modernisation projects any concerns for code quality are silenced until the end of the migration, to simplify an already complex process. Yet, we claim from experience that prioritising quality above many other issues has many benefits. In this experience report, we discuss a modernisation project of mBank, a big Polish bank, where bad smell detection and elimination, automated testing and refactoring played a crucial rule, provided pay-offs early in the project, increased buy-in, and ensured maintainability of the end result.}, 
keywords={C# languages;Databases;Testing;Production;Software;Product development;Aging}, 
doi={10.1109/SANER.2019.8668006}, 
ISSN={1534-5351}, 
month={Feb},}
@INPROCEEDINGS{1541840, 
author={Kajko-Mattsson, M. and Meyer, P.}, 
booktitle={2005 International Symposium on Empirical Software Engineering, 2005.}, 
title={Evaluating the acceptor side of EM/sup 3/: release management at SAS}, 
year={2005}, 
volume={}, 
number={}, 
pages={10 pp.-}, 
abstract={Today, there are no detailed standard process models encompassing the overall release management activities. To remedy this, we have created an individual release management process model, called EM/sup 3/: release management. In this paper, we evaluate its acceptor side against an industrial release management process performed at Scandinavian Airline Systems (SAS). We have observed some similarities and differences. Some of the observed differences provide feedback for the improvement and further extension of EM/sup 3/: release management.}, 
keywords={Synthetic aperture sonar;Technology management;Control systems;Software maintenance;Laboratories;Software standards;Performance evaluation;Feedback;Software systems;Lead}, 
doi={10.1109/ISESE.2005.1541840}, 
ISSN={}, 
month={Nov},}
@ARTICLE{7492282, 
author={Datta, Subhajit and Sarkar, Santonu and Sajeev, A. S. M.}, 
journal={IEEE Transactions on Big Data}, 
title={How Long Will This Live? Discovering the Lifespans of Software Engineering Ideas}, 
year={2016}, 
volume={2}, 
number={2}, 
pages={124-137}, 
abstract={We all want to be associated with long lasting ideas; as originators, or at least, expositors. For a tyro researcher or a seasoned veteran, knowing how long an idea will remain interesting in the community is critical in choosing and pursuing research threads. In the physical sciences, the notion of half-life is often evoked to quantify decaying intensity. In this paper, we study a corpus of 19,000+ papers written by 21,000+ authors across 16 software engineering publication venues from 1975 to 2010, to empirically determine the half-life of software engineering research topics. In the absence of any consistent and well-accepted methodology for associating research topics to a publication, we have used natural language processing techniques to semi-automatically identify and associate a set of topics with a paper. We adapted measures of half-life already existing in the bibliometric context for our study, and also defined a new measure based on publication and citation counts. We find evidence that some of the identified research topics show a mean half-life of close to 15 years, and there are topics with sustaining interest in the community. We report the methodology of our study in this paper, as well as the implications and utility of our results.}, 
keywords={Software engineering;Big data;Measurement;Collaboration;Context;Special issues and sections;Software;Big data;software engineering;research;half-life}, 
doi={10.1109/TBDATA.2016.2580541}, 
ISSN={2332-7790}, 
month={June},}
@ARTICLE{9448036, 
author={Sardar, Muhammad Usama and Musaev, Saidgani and Fetzer, Christof}, 
journal={IEEE Access}, 
title={Demystifying Attestation in Intel Trust Domain Extensions via Formal Verification}, 
year={2021}, 
volume={9}, 
number={}, 
pages={83067-83079}, 
abstract={In August 2020, Intel asked the research community for feedback on the newly offered architecture extensions, called Intel Trust Domain Extensions (TDX), which give more control to Trust Domains (TDs) over processor resources. One of the key features of these extensions is the remote attestation mechanism, which provides a unified report verification mechanism for TDX and its predecessor Software Guard Extensions (SGX). Based on our experience and intuition, we respond to the request for feedback by formally specifying the attestation mechanism in the TDX using ProVerif's specification language. Although the TDX technology seems very promising, the process of formal specification reveals a number of subtle discrepancies in Intel's specifications that could potentially lead to design and implementation flaws. After resolving these discrepancies, we also present fully automated proofs that our specification of TD attestation preserves the confidentiality of the secret and authentication of the report by considering the state-of-the-art Dolev-Yao adversary in the symbolic model using ProVerif. We have submitted the draft to Intel, and Intel is in the process of making the changes.}, 
keywords={Security;Software;Tools;Virtual machine monitors;Computer bugs;Analytical models;Runtime;Formal verification;symbolic security analysis;ProVerif;trusted execution environment;trust domains;Intel TDX;remote attestation}, 
doi={10.1109/ACCESS.2021.3087421}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8115704, 
author={Pietsch, Christopher and Ohrndorf, Manuel and Kelter, Udo and Kehrer, Timo}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Incrementally slicing editable submodels}, 
year={2017}, 
volume={}, 
number={}, 
pages={913-918}, 
abstract={Model slicers are tools which provide two services: (a) finding parts of interest in a model and (b) displaying these parts somehow or extract these parts as a new, autonomous model, which is referred to as slice or sub-model. This paper focuses on the creation of editable slices, which can be processed by model editors, analysis tools, model management tools etc. Slices are useful if, e.g., only a part of a large model shall be analyzed, compared or processed by time-consuming algorithms, or if sub-models shall be modified independently. We present a new generic incremental slicer which can slice models of arbitrary type and which creates slices which are consistent in the sense that they are editable by standard editors. It is built on top of a model differencing framework and does not require additional configuration data beyond those available in the differencing framework. The slicer can incrementally extend or reduce an existing slice if model elements shall be added or removed, even if the slice has been edited meanwhile. We demonstrate the usefulness of our slicer in several scenarios using a large UML model. A screencast of the demonstrated scenarios is provided at http://pi.informatik.uni-siegen.de/projects/SiLift/ase2017.}, 
keywords={Unified modeling language;Adaptation models;Tools;Servers;Load modeling;Computational modeling;Data models}, 
doi={10.1109/ASE.2017.8115704}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{4148978, 
author={Liu, Jing and Dehlinger, Josh and Sun, Hongyu and Lutz, Robyn}, 
booktitle={14th Annual IEEE International Conference and Workshops on the Engineering of Computer-Based Systems (ECBS'07)}, 
title={State-Based Modeling to Support the Evolution and Maintenance of Safety-Critical Software Product Lines}, 
year={2007}, 
volume={}, 
number={}, 
pages={596-608}, 
abstract={Changes to safety-critical product lines can jeopardize the safety properties that they must ensure. Thus, evolving software product lines must consider the impact that changes to requirements may have on the existing systems and their safety. The contribution of this work is a systematic, tool-supported technique to support safe evolution of product-line requirements using a model-based approach. We show how the potential feature interactions that need to be modeled are scoped and identified with the aid of product-line software fault tree analysis. Further, we show how reuse of the state-based models is effectively exploited in the evolution phase of product-line engineering. To illustrate this approach, we apply our technique to the evolution of a safety-critical cardiac pacemaker product line}, 
keywords={Software maintenance;Software safety;Product safety;Fault trees;Pacemakers;Software tools;Systems engineering and theory;Costs;Reliability engineering;Computer science}, 
doi={10.1109/ECBS.2007.66}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{4293588, 
author={Fletcher, Matt and Bereza, William and Karlesky, Mike and Williams, Greg}, 
booktitle={Agile 2007 (AGILE 2007)}, 
title={Evolving into Embedded Develop}, 
year={2007}, 
volume={}, 
number={}, 
pages={150-155}, 
abstract={In late 2005 we had the opportunity to start our first embedded development project. We apply agile practices to a variety of domains from web development to desktop applications to factory floor test equipment. The challenge for this new work was not learning the environment and technology. Our challenge was applying the practices of the agile world to the small and complex world of embedded systems. The hurdles were numerous: we battled the archaic state of many embedded tool sets, the lack of integration with tools like Rake that provide easy automation, and poor support for object oriented design. We've overcome each of these difficulties. This report is about our yearlong experience in introducing our development practices to embedded development.}, 
keywords={Velocity control;Microprogramming;Vehicle safety;Automatic testing;Embedded system;Vehicle driving;Automation;System testing;Sampling methods;Production facilities}, 
doi={10.1109/AGILE.2007.25}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{687918, 
author={Aljabri, A.S. and Bernard, D.E. and Dvorak, D.L. and Man, G.K. and Pell, B. and Starbird, T.W.}, 
booktitle={1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)}, 
title={Infusion of autonomy technology into space missions: DS1 lessons learned}, 
year={1998}, 
volume={2}, 
number={}, 
pages={315-329 vol.2}, 
abstract={The impact of infusing breakthrough autonomy technology into a flight project was a big surprise. Valuable technical and cultural lessons, many of general applicability when introducing system-level autonomy, have been learned by infusing the Remote Agent (RA) into NASA's Deep Space 1 (DS1) spacecraft. The RA's architecture embodies system-level autonomy in three major components: planning and scheduling, execution, and fault diagnosis and reconfiguration. Lessons learned include: the architecture was confirmed; active participation by nonautonomy personnel in the development is essential; communication of new concepts is essential, difficult, and hampered by differences in terminology; giving a spacecraft system-level autonomy changes organizational roles in operating the spacecraft after launch, and hence changes roles during development; software models supporting functions traditionally handled on the ground must be developed early enough to get on-board; shortfalls in planned features must be technically and developmentally accomodatable, in particular not to threaten the launch schedule; traditional commanding must be supported; testing must be emphasized. These lessons and others, on incremental system releases and use of autocode generation, are based on 16 months of spiral development from start of project through the project's decision to reduce the role of the RA from full-time control of the spacecraft to a separable experiment.}, 
keywords={Space technology;Space missions;Space vehicles;Scheduling;Cultural differences;Fault diagnosis;Computer architecture;Personnel;Terminology;Communication system software}, 
doi={10.1109/AERO.1998.687918}, 
ISSN={1095-323X}, 
month={March},}
@INPROCEEDINGS{6114166, 
author={Ioannides, Charalambos and Barrett, Geoff and Eder, Kerstin}, 
booktitle={2011 IEEE International High Level Design Validation and Test Workshop}, 
title={Introducing XCS to Coverage Directed test Generation}, 
year={2011}, 
volume={}, 
number={}, 
pages={57-64}, 
abstract={Coverage Directed test Generation (CDG) is rife with challenges and problems, despite the relative successes of machine learning methodologies over the years in automating it. This paper introduces the use of the eXtended Classifier System (XCS) in simulation-based digital design verification. It argues for the use of this novel genetics-based machine learning technique to perform effective CDG by learning the full mapping between coverage results and test generator directives. Using the resulting production rules, efficient test suites can be constructed, and inference on the validity of the verification environment can be made. There is great potential in using XCS for design verification and this paper forms an initial attempt to highlight the associated advantages. The technique requires no domain knowledge to setup and satisfies important CDG requirements. Once matured, it is expected to be utilized seamlessly in any industrial level simulation-based verification process.}, 
keywords={Generators;Fires;Pipelines;Genetic algorithms;Machine learning;Measurement;Learning systems;Electronic Design Automation and Methodology;Digital Simulation;Learning Systems;Learning Classifier Systems;XCS}, 
doi={10.1109/HLDVT.2011.6114166}, 
ISSN={1552-6674}, 
month={Nov},}
@INPROCEEDINGS{8952200, 
author={Rahat, Tamjid Al and Feng, Yu and Tian, Yuan}, 
booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={OAUTHLINT: An Empirical Study on OAuth Bugs in Android Applications}, 
year={2019}, 
volume={}, 
number={}, 
pages={293-304}, 
abstract={Mobile developers use OAuth APIs to implement Single-Sign-On services. However, the OAuth protocol was originally designed for the authorization for third-party websites not to authenticate users in third-party mobile apps. As a result, it is challenging for developers to correctly implement mobile OAuth securely. These vulnerabilities due to the misunderstanding of OAuth and inexperience of developers could lead to data leakage and account breach. In this paper, we perform an empirical study on the usage of OAuth APIs in Android applications and their security implications. In particular, we develop OAUTHLINT, that incorporates a query-driven static analysis to automatically check programs on the Google Play marketplace. OAUTHLINT takes as input an anti-protocol that encodes a vulnerable pattern extracted from the OAuth specifications and a program P. Our tool then generates a counter-example if the anti-protocol can match a trace of Ps possible executions. To evaluate the effectiveness of our approach, we perform a systematic study on 600+ popular apps which have more than 10 millions of downloads. The evaluation shows that 101 (32%) out of 316 applications that use OAuth APIs make at least one security mistake.}, 
keywords={Protocols;Authorization;Mobile applications;Authentication;Google;Computer bugs;Security, OAuth, Android, Static Analysis, Bug Finding}, 
doi={10.1109/ASE.2019.00036}, 
ISSN={2643-1572}, 
month={Nov},}
@INPROCEEDINGS{6550482, 
author={Esnaashari, Shadi and Welch, Ian and Komisarczuk, Peter}, 
booktitle={2013 27th International Conference on Advanced Information Networking and Applications Workshops}, 
title={Determining Home Users' Vulnerability to Universal Plug and Play (UPnP) Attacks}, 
year={2013}, 
volume={}, 
number={}, 
pages={725-729}, 
abstract={Universal Plug and Play (UPnP) technology is used worldwide since it has simplified the installation and management of the devices. As a result, many devices are now equipped with UPnP capabilities. Unfortunately using UPnP in home routers puts routers at risk of abuse. For example, it is easier for hackers to discover the devices and use device vulnerabilities in order to make malicious attacks to cause financial or reputation damage to the users. In this paper, we have analyzed the UPnP protocol and its different vulnerabilities. Furthermore, we have emphasized how common the problem is with the home users' devices. Hence, we suggest a tool to achieve transparency in the health of the Internet by detecting UPnP enabled devices which are likely to be attacked on home networks. The tool will look for UPnP based attacks when people's routers have been compromised. The tool is easy to install and use for novice home users and maintains their privacy too. This project aims not only to implement a tool for a user to determine whether his/her system is vulnerable to a particular attack, but also to measure the prevalence of vulnerabilities at national or global level. Thus a larger framework is required to collect and manage the results from individual users.}, 
keywords={Servers;Protocols;Ports (Computers);Logic gates;Plugs;IP networks;Internet;Security;Network Measurement;UPnP}, 
doi={10.1109/WAINA.2013.225}, 
ISSN={}, 
month={March},}
@ARTICLE{4012600, 
author={Ayers, Danny}, 
journal={IEEE Internet Computing}, 
title={The Shortest Path to the Future Web}, 
year={2006}, 
volume={10}, 
number={6}, 
pages={76-79}, 
abstract={This column's title could suggest that there is only one best path forward for the Web. The path begins with document metadata and travels through the world of microformats and embedded data. A waypoint is a semantic Web that leverages these approaches, along with those offered by an environment more capable of managing first-class data directly. This is only one path, however, and it probably isn't the shortest. The Internet is a rich environment with billions of active agents. Natural selection, mutation, and genetic breeding of sorts all happen to software systems, together with a significantly higher proportion of "intelligent design" than found in the real world. The net effect is that many different evolutionary paths are being explored simultaneously, and several could lead to a better Web}, 
keywords={Access protocols;HTML;Computer networks;Humans;Semantic Web;Network servers;Web server;Pediatrics;Internet;Electronic mail;Semantic Web;Web 2.0;Web programming}, 
doi={10.1109/MIC.2006.137}, 
ISSN={1941-0131}, 
month={Nov},}
@INPROCEEDINGS{4626838, 
author={Lutz, Robyn}, 
booktitle={2008 12th International Software Product Line Conference}, 
title={Enabling Verifiable Conformance for Product Lines}, 
year={2008}, 
volume={}, 
number={}, 
pages={35-44}, 
abstract={NASA is, with the rest of industry, turning to product-line engineering to reduce costs and improve quality by effectively managing reuse. Experience in industry has shown that it is the verifiable conformance of each system to the product-line specifications that makes or breaks the product-line practice. Verification that the software for each project satisfies its intended product-line constraints is thus essential. This paper reports early results from aneffort to assemble from previous, industrial experience a set of enablers of verifiable conformance for use in the application engineering of NASA product lines. Lessons learned may be useful for developers of safety-critical, long-lived, or highly autonomous productlines, as well as for companies that integrate product line subsystems developed by multiple contractors.}, 
keywords={NASA;Software;Computer architecture;Organizations;Industries;Evolution (biology);Safety;software product line;application engineering;verifying conformance;experience}, 
doi={10.1109/SPLC.2008.12}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8843284, 
author={Schulz, Henning and Angerstein, Tobias and Okanović, Dušan and van Hoorn, André}, 
booktitle={2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)}, 
title={Microservice-Tailored Generation of Session-Based Workload Models for Representative Load Testing}, 
year={2019}, 
volume={}, 
number={}, 
pages={323-335}, 
abstract={Load tests are commonly used to assess the performance of an application system. A representative load test uses workload characteristics according to the user behavior in production. Session-based systems have special workload characteristics as the system is used as sequences of inter-related requests. Approaches exist to automatically extract session-based workload models from production request logs. However, they focus on system-level testing, which is in stark contrast with modern development practices, where one development team is in charge of developing, testing, and deploying a single microservice. Hence, representative session-based workload models for testing single microservices and their integration are desirable. To deal with these issues, we propose a concept for tailoring a representative load test workload to target only certain services, instead of targeting the whole system. Our goal is to transform the workload for one or more specified service(s) from the system-level workload collected in production. Using this approach, only a subset of the application's microservices is deployed for a load test, specifically the targeted services and the services they depend on. We propose two algorithms. The log-based algorithm deals with extracting the workload for a specific service from collected production traces. The model-based algorithm performs the workload tailoring on the level of the workload model. In an experiment series with a representative microservice application, we compare both algorithms with system-level and request-based workoad models. The results show that when load testing a set of services, the tailored workload models outperform untailored workload models in terms of test duration and the capacity of the test infrastructure, and outperform request-based workload models in terms of representativeness.}, 
keywords={Load modeling;Testing;Unified modeling language;Production;Markov processes;Data mining;Transforms;load testing;microservices;workload model generation}, 
doi={10.1109/MASCOTS.2019.00043}, 
ISSN={2375-0227}, 
month={Oct},}
@INPROCEEDINGS{7313460, 
author={Berger, Christian and Block, Delf and Hons, Christian and Kühnel, Stefan and Leschke, André and Plotnikov, Dimitri and Rumpe, Bernhard}, 
booktitle={2015 IEEE 18th International Conference on Intelligent Transportation Systems}, 
title={Large-Scale Evaluation of an Active Safety Algorithm with EuroNCAP and US NCAP Scenarios in a Virtual Test Environment -- An Industrial Case Study}, 
year={2015}, 
volume={}, 
number={}, 
pages={2280-2286}, 
abstract={Context: Recently, test protocols from organizations like European New Car Assessment Programme (EuroNCAP) were extended to also cover active safety systems. Objective: The official EuroNCAP test protocol for Autonomous Emergency Braking (AEB)/Forward Collision Warning (FCW) systems explicitly defines to what extent a Vehicle-Under-Test (VUT) is allowed to vary in its lateral position. In addition, the United States New Car Assessment Programme (US NCAP) test protocol has broader tolerance ranges. The goal for automotive OEMs is to understand the impact of such allowed variations on a the overall vehicle's performance. Method: A simulation-based approach is outlined that allows systematic, large-scale analysis of such influences to effectively plan time-consuming and resource-intense real-world vehicle tests. Our models allow a profound analysis of an AEB algorithm by modeling and conducting more than 3,000 simulation runs with EuroNCAP's dynamic CCRm and CCRb scenarios including those with adopted USNCAP parameters. Results: Our structured analysis of such test procedures involving dynamic actors is the first of its kind in a relevant industrial setting. Several anomalies were unveiled under US NCAP conditions to support real-world test runs. Hence, we could show that the proposed method supports all possible scenarios in AEB consumer tests and scales as we had to timely process approx. 7.7GB of simulation data. Conclusion: To achieve the expected performance and to study a system's behavior in potential misuse cases from a functional point of view, large scale, model-based simulations complement traditional testing on proving ground.}, 
keywords={Vehicles;Safety;Data models;Protocols;Vehicle dynamics;Trajectory;Systematics}, 
doi={10.1109/ITSC.2015.368}, 
ISSN={2153-0017}, 
month={Sep.},}
@INPROCEEDINGS{4700654, 
author={Khoche, Ajay and Burlison, Phil and Rowe, John and Plowman, Glenn}, 
booktitle={2008 IEEE International Test Conference}, 
title={A Tutorial on STDF Fail Datalog Standard}, 
year={2008}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Advances in technology are making it imperative to collect detailed structural IC fail data during manufacturing test to improve yield. However, there is currently no standard format for communicating and storing such structural fail data efficiently. This leads to ad-hoc tool-specific solutions, which do not offer interoperability required in a typical multi-tool, multi-vendor customer environment. These ad-hoc solutions result in unnecessary investment in development of point-to-point interface solutions that are ultimately still not integrated with the traditional data collection for a unified yield analysis data format. Expanding an established datalogging standard to accommodate the new requirements solves these issues. Standard Test Data Format (STDF) is the predominant format used today for traditional failure datalogging storage, but in its current form falls far short in handling the new high-volume structural failures for yield learning. A group of more than 20 companies from ATE, EDA, Semiconductor and Yield Management companies has been working on a new enhanced STDF standard that addresses the new requirements. This paper provides the overview of the new enhanced standard.}, 
keywords={Tutorial;Electronic design automation and methodology;Production;Manufacturing;Silicon;Integrated circuit testing;Communication standards;Investments;Data analysis;Test pattern generators}, 
doi={10.1109/TEST.2008.4700654}, 
ISSN={2378-2250}, 
month={Oct},}
@ARTICLE{4267608, 
author={Bassett, Paul G.}, 
journal={IEEE Software}, 
title={The Case for Frame-Based Software Engineering}, 
year={2007}, 
volume={24}, 
number={4}, 
pages={90-99}, 
abstract={Frame technology adapts generic components into custom information structures. Its facility for maximizing reuse and minimizing redundancy has demonstrated dramatic improvements across software's life cycle.}, 
keywords={Software engineering;Programming profession;Software maintenance;Redundancy;Software systems;Software quality;Computer industry;Humans;Statistics;Organizing;reuse models;automatic programming;evolutionary programming;software engineering process}, 
doi={10.1109/MS.2007.119}, 
ISSN={1937-4194}, 
month={July},}
@INPROCEEDINGS{7073242, 
author={Biliri, Evmorfia and Petychakis, Michael and Alvertis, Iosif and Lampathaki, Fenareti and Koussouris, Sotirios and Askounis, Dimitrios}, 
booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
title={Infusing social data analytics into Future Internet applications for manufacturing}, 
year={2014}, 
volume={}, 
number={}, 
pages={515-522}, 
abstract={Today, a new age of engagement and collaboration has emerged with the proliferation of usergenerated content in social networks and generally the Web 2.0, rendering it particularly difficult for enterprises to monitor and act upon all content following conventional data mining methodologies. In this paper, we present our approach for a Future Internet enabler (FITMAN Anlzer) that provides automated, social data analytics and aims at assisting enterprises in becoming more tuned to their customer needs and gaining insights into current and future trends to early embed them into product design. The FITMAN Anlzer implementation is domainindependent and allows any manufacturer to effectively train it based on his needs and create personalized reports to timely capture the right information. Our methodology includes trend analytics, polarity detection through machine learning, data querying through flexible reports and finally informative charts to visualize the results in order to help companies in their decision making procedures.}, 
keywords={Media;Sentiment analysis;Market research;Facebook;Twitter;Data mining;Context;social media monitoring;trend analysis;opinion mining;natural language processing;sentiment analysis;social data analytics}, 
doi={10.1109/AICCSA.2014.7073242}, 
ISSN={2161-5330}, 
month={Nov},}
@INPROCEEDINGS{7357239, 
author={Barnett, Scott and Avazpour, Iman and Vasa, Rajesh and Grundy, John}, 
booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={A multi-view framework for generating mobile apps}, 
year={2015}, 
volume={}, 
number={}, 
pages={305-306}, 
abstract={This paper demonstrates a multi-view framework for Rapid APPlication Tool (RAPPT). RAPPT enables rapid development of mobile applications. It employs a multilevel approach to mobile application development: a Domain Specific Visual Language to define the high level structure of mobile apps, a Domain Specific Textual Language to define behavioural concepts, and concrete source code for fine grained improvements.}, 
keywords={Navigation}, 
doi={10.1109/VLHCC.2015.7357239}, 
ISSN={}, 
month={Oct},}
@ARTICLE{8747425, 
author={Lin, Ying-Dar and Lai, Yu-Kuen and Tsou, Yung-Liang and Lai, Yuan-Cheng and Liou, En-Cheng and Chiang, Yita}, 
journal={IEEE Systems Journal}, 
title={Generic Validation Criteria and Methodologies for SDN Applications}, 
year={2019}, 
volume={13}, 
number={4}, 
pages={3909-3920}, 
abstract={Programmable control plane in software-defined networking (SDN), plays an essential role in the SDN architecture. The network function provided by the specialized hardware in a legacy network can be created in the form of software-based “SDN application” running on the controllers to manipulate entire network configurations. Therefore, the risk of having software bugs and errors in the SDN applications may threaten the normal operations of SDN networks. This paper presents systematic validation criteria and test cases based on the proposed novel methodologies for SDN application testing. The test framework can perform testbed build-up, generate desired packet sequences, and analyze results automatically. According to the results of a generic test suite, several issues are unveiled in the application under test (AUT). Some AUTs, which need to check all the incoming packets from OpenFlow switches, fail to meet the test criteria of burst packet-in and flow self-recycling. For most of the applications based on the Ryu controller, the evaluation results reveal that some are unable to recycle flow entries after they are unloaded. It is recommended that all flows populated by SDN applications must have timeout value specified to prevent unnecessary entries kept in the flow table.}, 
keywords={Control systems;Benchmark testing;Computer bugs;Software;Stability criteria;OpenFlow;performance evaluation;software defined network (SDN);system performance;testing;validation}, 
doi={10.1109/JSYST.2019.2921599}, 
ISSN={1937-9234}, 
month={Dec},}
@INPROCEEDINGS{5614873, 
author={Abbors, Fredrik and Truşcan, Dragoş}, 
booktitle={2010 Second International Conference on Advances in System Testing and Validation Lifecycle}, 
title={Approaching Performance Testing from a Model-Based Testing Perspective}, 
year={2010}, 
volume={}, 
number={}, 
pages={125-128}, 
abstract={The paper introduces the concept of model-based performance testing, which we plan to pursue in our research. The underlying idea is to describe various performance aspects as well as functional aspects of a software system using modeling languages like UML, and from the resulting models to automatically design tests that can be used for performance testing. In our research, we also plan to focus on how the modeling and traceability of performance requirements can be achieved across the testing process.}, 
keywords={Unified modeling language;Testing;Adaptation model;Analytical models;Software systems;Load modeling;Model-Based Testing;Model Validation;Requirements Traceability}, 
doi={10.1109/VALID.2010.22}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{9476896, 
author={Mirabella, A. Giuliano and Martin-Lopez, Alberto and Segura, Sergio and Valencia-Cabrera, Luis and Ruiz-Cortés, Antonio}, 
booktitle={2021 IEEE/ACM Third International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)}, 
title={Deep Learning-Based Prediction of Test Input Validity for RESTful APIs}, 
year={2021}, 
volume={}, 
number={}, 
pages={9-16}, 
abstract={Automated test case generation for RESTful web APIs is a thriving research topic due to their key role in software integration. Most approaches in this domain follow a black-box approach, where test cases are randomly derived from the API specification. These techniques show promising results, but they neglect constraints among input parameters (so-called inter-parameter dependencies), as these cannot be formally described in current API specification languages. As a result, when testing real-world services, most random test cases tend to be invalid since they violate some of the inter-parameter dependencies of the service, making human intervention indispensable. In this paper, we propose a deep learning-based approach for automatically predicting the validity of an API request (i.e., test input) before calling the actual API. The model is trained with the API requests and responses collected during the generation and execution of previous test cases. Preliminary results with five real-world RESTful APIs and 16K automatically generated test cases show that test inputs validity can be predicted with an accuracy ranging from 86% to 100% in APIs like Yelp, GitHub, and YouTube. These are encouraging results that show the potential of artificial intelligence to improve current test case generation techniques.}, 
keywords={Deep learning;Conferences;Neural networks;Restful API;Software;Distance measurement;Specification languages;RESTful web API;web services testing;artificial neural network}, 
doi={10.1109/DeepTest52559.2021.00008}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{9081629, 
author={Darwesh, Darbaz Nawzad and Annighöfer, Björn and Reichel, Reinhard}, 
booktitle={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)}, 
title={Semi-automated deployment of a High-lift system on IMA using the Selective Middleware}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={This paper presents a case study concerning the implementation of a High-Lift System on an Integrated Modular Avionics (IMA) platform using a system management middleware and a knowledge-based automatic instantiation process. Safety-critical avionics systems usually consist several system management functions besides the specific control functions, which contribute to high development costs. System management functions such as redundancy, signal path, and fault management could be generally constructed on generic logics. The Flexible Avionics Platform approach developed at the Institute of Aircraft Systems at the University of Stuttgart describes these logics in generic software bricks to instantiate the system management. A model and knowledge-based tool suite assists to derive the system management instantiation from an abstract high-level system architecture model. Ongoing research has introduced the Flexible Avionics Platform into the IMA environment as a selectively useable middleware called “Selective Middleware” in a partition space according to the ARINC653P4 interface specification. Consequently, a development process for the Selective Middleware has to be defined and validated by the development of an appropriate system to show the applicability in an industrial development process. Hence, this paper presents a development process for a selective Middleware, which focuses on system development aspects and gives an outlook on relevant qualification procedures. Aspects of DO-297 and ARP4754A are considered to derive the development process. Subsequently, a High-Lift System function is established on grounds of the Selective Middleware approach. This High-Lift System function is divided into two development processes of (1) the generation of the system management and IMA configuration files and (2) the development of the specific control functions.}, 
keywords={IMA;DO-297;model-based;knowledge-based;process automation;system design;system management;safety-critical system;distributed system;High-Lift System}, 
doi={10.1109/DASC43569.2019.9081629}, 
ISSN={2155-7209}, 
month={Sep.},}
@INPROCEEDINGS{5280052, 
author={Krapfenbauer, Harald and Ertl, Dominik and Zoitl, Alois and Kupzog, Friederich}, 
booktitle={2009 Fourth International Multi-Conference on Computing in the Global Information Technology}, 
title={Improving Component Testing of Industrial Automation Software}, 
year={2009}, 
volume={}, 
number={}, 
pages={259-262}, 
abstract={Industrial automation systems are tested nowadays mainly via system tests at a very late stage of development. These tests are conducted manually, are time-consuming and cost-intensive. Earlier testing of automation software, e.g., component testing, is therefore desired in order to reduce the effort for system testing by detecting errors sooner. In this paper we present an improved concept for a test environment that enables developers of industrial control electronics to test the functionality of IEC 61499 software components. Components can be tested on any hardware with an IEC 61499 runtime environment, even on the target hardware. There is no need to change the automation software for testing. We propose using dynamically typed languages to implement tests because such languages have inherent properties that are useful for this task. We provide example code of a typical test case.}, 
keywords={Automatic testing;Software testing;Computer industry;Automation;System testing;Electronic equipment testing;IEC standards;Hardware;Industrial control;Industrial electronics;Industrial Automation Software;IEC 61499;Component Testing;Dynamically Typed Languages}, 
doi={10.1109/ICCGI.2009.46}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6949287, 
author={Angmo, Rigzin and Sharma, Monika}, 
booktitle={2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)}, 
title={Performance evaluation of web based automation testing tools}, 
year={2014}, 
volume={}, 
number={}, 
pages={731-735}, 
abstract={In today's 21st century era countless software applications are written as a web based application which runs in a web browsers. With new technologies and commercialization of I.T. sector, the web based system has undergoes frequent and rapid changes. Today Softwares are coded as a web based application, which help to access data from any part of the globe. Even the economic relevance of web based enhances the control and quality of software. The quality assurance of any system depends on its test. But to do manually testing in most of the cases is time consuming, expensive and hectic. For the better business purpose and to save time and money automation testing is required. There are variety of tools are available in the market for this. One of the best known tool is selenium suite which is a combination of different automation testing tool. In this paper we will discuss about the selenium suite. It provides testers with different framework for different test cases. The main objective of this paper is to find the best tool in selenium suite and then compare it with some other tool for same task. For this purpose, performance evaluation is done on the basis of some criteria.}, 
keywords={Automation;Performance evaluation;Browsers;Software;Software testing;Information technology;Web applications;Selenium;Performance;Watir-webdriver;Test case;Automation testing}, 
doi={10.1109/CONFLUENCE.2014.6949287}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{8839463, 
author={Hofer, Florian and Russo, Barbara}, 
journal={IEEE Transactions on Industrial Informatics}, 
title={IEC 61131-3 Software Testing: A Portable Solution for Native Applications}, 
year={2020}, 
volume={16}, 
number={6}, 
pages={3942-3951}, 
abstract={Programmable logic controllers (PLCs) are the most used digital systems in manufacturing industry, but there is little support for test automation of such systems. As net result, testing is mostly done manually or not at all despite the recommendations of the IEC 61131-3 Standard. Attempts to provide an automated testing framework for PLCs have been recently performed with first successful results. The most advanced and promising framework proposes an approach close to object orientation that relies on nonnative language and platform. In this article, we propose a testing library called Advanced Program Organization Unit Testing Framework written in native language, built according to the unit testing paradigm, and supporting automated testing for simple and complex scenarios of IEC 61131-3-compliant PLCs. In this article, we present such library, discuss its performance and advantages, and illustrate its application to a real case study.}, 
keywords={Testing;IEC Standards;Libraries;Tools;Software;Hardware;Automation;CoDeSys;control software;IEC 61131-3;testing;unit test}, 
doi={10.1109/TII.2019.2941584}, 
ISSN={1941-0050}, 
month={June},}
@INPROCEEDINGS{232773, 
author={Simeu, E. and Puissochet, A. and Rainard, J.L. and Tagant, A.M. and Poize, M.}, 
booktitle={Digest of Papers. 1992 IEEE VLSI Test Symposium}, 
title={A new tool for random testability evaluation using simulation and formal proof}, 
year={1992}, 
volume={}, 
number={}, 
pages={321-326}, 
abstract={A set of tools is described, allowing one to compute random testability measurement for combinational circuits, based on a black box worst case hypothesis. These tools provide enough information to allow circuit modification, in order to meet a prescribed testability value. The efficiency of these tools is due to the use of a statistical method combined with formal proof mechanisms. The random testability of the complete ISCAS benchmark of combinational circuits is computed. For the least testable circuits, a few modifications, guided by the testability measurements, are shown to be sufficient to make them randomly testable.<>}, 
keywords={Circuit testing;Circuit faults;Computational modeling;Circuit simulation;Switching circuits;Combinational circuits;Benchmark testing;Built-in self-test;Telecommunication computing;Semiconductor device modeling}, 
doi={10.1109/VTEST.1992.232773}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6821183, 
author={Meyer, Stefan and Healy, Philip and Lynn, Theo and Morrison, John}, 
booktitle={2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing}, 
title={Quality Assurance for Open Source Software Configuration Management}, 
year={2013}, 
volume={}, 
number={}, 
pages={454-461}, 
abstract={Commonly used open source configuration management systems, such as Puppet, Chef and CFEngine, allow for system configurations to be expressed as scripts. A number of quality issues that may arise when executing these scripts are identified. An automated quality assurance service is proposed that identifies the presence of these issues by automatically executing scripts across a range of environments. Test results are automatically published to a format capable of being consumed by script catalogues and social coding sites. This would serve as an independent signal of script trustworthiness and quality to script consumers and would allow developers to be made quickly aware of quality issues. As a result, potential consumers of scripts can be assured that a script is likely to work when applied to their particular environment. Script developers can be notified of compatibility issues and take steps to address them.}, 
keywords={Servers;Operating systems;Communities;Linux;Quality assurance;Testing;Automated configuration;configuration management;continuous integration;automated deployment;service orchestration;assurance}, 
doi={10.1109/SYNASC.2013.66}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7363910, 
author={Richardet, Renaud and Chappelier, Jean-Cédric and Tripathy, Shreejoy and Hill, Sean}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={Agile text mining with Sherlok}, 
year={2015}, 
volume={}, 
number={}, 
pages={1479-1484}, 
abstract={The successful development of an intelligent text mining application requires the collaboration of two main stakeholders: subject matter experts and text miners. In this paper, we describe a new methodology, agile text mining to improve that collaboration. Agile text mining is characterized by short development cycles, frequent tasks redefinition and continuous performance monitoring through integration tests. We introduce Sherlok, a system supporting the development of agile text mining applications and present an application to extract mention of neurons from a very large corpus of scientific articles. The resulting code and models are publicly available.}, 
keywords={Text mining;Pipelines;Ontologies;Engines;Proteins;Collaboration;natural language processing;text mining;big data;UIMA;agile data science}, 
doi={10.1109/BigData.2015.7363910}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6405436, 
author={Lauret, Jimmy and Waeselynck, Helene and Fabre, Jean-Charles}, 
booktitle={2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops}, 
title={Detection of Interferences in Aspect-Oriented Programs Using Executable Assertions}, 
year={2012}, 
volume={}, 
number={}, 
pages={165-170}, 
abstract={Aspect-oriented programming (AOP) is a technique that promotes separation of concerns. Unfortunately, it still suffers from well-known composition issues, in particular from undesirable interferences when multiple concerns are applied at the same join point. In this paper we propose an approach to detect interferences side effect using executable assertions. The assertions are inserted in the aspect chain to detect various types of interferences. The implementation is based on the AIRIA resolver construct, recently introduced to better control conflicting aspects in AspectJ. Resolvers add observation points that were lacking in AspectJ. We propose to take advantage of this to implement automated detection of interferences at execution time. We study the feasibility of this approach and demonstrate it on artificial examples.}, 
keywords={Interference;Monitoring;Instruments;Weaving;Encryption;Data structures;Programming;Aspect interference;executable assertions;verification}, 
doi={10.1109/ISSREW.2012.34}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7923802, 
author={Ramos De Oliveira, Ricardo and Martins, Rafael Messias and Da Silva Simao, Adenilso}, 
booktitle={2017 IEEE International Conference on Cloud Engineering (IC2E)}, 
title={Impact of the Vendor Lock-in Problem on Testing as a Service (TaaS)}, 
year={2017}, 
volume={}, 
number={}, 
pages={190-196}, 
abstract={Testing as a Service (TaaS) is a new business and service model that provides efficient and effective software quality assurance and enables the use of a cloud for the meeting of quality standards, requirements and consumer's needs. However, problems that limit the effective use of TaaS involve lack of standardization in writing, execution, configuration and management of tests and lack of portability and interoperability among TaaS platforms - the so-called lock-in problem. The lock-in problem is a serious threat to software testing in the cloud and may become critical when a provider decides to suddenly increase prices, or shows serious technical availability problems. This paper proposes a novel approach for solving the lock-in problem in TaaS with the use of design patterns. The aim to assist software engineers and quality control managers in building testing solutions that are both portable and interoperable and promote a more widespread adoption of the TaaS model in cloud computing.}, 
keywords={Cloud computing;Testing;Computational modeling;Interoperability;Browsers;Context;Cloud Computing;Testing as a Service (TaaS);Design Patterns;Vendor Lock-in;Testing Service}, 
doi={10.1109/IC2E.2017.30}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{5254276, 
author={Mei, Lijun and Chan, W. K. and Tse, T. H. and Kuo, Fei-Ching}, 
booktitle={2009 33rd Annual IEEE International Computer Software and Applications Conference}, 
title={An Empirical Study of the Use of Frankl-Weyuker Data Flow Testing Criteria to Test BPEL Web Services}, 
year={2009}, 
volume={1}, 
number={}, 
pages={81-88}, 
abstract={Programs using service-oriented architecture (SOA) often feature ultra-late binding among components. These components have well-defined interfaces and are known as Web services. Messages between every pair of Web services dually conform to the output interface of a sender and the input interface of a receiver. Unit testing of Web services should not only test the logic of Web services, but also assure the correctness of the Web services during input, manipulation, and output of messages. There is, however, little software testing research in this area. In this paper, we study the unit testing problem to assure components written in orchestration languages, WS-BPEL in particular. We report an empirical study of the effectiveness of the Frankl-Weyuker data flow testing criteria (particularly the all-uses criterion) on WS-BPEL subject programs. Our study shows that conventional data flow testing criteria can be much less effective in revealing faults in interface artifacts (WSDL documents) and message manipulations (XPath queries) than revealing faults in BPEL artifacts.}, 
keywords={Web services;Logic testing;XML;Software testing;Application software;Service oriented architecture;Councils;Information retrieval;Computer applications;Data flow computing;WS-BPEL;XPath;data flow testing}, 
doi={10.1109/COMPSAC.2009.21}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{8904799, 
author={Meyers, Bart and Gadeyne, Klaas and Oakes, Bentley and Bernaerts, Matthias and Vangheluwe, Hans and Denil, Joachim}, 
booktitle={2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
title={A Model-Driven Engineering Framework to Support the Functional Safety Process}, 
year={2019}, 
volume={}, 
number={}, 
pages={619-623}, 
abstract={The design of safety-related systems traditionally has long and costly development cycles due to the highly manual safety engineering process, which is guided by industry standards. In this paper, we present a modelling framework that supports DevOps principles of continuous testing and fast development iterations for the design of safety-critical systems. We show how modelling can help introducing DevOps in the context of functional safety analysis, and we also report how DevOps was used during the development of the framework.}, 
keywords={devops;safety critical;verification;automotive;iso26262}, 
doi={10.1109/MODELS-C.2019.00094}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7589821, 
author={Tang, C.M. and Keung, Jacky and Yu, Y.T. and Chan, W.K.}, 
booktitle={2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
title={DFL: Dual-Service Fault Localization}, 
year={2016}, 
volume={}, 
number={}, 
pages={412-422}, 
abstract={In engineering a service, software developers often construct and deploy a newer (forthcoming) version of the service to replace the current version. A forthcoming version is often placed online for users to consume and report feedback. In the case of observed failures, the forthcoming version should be debugged and further evolved. In this paper, we propose the model of dual-service fault localization (DFL) to aid this evolution process. Many prior research studies on spectrum-based fault localization (SBFL) consider each version separately. The DFL model correlates the dynamic execution spectra of the current and the forthcoming versions of the same service placed for live test of the forthcoming version, and dynamically generates an adaptive fault localization formula to estimate the code regions in the forthcoming service responsible for the observed failures. We report an experiment in which we initialized the DFL model into six instances, each using an ensemble technique dynamically composed from 11 existing SBFL formulas, and applied the model to four benchmarks. The results show that DFL is feasible and multiple instances are statistically more effective than, if not as effective as, the best of these individual SBFL formulas on each benchmark.}, 
keywords={Software;Debugging;Computer bugs;Adaptation models;Benchmark testing;Production;Companies;debugging;spectrum-based fault localization;ensemble techniques;dual-service fault localization}, 
doi={10.1109/QRS.2016.53}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7338271, 
author={Ali, Shaukat and Yue, Tao}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={Formalizing the ISO/IEC/IEEE 29119 Software Testing Standard}, 
year={2015}, 
volume={}, 
number={}, 
pages={396-405}, 
abstract={Model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in the literature and in the practice. However, all of the techniques have been developed using their own concepts and terminology of MBT, which are very often different than other techniques and at times have conflicting semantics. Moreover, while working on MBT projects with our industrial partners in the last several years, we were unable to find a unified way of defining MBT techniques based on standard terminology. To precisely define MBT concepts with the aim of providing common understanding of MBT terminology across techniques, we formalize a small subset of the recently released ISO/IEC/IEEE 29119 Software Testing Standard as a conceptual model (UML class diagrams) together with OCL constraints. The conceptual model captures all the necessary concepts based on the standard terminology that are mandatory or optional in the context of MBT techniques and can be used to define new MBT tools and techniques. To validate the conceptual model, we instantiated its concepts for various MBT techniques previously developed in the context of our industrial partners. Such instantiation automatically enforces the specified OCL constraints. This type of validation provided us feedback to further refine the conceptual model. Finally, we also provide our experiences and lessons learnt for such formalization and validation.}, 
keywords={Unified modeling language;Concrete;Testing;Terminology;Data models;ISO Standards;Model-Based Testing;ISO/IEC/IEEE 29119;UML;Test Case Generation;Modeling Methodology}, 
doi={10.1109/MODELS.2015.7338271}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6934560, 
author={Agnelli, S. and Feltz, P. and Griffiths, P-F. and Roth, D.}, 
booktitle={2014 7th Advanced Satellite Multimedia Systems Conference and the 13th Signal Processing for Space Communications Workshop (ASMS/SPSC)}, 
title={Satellite's role in the penetration of broadband connectivity within the European Union}, 
year={2014}, 
volume={}, 
number={}, 
pages={306-311}, 
abstract={The European Commission (EC) has recently acknowledged that broadband coverage for all - the 2013 target of the Digital Agenda for Europe (DAE) - has been achieved thanks to satellite broadband services, such as Tooway™, the consumer-grade Internet access at 20 Mbps via the Eutelsat KA-SAT satellite. However, broadband take-up in the European Union (EU) is still far from being satisfactory, notably in rural and remote areas where satellite solutions are ideally suited.}, 
keywords={Broadband communication;Satellites;Europe;Internet;Investment;Multimedia systems;Signal processing;Broadband Coverage;Broadband Take-Up;Digital Divide;Digital Agenda for Europe;European Union;KA-SAT;SABER Project;Voucher Schemes}, 
doi={10.1109/ASMS-SPSC.2014.6934560}, 
ISSN={2326-5949}, 
month={Sep.},}
@INPROCEEDINGS{9159060, 
author={Mai, Phu X. and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel}, 
booktitle={2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)}, 
title={Metamorphic Security Testing for Web Systems}, 
year={2020}, 
volume={}, 
number={}, 
pages={186-197}, 
abstract={Security testing verifies that the data and the resources of software systems are protected from attackers. Unfortunately, it suffers from the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior. In many situations where potential vulnerabilities are tested, a test oracle may not exist, or it might be impractical due to the many inputs for which specific oracles have to be defined. In this paper, we propose a metamorphic testing approach that alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture security properties of the system. Such MRs are then used to automate testing and detect vulnerabilities. We provide a catalog of 22 system-agnostic MRs to automate security testing in Web systems. Our approach targets 39% of the OWASP security testing activities not automated by state-of-the-art techniques. It automatically detected 10 out of 12 vulnerabilities affecting two widely used systems, one commercial and the other open source (Jenkins).}, 
keywords={Testing;Software;Uniform resource locators;Graphical user interfaces;Authorization;Transforms;Software Engineering;Software Security}, 
doi={10.1109/ICST46399.2020.00028}, 
ISSN={2159-4848}, 
month={Oct},}
@INPROCEEDINGS{7107547, 
author={Biňas, M.}, 
booktitle={2014 IEEE 12th IEEE International Conference on Emerging eLearning Technologies and Applications (ICETA)}, 
title={Identifying web services for automatic assessments of programming assignments}, 
year={2014}, 
volume={}, 
number={}, 
pages={45-50}, 
abstract={The main objective of this article is to verify the assumption, if the web services can be used in the process of automatic assessment of programming assignments. It tries to identify general services for such processes and presents the experimental part by creating platform based on set of web services.}, 
keywords={Web services;Programming;Testing;Electronic mail;Plagiarism;Uniform resource locators;Electronic learning}, 
doi={10.1109/ICETA.2014.7107547}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{9014711, 
author={Medhat, Noha and Moussa, Sherin and Badr, Nagwa and Tolba, Mohamed F.}, 
booktitle={2019 Ninth International Conference on Intelligent Computing and Information Systems (ICICIS)}, 
title={Testing Techniques in IoT-based Systems}, 
year={2019}, 
volume={}, 
number={}, 
pages={394-401}, 
abstract={Internet of Things (IoT) systems are fast evolving nowadays, in which huge amounts of data are produced rapidly from heterogeneous sources. The nature of IoT-based systems implies many challenges, in terms of operation, security, quality control and data management. Thus, testing such systems is a key element to their success. In this paper, we present a comprehensive study for the main testing techniques and tools that have been considered for the IoT-based systems. Detailed comparison and analytical criticism are conducted, identifying the different testing types that have been applied for the main application domains. The research gaps are addressed, which highlight the future directions that can be adopted.}, 
keywords={Testing;Sensors;Encryption;Internet of Things;Cloud computing;Tools;Testing;Internet of things (IoT);IoT-based systems;Testing Framework;Testing Tools;IoT Applications}, 
doi={10.1109/ICICIS46948.2019.9014711}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{1203468, 
author={De, P. and Neogi, A. and Chiueh, T.-C.}, 
booktitle={23rd International Conference on Distributed Computing Systems, 2003. Proceedings.}, 
title={VirtualWire: a fault injection and analysis tool for network protocols}, 
year={2003}, 
volume={}, 
number={}, 
pages={214-221}, 
abstract={The prevailing practice for testing protocol implementations is direct code instrumentation to trigger specific states in the code. This leaves very little scope for reuse of the test cases. In this paper, we present the design, implementation, and evaluation of VirtualWire, a network fault injection and analysis system designed to facilitate the process of testing network protocol implementations. VirtualWire injects user-specified network faults and matches network events against anticipated responses based on high-level specifications written in a declarative scripting language. With VirtualWire, testing requires no code instrumentation and fault specifications can be reused across versions of a protocol implementation. We illustrate the effectiveness of VirtualWire with examples drawn from testing Linux's TCP implementation and a real-time Ethernet protocol called Rether. In each case, 10 to 20 lines of script is sufficient to specify the test scenario. VirtualWire is completely transparent to the protocols under test, and additional overhead in protocol processing latency it introduces is below 10% of the normal.}, 
keywords={Instruments;Access protocols;Ethernet networks;Kernel;System testing;Filters;Laboratories;Computer science;Delay;Formal verification}, 
doi={10.1109/ICDCS.2003.1203468}, 
ISSN={1063-6927}, 
month={May},}
@INPROCEEDINGS{114002, 
author={Bonet, L. and Ganger, J. and Girardeau, J. and Greaves, C. and Pendleton, M. and Yatim, D.}, 
booktitle={Proceedings. International Test Conference 1990}, 
title={Test features of the MC145472 ISDN U-transceivers}, 
year={1990}, 
volume={}, 
number={}, 
pages={68-79}, 
abstract={The design of a single-chip implementation of a 2B1Q ISDN (integrated services digital network) U transceiver that meets the ANSI T1.601 standards has been completed. The MC145472 was designed with testability in mind and to be consistent with Motorola's design-for-manufacturability goals. The authors describe in detail the design-for-testability techniques specifically intended for the IC manufacturer production test and other ad hoc test/diagnostic structures for the customer to use in evaluating system performance. A global test strategy for testing the ISDN U transceiver is presented. The test features have been used extensively not only for testing the device in the production environment but also for conducting evaluations and design verification experiments during the chip debugging phase. The test features described are well integrated with the architecture of the chip, thus minimizing incremental cost.<>}, 
keywords={ISDN;Transceivers;Integrated circuit testing;System testing;ANSI standards;Manufacturing;Production systems;System performance;Debugging;Costs}, 
doi={10.1109/TEST.1990.114002}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{5389336, 
author={Wile, B. and Mullen, M. P. and Hanson, C. and Bair, D. G. and Lasko, K. M. and Duffy, P. J. and Kaminski, E. J. and Gilbert, T. E. and Licker, S. M. and Sheldon, R. G. and Wollyung, W. D. and Lewis, W. J. and Adkins, R. J.}, 
journal={IBM Journal of Research and Development}, 
title={Functional verification of the CMOS S/390 Parallel Enterprise Server G4 system}, 
year={1997}, 
volume={41}, 
number={4.5}, 
pages={549-566}, 
abstract={Verification of the S/390® Parallel Enterprise Server G4 processor and level 2 cache (L2) chips was performed using a different approach than previously. This paper describes the methods employed by our functional verification team to demonstrate that its logical system complied with the S/390 architecture while staying within the changing cost structure and time-to-market constraints. Verification proceeded at four basic levels defined by the breadth of logic being tested. The lowest level, designer macro verification, contained a single designer's hardware description language (in VHDL). Unit-level verification consisted of a logical portion of function that generally contained four or five designers' logic. The third level of verification was the chip level, in which the processor or L2 chips were individually tested. Finally, system-level verification was performed on symmetric multiprocessor (SMP) configurations that included bus-switching network (BSN) chips and I/O connection chips, designated as memory bus adaptors (MBAs), along with multiple copies of the processor and L2 chips.}, 
keywords={}, 
doi={10.1147/rd.414.0549}, 
ISSN={0018-8646}, 
month={July},}
@INPROCEEDINGS{7588753, 
author={Chen, Lei and James, Phillip and Kirkwood, David and Nguyen, Hoang Nga and Nicholson, Gemma L and Roggenbach, Markus}, 
booktitle={2016 IEEE International Conference on Intelligent Rail Transportation (ICIRT)}, 
title={Towards integrated simulation and formal verification of rail yard designs - an experience report based on the UK East Coast Main Line}, 
year={2016}, 
volume={}, 
number={}, 
pages={347-355}, 
abstract={The development of railway systems is often supported by a range of tools, each addressing individual, but overlapping concerns such as, e.g., performance or safety analysis. However, it is a challenge for users to organise work-flows; results are often in different, non-aligning data formats; furthermore, tools work on different levels of abstraction from macro to microscopic. Thus, tool integration would be beneficial, and also allow for more playful, experimental prototyping and design. This paper reports on lessons learned from the integration of BRaVE - the Birmingham Railway Virtual Environment - and OnTrack from Swansea University. BRaVE is an easy-to-use railway simulation software for development, modelling and flow analysis. OnTrack allows for the automatic verification of scheme plans against a number of safety properties via different formal methods. We present an approach that bridges the gap that occurs from varying details in data sources through automated transformations. This integration provides a first step towards a seamless environment for prototyping, concept development, and safety analysis under ”one roof”. We demonstrate the usefulness of our approach by giving integrated simulation and verification results for the UK East Coast Main Line. This work is part of the wider RSSB's Future Traffic Regulation Optimisation research programme.}, 
keywords={Rail transportation;Solid modeling;Analytical models;Data models;Computational modeling;Vehicles}, 
doi={10.1109/ICIRT.2016.7588753}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{9448913, 
author={Marksteiner, Stefan and Marko, Nadja and Smulders, Andre and Karagiannis, Stelios and Stahl, Florian and Hamazaryan, Hayk and Schlick, Rupert and Kraxberger, Stefan and Vasenev, Alexandr}, 
booktitle={2021 IEEE 93rd Vehicular Technology Conference (VTC2021-Spring)}, 
title={A Process to Facilitate Automated Automotive Cybersecurity Testing}, 
year={2021}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Modern vehicles become increasingly digitalized with advanced information technology-based solutions like advanced driving assistance systems and vehicle-to-x communications. These systems are complex and interconnected. Rising complexity and increasing outside exposure has created a steadily rising demand for more cyber-secure systems. Thus, also standardization bodies and regulators issued standards and regulations to prescribe more secure development processes. This security, however, also has to be validated and verified. In order to keep pace with the need for more thorough, quicker and comparable testing, today's generally manual testing processes have to be structured and optimized. Based on existing and emerging standards for cybersecurity engineering, this paper therefore outlines a structured testing process for verifying and validating automotive cybersecurity, for which there is no standardized method so far. Despite presenting a commonly structured framework, the process is flexible in order to allow implementers to utilize their own, accustomed toolsets.}, 
keywords={Regulators;Conferences;Manuals;Regulation;Complexity theory;Computer security;Standards;Security;Cybersecurity;Testing;Automotive;Validation;Verification;Process}, 
doi={10.1109/VTC2021-Spring51267.2021.9448913}, 
ISSN={2577-2465}, 
month={April},}
@INPROCEEDINGS{8901892, 
author={Emmerich, Paul and Ellmann, Simon and Bonk, Fabian and Egger, Alex and Sánchez-Torija, Esaú García and Günzel, Thomas and di Luzio, Sebastian and Obada, Alexandru and Stadlmeier, Maximilian and Voit, Sebastian and Carle, Georg}, 
booktitle={2019 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS)}, 
title={The Case for Writing Network Drivers in High-Level Programming Languages}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-13}, 
abstract={Drivers are written in C or restricted subsets of C++ on all production-grade server, desktop, and mobile operating systems. They account for 66 % of the code in Linux, but 39 out of 40 security bugs related to memory safety found in Linux in 2017 are located in drivers. These bugs could have been prevented by using high-level languages for drivers. We present user space drivers for the Intel ixgbe 10 Gbit/s network cards implemented in Rust, Go, C#, Java, OCaml, Haskell, Swift, JavaScript, and Python written from scratch in idiomatic style for the respective languages. We quantify costs and benefits of using these languages: High-level languages are safer (fewer bugs, more safety checks), but run-time safety checks reduce throughput and garbage collection leads to latency spikes. Out-of-order CPUs mitigate the cost of safety checks: Our Rust driver executes 63 % more instructions per packet but is only 4 % slower than a reference C implementation. Go's garbage collector keeps latencies below 100 μs even under heavy load. Other languages fare worse, but their unique properties make for an interesting case study. All implementations are available as free and open source at https://githud.com/ixy-languages/ixy-languages.}, 
keywords={Rust;Go;C#;Java;OCaml;Haskell;Swift;JavaScript;Python}, 
doi={10.1109/ANCS.2019.8901892}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5985928, 
author={Siegl, Sebastian and Hielscher, Kai-Steffen and German, Reinhard and Berger, Christian}, 
booktitle={2011 12th Latin American Test Workshop (LATW)}, 
title={Automated testing of embedded automotive systems from requirement specification models}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Embedded software for modern automotive and avionic systems is increasingly complex. In early design phases, even when there is still uncertainty about the feasibility of the requirements, valuable information can be gained from models that describe the expected usage and the desired system reaction. The generation of test cases from these models indicates the feasibility of the intended solution and helps to identify scenarios for which the realization is hardly feasible or the intended system behavior is not properly defined. In this paper we present the formalization of requirements by models to simulate the expected field usage of a system. These so called usage models can be enriched by information about the desired system reaction. Thus, they are the basis for all subsequent testing activities: First, they can be used to verify the first implementation models and design decisions w.r.t. the fulfillment of requirements and second, test cases can be derived in a random or statistic manner. The generation can be controlled with operational profiles that describe different classes of field usage. We have applied our approach at a large German car manufacturer in the early development phase of active safety functionalities. Test cases were generated from the usage models to assess the implementation models in MATLAB/Simulink. The parametrization of the systems could be optimized and a faulty transition in the implementation models was revealed. These design and implementation faults had not been discovered with the established test method.}, 
keywords={Safety;Testing;Timing;MATLAB;Belts;Clocks}, 
doi={10.1109/LATW.2011.5985928}, 
ISSN={2373-0862}, 
month={March},}
@INPROCEEDINGS{7128891, 
author={Wanderley, Fernando and Silva, Antonio and Araújo, João}, 
booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
title={Evaluation of BehaviorMap: A user-centered behavior language}, 
year={2015}, 
volume={}, 
number={}, 
pages={309-320}, 
abstract={In the software development process, one of the recurring problems is to ensure that the expectations of stakeholders are being met. These expectations must match the system's behavior and be present in the requirements specifications and models. The Requirements Engineering discipline studies how to capture, specify, validate and manage requirements. However, recent empirical studies show that stakeholders do not usually understand traditional requirements models. This paper focuses on the cognitive evaluation of a user-centered language called BehaviorMap that aims to specify behavioral user scenarios in a cognitive way, based on mind map modelling. This paper describes an experimental evaluation to verify the understandability of the BehaviorMap scenarios compared to the textual ones. The experiment gathered data from 15 individuals (naïve-users), with different backgrounds, that had to analyze 8 scenarios, being 4 graphical and 4 textual. To assess the participants' cognitive effort, it was used questionnaires. Also, the time effort to perform the tasks was measured. This experiment showed promising results for the BehaviorMap scenarios.}, 
keywords={Data structures;Boolean functions;Visualization;Software;Atmospheric measurements;Particle measurements;Agile Requirements;Mind Map Modelling;Behavior-Driven Design;User-Centred Requirements;Cognitive Effort}, 
doi={10.1109/RCIS.2015.7128891}, 
ISSN={2151-1357}, 
month={May},}
@INPROCEEDINGS{4577708, 
author={Gargantini, A. and Riccobene, E. and Scandurra, P.}, 
booktitle={2008 International Symposium on Industrial Embedded Systems}, 
title={A model-driven validation & verification environment for embedded systems}, 
year={2008}, 
volume={}, 
number={}, 
pages={241-244}, 
abstract={This paper presents a validation and verification tool component, based on the abstract state machine formal method, that we are developing to support high level formal analysis of embedded system model-driven design. This component is integrated into a model-driven environment for HW/SW co-design that provides a graphical high-level representation of HW and SW components by means of UML profiles for SystemC/multi-thread C, and allows C/C++/SystemC code generation/back-annotation from/to graphical UML models.}, 
keywords={Embedded system;Unified modeling language;Hardware;Diffusion tensor imaging;Application software;Computer architecture;Industrial control;Standardization;Job shop scheduling;Timing}, 
doi={10.1109/SIES.2008.4577708}, 
ISSN={2150-3117}, 
month={June},}
@ARTICLE{9179780, 
author={Mathieson, John T. J. and Mazzuchi, Thomas and Sarkani, Shahram}, 
journal={IEEE Systems Journal}, 
title={The Systems Engineering DevOps Lemniscate and Model-Based System Operations}, 
year={2021}, 
volume={15}, 
number={3}, 
pages={3980-3991}, 
abstract={Systems engineering is defined as a “full life cycle” discipline and provides methodologies and processes to support the design, development, verification, sustainment, and disposal of systems. While this cradle-to-grave concept is well documented throughout literature, there has been recent emphasis on evolving and digitally transforming systems engineering methodologies, practices, and tools for the latter phases of system life cycles. This article adapts principles from the software engineering domain DevOps concept (a collaborative merger of system development and operations) into a Systems Engineering DevOps (SEDevOps) life cycle model. This facilitates a merger of systems engineering processes, tools, and products into a surrogate operational environment in which the sustainment of a system is tied closely to the curation of a system model expanded to include the enabling system elements necessary for operations and sustainment (procedures, scripts, etc.). This progression of the systems engineering mindset, focused on digitally transforming and enhancing system operations and sustainment, improves agility in later life cycle phases. A framework for applying SEDevOps is introduced as a new systems modeling language profile. A use-case leveraging this “model-based system operations” framework, shows how merging support elements into a spacecraft system model improves adaptability during operations, exemplifying elements of a DevOps approach to cyber-physical system sustainment.}, 
keywords={Modeling;Tools;Unified modeling language;Systems operation;Software engineering;Adaptation models;Agile;DevOps;digital transformation;life cycle;model-based systems engineering (MBSE);systems engineering}, 
doi={10.1109/JSYST.2020.3015595}, 
ISSN={1937-9234}, 
month={Sep.},}
@ARTICLE{7140715, 
author={}, 
journal={IEEE Std 2030.2-2015}, 
title={IEEE Guide for the Interoperability of Energy Storage Systems Integrated with the Electric Power Infrastructure}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-138}, 
abstract={This guide applies the smart grid interoperability reference model (SGIRM) process (IEEE Std 2030-2011) to energy storage by highlighting the information relevant to energy=storage system (ESS) interoperability with the energy power system (EPS). The process can be applied to ESS applications located on customer premises, at the distribution level, and on the transmission level (i.e., bulk storage). This guide provides useful industry-derived definitions for ESS characteristics, applications, and terminology that, in turn, simplify the task of defining system information and communications technology (ICT) requirements. As a result. these requirements can be communicated more clearly and consistently in project specifications. This guide also presents a methodology that can be used for most common ESS projects to describe the power system, communications, and information technology (IT) perspectives based on the IEEE 2030 definitions. From this framework, a seemingly complex system can be more clearly understood by all project stakeholders. Emerging cybersecurity requirements can also be incorporated into the framework as appropriate. Additionally, this guide provides the templates that can be used to develop requirements for an ESS project and goes through several real-world ESS project examples step by step.}, 
keywords={IEEE Standards;Energy storage;Batteries;Smart grids;Electric power systems;Power system reliability;battery;communications technology;electric power system;energy storage system;IEEE 2030.2(TM);information technology;interoperability;power system;Smart Grid}, 
doi={10.1109/IEEESTD.2015.7140715}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7962325, 
author={Liechti, Olivier and Pasquier, Jacques and Reis, Rodney}, 
booktitle={2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST)}, 
title={Supporting Agile Teams with a Test Analytics Platform: A Case Study}, 
year={2017}, 
volume={}, 
number={}, 
pages={9-15}, 
abstract={Continuous improvement, feedback mechanisms and automated testing are cornerstones of agile methods. We introduce the concept of test analytics, which brings these three practices together. We illustrate the concept with an industrial case study and describe the experiments run by a team who had set a goal for itself to get better at testing. Beyond technical aspects, we explain how these experiments have changed the mindset and the behaviour of the team members. We then present an open source test analytics platform, later developed to share the positive learnings with the community. We describe the platform features and architecture and explain how it can be easily put to use. Before the conclusions, we explain how test analytics fits in the broader context of software analytics and present our ideas for future work.}, 
keywords={Testing;Software;Companies;Context;Collaboration;agile development;automated testing;gamification;feedback channels}, 
doi={10.1109/AST.2017.3}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6037562, 
author={Agaram, Mukundan K. and Liu, Chang}, 
booktitle={2011 IEEE 15th International Enterprise Distributed Object Computing Conference}, 
title={An Engine-Independent Framework for Business Rules Development}, 
year={2011}, 
volume={}, 
number={}, 
pages={75-84}, 
abstract={There is a compelling need for highly customized Domain Specific Languages and Business Vocabulary in certain industries such as insurance, mortgage, and finance to enable Knowledge Workers to articulate and to automate complex rules pertinent to their areas of function within their companies. Rule Engine vendors attempt to provide a solution to the problem by selling an integrated Rules Engine and Business Rules Management System. Usually, the BRMS's provided by vendors need to be customized and integrated into the overall Enterprise Architecture. This results in the Enterprise Architecture to be tightly coupled with the vendor's rule offering. Moreover, it poses a significant risk to the Enterprise as vendor solutions change between releases. The Enterprise Architecture needs a way to insulate itself from such impacts. This paper describes a framework that delivers the core BRMS functions of authoring and representation in a vendor neutral fashion. In addition, the paper sheds light on specific areas of the framework that can be standardized.}, 
keywords={Vocabulary;Business;Engines;Production;Dentistry;Computer architecture;Syntactics;Business Rules;Business Rules Languages;Business Rule Components;Business Vocabulary;Domain Specific Languages}, 
doi={10.1109/EDOC.2011.20}, 
ISSN={1541-7719}, 
month={Aug},}
@ARTICLE{7389278, 
author={Bellucci, Andrea and Romano, Marco and Aedo, Ignacio and Díaz, Paloma}, 
journal={IEEE Pervasive Computing}, 
title={Software Support for Multitouch Interaction: The End-User Programming Perspective}, 
year={2016}, 
volume={15}, 
number={1}, 
pages={78-86}, 
abstract={The hardware development of the past years favored the widespread diffusion of multitouch devices (such as smartphones, tablets, and interactive tabletops) to such an extent that a wide variety of users are now exploiting them to perform different activities on a daily basis. In the heterogeneous and manifold context of modern computation, it is impossible to predict, at design time, all the possible configurations of such technologies, and especially the way users will be willing to interact with them. Therefore, empowering end users with tools for developing multitouch interaction is a promising step toward the materialization of ubiquitous computing. The aim of this survey is to frame the state of the art of existing multitouch software development tools from an end-user programming (EUP) perspective.}, 
keywords={Software tools;Programming profession;Graphics;Complexity theory;Sensors;coding tools and techniques;ubiquitous computing;end-user programming;pervasive computing;software engineering;graphics;mobile}, 
doi={10.1109/MPRV.2016.3}, 
ISSN={1558-2590}, 
month={Jan},}
@INPROCEEDINGS{8502650, 
author={Winkler, Dietmar and Meixner, Kristof and Biffl, Stefan}, 
booktitle={2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Towards Flexible and Automated Testing in Production Systems Engineering Projects}, 
year={2018}, 
volume={1}, 
number={}, 
pages={169-176}, 
abstract={Automated and systematic testing of automation systems (AS) and production systems (PS) require an integrated testing tool chain for test case development, execution and reporting. In practice, the test automation tool chain cannot be fully automated because of missing links between different tools used in the test automation process. Closing these gaps typically require (high) human effort. Furthermore, domain and software testing expertise is often bundled by one (expensive) engineer who is responsible for the application domain (reflected in use cases and test cases) and software tests (software test code). This paper presents a flexible Testing Automation Framework (TAF) that enables the configuration of test processes involving different tools and various layers for test automation and enables separated roles for the application domain and software tests. We build on best-practice test automation from Software Engineering and design a test automation process for the automation systems domain. We demonstrate the feasibility with a use case, derived from production systems automation, with selected tools covering all test automation layers. First results showed the feasibility of the framework in the evaluation use case making test processes more flexible and automated. Although the successful implementation of the TAF can support the efficient configuration and execution of test processes, there is additional effort for preparing the flexible and automated tool chain.}, 
keywords={Automation;Tools;Testing;Software;Production systems;Unified modeling language;Software engineering;software and system testing;test automation framework;automation systems;production systems;test configuration;feasibility study}, 
doi={10.1109/ETFA.2018.8502650}, 
ISSN={1946-0759}, 
month={Sep.},}
@INPROCEEDINGS{9304609, 
author={Xinxin, Zhang and Fei, Li and Xiangbin, Wu}, 
booktitle={2020 IEEE Intelligent Vehicles Symposium (IV)}, 
title={CSG: Critical Scenario Generation from Real Traffic Accidents}, 
year={2020}, 
volume={}, 
number={}, 
pages={1330-1336}, 
abstract={Autonomous driving (AD) is getting closer to our life, but the severe traffic accidents of autonomous vehicle (AV) happened in the past several years warn us that the safety of AVs is still a big challenge for the AD industry. Before volume production, the automotive industry and regulators must ensure the AV can deal with dangerous scenarios. Although road test is the most common method to test the performance and safety of an AV, it has some manifest disadvantages, e.g., highly risky and unrepeatable, low efficiency and lack of useful critical scenarios. Critical-scenario-based simulation can effectively address these problems and become an important complement to road test. In this paper, we present a novel approach to extract critical scenarios from real traffic accident videos and re-generate them in a simulator. We also introduce our integrated toolkit for scenario extraction and scenario test. With the toolkit, we can build a critical scenario library quickly and use it as a benchmark for AV safety assessment, among other purposes. On top of this, we further introduce our safety assessment criteria and scoring method.}, 
keywords={Roads;Accidents;Safety;Videos;Libraries;Three-dimensional displays;Automobiles}, 
doi={10.1109/IV47402.2020.9304609}, 
ISSN={2642-7214}, 
month={Oct},}
@INPROCEEDINGS{7880429, 
author={Jamous, Naoum and Bosse, Sascha and Görling, Carsten and Hintsch, Johannes and Khan, Ateeq and Kramer, Frederik and Müller, Hendrik and Turowski, Klaus}, 
booktitle={2016 4th International Conference on Enterprise Systems (ES)}, 
title={Towards an IT Service Lifecycle Management (ITSLM) Concept}, 
year={2016}, 
volume={}, 
number={}, 
pages={29-38}, 
abstract={Information Technology (IT) usage in enterprises has evolved over the last years. This led to today's complex, heterogeneous, and dynamic IT system landscapes that support business processes in enterprises. To manage these landscapes, the IT Service Management (ITSM) concept is gaining more importance in today's business and research. Studies demonstrate that introducing ITSM standards lead to positive effects, such as improved customer-orientation as well as efficiency and transparency of IT support, which justify the costs of implementation. However, companies still face difficulties in deciding which processes to be implement (first), and to which extent. Questions like: "How can the currently applied ITSM be adapted or extended when new business-related or technological challenges appear?" arise. Goods producing companies started early relying on Product Lifecycle Management (PLM). PLM delivers a solid means to define, discuss, analyze, and better standardize value creation processes. With PLM in mind, we propose a concept to adopt and further develop it towards IT Service Lifecycle Management (ITSLM) suitable for the IT services provider environment. After introducing ITSLM, analyzing its processes, and its correlation to PLM, we design ITSLM as a model-driven process support. The selection of appropriate models with different complexity can be used to implement and adapt standard supporting tasks with minimum effort. Two use cases are detailed: fault-tolerance design optimization as well as automation of IT service provisioning. In these areas, suitable model complexity levels, computer-aided task support as well as the knowledge transfer among these models are discussed.}, 
keywords={Business;Biological system modeling;Computational modeling;Adaptation models;Standards;Complexity theory;Information technology;IT Service Management (ITSM);Moddeling;Information Technology Infrastructure Library (ITIL);A Model-Driven IT Service Engineering}, 
doi={10.1109/ES.2016.10}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6405446, 
author={Carrozza, Gabriella and Faella, Mauro and Fucci, Francesco and Pietrantuono, Roberto and Russo, Stefano}, 
booktitle={2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops}, 
title={Integrating MDT in an Industrial Process in the Air Traffic Control Domain}, 
year={2012}, 
volume={}, 
number={}, 
pages={225-230}, 
abstract={Air Traffic Control (ATC) systems are typical software-intensive mission-critical systems with stringent dependability requirements. The major providers of ATC systems are system integrators that address such requirements at the cost of a very expensive testing effort. They envisage Model Driven Testing (MDT) as a promising approach to reduce this effort while achieving better product quality. Within the context of a public-private partnership for software innovation in the ATC domain, we address the problem of integrating MDT into a software development process based on Model Driven Architecture. Specifically, we propose a solution to the integration of MDT into a V-model, focusing on a parallel MDA-MDT flow in a real industrial software process.}, 
keywords={Unified modeling language;Software;Testing;Computer architecture;Adaptation models;Atmospheric modeling;Europe;MDA;MDT;Testing automation}, 
doi={10.1109/ISSREW.2012.87}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6676515, 
author={Nascimento, Amanda S. and Rubira, Cecilia M.F. and Castor, Fernando}, 
booktitle={2013 IEEE 7th International Conference on Self-Adaptive and Self-Organizing Systems}, 
title={Using CVL to Support Self-Adaptation of Fault-Tolerant Service Compositions}, 
year={2013}, 
volume={}, 
number={}, 
pages={261-262}, 
abstract={We present a dynamic software product line to support fault-tolerant service compositions. Architectural variability is specified and resolved by Common Variability Language (CVL). CVL is a generic variability modeling language that enables the transformation of a product line model into a configured, new product model. At runtime, whenever it is necessary to determine a fault tolerance technique more adapted to the context (i.e. a new product) the correspondent product model is dynamically generated by executing CVL model-to-model transformation. Based on the comparison of the reflection model with the target product model, the adaptation process is fully automated.}, 
keywords={Adaptation models;Fault tolerance;Fault tolerant systems;Software;Unified modeling language;Runtime;Quality of service;Fault-tolerant Systems;Self-Adaptation;CVL}, 
doi={10.1109/SASO.2013.34}, 
ISSN={1949-3681}, 
month={Sep.},}
@INPROCEEDINGS{889571, 
author={Federici, D. and Bisgambiglia, P. and Santucci, J.-F.}, 
booktitle={Proceedings IEEE International High-Level Design Validation and Test Workshop (Cat. No.PR00786)}, 
title={High level fault simulation: experiments and results on ITC'99 benchmarks}, 
year={2000}, 
volume={}, 
number={}, 
pages={118-123}, 
abstract={In this paper we present our approach for performing Behavioral Fault Simulation (BFS). This approach involves three main steps (i) the definition of an internal modeling of behavioral descriptions, and the determination of a fault model; (ii) the definition of a fault simulation technique; (iii) the implementation of this technique. Finally, this paper deals with experiments conducted on ITC'99 benchmarks in order to validate a VHDL behavioral fault simulator (BFS). The effectiveness of the BFS software is clearly demonstrated through the obtained results.}, 
keywords={Circuit faults;Circuit simulation;Circuit testing;Benchmark testing;Very large scale integration;Test pattern generators;Electrical fault detection;Fault detection;Data structures;Software tools}, 
doi={10.1109/HLDVT.2000.889571}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6113487, 
author={Ren, Chunmei and Jiang, Daihong}, 
booktitle={2011 International Conference of Information Technology, Computer Engineering and Management Sciences}, 
title={A New Ontology of Resource Specification for Wireless Sensor Networks}, 
year={2011}, 
volume={2}, 
number={}, 
pages={138-140}, 
abstract={The practical usage of ontology for resource specification is for effective resource control in wireless sensor networks. In order to support geographically and logically distinct resources to be co-scheduled and co-allocated when test beds are federated, we build a new ontology. It provides a simple schema that can be used to present a clear overview of network and the relation between sensing elements. Our ontology has been actively integrated with the common-used control framework. The ontology also provides service abstraction between service providers and users, which defines generalized resource request for users to describe the desired resources on different test beds. It has been shown to be useful for describing heterogeneous networked sensing substrate and be feasible for allocating resources across various test beds.}, 
keywords={Ontologies;Sensors;Substrates;XML;Wireless sensor networks;Semantics;Wireless communication;Wireless Senor Networks;Ontology;Virtulization}, 
doi={10.1109/ICM.2011.282}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7133545, 
author={Cheng, Jing and Zhu, Yian and Zhang, Tao and Zhu, Chuanxi and Zhou, Wenqiang}, 
booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
title={Mobile Compatibility Testing Using Multi-objective Genetic Algorithm}, 
year={2015}, 
volume={}, 
number={}, 
pages={302-307}, 
abstract={Mobile compatibility testing has been identified as one urgent and challenging issue. Mobile apps are expected to work on thousand kinds of mobile devices with diverse device features and mobile platforms. So mobile compatibility testing is complex and costly, it is impossible to test mobile apps on all mobile devices and in all environments with limited test resources. Then the question is how to select test devices in cost-effective mobile app compatibility testing. This paper proposes a novel test device selection approach using multi-objective genetic algorithm. Using the proposed approach, the minimum number of mobile devices is selected, and the multiple test coverage requirements are met simultaneously. Furthermore, the case study results have successfully demonstrated that the proposed approach is effective for mobile compatibility testing.}, 
keywords={Mobile communication;Testing;Mobile handsets;Biological cells;Genetic algorithms;Sociology;Statistics;software testing;mobile testing;compatibility testing;clustering algorithm;test coverage}, 
doi={10.1109/SOSE.2015.36}, 
ISSN={}, 
month={March},}
@ARTICLE{1075437, 
author={Maxham, K. and Dugan, J. and McDonald, M. and Hogge, C.}, 
journal={Journal of Lightwave Technology}, 
title={1.13-Gbit/lightwave transmission system}, 
year={1987}, 
volume={5}, 
number={10}, 
pages={1510-1517}, 
abstract={A new high capacity lightwave transmission system has been developed using GaAs semicustom logic arrays and a DFB single-mode laser, and is presently in production. The architecture of this product is designed for in-service upgrade of a 565-Mbit/s product. This paper reviews the technical characteristics and design considerations of the Rockwell LTS-21130 lightwave transmission system.}, 
keywords={Optical transmitters;Protection;Gallium arsenide;Multiplexing;Circuits;Condition monitoring;Logic arrays;Production systems;High speed optical techniques;Optical receivers}, 
doi={10.1109/JLT.1987.1075437}, 
ISSN={1558-2213}, 
month={October},}
@INPROCEEDINGS{8712045, 
author={Joaquin, Montero and Alexander, Atzberger and Matthias, Bleckmann and Jens, Holtmannspötter and Kristin, Paetzold}, 
booktitle={2019 IEEE 10th International Conference on Mechanical and Intelligent Manufacturing Technologies (ICMIMT)}, 
title={Enhancing the Additive Manufacturing process for spare parts by applying Agile Hardware Development principles}, 
year={2019}, 
volume={}, 
number={}, 
pages={109-116}, 
abstract={The availability of spare parts can turn into an issue, when those components are out of stock and cannot be reproduced easily. In this article, the German Federal Armed Forces serves as a prime example, since the equipment used is several decades old and due to that extended lifespan, its availability can turn into a significant problem, especially when the original documentation is unavailable. This article further develops the design process of manufacturing spare parts by means of additive manufacturing (AM), using a component of the military industry as a case study. The design of spare parts by means of AM is a complex task, which designers must face and in order to encounter the issues arising, it is possible to use principles and practices of agile hardware development to clarify and promote the robustness of that process. Using the concept of iterations and increments, the traceability and the quality of the spare parts produced is improved as well as the readiness of the overall process.}, 
keywords={Three-dimensional printing;Uncertainty;Hardware;Metals;Three-dimensional displays;Additive Manufacturing;Design Processes;Design for Additive Manufacturing;Agile Hardware Development;Selective Laser Melting;Metal AM}, 
doi={10.1109/ICMIMT.2019.8712045}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{1687602, 
author={Caruso, F. and Milham, D. and Orobec, S.}, 
booktitle={2006 IEEE/IFIP Network Operations and Management Symposium NOMS 2006}, 
title={Emerging industry standard for managing next generation transport networks: TMF MTOSI}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-15}, 
abstract={There are enormous business benefits to being able to separate the business logic from the massive technical complexity at the network level. One of the greatest challenges to being able to achieve this abstraction at the OS level has been the requirement to communicate and manage many different sets of vendor technologies. The main inhibitor to overcoming this challenge has been the lack of standards supporting both the interface from an OS to an EMS and also that between OSs. Building upon the successful Multi Technology Network Management (MTNM) CORBA/IDL interface, MTOSI has extended MTNM work to support XML/Web Service interactions between various types of Operations Systems. MTOSI bridged the standard gap by defining a methodology and a framework to map the domain specific business activities into well defined TMF NGOSS contracts according to the Service Oriented Architecture principles. This session introduces the key aspects of MTOSI and presents a real use case of MTOSI in BT.}, 
keywords={Next generation networking;SOA;MDA;Web Services;TMF NGOSS;MTOSI;MTNM}, 
doi={10.1109/NOMS.2006.1687602}, 
ISSN={2374-9709}, 
month={April},}
@INPROCEEDINGS{6614319, 
author={Saadatmand, Mehrdad and Sjödin, Mikael}, 
booktitle={2013 10th International Conference on Information Technology: New Generations}, 
title={On Combining Model-Based Analysis and Testing}, 
year={2013}, 
volume={}, 
number={}, 
pages={260-266}, 
abstract={Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.}, 
keywords={Unified modeling language;Testing;Analytical models;Software;Security;Timing;Batteries;Model-based development;static analysis;model-based testing;non-functional requirements;test-case prioritization}, 
doi={10.1109/ITNG.2013.42}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7102628, 
author={Rodrigues, Elder and Bernardino, Maicon and Costa, Leandro and Zorzo, Avelino and Oliveira, Flavio}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={PLeTsPerf - A Model-Based Performance Testing Tool}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Performance testing is a highly specialized task, since it requires that a performance engineer knows the application to be tested, its usage profile, and the infrastructure where it will execute. Moreover, it requires that testing teams expend a considerable effort and time on its automation. In this paper, we present the PLeTsPerf, a model-based performance testing tool to support the automatic generation of scenarios and scripts from application models. PLetsPerf is a mature tool, developed in collaboration with an IT company, which has been used in several works, experimental studies and pilot studies. We present an example of use to demonstrate the process of generating test scripts and scenarios from UML models to test a Web application. We also present the lessons learned and discuss our conclusions about the use of the tool.}, 
keywords={Unified modeling language;Testing;Load modeling;Software;Companies;Generators;Visualization}, 
doi={10.1109/ICST.2015.7102628}, 
ISSN={2159-4848}, 
month={April},}
@ARTICLE{5389542, 
author={Brodnax, T. B. and Billings, R. V. and Glenn, S. C. and Patel, P. T.}, 
journal={IBM Journal of Research and Development}, 
title={Implementation of the PowerPC 601 microprocessor}, 
year={1994}, 
volume={38}, 
number={5}, 
pages={621-632}, 
abstract={To produce a marketable PowerPC™ microprocessor on a short development schedule, the logic had to be designed in a manner flexible enough to allow quick modifications without sacrificing high performance and density when customized cells were required. This was accomplished for the PowerPC 601™ microprocessor (601) with a high-level design-language description, which was synthesized for a gate-level implementation and simulated for functional verification. In a similar way, the physical design strategy for the 601 struck an attractive balance between a highly automated, flexible floorplan and the additional density that had to be available for limited, well-conceived manual placements. Finally, a rigorous test strategy was implemented, which has proved very useful in analyzing the processor and in assembling 601-based systems. Careful adherence to this methodology led to a successful first-pass physical implementation, leaving the second iteration for additional customer requests.}, 
keywords={}, 
doi={10.1147/rd.385.0621}, 
ISSN={0018-8646}, 
month={Sep.},}
@INPROCEEDINGS{5979373, 
author={Wang, Shuanqi and Wu, Yumei and Lu, Minyan and Li, Haifeng}, 
booktitle={The Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety}, 
title={Software reliability modeling based on test coverage}, 
year={2011}, 
volume={}, 
number={}, 
pages={665-671}, 
abstract={In order to incorporate the effect of test coverage, two novel software reliability growth models (SRGMs) are proposed in this paper using failure data and test coverage simultaneously. One is continuous using testing time, and the other is discrete with respect to the number of executed test cases instead of testing time. Since one of the most important factors of the coverage-based SRGMs is the test coverage function (TCF), we first discuss a discrete TCF based on Beta function. Then we develop mean value functions (MVF) of the two models integrating test coverage and imperfect debugging. Finally the proposed TCF and MVFs are evaluated and validated on actual software reliability data collected from real software development projects. The results demonstrate clearly that both the proposed TCF and SRGMs provide better estimation and fitting for the data sets under comparisons.}, 
keywords={Software reliability;Testing;Software;Mathematical model;Fault detection;Equations;Software reliability growth model;test coverage function;beta function;non-homogeneous poisson process;mean value function}, 
doi={10.1109/ICRMS.2011.5979373}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{5679044, 
author={Gallant, Scott and Gaughan, Chris}, 
booktitle={Proceedings of the 2010 Winter Simulation Conference}, 
title={Systems engineering for distributed live, virtual, and constructive (LVC) simulation}, 
year={2010}, 
volume={}, 
number={}, 
pages={1501-1511}, 
abstract={Designing a distributed simulation environment across multiple domains that typically have disparate middleware transport protocols, data exchange formats and applications increases the difficulty of capturing and linking system design decisions to the resultant implementation. Systems engineering efforts for distributed simulation environments are typically based on the middleware transport used, the applications available and the constraints placed on the technical team including network, computer and personnel limitations. To facilitate community re-use, systems engineering should focus on integrated operational function decomposition. This links data elements produced within the simulation to the functional capabilities required by the user. The system design should be captured at a functional level and subsequently linked to the technical design. Doing this within a data-driven systems engineering infrastructure allows generative programming techniques to assist accurate, flexible and rapid architecture development. This paper describes the MATREX program systems engineering process, infrastructure and path forward.}, 
keywords={Computer architecture;System analysis and design;Data models;Testing;Middleware}, 
doi={10.1109/WSC.2010.5679044}, 
ISSN={1558-4305}, 
month={Dec},}
@INPROCEEDINGS{4641444, 
author={Gargantini, A. and Riccobene, E. and Scandurra, P. and Carioni, A.}, 
booktitle={2008 Forum on Specification, Verification and Design Languages}, 
title={Scenario-based validation of embedded systems}, 
year={2008}, 
volume={}, 
number={}, 
pages={191-196}, 
abstract={This paper describes a scenario-based methodology for system-level design validation based on the Abstract State Machines formal method. This scenario-based approach complements an existing model-driven design methodology for embedded systems based on the SystemC UML profile. It allows the designer to functionally validate system components from SystemC UML designs early at high levels of abstraction and without requiring strong skills and expertise on formal methods. A validation tool integrated into an existing model-driven co-design environment to support the proposed scenario-based validation flow is also presented.}, 
keywords={Unified modeling language;Biological system modeling;Modeling;Embedded system;Analytical models;Libraries;Design methodology}, 
doi={10.1109/FDL.2008.4641444}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7966874, 
author={Souza, Rodrigo and Oliveira, Allan}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER)}, 
title={GuideAutomator: Continuous Delivery of End User Documentation}, 
year={2017}, 
volume={}, 
number={}, 
pages={31-34}, 
abstract={User guides, also known as user manuals, are a type of documentation aimed at helping a user operate a specific system. For software systems, user guides usually include screenshots that show users how to interact with the user interface. Because creating such screenshots is a slow, manual process, keeping the user guide up-to-date with changes in the user interface is challenging. We propose an approach in which the documentation writer interleaves the user guide text with source code that automates screen capturing. As a result, screenshots always reflect the latest software version, which makes the approach suitable for a project that uses continuous delivery. The approach was implemented as a prototype, called GuideAutomator.}, 
keywords={Documentation;Tools;Software;Browsers;Cascading style sheets;Programming;software documentation;automated documentation generator;literate programming;continuous delivery}, 
doi={10.1109/ICSE-NIER.2017.10}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5476763, 
author={Mathieu, Bertrand and Paris, Pierre and Guelvouit, Gaëtan Le and Rouibia, Soufiane}, 
booktitle={2010 Fifth International Conference on Internet and Web Applications and Services}, 
title={A Secure and Legal Network-Aware P2P VoD System}, 
year={2010}, 
volume={}, 
number={}, 
pages={194-199}, 
abstract={File sharing applications using Peer-to-Peer (P2P) networks such as Bittorrent or eDonkey rapidly attracted a lot of people and proved the efficiency and interest of this P2P technology. Distribution of video and of live contents also experienced the P2P mechanisms with success. PPLive, UUSee and others have many of customers, hundreds of channels and thousands of concurrent users. However, major content providers are reluctant to use this technology because no solution to ensure the distribution of only legal contents is provided. In the same way, network operators do not really push towards P2P content distribution because bad organization of the overlay can lead to overload the network and consume a lot of networks resources. In this paper, a secure and legal network-aware P2P video system is introduced, which aims at overcoming those two drawbacks. The design of the system and the evaluation of a prototype showed good results and let us be optimistic about a possible deployment of P2P systems for video delivery, having the support of content providers as well as network operators.}, 
keywords={Law;Legal factors;Peer to peer computing;Video sharing;Prototypes;Watermarking;Computer architecture;IP networks;Web and internet services;Design optimization;P2P Video Streaming;network-awareness;secure distribution;legal contents;watermarking}, 
doi={10.1109/ICIW.2010.35}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1321646, 
author={Hau Lam}, 
booktitle={IEEE/CPMT/SEMI 29th International Electronics Manufacturing Technology Symposium (IEEE Cat. No.04CH37585)}, 
title={New design-to-test software strategies accelerate time-to-market}, 
year={2004}, 
volume={}, 
number={}, 
pages={140-143}, 
abstract={Today's growing device complexity and new manufacturing requirements have presented significant challenges for manufacturers looking to speed time-to-market. One such challenge is the need to contain test costs, of which a major component is the time and resources required for test program development. Some test development tools that exist today can translate a device's functional events and scan patterns into test programs for targeted ATE. Identification and specification of critical timing parameters that require conversion into cycle-based ATE formats have become an increasing cost factor, which can also significantly impact test accuracy. Traditionally, timing specifications from microprocessor and IP cores, multiple bus types, and other device components can be established via published timing specifications and by a manageable, iterative process between design and test engineering. Likewise, automatic test pattern generation tools for structural test can address simple timing, and are capable of generating cycle-based timing. Today's complex SoC may consist of over 60 IP cores made more complicated by increased challenges from high-speed serial bus technology and multiple-time domain designs. Further complicating test program development is the need for compatibility with multiple ATE platforms to accommodate global manufacturing strategies. Next generation design-to-test software tools have to address these factors to help reduce the ever growing cost-of-test. Tools must support standard industry test languages such as standard test interface language (STIL), support both functional events and scan patterns, and validate outputs to ensure first-pass success of test programs pre- and post silicon, across multiple ATE platforms.}, 
keywords={Software design;Acceleration;Time to market;Testing;Timing;Manufacturing;Costs;Automatic test pattern generation;Microprocessors;Engineering management}, 
doi={10.1109/IEMT.2004.1321646}, 
ISSN={1089-8190}, 
month={July},}
@INPROCEEDINGS{4670300, 
author={Guelfi, Nicolas and Ries, Benoit}, 
booktitle={Testing: Academic & Industrial Conference - Practice and Research Techniques (taic part 2008)}, 
title={Selection, Evaluation and Generation of Test Cases in an Industrial Setting: A Process and a Tool}, 
year={2008}, 
volume={}, 
number={}, 
pages={47-51}, 
abstract={The test phase in safety-critical systems industry is a crucial phase of the development process. Some companies of these industries have their own test methods which do not reuse the notions available in the theory of software testing or model driven engineering. This paper reports on an experience in a testing process improvement made inside a safety-critical systems company in order to improve the quality of the test phase improvement. We present the initial situation, the objectives, the proposed process and the tools that are used to support it. In particular, we show that the most efficient improvements were achieved concerning the test process definition and in allowing a tailored and precise delimitation of the systempsilas elements to be tested.}, 
keywords={System testing;Software testing;Computer industry;Embedded software;Performance evaluation;Laboratories;Software systems;Model driven engineering;Software performance;Software engineering;test selection;model-driven testing;industrial;tool-support;process}, 
doi={10.1109/TAIC-PART.2008.12}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6389024, 
author={Gananchchelvi, Parameshwaran and Jiao Yu and Pukish, Michael S.}, 
booktitle={IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society}, 
title={Current trends in in-vehicle electrical engineering applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={6268-6273}, 
abstract={Novel electrical engineering applications play a major role in in-vehicle technology. Today's automobile industry is transforming from mechanically driven functions to electronic and software driven functions. We can see the impact of electrical engineering technology in many parts of vehicle and in all of its operations such as control, power conversion, navigation, communication, entertainment, safety and security. In addition, today's automobile industry is showing a renewed interest in electric transportation. This paper presents a review on the recent developments in the in-vehicle electrical engineering applications classified into two sections, the embedded systems and the power and energy system, both of which can strongly influence the automobile industry in the future.}, 
keywords={Field programmable gate arrays;Supercapacitors;Lead;Batteries;Digital signal processing;Frequency conversion;Frequency estimation;in-vehicle electronics;embedded systems;ECU;FPGA;EV;ultracapacitors},
doi={10.1109/IECON.2012.6389024}, 
ISSN={1553-572X}, 
month={Oct},}
@INPROCEEDINGS{8945647, 
author={Bhagya, Thilini and Dietrich, Jens and Guesgen, Hans}, 
booktitle={2019 26th Asia-Pacific Software Engineering Conference (APSEC)}, 
title={Generating Mock Skeletons for Lightweight Web-Service Testing}, 
year={2019}, 
volume={}, 
number={}, 
pages={181-188}, 
abstract={Modern application development allows applications to be composed using lightweight HTTP services. Testing such an application requires the availability of services that the application makes requests to. However, access to dependent services during testing may be restrained. Simulating the behaviour of such services is, therefore, useful to address their absence and move on application testing. This paper examines the appropriateness of Symbolic Machine Learning algorithms to automatically synthesise HTTP services' mock skeletons from network traffic recordings. These skeletons can then be customised to create mocks that can generate service responses suitable for testing. The mock skeletons have human-readable logic for key aspects of service responses, such as headers and status codes, and are highly accurate.}, 
keywords={Testing;Artificial intelligence;Decision trees;Semantics;Web services;Prediction algorithms;Skeleton;HTTP, Web services, REST, service oriented computing, mocking, service virtualisation, application testing, symbolic machine learning}, 
doi={10.1109/APSEC48747.2019.00033}, 
ISSN={2640-0715}, 
month={Dec},}
@INPROCEEDINGS{7781923, 
author={Klein, Eduard and Gschwend, Adrian and Neuroni, Alessia C.}, 
booktitle={2016 Conference for E-Democracy and Open Government (CeDEM)}, 
title={Towards a Linked Data Publishing Methodology}, 
year={2016}, 
volume={}, 
number={}, 
pages={188-196}, 
abstract={Linked open government data (LOGD) can be a catalyst in the development of value-added services and products. The vision of many Linked Open Data (LOD) projects is to make publishing and reuse of linked data as easy as possible for the end user thanks to a thriving marketplace with data publishers, developers, and consumers along the value chain. In the large scale LOD project "Fusepool P3", tourism-related applications and software components were developed that support data owners and open data enthusiasts in transforming legacy data to linked data. Based on experiences from this project, we present reflections and discuss pitfalls in drawing a linked data publishing methodology. An integrated view on all phases of the publishing process has not been described so far, for the technical phases linked data life-cycles have been identified only. The methodology developed enables stakeholders to transfer the lessons learned to other use cases and application contexts. This allows for better estimation of efforts and skills for future LOD projects.}, 
keywords={Publishing;Stakeholders;Context;Government;Data models;Software;Portals;linked open data;data publishing;linked data life-cycle;publishing methodology;linked data platform}, 
doi={10.1109/CeDEM.2016.12}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8109427, 
author={Nobach, Leonhard and Blendin, Jeremias and Kolbe, Hans-Jörg and Schyguda, Georg and Hausheer, David}, 
booktitle={2017 IEEE 42nd Conference on Local Computer Networks (LCN)}, 
title={Bare-Metal Switches and Their Customization and Usability in a Carrier-Grade Environment}, 
year={2017}, 
volume={}, 
number={}, 
pages={649-657}, 
abstract={The current ecosystem of network elements, such as switches and appliances, is largely dominated by devices supplied and sold with a bundled operating system, and software dedicated to manage the device's forwarding hardware, however, these platforms are not open-source and cannot be arbitrarily customized, and there is no cost transparency or flexibility in choosing software different to the bundled components.,,,,In this paper, we explore the capabilities of bare-metal switches, which are equipped with commodity switching hardware components, but shipped without an operating system. We evaluate the feasibility of these commonly lower-cost devices to meet the requirements of a customized, carrier-grade network function. Therefore, we have implemented a prototype on generic hardware, re-using as much open-source software as possible. Our Broadband Remote Access Server (BRAS) prototype can lower the cost compared to proprietary network appliances, and, known to have a hardware backplane capacity of 720 Gbps, the merchant-silicon / ASIC approach can highly outperform the state of the art of current x86-based virtualized network functions, while implementing the most important BRAS features.}, 
keywords={Hardware;Software;Servers;Linux;Ports (Computers);Switches;Bare-Metal Switching;Dataplanes;Network Functions;Middleboxes;Sofware-Defined Networking;Cost-Efficiency}, 
doi={10.1109/LCN.2017.104}, 
ISSN={0742-1303}, 
month={Oct},}
@INPROCEEDINGS{6030046, 
author={Hutchesson, Stuart and McDermid, John}, 
booktitle={2011 15th International Software Product Line Conference}, 
title={Towards Cost-Effective High-Assurance Software Product Lines: The Need for Property-Preserving Transformations}, 
year={2011}, 
volume={}, 
number={}, 
pages={55-64}, 
abstract={Generative programming and model transformation techniques are becoming widely used for the development of software components for product lines. The ability to develop components with identified common and variable parts, and rapidly instantiate product-specific versions is key to many software product line approaches. However if this approach is to be truly cost effective for high assurance applications, the instantiation process must be property-preserving, any verification evidence acquired on the product-line component must be demonstrably applicable to the instantiated component. In this paper we outline an approach that uses static analysis techniques and the SPARK language that can potentially demonstrate the correctness of model transformations.}, 
keywords={Software;Unified modeling language;Sparks;Programming;Ignition;Contracts;UML;SPARK;M2M;Safety Critical;High Integrity;Software Product Lines;Verification;Static Analysis;DO-178B/ED-12B}, 
doi={10.1109/SPLC.2011.32}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6602477, 
author={Morrison, Patrick and Holmgreen, Casper and Massey, Aaron and Williams, Laurie}, 
booktitle={2013 5th International Workshop on Software Engineering in Health Care (SEHC)}, 
title={Proposing regulatory-driven automated test suites for electronic health record systems}, 
year={2013}, 
volume={}, 
number={}, 
pages={46-49}, 
abstract={In regulated domains such as finance and health care, failure to comply with regulation can lead to financial, civil and criminal penalties. While systems vary from organization to organization, regulations apply across organizations. We propose the use of Behavior-Driven-Development (BDD) scenarios as the basis of an automated compliance test suite for standards such as regulation and interoperability. Such test suites could become a shared asset for use by all systems subject to these regulations and standards. Each system, then, need only create their own system-specific test driver code to automate their compliance checks. The goal of this research is to enable organizations to compare their systems to regulation in a repeatable and traceable way through the use of BDD. To evaluate our proposal, we developed an abbreviated HIPAA test suite and applied it to three open-source electronic health record systems. The scenarios covered all security behavior defined by the selected regulation. The system-specific test driver code covered all security behavior defined in the scenarios, and identified where the tested system lacked such behavior.}, 
keywords={Data structures;Boolean functions;NIST;Certification;Behavior-Driven-Development Healthcare IT;Regulatory Compliance;Security;Software Engineering;Software Testing}, 
doi={10.1109/SEHC.2013.6602477}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9445321, 
author={Jayasuryapal, G and Pranay, P. Meher and Kaur, Harpreet and Swati}, 
booktitle={2021 2nd International Conference on Intelligent Engineering and Management (ICIEM)}, 
title={A Survey on Network Penetration Testing}, 
year={2021}, 
volume={}, 
number={}, 
pages={373-378}, 
abstract={Penetration on network is an important security measurements every company want to take into the consideration. Day-to-Day life as it is seen that cybercrimes are increasing due to lack of security practice. Penetration testing is an outstanding approach where pen tester evaluates the security of network and numerous applications by simulating attacks from attacker's view point .Additionally, penetration testing process follow certain rules and agreement that both the parties ( client and pen testing team). By this testing, the company's weakness will be detected like open servers and open ports etc. so we can take a countermeasure by doing penetration testing on the company. This paper contains some of the important terms and steps to do a strong penetration testing on organizations. Hence, this paper covered all the mechanisms including information gathering to the post exploitation.}, 
keywords={Productivity;Companies;Security;Servers;Computer crime;Robots;Penetration testing;Network Penetration Testing;Server Security;Vulnerability examination;External Enumeration}, 
doi={10.1109/ICIEM51511.2021.9445321}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8816743, 
author={Yang, Aidan Z.H. and Alencar da Costa, Daniel and Zou, Ying}, 
booktitle={2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)}, 
title={Predicting Co-Changes between Functionality Specifications and Source Code in Behavior Driven Development}, 
year={2019}, 
volume={}, 
number={}, 
pages={534-544}, 
abstract={Behavior Driven Development (BDD) is an agile approach that uses. feature files to describe the functionalities of a software system using natural language constructs (English-like phrases). Because of the English-like structure of. feature files, BDD specifications become an evolving documentation that helps all (even non-technical) stakeholders to understand and contribute to a software project. After specifying a. feature files, developers can use a BDD tool (e.g., Cucumber) to automatically generate test cases and implement the code of the specified functionality. However, maintaining traceability between. feature files and source code requires human efforts. Therefore,. feature files can be out-of-date, reducing the advantages of using BDD. Furthermore, existing research do not attempt to improve the traceability between. feature files and source code files. In this paper, we study the co-changes between. feature files and source code files to improve the traceability between. feature files and source code files. Due to the English-like syntax of. feature files, we use natural language processing to identify co-changes, with an accuracy of 79%. We study the characteristics of BDD co-changes and build random forest models to predict when a. feature files should be modified before committing a code change. The random forest model obtains an AUC of 0.77. The model can assist developers in identifying when a. feature files should be modified in code commits. Once the traceability is up-to-date, BDD developers can write test code more efficiently and keep the software documentation up-to-date.}, 
keywords={Software;Java;Stakeholders;Documentation;Testing;Registers;Tools;Behavior Driven Development;Traceability;Co-Changes;Empirical Software Engineering}, 
doi={10.1109/MSR.2019.00080}, 
ISSN={2574-3864}, 
month={May},}
@INPROCEEDINGS{9042019, 
author={Ta, Tuan and Zhang, Xianwei and Gutierrez, Anthony and Beckmann, Bradford M.}, 
booktitle={2019 IEEE International Symposium on Workload Characterization (IISWC)}, 
title={Autonomous Data-Race-Free GPU Testing}, 
year={2019}, 
volume={}, 
number={}, 
pages={81-92}, 
abstract={As the deep learning and high-performance computing markets continue to grow, hardware designers are increasingly optimizing future GPUs to run compute (a.k.a. GPGPU) workloads. A key area of optimization for these compute-oriented designs, which was not emphasized when GPUs exclusively executed graphics workloads, is inter-thread data sharing and synchronization. GPU cache coherence protocols now support these operations and are governed by a specified memory consistency model. In general, current GPU models are based on sequential consistency for data-race-free (SC for DRF), which mandates data written to memory must be globally visible only after certain synchronization points. GPU coherence protocols based on such relaxed memory models are particularly difficult to design and test due to the large number of memory accesses that may be reordered. This leaves GPU hardware designers struggling to validate the correctness of GPU cache coherence optimizations. To address this issue, this paper introduces a novel, completely autonomous random testing methodology for complex GPU cache coherence protocols. Our framework continuously generates sequences of memory requests with minimal user intervention using a mix of load, store, and atomic operations. The tester dynamically and autonomously checks each response against an expected global view of memory and immediately detects any inconsistencies in a target coherence protocol, providing designers detailed feedback on the issue. We then demonstrate the methodology on the popular cycle-level gem5 simulator by replacing its GPU core model with our unique testing framework. The results show that the GPU tester can cover 94% and 100% of all reachable state transitions in L1 and L2 caches respectively of a representative GPU coherence protocol. This coverage is 6.25% and 25% higher than the one achieved by a wide selection of 26 applications. In addition, the tester runs more than 50 times faster than those applications, which enables efficient and fast protocol debugging.}, 
keywords={Cache coherence;central processing unit (CPU);graphics processing unit (GPU);memory consistency;simulation;testing}, 
doi={10.1109/IISWC47752.2019.9042019}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8247727, 
author={Pfrang, Steffen and Meier, David and Kautz, Valentin}, 
booktitle={2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Towards a modular security testing framework for industrial automation and control systems: ISuTest}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Industrial automation and control systems (IACS) play a key role in modern production facilities. On the one hand, they provide real-time functionality to the connected field devices. On the other hand, they get more and more connected to local networks and the internet in order to facilitate use cases promoted by “Industry 4.0”. This makes IACS susceptible to cyber-attacks which exploit vulnerabilities, for example in order to interrupt the automation process. Security testing targets at discovering those vulnerabilities before they are exploited. In order to enable IACS manufacturers and integrators to perform security testing for their devices, we present ISuTest, a modular security testing framework for IACS. ISuTest is designed to be extendable regarding all kinds of automation protocols, different connection paths as well as evaluating arbitrary outputs of the tested devices. This paper describes the fundamental ideas behind ISuTest, its design and a basic evaluation in which the ISuTest framework was able to discover a vulnerability in a programmable logic controller (PLC). The paper concludes with a broad overview of the planned future work.}, 
keywords={Testing;Security;Automation;Protocols;Control systems;Software;Hardware}, 
doi={10.1109/ETFA.2017.8247727}, 
ISSN={1946-0759}, 
month={Sep.},}
@INPROCEEDINGS{1617602, 
author={Mingjing Chen and Haggag, H. and Orailoglu, A.}, 
booktitle={24th IEEE VLSI Test Symposium}, 
title={Decision tree based mismatch diagnosis in analog circuits}, 
year={2006}, 
volume={}, 
number={}, 
pages={6 pp.-285}, 
abstract={Mismatch is a critical consideration in analog circuit design. Knowledge of mismatch locations and an understanding of their impact on circuit performance are crucial for design optimization and process improvement. We present a circuit level mismatch diagnosis methodology in this paper. The functional parameters with abnormal values are measured as manifestations of mismatch, from which reverse tracing is employed to determine the mismatch source. The methodology is implemented on a representative benchmark and its efficiency confirmed by simulation results.}, 
keywords={Decision trees;Analog circuits;Design optimization;Circuit simulation;Fabrication;Degradation;Design automation;Predictive models;Circuit testing;Circuit optimization}, 
doi={10.1109/VTS.2006.26}, 
ISSN={2375-1053}, 
month={April},}
@INPROCEEDINGS{4610718, 
author={Nicolae, Maximilian and Dobrescu, Radu and Dobrescu, Matei and Popescu, Dan}, 
booktitle={2008 6th International Symposium on Communication Systems, Networks and Digital Signal Processing}, 
title={Embedded node around a DSP core for mobile sensor networks over 802.11 infrastructure}, 
year={2008}, 
volume={}, 
number={}, 
pages={643-646}, 
abstract={Digital signal processors (DSPs) are very efficient devices to implement algorithms for signal processing and analyzing. Endowing sensorial nodes from a sensor network with such processing cores it could lead to high performance because of the possibility of parallel and distribute processing and thus reducing the quantity of information spread into the network (network load). If such a node uses for communication a mature infrastructure like 802.11 standard, will be obtained a solution for implementing mobile sensor networks with high performance end cost efficient. In the same way of obtaining high performances with low costs we suggest that on the information chain from the physical quantity to the numerical result, the acquisition part to be done by an audio codec. In this way, the entire network may look like a Voice over IP (VoIP) mobile network, yet the information exchanged will not be voice but measures and commands with quality of service (QoS) inherited from VoIP.}, 
keywords={Digital signal processing;Computer architecture;Wireless sensor networks;Codecs;Mobile communication;Mobile computing;Hardware}, 
doi={10.1109/CSNDSP.2008.4610718}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{5655567, 
author={Wieczorek, Sebastian and Stefanescu, Alin and Roth, Andreas}, 
booktitle={2010 Seventh International Conference on the Quality of Information and Communications Technology}, 
title={Model-Driven Service Integration Testing - A Case Study}, 
year={2010}, 
volume={}, 
number={}, 
pages={292-297}, 
abstract={This paper presents a case study for the modeling and model-based testing (MBT) of enterprise service choreographies. Our proposed MBT approach uses proprietary models called Message Choreography Models (MCM) as test models. The case study illustrates how MCM-based service integration testing allows to formalize design decisions and enables full integration into an existing industrial test infrastructure by using the concepts of domain specific languages and model transformations. Further, the MBT tools integrated into the testing framework have been compared based on one concrete use case.}, 
keywords={Testing;Business;Service oriented architecture;Generators;Unified modeling language;Data models;Context;Model-based Testing;Enterprise Systems;Service-oriented Architecture;Case Study;Service Choreographies}, 
doi={10.1109/QUATIC.2010.49}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6151484, 
author={Jabeen, Aneela and Tariq, Sidra and Farooq, Qurat-ul-ann and Malik, Zafar I.}, 
booktitle={2011 IEEE 14th International Multitopic Conference}, 
title={A lightweight aspect modelling approach for BPMN}, 
year={2011}, 
volume={}, 
number={}, 
pages={255-260}, 
abstract={Aspectual Business Process Modelling is not a new concept in business process based development to support the separation of the cross cutting concerns. Most of the researchers use the concept of the heavyweight extensions of the business processes to incorporate the aspects. This requires changes in the meta-models of the languages and the tool infrastructures, which is not a feasible option. Some of the researchers also provide lightweight extensions in the form of profiles, but these are mostly incomplete and do not provide solutions for modelling some important aspectual concepts like Pointcuts effectively. To overcome this issue, in this paper, we provide a lightweight extension of the business processes expressed in BPMN to incorporate the aspect specific concepts in it. We propose a profile ABPMN which uses the existing notations of the BPMN models for expressing aspectual concepts. Further, we developed a language PCDL to express the pointcuts in an effective way. The language is implemented using XText in Eclipse. To evaluate the applicability of our approach, we applied it on a case study of the E-Bidding system.}, 
keywords={Weaving;Aspect-oriented Modelling;BPMN;Xtext;Metamodel;ABPMN}, 
doi={10.1109/INMIC.2011.6151484}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6209957, 
author={Kanstrén, Teemu and Puolitaival, Olli-Pekka and Rytky, Veli-Matti and Saarela, Asmo and Keränen, Janne S.}, 
booktitle={2012 IEEE International Conference on Industrial Technology}, 
title={Experiences in setting up domain-specific model-based testing}, 
year={2012}, 
volume={}, 
number={}, 
pages={319-324}, 
abstract={Model-based testing is a technique for generating test cases based on a model of the system under test. Typically the model is expressed in a specific notation of the test tool, using a generic notation intended to describe any system under test. In this paper we present experiences in using a domain-specific modeling layer on top of the specific model-based testing tools. This allows for easier change of the used testing tool, while providing a more familiar modeling notation in terms of the domain concepts familiar to the user. Our experiences show how this can significantly help in adopting the model-based testing approach and provide improved test results.}, 
keywords={Generators;Encoding;Testing}, 
doi={10.1109/ICIT.2012.6209957}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{9274755, 
author={Kannengiesser, Udo and Krenn, Florian and Stary, Christian}, 
booktitle={2020 IEEE Conference on Industrial Cyberphysical Systems (ICPS)}, 
title={A Behaviour-Driven Development Approach for Cyber-Physical Production Systems}, 
year={2020}, 
volume={1}, 
number={}, 
pages={179-184}, 
abstract={This paper proposes a method for iterative engineering of cyber-physical production systems (CPPS) that allows early testing of virtual prototypes and early involvement of domain experts. It is based on behaviour-driven development (BDD) from agile software engineering, which is adapted to address a set of issues relevant for CPPS engineering including the use of standardised CPPS models, integration testing, test environments, and brownfield development. The paper describes these adaptations and synthesises them into a procedural model of BDD for CPPS. Finally, a prototypical test system for CPPS is presented that partially implements the approach.}, 
keywords={Testing;Modeling;Unified modeling language;Software;Production systems;Task analysis;Iterative methods;CPPS;Behaviour-Driven Development;Testing;Iterative Development}, 
doi={10.1109/ICPS48405.2020.9274755}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7374941, 
author={Suthar, Prakash and Stolic, Milan}, 
booktitle={2015 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob)}, 
title={Carrier grade Telco-Cloud}, 
year={2015}, 
volume={}, 
number={}, 
pages={101-107}, 
abstract={The Telco service providers business is undergoing a fundamental shift, and operators are transforming their network to meet new business challenges. Biggest focus area for Telco Cloud is rapid time to market (TTM) for new services and reduction in total cost of ownership (TCO). Telco service providers are facing challenges from web and content providers because of agility and convergence of voice and data. Telco Cloud is transformation of traditional wireline, wireless, voice, text, data, and web etc. services to common compute cloud infrastructures. Cloud infrastructures can be set-up on-premise, off-premise or hybrid based upon service level agreements (SLA), security and maturity of services. Key component of Telco Cloud is IP Multimedia System (IMS) which provides convergence of voice, data, video, multimedia messaging etc. Designing and developing Telco Cloud, which meets criteria of “carrier grade”, is very important to gain confidence and comfort level of different stakeholders. This paper discusses design and deployment criteria for building high quality Telco Cloud.}, 
keywords={Cloud computing;Business;Logic gates;Mobile communication;Hardware;Wireless communication}, 
doi={10.1109/APWiMob.2015.7374941}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7342393, 
author={McGinty, Stephen and Hadad, Daniel and Nappi, Chris and Caquelin, Brian}, 
booktitle={2015 IEEE International Test Conference (ITC)}, 
title={Developing a modern platform for test engineering — Introducing the origen semiconductor developer's kit}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Many of the tools used today in semiconductor test engineering are single-point solutions that are concerned with the mechanics of translating test IP between domains and formats. There is no cohesive standardized framework to bind them all together; and workflow and application architecture choices are largely left up to the individual engineer. Learning from the state-of-the-art in other software engineering domains, we have developed a modern framework for semiconductor engineering that favors a convention-based approach to application architectures. By following conventions, powerful abstractions can be created to enable truly modular test development within a unified environment for the creation of test patterns, test programs, and all other test collateral. The paper reviews the background and some of the main capabilities of the framework and discusses how it is being used in production today to replace many conventional pattern flows. This is also a formal announcement that Freescale Semiconductor is open-sourcing the Origen Semiconductor Developer's Kit (SDK) to enable future development to be done in collaboration with the global semiconductor engineering community.}, 
keywords={IP networks;Companies;Complexity theory;Testing;Industries;Computer architecture;Silicon}, 
doi={10.1109/TEST.2015.7342393}, 
ISSN={}, 
month={Oct},}
@ARTICLE{5233611, 
author={Duenas, Juan C. and Ruiz, José L. and Cuadrado, Félix and Garcia, Boni and Parada G., Hugo A.}, 
journal={IEEE Internet Computing}, 
title={System Virtualization Tools for Software Development}, 
year={2009}, 
volume={13}, 
number={5}, 
pages={52-59}, 
abstract={The configuration complexity of preproduction sites coupled with access-control mechanisms often impede the software development life cycle. Virtualization is a cost-effective way to remove such barriers and provide a test environment similar to the production site, reducing the burden in IT administrators. An Eclipse-based virtualization tool framework can offer developers a personal runtime environment for launching and testing their applications. The authors have followed a model-driven architecture (MDA) approach that integrates best-of-breed virtualization technologies, such as Xen and VDE.}, 
keywords={Programming;Testing;Impedance;Production;Application virtualization;Runtime environment;Application software;virtualization;software development;distributed systems;Eclipse;model-driven architecture;MDA}, 
doi={10.1109/MIC.2009.115}, 
ISSN={1941-0131}, 
month={Sep.},}
@INPROCEEDINGS{7102608, 
author={Jensen, Simon Holm and Thummalapenta, Suresh and Sinha, Saurabh and Chandra, Satish}, 
booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Test Generation from Business Rules}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Enterprise applications are difficult to test because their intended functionality is either not described precisely enough or described in cumbersome business rules. It takes a lot of effort on the part of a test architect to understand all the business rules and design tests that "cover" them, i.e., exercise all their constituent scenarios. Part of the problem is that it takes a complicated set up sequence to drive an application to a state in which a business rule can even fire. In this paper, we present a business rule modeling language that can be used to capture functional specification of an enterprise system. The language makes it possible to build tool support for rule authoring, so that obvious deficiencies in rules can be detected mechanically. Most importantly, we show how to mechanically generate test sequences--i.e., test steps and test data--needed to exercise these business rules. To this end, we translate the rules into logical formulae and use constraint solving to generate test sequences. One of our contributions is to overcome scalability issues in this process, and we do this by using a novel algorithm for organizing search through the space of candidate sequences to discover covering sequences. Our results on three case studies show the promise of our approach.}, 
keywords={Business;Databases;Testing;Syntactics;Algorithm design and analysis;Systematics;Context}, 
doi={10.1109/ICST.2015.7102608}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{9402034, 
author={Mayr-Dorn, Christoph and Vierhauser, Michael and Bichler, Stefan and Keplinger, Felix and Cleland-Huang, Jane and Egyed, Alexander and Mehofer, Thomas}, 
booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, 
title={Supporting Quality Assurance with Automated Process-Centric Quality Constraints Checking}, 
year={2021}, 
volume={}, 
number={}, 
pages={1298-1310}, 
abstract={Regulations, standards, and guidelines for safety-critical systems stipulate stringent traceability but do not prescribe the corresponding, detailed software engineering process. Given the industrial practice of using only semi-formal notations to describe engineering processes, processes are rarely "executable" and developers have to spend significant manual effort in ensuring that they follow the steps mandated by quality assurance. The size and complexity of systems and regulations makes manual, timely feedback from Quality Assurance (QA) engineers infeasible. In this paper we propose a novel framework for tracking processes in the background, automatically checking QA constraints depending on process progress, and informing the developer of unfulfilled QA constraints. We evaluate our approach by applying it to two different case studies; one open source community system and a safety-critical system in the air-traffic control domain. Results from the analysis show that trace links are often corrected or completed after the fact and thus timely and automated constraint checking support has significant potential on reducing rework.}, 
keywords={Quality assurance;Manuals;Regulation;Complexity theory;Standards;Software engineering;Guidelines;software engineering process;traceability;developer support}, 
doi={10.1109/ICSE43902.2021.00118}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{8590169, 
author={García, Boni and Lonetti, Francesca and Gallego, Micael and Miranda, Breno and Jiménez, Eduardo and De Angelis, Guglielmo and Santos, Carlos and Marchetti, Eda}, 
booktitle={2018 11th International Conference on the Quality of Information and Communications Technology (QUATIC)}, 
title={A Proposal to Orchestrate Test Cases}, 
year={2018}, 
volume={}, 
number={}, 
pages={38-46}, 
abstract={This paper presents the concept of test orchestration, understood as a novel way to select, order, and execute in parallel a group of tests. Our view of test orchestration can be seen as a process in which different test cases are organized, assembled and executed following a topology that determines how their executions coordinate. We distinguish two types of orchestrations techniques: i) verdict-driven, which organizes tests using their outcome (i.e., passed or failed) to drive the workflow; and ii) data-driven, in which test data (input) and test outcomes (output) are handled within the graph. Both approaches are being implemented in the project ElasTest, an open source platform aimed to simplify the end-to-end test process of large software systems.}, 
keywords={Testing;Topology;Pipelines;Tools;Signal to noise ratio;Software systems;Engines;Software testing;test composition;test parallelization}, 
doi={10.1109/QUATIC.2018.00016}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{1653662, 
author={Pei Hsia and Petry}, 
journal={Computer}, 
title={A Systematic Approach to Interactive Programming}, 
year={1980}, 
volume={13}, 
number={6}, 
pages={27-34}, 
abstract={Designed for program development in an interactive environment, this framework can help create a new generation of programmers with an invaluable disciplined approach to software development.}, 
keywords={Programming profession;Software engineering;Software design;Automatic programming;Programming environments;Production systems;Software quality}, 
doi={10.1109/MC.1980.1653662}, 
ISSN={1558-0814}, 
month={June},}
@INPROCEEDINGS{4259241, 
author={Van Roijen, R. and Collins, C. and Ayala, J. and Barker, K. and Boiselle, H. and Catlett, S. and Dezfulian, K. and Logan, R. and Maxson, J. and Ramachandran, R. and Rawlins, B. and Ruegsegger, S. and Rust, T. and Shepard, J. and Singh, R.}, 
booktitle={2007 IEEE/SEMI Advanced Semiconductor Manufacturing Conference}, 
title={Reducing Time-to-Respond in a Modern Manufacturing Environment}, 
year={2007}, 
volume={}, 
number={}, 
pages={29-33}, 
abstract={The complexity of modern manufacturing processes has sharply increased the number of steps affecting device and circuit performance. We discuss a number of critical steps, their control methodology and how to minimize the time to detect. Product test results and data-mining are used to identify critical steps and to determine which inline signals require most attention. The last section is devoted to optimizing the analysis of inline electrical signals and their application to tool control.}, 
keywords={Implants;Manufacturing processes;Temperature control;Manufacturing automation;Rapid thermal annealing;Microelectronics;Circuit optimization;Circuit testing;Signal processing;Signal analysis;300mm manufacturing;SOI;Process control;Manufacturing automation}, 
doi={10.1109/ASMC.2007.375075}, 
ISSN={2376-6697}, 
month={June},}
@INPROCEEDINGS{9287916, 
author={Breitenhuber, Guido}, 
booktitle={2020 Fourth IEEE International Conference on Robotic Computing (IRC)}, 
title={Towards application level testing of ROS networks}, 
year={2020}, 
volume={}, 
number={}, 
pages={436-442}, 
abstract={Robotics is a highly interdisciplinary field where applications of complex architectures are built to solve challenging problems. Integration of multiple components in this environment continues to be a time-consuming, tedious and error-prone activity. Typical approaches in parallel component development or composition from re-usable components focus on structural and static aspects like ROS message and topic definitions. However, the behavioural part of robot software components is hardly ever specified in sufficient detail. In this work, we tackle this issue by proposing an application-level testing framework for robot software applications that uses a fluent API to describe the expected behaviour of an application or its components. This is a first step towards test-first based development of robot software in order to increase robot software quality.}, 
keywords={Conferences;Software quality;Computer architecture;Robots;Testing;Software testing;ROS testing;Software quality;Automated testing;Testing frameworks;ROS}, 
doi={10.1109/IRC.2020.00081}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6166261, 
author={Edmondson, James and Gokhale, Aniruddha and Neema, Sandeep}, 
booktitle={2011 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)}, 
title={Automating testing of service-oriented mobile applications with distributed knowledge and reasoning}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Automated testing of distributed, service-oriented applications, particularly mobile applications, is a hard problem due to challenges testers often must deal with, such as (1) heterogeneous platforms, (2) difficulty in introducing additional resources or backups of resources that fail during testing, and (3) lack of fine-grained control over test sequencing. To address these challenges, this paper describes an approach that combines portable operating system libraries with knowledge and reasoning, which together leverage the best features of centralized and decentralized testing infrastructures to support both heterogeneous systems and distributed control by reasoning on distributed testing events.}, 
keywords={Testing;Cognition;Servers;Engines;Knowledge engineering;Real time systems;Conferences;test sequencing;distributed control in testing;portability;knowledge dissemination}, 
doi={10.1109/SOCA.2011.6166261}, 
ISSN={2163-2871}, 
month={Dec},}
@ARTICLE{8472900, 
author={Duan, Pengfei and Zhou, Ying and Gong, Xufang and Li, Bixin}, 
journal={IEEE Access}, 
title={A Systematic Mapping Study on the Verification of Cyber-Physical Systems}, 
year={2018}, 
volume={6}, 
number={}, 
pages={59043-59064}, 
abstract={Cyber-physical system (CPS) is a kind of complex real-time hybrid system which involves deep interactions between computation processors, communication network, and physical environments are deemed as the key enablers of next generation computer applications. However, how to verify CPS effectively is always a great challenge. Based on current scientific works about CPS verification, this paper aims at identifying the gap of current studies and suggesting promising areas for the future works. For this purpose, we conduct a systematic mapping study over the topic on verification of cyber-physical system. We carry out a widely search of publications from 2006 to 2018 in 11 electronic databases. After the step of study selection, 80 papers are selected as primary studies for answering proposed research questions, focused questions, and statistical questions. According to these questions and their answers, this paper not only presents a quantitative and comprehensive analysis of verification challenges, abstraction methods, verification techniques, assistance tools, and verification scenarios that represent each step of verification works, but also summarizes CPS systematic natures, main routine of verification and future research directions. We believe that this survey can identify gaps in current research works and reveal new insights for the future works.}, 
keywords={Systematics;Cyber-physical systems;Tools;Databases;Guidelines;Sociology;Statistics;Systematic mapping study;verification of cyber-physical system;verification challenges;abstraction methods;verification techniques;assistance tools;verification scenarios}, 
doi={10.1109/ACCESS.2018.2872015}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{5767210, 
author={Mohammed, Rahima and Sahan, Ridvan and Xia, Yi and Pang, Ying-feng}, 
booktitle={2011 27th Annual IEEE Semiconductor Thermal Measurement and Management Symposium}, 
title={High performance air-cooled temperature margining thermal tools for silicon validation}, 
year={2011}, 
volume={}, 
number={}, 
pages={265-271}, 
abstract={Thermal tools provide temperature margining capability by varying the case temperature at silicon thermal design power (TDP). They are used for process, voltage, temperature and frequency (PVTF) testing by Intel's post-silicon validation customers across servers, desktops, mobile and graphics segments. Thermal margining tools are widely used in silicon debug validation by varying the case temperature over a wide operating range of specifications of the Silicon to i) validate the silicon, ii) accelerate fault detection, and iii) reduce escapes and identify bugs. Thermal tool is controlled by a thermal controller to provide a temperature set-point based on the device under test's (DUT's) case or junction diode temperature. Air cooled thermal tool (AC-TT) employs a controller card to achieve the margining capability by running the tool's thermoelectric cooler (TEC), a Peltier device, within the optimal temperature range. AC-TT has an active heat sink design to remove the heat dissipated by the TEC and the silicon. Although AC-TT is expected to provide narrower range of margining capability due to the limitations of air cooling, they still can be an excellent solution for some specific thermal margining applications. Therefore, a new line of AC-TTs were developed for validation customers whose needs can be addressed without requiring costly controllers and noisy chillers while enhancing the user-experience. This paper presents the design improvement strategies implemented for developing the new line of CPU, Chipset and ASIC AC-TTs. Improved designs provide wider margining capability by using i) high performance active heat sink designs, ii) high power thermo-electric cooler (TEC), iii) cold plate designs compatible to keep out volume (KOV), iv) new choice of thermal interface material (TIM), and v) new retention design. This paper discusses the details of the design process and how multiple design strategies are implemented to finalize the design and to achieve the overall performance improvement while keeping the cost of the AC-TT low. The new line of AC-TT designs have performance improvement of 44% (~25C) for 130W CPU TT compared to existing CPU AC-TT, of 32% (~19C) for 60W chipset compared to existing chipset AC-TT, and of 41% (~8C) compared to existing 15W PCH (Peripheral Component Hub) AC-TT. Design strategies provided here can be easily adapted to develop future generation of low-cost CPU, chipset, and ASIC AC-TTs with a wider margining capability.}, 
keywords={Heat sinks;Silicon;Cold plates;Heating;Heat transfer;Thermal resistance;Thermal tool;CPU;chipset;ASIC;retention design;CFD;TEC;air cooling;TIM;temperature margining}, 
doi={10.1109/STHERM.2011.5767210}, 
ISSN={1065-2221}, 
month={March},}
@INPROCEEDINGS{6130706, 
author={Svendsen, Andreas and Haugen, Øystein and Moller-Pedersen, Birger}, 
booktitle={2011 18th Asia-Pacific Software Engineering Conference}, 
title={Using Variability Models to Reduce Verification Effort of Train Station Models}, 
year={2011}, 
volume={}, 
number={}, 
pages={348-356}, 
abstract={We show how the effort needed to verify a transformed base model can be reduced by analyzing the definition of the modification. The Common Variability Language (CVL) is a generic language for modeling variability, where a CVL model describes the increment from one base model to another (transformed) base model. Assuming that a property of the base model has been verified, we use the CVL model to reduce the effort needed to verify the property of the transformed model. Based on the CVL model, we narrow down the set of traces required to be verified, including the increment and the cascading effects. We apply CVL to several models of the Train Control Language (TCL) to illustrate how the effort of verifying safety properties of transformed train station models can be reduced.}, 
keywords={Safety;Biological system modeling;Analytical models;Metals;Switches;Semantics;Mathematical model;analysis;variability;safety property;Common Variability Language;Train Control Language}, 
doi={10.1109/APSEC.2011.21}, 
ISSN={1530-1362}, 
month={Dec},}
@INPROCEEDINGS{6606719, 
author={Guana, Victor}, 
booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 
title={Supporting maintenance tasks on transformational code generation environments}, 
year={2013}, 
volume={}, 
number={}, 
pages={1369-1372}, 
abstract={At the core of model-driven software development, model-transformation compositions enable automatic generation of executable artifacts from models. Although the advantages of transformational software development have been explored by numerous academics and industry practitioners, adoption of the paradigm continues to be slow, and limited to specific domains. The main challenge to adoption is the fact that maintenance tasks, such as analysis and management of model-transformation compositions and reflecting code changes to model transformations, are still largely unsupported by tools. My dissertation aims at enhancing the field's understanding around the maintenance issues in transformational software development, and at supporting the tasks involved in the synchronization of evolving system features with their generation environments. This paper discusses the three main aspects of the envisioned thesis: (a) complexity analysis of model-transformation compositions, (b) system feature localization and tracking in model-transformation compositions, and (c) refactoring of transformation compositions to improve their qualities.}, 
keywords={Maintenance engineering;Object oriented modeling;Analytical models;Complexity theory;Software;Games;Semantics;software maintenance;transformation composition;transformation complexity;transformation refactoring}, 
doi={10.1109/ICSE.2013.6606719}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{6227107, 
author={Devos, Nicolas and Ponsard, Christophe and Deprez, Jean-Christophe and Bauvin, Renaud and Moriau, Benedicte and Anckaerts, Guy}, 
booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
title={Efficient reuse of domain-specific test knowledge: An industrial case in the smart card domain}, 
year={2012}, 
volume={}, 
number={}, 
pages={1123-1132}, 
abstract={While testing is heavily used and largely automated in software development projects, the reuse of test practices across similar projects in a given domain is seldom systematized and supported by adequate methods and tools. This paper presents a practical approach that emerged from a concrete industrial case in the smart card domain at STMicroelectronics Belgium in order to better address this kind of challenge. The central concept is a test knowledge repository organized as a collection of specific patterns named QPatterns. A systematic process was followed, first to gather, structure and abstract the test practices, then to produce and validate an initial repository, and finally to make it evolve later on Testers can then rely on this repository to produce high quality test plans identifying all the functional and nonfunctional aspects that have to be addressed, as well as the concrete tests that have to be developed within the context of a new project. A tool support was also developed and integrated in a traceable way into the existing industrial test environment. The approach was validated and is currently under deployment at STMicroelectronics Belgium.}, 
keywords={Smart cards;Testing;Libraries;Software;Security;Concrete;patterns;test;generation;smartcard}, 
doi={10.1109/ICSE.2012.6227107}, 
ISSN={1558-1225}, 
month={June},}
@INPROCEEDINGS{8327233, 
author={Chodarev, Sergej and Bačíková, Michaela}, 
booktitle={2017 IEEE 14th International Scientific Conference on Informatics}, 
title={Development of Oberon-0 using YAJCo}, 
year={2017}, 
volume={}, 
number={}, 
pages={122-127}, 
abstract={YAJCo is a tool for the development of software languages based on an annotated language model. The model is represented by Java classes with annotations defining its mapping to concrete syntax. This approach to language definition enables the abstract syntax to be central point of the development process, instead of concrete syntax. In this paper a case study of Oberon-0 programming language development is presented. The study is based on the LTDA Tool Challenge and showcases details of abstract and concrete syntax definition using YAJCo, as well as implementation of name resolution, type checking, model transformation and code generation.}, 
keywords={Syntactics;Tools;Grammar;Java;Generators;Analytical models;Abstract syntax;experience report;language development;Oberon-0;parser generator;YAJCo}, 
doi={10.1109/INFORMATICS.2017.8327233}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8591171, 
author={Chen, Yingxin and Dai, Wenbin and Zhang, Zhijie and Pang, Cheng and Vyatkin, Valeriy}, 
booktitle={IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society}, 
title={A Case Study on Knowledge Driven Code Generation for Software-Defined Industrial Cyber-Physical Systems}, 
year={2018}, 
volume={}, 
number={}, 
pages={4687-4692}, 
abstract={Industrial Cyber-Physical Systems (iCPS) enables coordination between various subsystems and devices based on real-time feedback data from sensors. iCPS must react rapidly to new requirements and adjust itself to fulfill new functionalities in no time. On the software side, control programs of iCPS need to be reconfigured dynamically. An efficient way for massive reconfiguration is automatic code generation. In this paper, a knowledge-driven code generation method is experimented for software-defined iCPS. Based on sensor values, actuators are controlled by the reasoning process with support of ontological knowledge base. The results demonstrate that iCPS could be driven by rules completely without programming control software.}, 
keywords={IEEE Senior Members;Indexes;Cyber-physical systems;Requirements engineering;Ontologies;Cognition;Industrial Cyber-Physical Systems;Code Generation;Software-Defined Systems;Requirement Engineering;Ontology Reasoning;SWRL;SQWRL}, 
doi={10.1109/IECON.2018.8591171}, 
ISSN={2577-1647}, 
month={Oct},}
@INPROCEEDINGS{1390775, 
author={Hicks, B.}, 
booktitle={The 23rd Digital Avionics Systems Conference (IEEE Cat. No.04CH37576)}, 
title={Transforming avionics architectures to support network centric warfare}, 
year={2004}, 
volume={2}, 
number={}, 
pages={8.E.3-81}, 
abstract={Network centric warfare was applied to different layers in the military force structure to enable commanders and direct combatants to monopolize information to increase lethality and survivability. The flow of information, the amount, type, and other attributes to be discussed, heavily impact the aviation sector of military operations and acquisition. This work concentrates on the impact of NCW on avionics architectures and provides insight to the changes required of aircraft systems to fully utilize the NCW tenets. The NCW concepts are described along with the properties of information necessary for network centric operations.}, 
keywords={Aerospace electronics;Information systems;Military aircraft;Privacy;Information security;Logistics;Business;Information resources;Radar;Electrooptic devices}, 
doi={10.1109/DASC.2004.1390775}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8491148, 
author={Jorge, Dalton N. and Machado, Patrícia D. L. and Alves, Everton L. G. and Andrade, Wilkerson L.}, 
booktitle={2018 IEEE 26th International Requirements Engineering Conference (RE)}, 
title={Integrating Requirements Specification and Model-Based Testing in Agile Development}, 
year={2018}, 
volume={}, 
number={}, 
pages={336-346}, 
abstract={In agile development, Requirements Engineering (RE) and testing have to cope with a number of challenges such as continuous requirement changes and the need for minimal and manageable documentation. In this sense, extensive research has been conducted to automatically generate test cases from (structured) natural language documents using Model-Based Testing (MBT). However, the imposed structure may impair agile practices or test case generation. In this paper, inspired by cooperation with industry partners, we propose CLARET, a notation that allows the creation of use case specifications using natural language to be used as central artifacts for both RE and MBT practices. A tool set supports CLARET specification by checking syntax of use cases structure as well as providing visualization of flows for use case revisions. We also present exploratory studies on the use of CLARET to create RE documents as well as on their use as part of a system testing process based on MBT. Results show that, with CLARET, we can document use cases in a cost-effective way. Moreover, a survey with professional developers shows that CLARET use cases are easy to read and write. Furthermore, CLARET has been successfully applied during specification, development and testing of industrial applications.}, 
keywords={Testing;Industries;Requirements engineering;Tools;Natural languages;Manuals;Syntactics;Agile Development;Requirements Engineering;Model-Based Testing;Use case specification}, 
doi={10.1109/RE.2018.00041}, 
ISSN={2332-6441}, 
month={Aug},}
@INPROCEEDINGS{7811570, 
author={Hoyos, Luis Cuellar and Rothenberg, Christian Esteve}, 
booktitle={2016 8th IEEE Latin-American Conference on Communications (LATINCOM)}, 
title={NOn: Network function virtualization ontology towards semantic service implementation}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={A hazard of ongoing Network Function Virtualization (NFV) realizations is the lack of a common understanding in support of development, deployment and operation tasks related to Virtual Function Networks (VNFs), NFV components and interfaces. In the current state of affairs, NFV stakeholders commonly create their own terminology to define and describe NFV components, following going the specifications led by European Telecommunications Standard Institute but also adopting telecommunication- and software-centric definitions. As a consequence, portability and interoperability goals of NFV get compromised since NFV technology providers have hard times in understanding and using definitions and descriptions across different domains. Furthermore, VNF data models of operational systems and deployment configuration software need to be re-defined, re-coded, and re-compiled to make them work over different NFV platforms. In this work, we present the design and implementation of our proposed NFV Ontology (NOn) enabling Semantic nFV Services (SnS) to reduce manual intervention during the integration process of heterogeneous NFV domains and effectively overcome the costly re-work hazards of current NFV implementation approaches. We present the proof of concept implementation of a Generic Client leveraging SnS/NOn to create and consume dynamic workflows in an open source testbed based on OpenStack and OpenBaton.}, 
keywords={Semantics;Ontologies;Interoperability;Software;Network function virtualization;Manuals;Engines;Network Function Virtualization;NFV;Semantic Services;Ontology}, 
doi={10.1109/LATINCOM.2016.7811570}, 
ISSN={}, 
month={Nov},}
@ARTICLE{5621965, 
author={Schwartz, Mischa}, 
journal={IEEE Communications Magazine}, 
title={X.25 Virtual Circuits - TRANSPAC IN France - Pre-Internet Data Networking [History of communications]}, 
year={2010}, 
volume={48}, 
number={11}, 
pages={40-46}, 
abstract={The following article , by Remi Despres, is the second on the history of X.25 systems to appear in this column. As noted by Dr. Despres, the previous article focused on the Canadian Datapac system. Earlier articles on packet switching in this column have included one on the history of the Arpanet/Internet and one on early British packet switching systems. What makes this article particularly distinctive, aside, of course, from the fact that it focuses on the major contributions of French engineers to the development of packet switching as well as to X.25 standardization, is that it carefully outlines the reasons for the choice of connection-oriented virtual circuits for the Transpac network, as contrasted with datagram-based packet switching adopted for Arpanet. Interestingly, Dr. Despres notes that the idea of using virtual-circuit connection-oriented packet switching in the Transpac development came from the British packet switching activity. It is to be noted that early commercial packet switching networks in the United States, such as Tymnet and Telenet, also adopted the virtual circuit paradigm.}, 
keywords={Packet switching;Virtual circuits;Protocols;Internet;History;Software}, 
doi={10.1109/MCOM.2010.5621965}, 
ISSN={1558-1896}, 
month={November},}
@ARTICLE{9534688, 
author={Cherrueau, Ronan-Alexandre and Delavergne, Marie and van Kempen, Alexandre and Lebre, Adrien and Pertin, Dimitri and Balderrama, Javier Rojas and Simonet, Anthony and Simonin, Matthieu}, 
journal={IEEE Transactions on Parallel and Distributed Systems}, 
title={EnosLib: A Library for Experiment-Driven Research in Distributed Computing}, 
year={2022}, 
volume={33}, 
number={6}, 
pages={1464-1477}, 
abstract={Despite the importance of experiment-driven research in the distributed computing community, there has been little progress in helping researchers conduct their experiments. In most cases, they have to achieve tedious and time-consuming development and instrumentation activities to deal with the specifics of testbeds and the system under study. In order to relieve researchers of the burden of those efforts, we have developed EnosLib: a Python library that takes into account best experimentation practices and leverages modern toolkits on automatic deployment and configuration systems. EnosLib helps researchers not only in the process of developing their experimental artifacts, but also in running them over different infrastructures. To demonstrate the relevance of our library, we discuss three experimental engines built on top of EnosLib, and used to conduct empirical studies on complex software stacks between 2016 and 2019 (database systems, communication buses and OpenStack). By introducing EnosLib, our goal is to gather academic and industrial actors of our community around a library that aggregates everyday experiment-driven research operations. A library that has been already adopted by open-source projects and members of the scientific community thanks to its ease of use and extension.}, 
keywords={Libraries;Software;Task analysis;Tools;Protocols;Codes;Benchmark testing;Experiment-driven research;performance evaluation;distributed computing experimentation library}, 
doi={10.1109/TPDS.2021.3111159}, 
ISSN={1558-2183}, 
month={June},}
@INPROCEEDINGS{6597883, 
author={Fan, Guisheng and Yu, Huiqun and Chen, Liqiong and Liu, Dongmei}, 
booktitle={2013 International Symposium on Theoretical Aspects of Software Engineering}, 
title={Aspect Orientation Based Test Case Selection Strategy for Service Composition}, 
year={2013}, 
volume={}, 
number={}, 
pages={95-104}, 
abstract={Software testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may select part of their test cases so that those that are more important are run earlier in the testing process. However, the methods that can be used to select test cases for service composition and its analysis are still lacking at present. This paper proposes an aspect orientation based test case selection strategy for service composition. Aspect-orientation is used to weave testing crosscutting concerns of service composition, which includes component testing concern and testing concern of service composition, the weaving mechanism dynamically integrates these schemas into a testing enforcement model. Based on this, the test cases selection strategy for service composition is given, and abstract it as a crosscutting concern to weave into testing model, the corresponding enforcement algorithm is also given, the operation semantics and related theories of Petri nets help prove its effectiveness and feasibility. A case study explains the testing process of service composition, and a series of experiments are done to explain that the use of aspects for testing Web service is more efficient than conventional techniques, which can improve the testing quality and efficiency.}, 
keywords={Testing;Weaving;Computational modeling;Analytical models;Semantics;Web services;Service composition; testing model; test case; aspect orientation; Petri nets}, 
doi={10.1109/TASE.2013.21}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7872763, 
author={Akbar, Sabriansyah Rizqika and Kurniawan, Wijaya and Ichsan, Mochammad Hannats Hanafi and Arwani, Issa and Handono, Maystya Tri}, 
booktitle={2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, 
title={Pervasive device and service discovery protocol in XBee sensor network}, 
year={2016}, 
volume={}, 
number={}, 
pages={79-84}, 
abstract={Internet of Things is a novel paradigm that combined microcomputer and wireless communication technology. IoT device will be considered as a pervasive and ubiquitous device that able to interact with it user and environment autonomously with minimum human intervention. At present, wireless technology already has pervasive features in the link and the network layer. They able to do the dynamic addressing, finding a neighbor and do the routing task such as in Xbee technology. In the future, pervasive sensing will support adaptive context-aware services that are not provided by the wireless sensor network protocol in the link and the network layer. Our research proposed pervasive device and service discovery protocol at the application level by creating a protocol in the smart sensor device and the smart sensor gateway. By implementing our protocol, the sensor network gateway is able to find each of the sensor network device and service descriptions and request the on-demand service to the smart sensor device. The protocol is implemented in two Arduino Uno integrated with XBee transceiver as the smart sensor device and the raspberry pi as the smart sensor gateway. Result shows, the gateway was able to find both device and service description in the smart sensor network with 4.13 seconds average time. The average round trip time for request and response data from the gateway is 0.201 seconds.}, 
keywords={Logic gates;Intelligent sensors;Protocols;Machine-to-machine communications;Temperature sensors;Humidity;Wireless sensor networks;Internet of Things;pervasive discovery;sensor network}, 
doi={10.1109/ICACSIS.2016.7872763}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6297159, 
author={Nagowah, Leckraj and Doorgah, Kishan}, 
booktitle={2012 International Conference on Computer & Information Science (ICCIS)}, 
title={Improving test data management in record and playback testing tools}, 
year={2012}, 
volume={2}, 
number={}, 
pages={931-937}, 
abstract={It is almost impossible to prevent requirement change in the web development life cycle. Selenium despite being a widely used open source automated tool for testing web application, has its limitation when it concerns test data management. Frequent changes in requirement result in changes in the user interface which in turn requires additional effort to re-record the test script. Eventually keeping track of test data used for each test script becomes very problematic for the tester. In this paper, we analyse existing tools and provide a design of an automated testing tool, Kishanium that also manages the set of test data. A prototype was created during experimentation phase to prove the concept of the underlying ideas of the proposed tool. The prototype has been implemented based on the core technologies of DomDocument, XPath and Curl. The testing carried out proves that Kishanium is a useful automated tool that can be used on its own or in conjunction with Selenium. With a very systematic approach it automatically searches input and button objects, allows testers to add new test data, edit existing test data and delete previous test data in order to respond to frequent requirement changes. The power of Kishanium is that it is able to re-use existing test data even if there are a number of changes in the user interface. It also automatically runs the tests with the appropriate set of test data using its Poster Component. Moreover the Kishanium automated tool provides additional features such as Data generator, Spylink and Snapshot.}, 
keywords={Presses;Fires;Manuals;Libraries;record and playback problem;automated testing;test data management}, 
doi={10.1109/ICCISci.2012.6297159}, 
ISSN={}, 
month={June},}
@ARTICLE{1638205, 
author={}, 
journal={IEEE Std 1100-2005 (Revision of IEEE Std 1100-1999)}, 
title={IEEE Recommended Practice for Powering and Grounding Electronic Equipment}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-703}, 
abstract={The IEEE Emerald Book(TM) presents a collection of consensus best practices for the powering and grounding of electronic equipment used in commercial and industrial applications.The main objective is to provide consensus recommended practices in an area where conflicting information and conflicting design philosophies have dominated. The recommended practices described are intended to enhance equipment performance while maintaining a safe installation. A description of the nature and origin of power disturbances is provided, followed by theory on the various parameters that impact power quality. Information on quantifying and resolving power and grounding related concerns using measurement and diagnostic instrumentation and standardized investigative procedures are included. Recommended power protection equipment and wiring and grounding system design practices are presented. Information on telecommunications system power protection as well as grounding, industrial system grounding, and noise control is included.Finally a selection of case studies are presented to support the recommended practices presented throughout the book.}, 
keywords={IEEE Standards;Commercialization;Electronic equipment;Industry applications;Power conditioning;Power system quality;Monitoring;Grounding;commercial applications;electrical power;electronic equipment;grounding;industrial applications;power conditioning;power disturbance;power monitor;power quality;color book}, 
doi={10.1109/IEEESTD.2006.216391}, 
ISSN={}, 
month={May},}
@ARTICLE{9622288, 
author={Leroy, Dorian and Sallou, June and Bourcier, Johann and Combemale, Benoit}, 
journal={Computer}, 
title={When Scientific Software Meets Software Engineering}, 
year={2021}, 
volume={54}, 
number={12}, 
pages={60-71}, 
abstract={We investigate the different levels of abstraction, linked to the diverse artifacts of the scientific software development process, that a software language can propose and the validation and verification facilities associated with the corresponding level of abstraction the language can provide to the user.}, 
keywords={Software;Software engineering}, 
doi={10.1109/MC.2021.3102299}, 
ISSN={1558-0814}, 
month={Dec},}
@INPROCEEDINGS{8760965, 
author={Cavalcante, Maria Gerliane and Sales, José Iranildo}, 
booktitle={2019 14th Iberian Conference on Information Systems and Technologies (CISTI)}, 
title={The Behavior Driven Development Applied to the Software Quality Test:}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={The mechanisms of private investment in Brazilian sport are one of the main sources of resources that athletes hold to promote participation in high-performance competitions. In order to improve this flow, softwares are designed to shorten the path between donors and recipients. These tools, such as “Meu Campeãu” used in this paper, require efficient analysis of the quality of the product offered. Concepts known as agile methodologies have brought a new style to the analysis of the software developed, and one of them has gained considerable prominence, known such as BDD (Behavior Driven Development), so this paper aims to analyze the implementation of Behavior Driven Development in the software quality verification process “Meu Campeãu”.}, 
keywords={Testing;Business;Sports;Software quality;Tools;Games;Behavior Driven Development;Software Quality Test;Sports Financing}, 
doi={10.23919/CISTI.2019.8760965}, 
ISSN={2166-0727}, 
month={June},}
@INPROCEEDINGS{8887378, 
author={Sachidananda, Vinay and Bhairav, Suhas and Ghosh, Nirnay and Elovici, Yuval}, 
booktitle={2019 18th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/13th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)}, 
title={PIT: A Probe Into Internet of Things by Comprehensive Security Analysis}, 
year={2019}, 
volume={}, 
number={}, 
pages={522-529}, 
abstract={One of the major issues which are hindering widespread and seamless adoption of Internet of Thing (IoT) is security. The IoT devices are vulnerable and susceptible to attacks which became evident from a series of recent large-scale distributed denial-of-service (DDoS) attacks, leading to substantial business and financial losses. Furthermore, in order to find vulnerabilities in IoT, there is a lack of comprehensive security analysis framework. In this paper, we present a modular, adaptable and tunable framework, called PIT, to probe IoT systems at different layers of design and implementation. PIT consists of several security analysis engines, viz., penetration testing, fuzzing, static analysis, and dynamic analysis and an exploitation engine to discover multiple IoT vulnerabilities, respectively. We also develop a novel grey-box fuzzer, called Applica, as a part of the fuzzing engine to overcome the limitations of the present day fuzzers. The proposed framework has been evaluated on a real-world IoT testbed comprising of the state-of-the-art devices. We discovered several network and system-level vulnerabilities such as Buffer Overflow, Denial-of-Service, SQL Injection, etc., and successfully exploited them to demonstrate the presence of security loopholes in the IoT devices.}, 
keywords={Security;Engines;Fuzzing;Generators;Static analysis;Protocols;Internet of Things;Internet of Things;Security and Privacy;Security Analysis;Vulnerability;Framework;Fuzzing}, 
doi={10.1109/TrustCom/BigDataSE.2019.00076}, 
ISSN={2324-9013}, 
month={Aug},}
@INPROCEEDINGS{9527970, 
author={Gylling, Andreas and Ekstedt, Mathias and Afzal, Zeeshan and Eliasson, Per}, 
booktitle={2021 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
title={Mapping Cyber Threat Intelligence to Probabilistic Attack Graphs}, 
year={2021}, 
volume={}, 
number={}, 
pages={304-311}, 
abstract={As cyber threats continue to grow and expertise resources are limited, organisations need to find ways to evaluate their resilience efficiently and take proactive measures against an attack from a specific adversary before it occurs. Threat modelling is an excellent method of assessing the resilience of ICT systems, forming Attack (Defense) Graphs (ADGs) that illustrate an adversary’s attack vectors. Cyber Threat Intelligence (CTI) is information that helps understand the current cyber threats, but has little integration with ADGs. This paper contributes with an approach that resolves this problem by using CTI feeds of known threat actors to enrich ADGs under multiple reuse. This enables security analysts to take proactive measures and strengthen their ICT systems against current methods used by any threat actor that is believed to pose a threat to them.}, 
keywords={Current measurement;Conferences;Probabilistic logic;Security;Feeds;Computer crime;Resilience}, 
doi={10.1109/CSR51186.2021.9527970}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7329720, 
author={Sroka, Michal and Nagy, Roman and Fisch, Dominik}, 
booktitle={2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)}, 
title={Impact of mutation intensity on evolutionary test model learning}, 
year={2015}, 
volume={}, 
number={}, 
pages={271-276}, 
abstract={Automation in the software testing process has significant impact on the overall software development in industry. The focus of this paper is on automation of test case design via model-based testing for automotive embedded software. A new method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information was designed and this paper investigates the impact of mutation intensity on the evolutionary learning process.}, 
keywords={Biological cells;Sociology;Statistics;Software;Testing;Evolutionary computation;Software algorithms}, 
doi={10.1109/INES.2015.7329720}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8900962, 
author={Kritzinger, Lisa Maria and Krismayer, Thomas and Rabiser, Rick and Grünbacher, Paul}, 
booktitle={2019 Working Conference on Software Visualization (VISSOFT)}, 
title={A User Study on the Usefulness of Visualization Support for Requirements Monitoring}, 
year={2019}, 
volume={}, 
number={}, 
pages={56-66}, 
abstract={Many requirements monitoring approaches have been proposed that check key properties of systems and their interactions at runtime. Some of these approaches also visualize monitoring results and provide details on requirements violations to end users. However, only few studies exist about the usefulness of requirements monitoring tools for practitioners, particularly regarding visualization. In this paper, we present a user study we have conducted with both industrial practitioners and researchers to assess the usefulness of visualization capabilities we have been developing for an event-based requirements monitoring tool. These capabilities allow users to monitor the status of the involved systems, to view trends and statistics, and to inspect the events and data that led to specific violations when diagnosing their root cause. We first performed a walkthrough of the tool using the cognitive dimensions of notations framework from the field of human-computer interaction. We then conducted a user study involving five software engineers of a large company from the automation software domain and four researchers. Using the tool's visualization capabilities all subjects succeeded in monitoring a real-world automation system and in diagnosing violations. Subjects regarded the visualization capabilities as essential for understanding the behavior of a complex system. Based on the study results we derive implications, opportunities, and risks of using visualization in requirements monitoring tools.}, 
keywords={Monitoring;Tools;Visualization;Data visualization;Usability;Market research;requirements monitoring, visualization, systems of systems, usability study, user study}, 
doi={10.1109/VISSOFT.2019.00015}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{9377950, 
author={Wright, Steven A.}, 
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, 
title={AI in the Law: Towards Assessing Ethical Risks}, 
year={2020}, 
volume={}, 
number={}, 
pages={2160-2169}, 
abstract={The exponential growth in data over the past decade has impacted the legal industry; both requiring automated solutions for the cost effective and efficient management of the volume and variety of big (legal) data; and, enabling artificial intelligence techniques based on machine learning for the analysis of that data. While many legal practitioners focus on specific services niches, the impact of AI in the law is much broader than individual niches. While AI systems and concerns for their ethical operation are not new, the scale of impact and adoption of AI systems in legal practice makes consideration of the ethics of these systems timely. While there has been recent progress in development of ethical guidelines for AI systems, much of this is targeted at the developers of these systems in general, or the actions of these AI systems as autonomous entities, rather than in the legal practice context. Much of the ethical guidance - whether for AI systems or legal professional is captured in high level principles within more narrowly defined domains, more specific guidance may be appropriate to identify and assess ethical risks. As adoption and operation of AI software in routine legal practice becomes more commonplace, more detailed guidance on assessing the scope and scale of ethical risks is needed.}, 
keywords={Industries;Ethics;Law;Software;Artificial intelligence;Standards;Guidelines;Ethics;AI;Law;Assessment}, 
doi={10.1109/BigData50022.2020.9377950}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6120072, 
author={Jiao Yu and Wilamowski, Bogdan M.}, 
booktitle={IECON 2011 - 37th Annual Conference of the IEEE Industrial Electronics Society}, 
title={Recent advances in in-vehicle embedded systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={4623-4625}, 
abstract={The number of computer based functions embedded in vehicles has increased significantly in the past two decades. An in-vehicle embedded electronic architecture is a complex distributed system; the development of which is a cooperative work involving different manufacturers and suppliers. There are several key demands in the development process, such as safety requirements, real-time assessment, schedulability, composability, etc. Intensive research is being conducted to address these issues. This paper reviews recent technology advances in relevant aspects and covers a range of topics highlighted above.}, 
keywords={Real time systems;Embedded systems;Automotive engineering;Field programmable gate arrays;Multicore processing;In-vehicle embedded electronic architecture;FPGA;real-time assessment;composability}, 
doi={10.1109/IECON.2011.6120072}, 
ISSN={1553-572X}, 
month={Nov},}
@INPROCEEDINGS{1342755, 
author={Andrews, J.H.}, 
booktitle={Proceedings. 19th International Conference on Automated Software Engineering, 2004.}, 
title={A case study of coverage-checked random data structure testing}, 
year={2004}, 
volume={}, 
number={}, 
pages={316-319}, 
abstract={We study coverage-checked random unit testing (CRUT), the practice of repeatedly testing units on sequences of random function calls until given code coverage goals are achieved. Previous research has shown that this practice can be a useful complement to traditional testing methods. However, questions remained as to the breadth of its applicability. In this paper, we report on a case study in which we applied CRUT to the testing of two mature public-domain data structures packages. We show that CRUT helped in identifying faults, in debugging, in extracting and specifying actual behaviour, and in achieving greater assurance of the correctness of the debugged software}, 
keywords={Computer aided software engineering;Data structures;Software testing;Packaging;Fault diagnosis;Software engineering;Automatic testing;Documentation;Computer science;Debugging}, 
doi={10.1109/ASE.2004.1342755}, 
ISSN={1938-4300}, 
month={Sep.},}
@ARTICLE{9195875, 
author={Jiang, Shunning and Ou, Yanghui and Pan, Peitian and Cheng, Kaishuo and Zhang, Yixiao and Batten, Christopher}, 
journal={IEEE Design & Test}, 
title={PyH2: Using PyMTL3 to Create Productive and Open-Source Hardware Testing Methodologies}, 
year={2021}, 
volume={38}, 
number={2}, 
pages={53-61}, 
abstract={Editor's note: This article proposes a new model testing and verification methodology, PyH2, using property-based random testing in Python. PyH2 leverages the whole Python ecosystem to build test benches and models. - Sherief Reda, Brown University - Leon Stock, IBM - Pierre-Emmanuel Gaillardon, University of Utah}, 
keywords={Design automation;Hardware;Open source software;Object oriented modeling;Python;Computer bugs;Cathode ray tubes}, 
doi={10.1109/MDAT.2020.3024144}, 
ISSN={2168-2364}, 
month={April},}
@INPROCEEDINGS{9286236, 
author={Fredin, Zach and Zemanek, Jiri and Blackburn, Camron and Strand, Erik and Abdel-Rahman, Amira and Rowles, Premila and Gershenfeld, Neil}, 
booktitle={2020 IEEE High Performance Extreme Computing Conference (HPEC)}, 
title={Discrete Integrated Circuit Electronics (DICE)}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={We introduce DICE (Discrete Integrated Circuit Electronics). Rather than separately develop chips, packages, boards, blades, and systems, DICE spans these scales in a direct-write process with the three-dimensional assembly of computational building blocks. We present DICE parts, discuss their assembly, programming, and design workflow, illustrate applications in machine learning and high performance computing, and project performance.}, 
keywords={Performance evaluation;Thermodynamics;Three-dimensional displays;High performance computing;Memory management;Programming;Packaging;Automated assembly;additive manufacturing;chiplets;system integration;machine learning;high-performance computing}, 
doi={10.1109/HPEC43674.2020.9286236}, 
ISSN={2643-1971}, 
month={Sep.},}
@ARTICLE{715185, 
author={}, 
journal={IEEE Spectrum}, 
title={What's ahead for design on the web}, 
year={1998}, 
volume={35}, 
number={9}, 
pages={53-63}, 
abstract={Panel discussion: Experts: the Web offers better design collaboration and a path toward greater tool interoperability.}, 
keywords={Internet;Computer aided engineering;Design engineering;Design automation;Computer science;Circuits;Process design;Web sites;Electrical engineering;Workstations}, 
doi={10.1109/MSPEC.1998.715185}, 
ISSN={1939-9340}, 
month={Sep.},}
@INPROCEEDINGS{4384854, 
author={Ferreira, Joao and Carvalho, Andre and Pimentel, Joao and Guedes, Marco and Furini, Francesco and Silva, Nuno}, 
booktitle={2007 5th IEEE International Conference on Industrial Informatics}, 
title={Modeling Engineering and Manufacturing Activity in Vehicle Development Process}, 
year={2007}, 
volume={2}, 
number={}, 
pages={675-680}, 
abstract={Designing and consequent assembly of a new vehicle is a complex process as it requires close coordination and inputs from a number of disciplines in developing a number of systems and sub-systems in the vehicle that should fit within the confined vehicle space, function and provide the customers an acceptable combination of all relevant vehicle attributes. Understanding how these processes interact and how they are aligned with other while they should support the tasks involved in the conception of a new vehicle at a minimum time and cost. The first step to achieve this goal is the definition of a new UML profile (called VDML, vehicle development modeling language) based on the extension mechanics of UML (industry standard language) to assist business process description and consequent improvements achieved by the high level vision. To show the benefits of this new language applied to this specific business we model the engineer and manufacturing activity process using VDML.}, 
keywords={Automotive engineering;Manufacturing processes;Virtual manufacturing;Space vehicles;Unified modeling language;Process design;Assembly systems;Costs;Manufacturing industries;Standards development}, 
doi={10.1109/INDIN.2007.4384854}, 
ISSN={2378-363X}, 
month={June},}
@INPROCEEDINGS{9282808, 
author={Jebbar, Oussama and Khendek, Ferhat and Toeroe, Maria}, 
booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)}, 
title={Architecture for the Automation of Live Testing of Cloud Systems}, 
year={2020}, 
volume={}, 
number={}, 
pages={142-151}, 
abstract={Live testing is performed in the production environment. In such environment, test activities have to be orchestrated properly to avoid interferences with normal usage traffic. Conducting live testing activities manually is error prone because of the size and the complexity of the system as well as the required complex orchestration of different tasks. Furthermore, it would be impossible to react to failures and contain them in due time without automation. Live testing requires a high level of automation. This automation comes with several challenges especially in contexts such as cloud and zero touch networks because of the diversity of the software composing them. In this paper we discuss the challenges of automating live testing for cloud systems. We propose an architecture that relies on a modeling framework to decouple the specification of testing activities from the platforms needed to conduct them. We propose a solution for conducting testing activities on a live system according to such a specification.}, 
keywords={Automation;Computer architecture;Production;Complexity theory;Specification languages;Task analysis;Testing;live testing;cloud;UML Testing Profile;test architecture;automation}, 
doi={10.1109/QRS51102.2020.00030}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6912285, 
author={Wohlrab, Rebekka and de Gooijer, Thijmen and Koziolek, Anne and Becker, Steffen}, 
booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)}, 
title={Experience of pragmatically combining RE methods for performance requirements in industry}, 
year={2014}, 
volume={}, 
number={}, 
pages={344-353}, 
abstract={To meet end-user performance expectations, precise performance requirements are needed during development and testing, e.g., to conduct detailed performance and load tests. However, in practice, several factors complicate performance requirements elicitation: lacking skills in performance requirements engineering, outdated or unavailable functional specifications and architecture models, the specification of the system's context, lack of experience to collect good performance requirements in an industrial setting with very limited time, etc. From the small set of available non-functional requirements engineering methods, no method exists that alone leads to precise and complete performance requirements with feasible effort and which has been reported to work in an industrial setting. In this paper, we present our experiences in combining existing requirements engineering methods into a performance requirements method called PROPRE. It has been designed to require no up-to-date system documentation and to be applicable with limited time and effort. We have successfully applied PROPRE in an industrial case study from the process automation domain. Our lessons learned show that the stakeholders gathered good performance requirements which now improve performance testing.}, 
keywords={Measurement;Context;Time factors;Documentation;Adaptation models;Throughput;Testing}, 
doi={10.1109/RE.2014.6912285}, 
ISSN={2332-6441}, 
month={Aug},}
@INPROCEEDINGS{9843754, 
author={Bocchino, Robert L and Levison, Jeffrey W. and Starch, Michael D.}, 
booktitle={2022 IEEE Aerospace Conference (AERO)}, 
title={FPP: A Modeling Language for F Prime}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-15}, 
abstract={We present F Prime Prime (FPP), a new open-source modeling language for F Prime. F Prime is an open-source flight software framework developed at JPL and deployed, among other places, on the Mars helicopter Ingenuity. FPP provides a convenient way to model the architectural elements of an F Prime application, e.g., components, ports, and their connections. It has a succinct and readable syntax, a well-defined semantics, and robust error checking and reporting. The FPP tool suite, written in Scala, analyzes FPP models, reports errors, and translates correct FPP models to a combination of XML and C++. Existing F Prime tools translate the XML to a partial implementation in C++, to be completed by the developers. The model elements have clean interfaces and are highly reusable. An accompanying visualization tool constructs diagrams of components and connections that FSW developers can use to understand and communicate their designs, for example at reviews. We discuss the design and implementation of FPP and the integration of FPP into F Prime. We also discuss our experience using FPP to construct F Prime models. Finally, we discuss our plans for future work, including improved code generation, improved visualization, and more advanced analysis capabilities.},
keywords={Ports (computers);Visualization;Analytical models;Mars;Semantics;XML;C++ languages}, 
doi={10.1109/AERO53065.2022.9843754}, 
ISSN={1095-323X}, 
month={March},}
@INPROCEEDINGS{7755285, 
author={Jain, Manish and Gopalani, Dinesh}, 
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, 
title={Testing application security with aspects}, 
year={2016}, 
volume={}, 
number={}, 
pages={3161-3165}, 
abstract={For the purpose of security of the computer systems, organizations now a days plan a lot of things like firewalls, network scanning tools, secure sockets layer (SSL) etc. However security bugs present at the application layer (code level) caused by unawareness or mistakes of the developers are usually ignored. Such security bugs can lead to unauthorized privileges on a computer system. For example most web applications connect back to databases which contain sensitive information. Malicious input can allow the attacker to alter the flow of the web application and provide unauthorized access to the confidential database. Hence proper security tests are required to be conducted in order to assess the security of applications. In this paper, we propose the use of Aspect Oriented Programming (AOP) for the purpose of security testing of Java applications. With the examples of fuzz testing and servlet testing using aspects, we will show how AOP can be used for detection of security bugs in Java applications by creeping inside the program without making any changes to the source code.}, 
keywords={Security;Testing;Java;Databases;Computer bugs;HTML;Programming;Aspect Oriented Programming;Security Testing;Software Testing;Aspects;AOP}, 
doi={10.1109/ICEEOT.2016.7755285}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{9794130, 
author={Gerten, Michael C. and Marsh, Alexis L. and Lathrop, James I. and Cohen, Myra B. and Miner, Andrew S. and Klinge, Titus H.}, 
booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)}, 
title={Inference and Test Generation Using Program Invariants in Chemical Reaction Networks}, 
year={2022}, 
volume={}, 
number={}, 
pages={1193-1205}, 
abstract={Chemical reaction networks (CRNs) are an emerging distributed computational paradigm where programs are encoded as a set of abstract chemical reactions. CRNs can be compiled into DNA strands which perform the computations in vitro, creating a foundation for intelligent nanodevices. Recent research proposed a software testing framework for stochastic CRN programs in simulation, however, it relies on existing program specifications. In practice, specifications are often lacking and when they do exist, transforming them into test cases is time-intensive and can be error prone. In this work, we propose an inference technique called ChemFlow which extracts 3 types of invariants from an existing CRN model. The extracted invariants can then be used for test generation or model validation against program implementations. We applied ChemFlow to 13 CRN programs ranging from toy examples to real biological models with hundreds of reactions. We find that the invariants provide strong fault detection and often exhibit less flakiness than specification derived tests. In the biological models we showed invariants to developers and they confirmed that some of these point to parts of the model that are biologically incorrect or incomplete suggesting we may be able to use ChemFlow to improve model quality.}, 
keywords={Software testing;Biological system modeling;Toy manufacturing industry;Stochastic processes;Chemical reactions;Nanoscale devices;Test pattern generators;chemical reaction networks;test generation;invariants;Petri nets}, 
doi={10.1145/3510003.3510176}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{9236829, 
author={Li, Hongwei and Wang, Junsheng and Dai, Hua and Lv, Bang}, 
booktitle={2020 IEEE 3rd International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
title={Research on Microservice Application Testing System}, 
year={2020}, 
volume={}, 
number={}, 
pages={363-368}, 
abstract={In response to the development plan of national informatization construction, The State Grid Corporation of China attaches great importance to the development of information technology and pioneers the successful application of a new information technology generation in the power grid business. The development of the core components of “State Grid Cloud” and site deployment are also the company's focus. Microservices, as a key technology for deployment in the cloud, because of its independent development, independent deployment, independent release, decentralized management, and support for high concurrency and feature to support the rich technology stack, have been applied to cloud service construction by The State Grid. However, in the process of wide application of microservice applications, the entire microservice system hasn't been perfected, and the research of the corresponding test scheme is still in its infancy. This paper compares the difference between microservice and traditional service, and puts forward a set of microservice application testing system based on the current microservice development and software testing technology. This testing system divides the microservice test into six stages, unifies the test tools of each stage and explains the test methods. In addition, it also explains the automation strategy of the microservice test. It is of great significance to perfect the microservice testing system of The State Grid Corporation.}, 
keywords={Software testing;Automation;Tools;Software;Information technology;Research and development;Testing;Microservices;Cloud Platform;Microservice Testing System;Automatic Deployment}, 
doi={10.1109/ICISCAE51034.2020.9236829}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{1541696, 
author={Karayannis, F. and Serrat-Fernandez, J. and Baliosian, J. and Rubio-Loyola, J. and Vaxevanakis, K.G. and Pagomenos, G. and Zahariadis, T.B.}, 
journal={IEEE Communications Magazine}, 
title={In-field evaluation of a managed IP/MPLS over WDM provisioning solution}, 
year={2005}, 
volume={43}, 
number={11}, 
pages={S26-S33}, 
abstract={This article demonstrates results and experiences gained in the area of multilayer internetworking, with emphasis on bandwidth on-demand provisioning as well as resource and restoration management. Behavioral characteristics and numerical results were obtained from a management. system prototype implemented and tested in an appropriately adapted commercial WDM environment enhanced with multivendor gigabit IP routers. The management solution, the testbed environment, and a representative evaluation scenario are presented as a means of explaining in detail the results that finally allow a global system assessment.}, 
keywords={Multiprotocol label switching;Wavelength division multiplexing;Telecommunication network management;Testing;Optical scattering;Biomedical optical imaging;Asynchronous transfer mode;Information management;Inventory management;Synchronous digital hierarchy}, 
doi={10.1109/MCOM.2005.1541696}, 
ISSN={1558-1896}, 
month={Nov},}
@INPROCEEDINGS{6984580, 
author={Oliveira, Pedro and Souza, Matheus and Braga, Ronyerison and Britto, Ricardo and Rabêlo, Ricardo Lira and Neto, Pedro Santos}, 
booktitle={2014 IEEE 26th International Conference on Tools with Artificial Intelligence}, 
title={Athena: A Visual Tool to Support the Development of Computational Intelligence Systems}, 
year={2014}, 
volume={}, 
number={}, 
pages={950-959}, 
abstract={Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.}, 
keywords={Visualization;Productivity;Remuneration;Computational modeling;Algorithm design and analysis;Computational intelligence;Resource management;Computational Intelligence;Artificial Intelligence;Visual Programming;Tool;Service}, 
doi={10.1109/ICTAI.2014.144}, 
ISSN={2375-0197}, 
month={Nov},}
@INPROCEEDINGS{4702753, 
author={Schavey, Todd and Duba, Shane}, 
booktitle={2008 IEEE/AIAA 27th Digital Avionics Systems Conference}, 
title={Streamlining IMA integration through model-driven methodologies}, 
year={2008}, 
volume={}, 
number={}, 
pages={1.B.3-1-1.B.3-5}, 
abstract={Avionics systems integration is an inherently complex undertaking. In addition to ensuring that basic functionality is satisfied, the systems integrator must maximize the systempsilas flexibility and reliability while minimizing weight and cost of change. With the introduction of integrated modular architectures (IMA) based on open standards, many traditional integration issues have been greatly improved. However, additional integration responsibilities arise due to having a large number of functions developed by independent suppliers all sharing the same physical resources. This paper describes the benefits of incorporating model-driven methodologies and associated tools into the systems integration process through Model Driven Integration (MDI). It describes how a system model can not only streamline the additional responsibilities but also establish a highly productive systems development environment and allow for virtual integration. In addition, this paper discusses a number of side- benefits that grow out of having a modeling tool platform such as enhanced team communication and automation opportunities. The analysis of avionics architectures and the use of model-driven methodologies to increase IMA manageability is based upon the authors' experience in developing platform computing systems at GE Aviation. GE Aviation has developed open system IMA architectures for both commercial aircraft and military aircraft.}, 
keywords={Aerospace electronics;Hardware;Cost function;Computer architecture;Humans;Biological system modeling;Manufacturing;Military aircraft;Muscles;Automotive engineering}, 
doi={10.1109/DASC.2008.4702753}, 
ISSN={2155-7209}, 
month={Oct},}
@INPROCEEDINGS{7546576, 
author={Jain, Manish and Gopalani, Dinesh}, 
booktitle={2016 Second International Conference on Computational Intelligence & Communication Technology (CICT)}, 
title={Aspect Oriented Programming and Types of Software Testing}, 
year={2016}, 
volume={}, 
number={}, 
pages={64-69}, 
abstract={Software testing is a process to determine that a software product satisfies the specified requirements. Software testing spans over all phases of the Software Development Life Cycle namely, requirement specification, analysis, designing, development, deployment and maintenance of a software. Software testing is important to point out the defects in the software and to ensure that the developed software works fine in the real environment with different operating systems, devices, browsers and concurrent users. Further software testing can be classified into various types based on the objective of testing, level at which the testing is performed, knowledge of the system or the degree of automation. In this paper, we examine the suitability of Aspect Oriented Programming (AOP) for the purpose of performing various types of software testing. AOP is a programming paradigm which modularizes the crosscutting concerns into units called aspects and separates them from the modules implementing the primary business logic. This leads to a system that is easier to understand and simpler to maintain. The basis of the idea behind using AOP for software testing is that aspects in AOP can be used to capture execution points within the program's modules and thus we can test components where we suspect bugs without even modifying the source code.}, 
keywords={Computational intelligence;Communications technology;Aspect Oriented Programming;Types of Software Testing;Aspects;AOP}, 
doi={10.1109/CICT.2016.22}, 
ISSN={}, 
month={Feb},}
@ARTICLE{9631239, 
author={Zhu, Penghua and Li, Ying and Li, Tongyu and Yang, Wei and Xu, Yihan}, 
journal={IEEE Access}, 
title={GUI Widget Detection and Intent Generation via Image Understanding}, 
year={2021}, 
volume={9}, 
number={}, 
pages={160697-160707}, 
abstract={Aerospace control software is the most important part of aerospace software. Since its potential defects endanger life and safety, there are strict requirements on product quality. Therefore, efficient and reliable software testing is essential. The traditional testing method has been challenging to meet its development requirements, and software automation testing has gradually become the main tool for testing aerospace control software. For the automation testing of aerospace control software, the core problem is to locate the GUI widgets on the software screenshots and identify their intent, which directly affects the accuracy of the test. Because of this, we use the widget recognition technology based on image matching and use the image understanding and analysis technology to extract the widget image in the screenshots. After obtaining the widget image, we use a convolutional neural network to extract image features and use the encoder module to encode the extracted information features as a tensor. The decoder module generates a word sequence conditional on tensor and previous output based on the encoded information. We also conduct an empirical study to evaluate the accuracy of widget recognition and intention generation. For widget recognition, our average IoU reached 0.81. For widget intent generation, our model BLEU-1 is 0.567, BLEU-2 is 0.356, BLEU-3 is 0.261, BLEU-4 is 0.131. The results show that our method is very effective.}, 
keywords={Software;Testing;Aerospace control;Graphical user interfaces;Feature extraction;Image edge detection;Convolutional neural networks;GUI widget detection;GUI widget intent generation;aerospace control software}, 
doi={10.1109/ACCESS.2021.3131753}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8942689, 
author={Yang, Yi-Chang and Jiang, Jehn-Ruey}, 
booktitle={2019 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)}, 
title={Web-based Machine Learning Modeling in a Cyber-Physical System Construction Assistant}, 
year={2019}, 
volume={}, 
number={}, 
pages={478-481}, 
abstract={The Cyber-Physical System (CPS) is critical for smart manufacturing of the Industry 4.0 vision. This study shows the design and implementation of machine learning modeling modules for a web-based CPS construction assistant, called PINE. The modules make easy the modeling of support vector classification (SVC), support vector regression (SVR), deep neural network (DNN), and convolutional neural network (CNN). They facilitate users to set modeling hyper-parameters and can generate source codes for the modeling. Examples are given to show how to use the modules to assist in training CNN models for an automated optical inspection (AOI) system.}, 
keywords={Training;Neural networks;Static VAr compensators;Support vector machine classification;Machine learning;Cyber-physical systems;Soldering;smart manufacturing;machine learning;support vector machine;deep neural network;convolutional neural network}, 
doi={10.1109/ECICE47484.2019.8942689}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7430116, 
author={Marroquin, Alberto and Gonzalez, Douglas and Maag, Stephane}, 
booktitle={2015 7th IEEE Latin-American Conference on Communications (LATINCOM)}, 
title={Testing distributed systems with test cases dependencies architecture}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In this work, we present a novel distributed testing architecture based on a formal definition of test cases dependencies to test the conformance of distributed systems in a black box context. Utilizing the European Telecommunication Standards Institute, Test Description Language standard, we apply our approach to a real Internet Multimedia Subsystem (IMS)/ SIP (Session Initiation Protocol) test bed and perform the tests through two use cases. This crucial activity belongs to the conformance testing context. Which aims at stimulating the communication system under test (SUT) to detect errors and unexpected behaviors with regards to the standards. When handling distributed systems, a major difficulty arises when testing these, due to the joint and linked stimulation of distributed entities. The main reason for it is the correlation of verdicts obtained from these entities.}, 
keywords={Testing;Protocols;Standards;Context;Synchronization;Correlation;Conformance Testing;Distributed Systems;Test Cases;SIP;TDL}, 
doi={10.1109/LATINCOM.2015.7430116}, 
ISSN={}, 
month={Nov},}
@INBOOK{7827467, 
author={Brooks, Tyson T.}, 
booktitle={Cyber-Assurance for the Internet of Things}, 
title={Cyber‐Assurance Through Embedded Security for the Internet of Things}, 
year={2017}, 
volume={}, 
number={}, 
pages={101-127}, 
abstract={The Internet of Things (IoT) comprises billions of Internet‐connected devices (ICD) or "things", each of which can sense, communicate, compute, and potentially actuate and can have intelligence, multimodal interfaces, physical/virtual identities, and attributes. Cyber‐assurance is the justified confidence that networked systems are adequately secure to meet operational needs, even in the presence of attacks, failures, accidents, and unexpected events. The cyber‐assurance recognition strategy is to define only the service‐level interfaces and leave out domain‐specific implementation details. Once the recognition of a cyber‐attack has been identified from the recognition process, the fortification process takes place. Reestablishment is a means to return the ICDs to its operational condition after the cyber‐attack through remapping to a different route since the ICD was under attack. When the IoT technologies are used as part of mission critical systems, the IoT services should be survivable in order to support the important missions.}, 
keywords={Embedded systems;Sensors;Protocols;Wireless sensor networks;Internet of Things;Authentication}, 
doi={10.1002/9781119193784.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781119193883}, 
url={https://ieeexplore.ieee.org/document/7827467},}
@INPROCEEDINGS{8544381, 
author={Ernadote, Dominique}, 
booktitle={2018 IEEE International Systems Engineering Symposium (ISSE)}, 
title={A Framework for Descriptive Models Quality Assessment}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Numerous systems engineering projects use descriptive models structured by standard metamodels such as SysML, UML, or NAF. Model-based systems engineering is now so well established in the world of systems engineering that there is no longer questions about the usefulness of such modeling activities. However, it is still difficult to determine what is the current level of quality and how far the models are from the expected project objectives. This paper proposes a framework which reduces the pain of elaborating a quality process aside of the main systems engineering tasks so that a project manager can quickly be aware of the modeling progress statuses. The proposed framework is based on a modeling method which is itself modeled so that to allow a predefined set of modeling artefacts self-learning the quality aspects, and generating the appropriate dashboards at different levels of details.}, 
keywords={Unified modeling language;Mathematical model;Quality assessment;Standards;Data models;Model Quality Assessment;MBSE;Model-Based Systems Engineering;Systems Engineering;Operations Research}, 
doi={10.1109/SysEng.2018.8544381}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8883668, 
author={Hăjmăȿan, Gheorghe and Mondoc, Alexandra and Creț, Octavian}, 
booktitle={2019 Conference on Next Generation Computing Applications (NextComp)}, 
title={Bytecode Heuristic Signatures for Detecting Malware Behavior}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={For a long time, the most important approach for detecting malicious applications was the use of static, hash-based signatures. This approach provides a fast response time, has a low performance overhead and is very stable due to its simplicity. However, with the rapid growth in the number of malware, as well as their increased complexity in terms of polymorphism and evasion, the era of reactive security solutions started to fade in favor of new, proactive approaches such as behavior based detection. We propose a novel approach that uses an interpreter virtual machine to run proactive behavior heuristics from bytecode signatures, thus combining the advantages of behavior based detection with those of signatures. Based on our approximation, using this approach we succeeded to reduce by 85% the time required to update a behavior based detection solution to detect new threats, while continuing to benefit from the versatility of behavior heuristics.}, 
keywords={Malware;Security;Prototypes;Instruction sets;Monitoring;Virtual machining;Computer languages;malware detection;behavior;bytecode;signature;heuristic;response time}, 
doi={10.1109/NEXTCOMP.2019.8883668}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{4382117, 
author={Ramanathan, S. and Lac, C.}, 
booktitle={2007 IEEE International Symposium on Consumer Electronics}, 
title={Use of fault tree analysis to improve residential gateway testing}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={A residential gateway, heart of the strategy of most Telcos, is a centralized intelligent device between the operator's access network and the home's network. It terminates all external access networks and enables residential services to be delivered to the consumer. Besides a plethora of useful services, the growth in market depends upon the reputation of its resilience (availability, reliability and security). This emphasizes a near zero fault design and efficient testing should be taken care before its launch into the market. This paper deals with the analysis of failures, both from test and field data, aiming to increase the efficiency of laboratory testing. Using fault tree analysis, we study the faults that have passed through the testing phase and created failures in the customer premises. With the help of defined specifications, we have identified the zones in which testing in the laboratory needs to be improved.}, 
keywords={Fault trees;Testing;Failure analysis;Laboratories;Heart;Intelligent networks;Home automation;Resilience;Availability;Data security}, 
doi={10.1109/ISCE.2007.4382117}, 
ISSN={2159-1423}, 
month={June},}
@INPROCEEDINGS{7338996, 
author={Borek, Marian and Stenzel, Kurt and Katkalov, Kuzman and Reif, Wolfgang}, 
booktitle={2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
title={Abstracting security-critical applications for model checking in a model-driven approach}, 
year={2015}, 
volume={}, 
number={}, 
pages={11-14}, 
abstract={Model checking at the design level makes it possible to find protocol flaws in security-critical applications automatically. But depending on the size of the application and especially on the abstraction of the application model, model checking may need a lot of resources, primarily time. To reduce the complexity, the application models are usually highly abstracted. But in a model-driven approach with automatic generation of runnable applications the application models need to be detailed and are often too complex to check in reasonable time. In this paper we describe an approach to handle this problem by using additional UML models to restrict the protocol runs, the attacker abilities and the numbers of participants. This makes model checking of large applications in our model-driven approach called SecureMDD possible without manual abstraction of the generated specifications. For model checking we use AVANTSSAR and show how the restrictions modeled within UML are translated. We demonstrate our approach with a smart card based electronic ticketing example.}, 
keywords={Unified modeling language;Model checking;Security;Protocols;Complexity theory;Manuals;Smart cards;UML;model checking;security-critical systems;model-driven development;transformations;SecureMDD}, 
doi={10.1109/ICSESS.2015.7338996}, 
ISSN={2327-0594}, 
month={Sep.},}
@INPROCEEDINGS{5541042, 
author={Yan Huan and Miao Changyun and Wu Zhigang}, 
booktitle={2010 International Conference On Computer Design and Applications}, 
title={The design of IP telephony media gateway which is based on soft-switching technology}, 
year={2010}, 
volume={5}, 
number={}, 
pages={V5-379-V5-381}, 
abstract={This paper presents a new type of design method for IP telephony gateway which is base on the soft-switch technology. It designs the hardware circuit with TMS320C5402 as its core and develops a telephone communication protocol which is based on UDP / IP. The Trunk Gateway supports MGCP protocol standards. Besides, it can fulfill lots of functions including audio processing, voice codec, and signaling tone generation and detection.}, 
keywords={Telephony;Protocols;Design methodology;Hardware;Circuits;Communication standards;Standards development;Codecs;Signal processing;Signal generators;soft-switching;IP phones gateways;Media Gateway Controller (MGC);Media Gateway (MG);signaling;protocol}, 
doi={10.1109/ICCDA.2010.5541042}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8088307, 
author={Gröning, Sven and Rosas, Christopher and Wietfeld, Christian}, 
booktitle={2017 IEEE International Systems Engineering Symposium (ISSE)}, 
title={Validating electric vehicle to grid communication systems based on model checking assisted test case generation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In last decades software development processes changed in order to address increasing complexity within decreasing implementation time. Hence, new practices like Kanban, Extreme Programming or Agile Software Development emerged. Model-based development is one potential option, which is more and more used to cope these new demands. However, adapting testing processes to the needs is still an open topic. This paper describes how model checking assisted test case generation can be used to integrate testing in new software development processes, focusing on a protocol implementation for electric vehicle charging communication as a case study. Therefore, it describes certain extensions made in the COMmunication Protocol vaLidation Toolchain COMFLgTg in order to enable test case generation in TTCN-3 core language using counterexamples of SPIN model checker.}, 
keywords={Unified modeling language;Protocols;Machine-to-machine communications;Model checking;Adaptation models;Grammar}, 
doi={10.1109/SysEng.2017.8088307}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{9951197, 
author={Riesebos, Leon and Brown, Kenneth R.}, 
booktitle={2022 IEEE International Conference on Quantum Computing and Engineering (QCE)}, 
title={Functional simulation of real-time quantum control software}, 
year={2022}, 
volume={}, 
number={}, 
pages={535-544}, 
abstract={Modern quantum computers rely heavily on real-time control systems for operation. Software for these systems is becoming increasingly more complex due to the demand for more features and more real-time devices to control. Unfortunately, testing real-time control software is often a complex process, and existing simulation software is not usable or practical for software testing. For this purpose, we implemented an interactive simulator that simulates signals at the application programming interface level. We show that our simulation infrastructure simulates kernels 6.9 times faster on average compared to execution on hardware, while the position of the timeline cursor is simulated with an average accuracy of 97.9% when choosing the appropriate configuration.}, 
keywords={Software testing;Quantum computing;Computational modeling;Process control;Real-time systems;Software;Device drivers;real-time control software;signal simulation;software testing;quantum computing}, 
doi={10.1109/QCE53715.2022.00076}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8502653, 
author={Nezhad, Amir Soltani and Lukkien, Johan J. and Mak, Rudolf. H.}, 
booktitle={2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Behavior-driven Development for Real-time Embedded Systems}, 
year={2018}, 
volume={1}, 
number={}, 
pages={59-66}, 
abstract={Embedded systems are a class of computer systems that are typically characterized by a tight interaction with the physical environment. Various methodologies have been adopted for the development of such systems, ranging from traditional waterfall to modern agile techniques. One of the agile techniques that has recently attracted increasing attention is Behavior-Driven Development (BDD). BDD promotes the engagement of all stakeholders in every development iteration to minimize the misunderstanding between technical and non-technical stakeholders and, consequently, to speed up the development process and lower the costs. In this paper, we investigate the application of BDD to the development of embedded systems, especially focusing on the testing of timing requirements for real-time embedded software. In particular, we extend BDD with time-related concepts and propose an approach to generate test code for the verification of timing behavior of real-time embedded systems. Our approach offers more automation for the development of test code compared to existing BDD tools, thus minimizing the risk of timing faults and reducing development costs and time-to-market.}, 
keywords={Logic gates;Testing;Timing;Embedded systems;Real-time systems;Stakeholders}, 
doi={10.1109/ETFA.2018.8502653}, 
ISSN={1946-0759}, 
month={Sep.},}
@INPROCEEDINGS{9589614, 
author={Käyrä, Matti and Hämäläinen, Timo D.}, 
booktitle={IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society}, 
title={A Survey on System-on-a-Chip Design Using Chisel HW Construction Language}, 
year={2021}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper presents a survey of functional programming languages in System-on-a-Chip (SoC) design. The motivation is improving the design productivity by better source code expressiveness, increased abstraction level in design entry, or improved automation. The survey focuses on Chisel that is one of the most potential High Level Language (HLL) based design frameworks. We include 26 papers that report implementations ranging from IP blocks to complete chips. The result is that functional programming languages are viable for SoC design and can also be deployed in production use. However, Chisel does not increase the abstraction level in a similar way as High Level Synthesis (HLS), since it is used to create circuit generators instead of direct descriptions. Additional benefit is that Chisel offloads user effort from control and connectivity structures, and makes reusability and configurability improved over traditional Hardware Description Language (HDL) designs.}, 
keywords={Productivity;Industrial electronics;Codes;Tools;Generators;System-on-chip;Functional programming;Survey;RTL Design;HDL;Chisel;FPGA;ASIC}, 
doi={10.1109/IECON48115.2021.9589614}, 
ISSN={2577-1647}, 
month={Oct},}
@INPROCEEDINGS{5501495, 
author={Chatterjee, Shreeshankar}, 
booktitle={2010 Seventh International Conference on Information Technology: New Generations}, 
title={Modeling, Debugging, and Tuning QoE Issues in Live Stream-Based Applications - A Case Study with VoIP}, 
year={2010}, 
volume={}, 
number={}, 
pages={1044-1050}, 
abstract={End-user acceptance criteria of live-stream based, interactive applications are different from traditional B2B or B2C applications. For example, if users sense disruptions in audio or video stream quality, they may quickly form a negative opinion. The Quality of Experience (QoE) in such live-stream applications is, thus, based on perception, and is open to subjective interpretations. QoE can be affected by hundreds of possible variables. QoE problems (QoE bugs), however, require an objective solution (fix in the product's code or tuning of product parameters). Understanding and debugging QoE bugs in such scenarios starts with designing relevant metrics and analysis tools. Thereafter, smart test-designs and strategies are required to gain insights into bottlenecks. This paper builds a discussion around these thoughts, and through a case study (sample QoE problem in a Voice over Internet Protocol (VoIP) application), collates some generic guidelines to investigate QoE bugs in live-streaming scenarios.}, 
keywords={Debugging;Streaming media;Computer bugs;Collaboration;Delay;Information technology;Videoconference;Product codes;Testing;Internet telephony;VOIP;QoE;QoS;Media Streaming;VOIP Metric;Modeling}, 
doi={10.1109/ITNG.2010.44}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{4839257, 
author={Dechev, Damian and Stroustrup, Bjarne}, 
booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems}, 
title={Model-Based Product-Oriented Certification}, 
year={2009}, 
volume={}, 
number={}, 
pages={295-304}, 
abstract={Future space missions such as the Mars Science Laboratory and Project Constellation suggest the engineering of some of the most complex man-rated software systems. The present process-oriented certification methodologies employed by NASA are becoming prohibitively expensive when applied to systems of such complexity. The process of software certification establishes the level of confidence in a software system in the context of its functional and safety requirements. Providing such certification evidence may require the application of a number of software development, analysis, and validation techniques. We define product-oriented certification as the process of measuring the system's reliability and efficiency based on the analysis of its design (expressed in models) and implementation (expressed in source code). In this work we introduce a framework for model-based product-oriented certification founded on the concept of source code enhancement and analysis. We describe a classification of the certification artifact types, the development and validation tools and techniques, the application domain-specific factors, and the levels of abstraction. We demonstrate the application of our certification platform by analyzing the process of model-based development of the parallel autonomic goals network, a critical component of the Jet Propulsion Laboratory's Mission Data System (MDS). We describe how we identify and satisfy seven critical certification artifacts in the process of model-driven development and validation of the MDS goal network. In the analysis of this process, we establish the relationship among the seven certification artifacts, the applied development and validation techniques and tools, and the level of abstraction of system design and development.}, 
keywords={Certification;Laboratories;Software systems;Software safety;Space missions;Mars;NASA;Application software;Programming;Reliability;product-oriented certification;nonblocking synchronization;semantic enhancement;concurrent real-time systems}, 
doi={10.1109/ECBS.2009.15}, 
ISSN={}, 
month={April},}
@ARTICLE{9869831, 
author={Sarjan, Hamed and Ameli, Amir and Ghafouri, Mohsen}, 
journal={IEEE Access}, 
title={Cyber-Security of Industrial Internet of Things in Electric Power Systems}, 
year={2022}, 
volume={10}, 
number={}, 
pages={92390-92409}, 
abstract={Electric Power Systems (EPSs) are among the most critical infrastructures of any society, since they significantly impact other infrastructures. Recently, there has been a trend toward implementing modern technologies, such as Industrial Internet of Things (IIoT), in EPSs to enhance their real-time monitoring, control, situational awareness, and intelligence. This movement, however, has exposed EPSs to various cyber intrusions that originate from the IIoT ecosystem. Statistics show that 38% of reported attacks have been against power and water infrastructure, and so far at least 91% of power utilities have experienced a cyber-attack. The cyber-security problem is even more severe for IIoT applications in EPSs due to the vulnerabilities and resource limitations of such applications. Thus, based on the above statistics, it is necessary to investigate the vulnerabilities of IIoT-based applications in EPSs, identify probable attacks and their consequences, and develop intrusion prevention and detection approaches to secure IIoT systems. On this basis, this paper first elaborates on the applications of IIoT-based systems in EPSs, and evaluates their security challenges. Afterwards, it comprehensively reviews various cyber-attacks against IIoT-assisted EPSs, with a particular focus on attack entry points and adversarial methods. Finally, efforts to prevent cyber-intrusions against IIoT systems in EPSs are explained, and different attack detection techniques are discussed.}, 
keywords={Industrial Internet of Things;Security;Ecosystems;Reliability;Real-time systems;Protocols;Power generation;Computer crime;Computer security;Intrusion detection;Cyber-attacks;cyber-security;electric power systems;industrial internet of things;intrusion detection systems}, 
doi={10.1109/ACCESS.2022.3202914}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{5770632, 
author={Navarro, Pedro Luis Mateo and Pérez, Gregorio Martínez and Ruiz, Diego Sevilla}, 
booktitle={2011 Fourth IEEE International Conference on Software Testing, Verification and Validation}, 
title={Towards Software Quality and User Satisfaction through User Interfaces}, 
year={2011}, 
volume={}, 
number={}, 
pages={415-418}, 
abstract={With this PhD we expect to provide the community and the industry with a solid basis for the development, integration, and deployment of software testing tools. As a solid basis we mean, on one hand, a set of guidelines, recommendations, and clues to better comprehend, analyze, and perform software testing processes, and on the other hand, a set of robust software frameworks that serve as a starting point for the development of future testing tools.}, 
keywords={Graphical user interfaces;Software testing;Usability;Open source software;Computer architecture;software testing;GUI testing;automatic test case generation;usability evaluation;user experience evaluation;GUI-data verification}, 
doi={10.1109/ICST.2011.13}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{6972271, 
author={Tokmakoff, Andrew and Sparrow, Ben and Turner, David and Lowe, Andrew}, 
booktitle={2014 IEEE 10th International Conference on e-Science}, 
title={AusPlots Rangelands Field Data Collection and Publication: Infrastructure for Ecological Monitoring}, 
year={2014}, 
volume={1}, 
number={}, 
pages={249-255}, 
abstract={The TERN AusPlots Rangelands field data collection system has been developed to facilitate simple and efficient data collection by ecologists operating in the Australian outback. The infrastructure provides tooling for 'clean' data collection on mobile (tablet) devices, associated data storage in a cloud-based server infrastructure, facilities for data curation and management and interfaces with the Australian Ecological Knowledge and Observation System (ÆKOS) data repository for long-term data management and semantic enrichment. In this paper, we introduce the AusPlots Rangelands field data collection solution, providing a systems-level view and motivating its development through the discussion of key functional requirements. We provide an outline of the ÆKOS data repository and demonstrate that the combined system provides a unique end-to-end data collection, curation, archiving and publishing mechanism for ecological data.}, 
keywords={Data collection;Vegetation;Databases;Servers;Soil;Protocols;Vegetation mapping;ecological data;mobile;data collection;data publishing}, 
doi={10.1109/eScience.2014.55}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7965258, 
author={Mazinanian, Davood and Tsantalis, Nikolaos}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
title={CSSDev: Refactoring Duplication in Cascading Style Sheets}, 
year={2017}, 
volume={}, 
number={}, 
pages={63-66}, 
abstract={Cascading Style Sheets (CSS) is a widely-used language for defining the presentation of structured documents and user interfaces. Despite its popularity, CSS still lacks adequate tool support for everyday maintenance tasks, such as debugging and refactoring. In this paper, we present CSSDEV, a tool suite for analyzing CSS code to detect refactoring opportunities.(https://youtu.be/lu3oITi1XrQ).}, 
keywords={Cascading style sheets;Tools;HTML;Browsers;Crawlers;Maintenance engineering;Runtime;Cascading Style Sheets;Preprocessors;Refactoring}, 
doi={10.1109/ICSE-C.2017.7}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7905631, 
author={Fourati, Farah and Bhiri, Mohamed Tahar and Robbana, Riadh}, 
booktitle={2016 5th International Conference on Multimedia Computing and Systems (ICMCS)}, 
title={Verification and validation of PDDL descriptions using Event-B formal method}, 
year={2016}, 
volume={}, 
number={}, 
pages={770-776}, 
abstract={The automatic planning community of Artificial Intelligence AI have developed a de facto standard language for PDDL, producing formal modeling of Planning problems. Equally it have conceived and produced tools called planners to automatically generate plans for PDDL descriptions. But the verification and validation of PDDL descriptions is little treated topic. In this paper, we shall treat this issue through the Event-B formal method. We illustrate the contribution of the static analysis tools associated with Event-B (provers, model checker, animator, and simulator) for verification and validation of PDDL descriptions.}, 
keywords={Planning;Context;Boats;Context modeling;Poles and towers;Syntactics;Electronic mail;Artificial Intelligence;Planning and Scheduling;PDDL;Verification and Validation;Event-B;Transformation;Static and Dynamic analysis}, 
doi={10.1109/ICMCS.2016.7905631}, 
ISSN={2472-7652}, 
month={Sep.},}
@INPROCEEDINGS{8071326, 
author={Mordvinov, Dmitry and Litvinov, Yurii and Bryksin, Timofey}, 
booktitle={2017 20th Conference of Open Innovations Association (FRUCT)}, 
title={TRIK studio: Technical introduction}, 
year={2017}, 
volume={}, 
number={}, 
pages={296-308}, 
abstract={This paper presents TRIK Studio - an environment for visual (and textual) programming of robotic kits, which is used in educational organizations across Russia and Europe. First part of the article provides overview of the system - its purpose, features, differences from similar programming environments, general difficulties of robot programming and solutions proposed by TRIK Studio. Second part presents implementation details of TRIK Studio and its most interesting components. This article combines five fields of study: robotics, domain-specific visual modeling, education, formal methods and methods of program analysis. Main contribution of this article is detailed technical description of TRIK Studio as complex and successful open-source cross-platform robot programming environment written in C++/Qt, and first part of the article can also be interesting for teachers as it provides an overview of existing robot programming tools and related problems.}, 
keywords={Visualization;Technological innovation;Semantics;Tools;Task analysis;Middleware;Pupils}, 
doi={10.23919/FRUCT.2017.8071326}, 
ISSN={2305-7254}, 
month={April},}
@ARTICLE{9950507, 
author={De Saqui-Sannes, Pierre and Vingerhoeds, Rob A. and Garion, Christophe and Thirioux, Xavier}, 
journal={IEEE Access}, 
title={A Taxonomy of MBSE Approaches by Languages, Tools and Methods}, 
year={2022}, 
volume={10}, 
number={}, 
pages={120936-120950}, 
abstract={Systems engineering has gained in maturity over the last decades and started a transition from document-centric approaches to Model-Based Systems Engineering (MBSE). Several papers have discussed the benefits and potential, but also the limitations, of using MBSE, based on literature surveys and analyze feedback from academia and industry. The current paper explores a complementary avenue and aims at giving students and industry practitioners a set of keys and decision criteria to select MBSE languages, tools and methods. Languages, tools and methods are categorised and selection criteria are proposed for a panorama of languages that goes beyond SysML and other techniques commonly associated with MBSE. In addition, research avenues for the future of MBSE are identified. The discussion relies on the authors’ experience in teaching and using system engineering and MBSE in both academia and industry, as well as on the experience shared within the framework of Concorde, a French project dedicated to drone systems design methodologies.}, 
keywords={Unified modeling language;Modeling;Analytical models;Systems engineering and theory;Taxonomy;Object oriented modeling;MBSE;formal methods;method;modeling tools;safety-critical systems;SysML;systems engineering}, 
doi={10.1109/ACCESS.2022.3222387}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8666862, 
author={Zhang, Hailong and Liu, Xuan and Zheng, Ke}, 
booktitle={2018 International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
title={Design of Broadband PLC Conformance Testing System Based on TTCN-3}, 
year={2018}, 
volume={}, 
number={}, 
pages={35-41}, 
abstract={Protocol conformance test plays a necessary and important role in guaranteeing that broadband PLC (Power Line Communication) devices to meet the standards and achieve interoperability. A broadband PLC Conformance Testing System is designed based on TTCN-3 framework. The implementation scheme for conformance testing platform is proposed and the module mechanisms as well as key techniques are described. Test suite are designed according to broadband PLC testing requirements, and finally test cases are executed based on testing environment constructed. The work is of significance for promoting the broadband PLC standardization and interoperability.}, 
keywords={Protocols;Broadband communication;Testing;Software;Hardware;Physical layer;Decoding;Broadband PLC;Conformance Test;TTCN-3}, 
doi={10.1109/ICISCAE.2018.8666862}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8109258, 
author={Duffau, Clément and Grabiec, Bartosz and Blay-Fornarino, Mireille}, 
booktitle={2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={Towards Embedded System Agile Development Challenging Verification, Validation and Accreditation: Application in a Healthcare Company}, 
year={2017}, 
volume={}, 
number={}, 
pages={82-85}, 
abstract={When Agile development meets critical embedded systems, verification, validation and accreditation activities are impacted. Challenges such as tests increase or accreditation documents production have to be managed in terms of time and resources. In this paper, we highlight these challenges and present a continuous integration ecosystem that aims to tackle these issues. We report on how this approach has been applied in a research and development healthcare company named AXONIC.}, 
keywords={Accreditation;Testing;Embedded systems;Hardware;Companies;Ecosystems;agile development;embedded systems;justification;VV&A;continuous integration}, 
doi={10.1109/ISSREW.2017.8}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7372061, 
author={Wölfl, Andreas and Siegmund, Norbert and Apel, Sven and Kosch, Harald and Krautlager, Johann and Weber-Urbina, Guillermo}, 
booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Generating Qualifiable Avionics Software: An Experience Report (E)}, 
year={2015}, 
volume={}, 
number={}, 
pages={726-736}, 
abstract={We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.}, 
keywords={Aerospace electronics;Helicopters;System software;Interviews;Hardware;Encoding}, 
doi={10.1109/ASE.2015.35}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7000095, 
author={Staron, Miroslaw and Rana, Rakesh and Meding, Wilhelm and Nilsson, Martin}, 
booktitle={2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement}, 
title={Consequences of Mispredictions of Software Reliability: A Model and its Industrial Evaluation}, 
year={2014}, 
volume={}, 
number={}, 
pages={157-162}, 
abstract={Predicting reliability of software under development is an important part of estimations in software engineering projects. In many organizations as the goal is that software products are released with no known defects, the process of finding and removing defects correlates with the effort for software projects. Software development projects estimate the resources needed to design, develop, test and release software products, and the number of defects which have to be handled. In this paper we present a model for consequence analysis of inaccurate predictions of quality in software projects. The model is a result of multiple case studies and is evaluated at two companies. The model recognizes the most common mispredictions - e.g. Over- and under-prediction, early- and late-predictions - and the combination of theses. The results from the industrial evaluation show that the consequences can be grouped according to under- and over-predictions and that the late- and early-predictions have the same consequences. The results show also that mispredicting the shape of the reliability curve has a significant consequence with regard to assessment of release readiness and resource planning.}, 
keywords={Software;Testing;Software reliability;Shape;Organizations;Predictive models;Software Reliability;SRGMs;Consequence;Mispredictions;Software;Forecasting}, 
doi={10.1109/IWSM.Mensura.2014.16}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6575271, 
author={Mulkey, Nick and Liu, Brian and Medda, Alessio}, 
booktitle={2013 8th International Conference on System of Systems Engineering}, 
title={The Integrated Blast Effects Sensor Suite: A rapidly developed, complex, system of systems}, 
year={2013}, 
volume={}, 
number={}, 
pages={224-228}, 
abstract={The need for rapid development of tactical system of systems solutions for military applications requires the use of system modeling techniques and simulation and validation methods to be applied throughout the lifecycle of the system. This combined approach of development and verification is preferred to traditional approaches for risk mitigation and cost effectiveness. This paper examines the Integrated Blast Effects Sensor Suite developed at the Georgia Tech Research Institute and its architecture as a complex system of systems.}, 
keywords={Vehicles;Systems engineering and theory;Computer architecture;Databases;Complexity theory;Explosives;Data collection;System of System;SoS lifecycle;Fast Development;High Complexity}, 
doi={10.1109/SYSoSE.2013.6575271}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{9226327, 
author={Haindl, Philipp and Plösch, Reinhold and Kömer, Christian}, 
booktitle={2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
title={An Operational Constraint Language To Evaluate Feature-Dependent Non-Functional Requirements}, 
year={2020}, 
volume={}, 
number={}, 
pages={34-42}, 
abstract={Features in a software system usually must satisfy different quality expectations, arising e.g., from their usage context or the long-term strategy of the manufacturer. As an example, the maintainability of the source code will likely be more important if the associated feature is frequently used by customers or if it has strategic value for the software manufacturer. Accordingly, features that process user-generated near real-time data will likely impose higher requirements towards performance efficiency than other features of the same application for maintaining the user profile. In order to practically approach these qualitative subtleties particularly in a DevOps context, we need an operational means to specify and automatically evaluate the fulfillment of these feature-dependent non-functional requirements, e.g., through quantitative constraints. However, the multitude of systems involved in DevOps and the heterogeneous data types of measures accruing on these systems hinder their effortless acquisition and automated evaluation.In this paper we present an operational constraint language for specifying and evaluating feature-dependent non-functional requirements quantitatively. Our language provides a compact set of time series operations, time filters, and comparison operators and allows to define metrical and ordinal threshold values. A comprehensive evaluation based on a large-scale software project with measures spanning the period over one year shows the performance and suitability of the approach for evaluating feature- dependent non-functional requirements specially in DevOps.}, 
keywords={Time measurement;Time series analysis;Instruments;Complexity theory;Benchmark testing;Time factors;Software measurement;Constraint Languages;Non-Functional Requirements;Constraint Evaluation;Operational Software Quality}, 
doi={10.1109/SEAA51224.2020.00017}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6115389, 
author={Cruz, Tiago and Simões, Paulo and Almeida, João and Rodrigues, João and Monteiro, Edmundo and Bastos, Fernando and Laranjeira, Alexandre}, 
booktitle={2011 IEEE 36th Conference on Local Computer Networks}, 
title={How to provision and manage off-the-shelf SIP phones in domestic and SOHO environments}, 
year={2011}, 
volume={}, 
number={}, 
pages={42-49}, 
abstract={Integrated services delivered over broadband connections are becoming the norm in domestic households, as it is the case with triple-play bundles which offer combined Voice, Television and Data services delivered using IP-based technologies and protocols. As a result, the usage of SIP-based (Session Initiation Protocol) VoIP devices has known a significant growth in domestic environments, either in the form of standalone (e.g. SIP telephones) or embedded devices (as it happens with some domestic gateways, which embed analog-to-SIP adaptors). For Internet Service Providers (ISPs), the provisioning and management of those devices is a challenge - especially standalone SIP phones, since most of them were exclusively designed for corporate LAN usage, not supporting adequate mechanisms for remote management over broadband access networks. In this paper we propose a framework which allows the integration of off-the-shelf SIP phones with the CWMP protocol suite, the prevailing standard for remote management of Customer Premises Devices (CPEs) in broadband access networks. This integration framework supports the vast majority of commercially available SIP phones whilst maintaining full compatibility with the original CWMP specification - thus allowing ISPs to reuse their CWMP management infrastructure to configure and provision off-the-shelf SIP telephones.}, 
keywords={Data models;Logic gates;Servers;Protocols;Broadband communication;Local area networks;Runtime;CWMP;VoIP;SIP;Home Networks}, 
doi={10.1109/LCN.2011.6115389}, 
ISSN={0742-1303}, 
month={Oct},}
@INPROCEEDINGS{4400351, 
author={Lengyel, Laszlo and Levendovszky, Tihamer and Mezei, Gergely and Vajk, Tamas and Charaf, Hassan}, 
booktitle={EUROCON 2007 - The International Conference on "Computer as a Tool"}, 
title={Practical Uses of Validated Model Transformation}, 
year={2007}, 
volume={}, 
number={}, 
pages={2200-2207}, 
abstract={Model-based approaches in development are widely recognized as a potential way of increasing productivity in software engineering. Model-based development is driven by model transformations that attempt to bridge the large semantic gaps between high-level models and low-level languages. There is a demand for researching the ways in which model transformation can become more flexible, efficient, highly-configurable as well as validated. This paper addresses issues of visually defined metamodel-based model transformations that support validated model transformations. We introduce our model transformation framework, visual modeling and transformation system (VMTS), and a list of applications realized with VMTS on metamodel-based model transformation basis. Furthermore, a comprehensive comparison is given related to other model transformation approaches.}, 
keywords={Algorithm design and analysis;Application software;Programming;Microwave integrated circuits;Productivity;Software engineering;System analysis and design;Automation;Informatics;Electronic mail;metamodel-based model transformation;graph rewriting;validated model transformation}, 
doi={10.1109/EURCON.2007.4400351}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6616325, 
author={Zech, Philipp and Felderer, Michael and Farwick, Matthias and Breu, Ruth}, 
booktitle={2013 IEEE Seventh International Conference on Software Security and Reliability Companion}, 
title={A Concept for Language-Oriented Security Testing}, 
year={2013}, 
volume={}, 
number={}, 
pages={53-62}, 
abstract={Today's ongoing trend towards intense usage of web service based applications in daily business and everybody's daily life poses new challenges for security testing. Additionally, such applications mostly not execute in their own runtime environment but instead are deployed in some data center, run alongside multiple other applications, and serve different purposes for sundry user domains with diverging security requirements. As a consequence, security testing also has to adapt to be able to meet the necessary requirements for each application in its domain and its specific security requirements. In addition, security testing needs to be feasible for both service providers and consumers. In our paper we identify drawbacks of existing security testing approaches and provide directions for meeting emerging challenges in future security testing approaches. We also introduce and describe the idea of language-oriented security testing, a novel testing approach building upon domain-specific languages and domain knowledge to meet future requirements in security testing.}, 
keywords={Testing;Security;Business;Cloud computing;Automation;Security Testing;Domainspecific Language;Languageoriented Programming;Servicecentric Systems}, 
doi={10.1109/SERE-C.2013.16}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{9488772, 
author={Shukla, Apoorv and Hudemann, Kevin and Vági, Zsolt and Hügerich, Lily and Smaragdakis, Georgios and Hecker, Artur and Schmid, Stefan and Feldmann, Anja}, 
booktitle={IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}, 
title={Fix with P6: Verifying Programmable Switches at Runtime}, 
year={2021}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={We design, develop, and evaluate P6, an automated approach to (a) detect, (b) localize, and (c) patch software bugs in P4 programs. Bugs are reported via a violation of pre-specified expected behavior that is captured by P6. P6 is based on machine learning-guided fuzzing that tests P4 switch non-intrusively, i.e., without modifying the P4 program for detecting runtime bugs. This enables an automated and real-time localization and patching of bugs. We used a P6 prototype to detect and patch existing bugs in various publicly available P4 application programs deployed on two different switch platforms: behavioral model (bmv2) and Tofino. Our evaluation shows that P6 significantly outperforms bug detection baselines while generating fewer packets and patches bugs in large P4 programs such as switch.p4 without triggering any regressions.}, 
keywords={Location awareness;Runtime;Automation;Conferences;Computer bugs;Prototypes;Switches}, 
doi={10.1109/INFOCOM42981.2021.9488772}, 
ISSN={2641-9874}, 
month={May},}
@INPROCEEDINGS{9212003, 
author={Meixner, Kristof and Kathrein, L. and Winkler, D. and Lüder, Arndt and Biffl, Stefan}, 
booktitle={2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Efficient Test Case Generation from Product and Process Model Properties and Preconditions}, 
year={2020}, 
volume={1}, 
number={}, 
pages={859-866}, 
abstract={In Cyber-Physical Production System (CPPS) engineering for discrete manufacturing, the definition of test cases is vital to ensure correct behavior of production processes and to test risky cases. Unfortunately, the definition of test cases requires know-how both from the CPPS engineering domain and on software test automation, and is time-consuming. In this paper, we investigate how domain experts can efficiently derive test cases for an assembly process step from process preconditions concerning product properties. We introduce the Test Case Derivation for PPR Models (TCD4PPR) method building on the Formalised Process Description and best practices from software testing. We evaluate the TCD4PPR method with an illustrative use case from industry in a feasibility study with domain experts at a large production systems engineering company for discrete manufacturing. The main result was that the domain experts found the TCD4PPR method efficient, usable, and useful. The evaluation results indicate that investing reasonable effort into modeling Product, Process, Resource (PPR) knowledge with preconditions can considerably reduce risks of untested production process behavior.}, 
keywords={Software testing;Industries;Production systems;Conferences;Buildings;Companies;Software;Production Systems Engineering;Model-based Testing;Software Testing;Equivalence Class}, 
doi={10.1109/ETFA46521.2020.9212003}, 
ISSN={1946-0759}, 
month={Sep.},}
@INPROCEEDINGS{8432020, 
author={Eberhardinger, Benedikt and Ponsar, Hella and Siegert, Gerald and Reif, Wolfgang}, 
booktitle={2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={Case Study: Adaptive Test Automation for Testing an Adaptive Hadoop Resource Manager}, 
year={2018}, 
volume={}, 
number={}, 
pages={513-518}, 
abstract={Coping with adaptive software systems is one of the key challenges testing is currently faced with. In our previous work, we proposed to enable the test system itself to be adaptive to the system under test as a solution. The adaptation is built up on the concepts of a self-aware test automation enabling to use this information to sequence, instantiate, or update the test suite to the current situation. In our test framework the modeling language S# allows to use a run-time model to do so in a model-based testing approach. In this paper, we demonstrate how our concepts of adaptive, self-aware test automation are applied to a real world scenario: testing an adaptive resource manager of Hadoop. We show the steps necessary to implement the approach and discuss our experiences in this case study paper.}, 
keywords={Adaptation models;Automation;Adaptive systems;Testing;Software systems;Yarn;Task analysis;Software Testing;Hadoop;Test Automation;Adaptive Testing;Adaptive Systems}, 
doi={10.1109/QRS-C.2018.00092}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7018469, 
author={Badreddin, Omar and Forward, Andrew and Lethbridge, Timothy C.}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={A test-driven approach for developing software languages}, 
year={2014}, 
volume={}, 
number={}, 
pages={225-234}, 
abstract={Test-Driven Development (TDD) is the practice of attempting to use the software you intend to write, before you write it. The premise is straightforward, but the specifics of applying it in different domains can be complex. In this paper, we provide aTDD approach for language development. The essence is to apply TDD at each of four levels of language processing, hence we call our approach Multi-Level TDD, or MLTDD. MLTDD can be applied to programming languages, preprocessors, domain specific languages, and transformation engines. MLTDD was used to build Umple, a model-oriented programming language available for Java, Ruby, and PHP. We present two case studies where this approach was implemented to develop two other domain specific languages.}, 
keywords={Testing;Syntactics;Semantics;Java;Generators;Unified modeling language;Software;Test Driven Development;Model Oriented Programming Language;UML}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{9222867, 
author={Bicevskis, Janis and Bicevska, Zane and Nikiforova, Anastasija and Oditis, Ivo}, 
booktitle={2020 15th Conference on Computer Science and Information Systems (FedCSIS)}, 
title={Data Quality Model-based Testing of Information Systems}, 
year={2020}, 
volume={}, 
number={}, 
pages={595-602}, 
abstract={This paper proposes a model-based testing approach by offering to use the data quality model (DQ-model) instead of the program’s control flow graph as a testing model. The DQ-model contains definitions and conditions for data objects to consider the data object as correct. The study proposes to automatically generate a complete test set (CTS) using a DQmodel that allows all data quality conditions to be tested, resulting in a full coverage of DQ-model. In addition, the possibility to check the conformity of the data to be entered and already stored in the database is ensured. The proposed alternative approach changes the testing process: (1) CTS can be generated prior to software development; (2) CTS contains not only input data, but also database content required for complete testing of the system; (3) CTS generation from DQ-model provides values against which the system can be further tested. If the test results correspond to the values obtained during CTS generation, the system under test shall be considered to have been tested according to DQ-model. Otherwise, the user can verify the cause of the differences that may occur due incorrect software, as well as an inaccurate specification.}, 
keywords={Systematics;Databases;Data integrity;Switches;Data models;Software;Software reliability;complete test set;data quality model;information system;model-based testing;symbolic execution.}, 
doi={10.15439/2020F25}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8116506, 
author={Jimenez, Ivo and Hamedian, Sina and Lofstead, Jay and Maltzahn, Carlos and Mohror, Kathryn and Arpaci-Dusseau, Remzi and Arpaci-Dusseau, Andrea and Ricci, Robert}, 
booktitle={2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
title={Demo abstract: PopperCI: Automated reproducibility validation}, 
year={2017}, 
volume={}, 
number={}, 
pages={952-953}, 
abstract={In this demo we illustrate the usage of PopperCI [1], a continous integration (CI) service for experiments hosted at UC Santa Cruz that allows researchers to automate the end-to-end execution and validation of experiments. PopperCI assumes that experiments follow Popper [2], a convention for implementing experiments and writing articles following a DevOps approach that has been proposed recently.}, 
keywords={Tools;Measurement;Runtime;Software;Conferences;Laboratories;Guidelines}, 
doi={10.1109/INFCOMW.2017.8116506}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4031279, 
author={Langlois, Benoit and Exertier, Daniel and Bonnet, Stephane}, 
booktitle={2006 10th IEEE International Enterprise Distributed Object Computing Conference Workshops (EDOCW'06)}, 
title={Performance Improvement of MDD Tools}, 
year={2006}, 
volume={}, 
number={}, 
pages={19-19}, 
abstract={From first to mature versions of Model-Driven Development (MDD) tools, there is a gap, as for any other software applications. All functional requirements must be met, including qualities of services, at the risk of seeing MDD tools rejected by users. In this paper, we focus on performance, especially for large-scale developments. After an overview of methodological elements, we give a list of reusable practices on performance. We conclude by a set of observations and stakes in order to understand where efforts must be applied during the development process.}, 
keywords={Model driven engineering;Software tools;Scalability;Unified modeling language;Aircraft;Bridges;Software performance;Application software;Quality of service;Large-scale systems}, 
doi={10.1109/EDOCW.2006.54}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{5533639, 
author={Piho, Gunnar and Tepandi, Jaak and Parman, Marko and Perkins, David}, 
booktitle={The 33rd International Convention MIPRO}, 
title={From archetypes-based domain model of clinical laboratory to LIMS software}, 
year={2010}, 
volume={}, 
number={}, 
pages={1179-1184}, 
abstract={We present our approach for developing a laboratory information management system (LIMS) software by combining Björners software triptych methodology (from domain models via requirements to software) with Arlow and Neustadt archetypes and archetype patterns based initiative. The fundamental hypothesis is that through this Archetypes Based Development (ABD) approach to domains, requirements and software, it is possible to improve the software development process as well as to develop more dependable software. We use ADB in developing LIMS software for the Clinical and Biomedical Proteomics Group (CBPG), University of Leeds.}, 
keywords={Laboratories;Abstracts;Programming;Production facilities;Software engineering;Proteomics;Information management;Software systems;Application software;Customer relationship management}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9594375, 
author={Darwesh, Darbaz Nawzad and Annighöfer, Björn and Lehmann, Matthias}, 
booktitle={2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)}, 
title={A qualification effort assessment framework for development processes of safety-critical system functions}, 
year={2021}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={This paper proposes an evaluation framework for quantifying the impact on the qualification effort using the Selective Middleware approach for safety-critical avionics functions. The qualification process of safety-critical avionics functions is troublesome and time-consuming. Most of the effort is spent on the qualification process to meet the qualification means or so-called objectives defined by recommended standards and guidelines. Therefore, tools or automation approaches are usually used to minimize the effort in the qualification process. In this research, the platform development approach called Selective Middleware (SMW) is investigated. It provides an efficient development approach for deploying a platform management middleware used for safety-critical avionics functions on IMA. The SMW approach provides a knowledge-based toolchain that automatically derives the platform management middleware instance from a manually designed high-level system architecture model. In addition to the platform management software, the required design and verification artifacts are also automatically generated. Therefore, this paper investigates to what extent the SMW approach reduces the qualification effort considering DO-178C, ARP4754A, and DO-297. To this object, a detailed assessment framework is defined in terms that captures and evaluates the impact on relevant effort reduction in each aircraft function life cycle process. The assessment framework is applied to the SMW approach concerning the guidelines, and a detailed and quantified analysis is performed.}, 
keywords={Knowledge based systems;Systems architecture;Aerospace electronics;Tools;Middleware;Task analysis;Aircraft;assessment framework;safety-critical avionics functions;model-based development;software development effort estimation;fault-tolerant;integrated modular avionics;qualification;certification}, 
doi={10.1109/DASC52595.2021.9594375}, 
ISSN={2155-7209}, 
month={Oct},}
@ARTICLE{5525316, 
author={Schavey, Todd and Duba, Shane}, 
journal={IEEE Aerospace and Electronic Systems Magazine}, 
title={Streamlining IMA integration through model-driven methodologies}, 
year={2010}, 
volume={25}, 
number={6}, 
pages={21-24}, 
abstract={Avionics systems integration is an inherently complex undertaking. In addition to ensuring that basic functionality is satisfied, the systems integrator must maximize the system's flexibility and reliability while minimizing weight and cost of change. With the introduction of integrated modular architectures (IMA) based on open standards, many traditional integration issues have been greatly improved. However, additional integration responsibilities arise due to having a large number of functions developed by independent suppliers all sharing the same physical resources.}, 
keywords={Aerospace electronics;Cost function}, 
doi={10.1109/MAES.2010.5525316}, 
ISSN={1557-959X}, 
month={June},}
@INPROCEEDINGS{8411751, 
author={Pröll, Reinhard and Bauer, Bernhard}, 
booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={A Model-Based Test Case Management Approach for Integrated Sets of Domain-Specific Models}, 
year={2018}, 
volume={}, 
number={}, 
pages={175-184}, 
abstract={Due to rapid improvements in the area of embedded processing hardware, the complexity of developed systems constantly increases. In order to ensure a high quality level of such systems, related quality assurance concepts have to evolve. The introduction of Model-Based Testing (MBT) approaches has shown promising results by automating and abstracting multiple activities of the software testing life cycle. Nevertheless, there is a strong need for approaches supporting scoped test models, i.e. subsets of test cases, reflecting specific test purposes driven by risk-oriented development strategies. Therefore, we developed an integrated and model-based approach supporting test case management, which incorporates the beneficial aspects of abstract development methodologies with predominant research for test case management in non-model-based scenarios. Based on a new model artifact, the integration model, tasks like cross-domain information mapping and the integration of domain-specific KPIs derived by analyses favor the subsequently applied constraint-based mechanism for test case management. Further, a prototypical implementation of these concepts within the Architecture And Analysis Framework (A3F) is elaborated and further evaluated based on representative application scenarios. A comparative view on related work leads to a conclusive statement regarding our future work.}, 
keywords={Analytical models;Data models;Context modeling;Software;Software testing;Model-Based Testing;Test Case Management;Test Selection;Test Prioritization;Test Suite Reduction;Test Model Scoping}, 
doi={10.1109/ICSTW.2018.00048}, 
ISSN={}, 
month={April},}
@ARTICLE{531663, 
author={}, 
journal={IEEE Std 743-1995}, 
title={IEEE Standard Equipment Requirements and Measurement Techniques for Analog Transmission Parameters for Telecommunications}, 
year={1996}, 
volume={}, 
number={}, 
pages={i-}, 
abstract={Performance requirements for test equipment that measures the analog transmission parameters of subscriber loops, message trunks, PBX trunks, and ties lines are specified. Requirements for these measurements with DS1 bit stream access are also provided. The measurement of loss, noise, and impulse noise on non-loaded cable pairs used for digital subscriber lines is also addressed.}, 
keywords={Communication standards}, 
doi={10.1109/IEEESTD.1996.81076}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{7081882, 
author={Laverdière, Marc-André and Berger, Bernhard J. and Merloz, Ettore}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Taint analysis of manual service compositions using Cross-Application Call Graphs}, 
year={2015}, 
volume={}, 
number={}, 
pages={585-589}, 
abstract={We propose an extension over the traditional call graph to incorporate edges representing control flow between web services, named the Cross-Application Call Graph (CACG). We introduce a construction algorithm for applications built on the Jax-WS standard and validate its effectiveness on sample applications from Apache CXF and JBossWS. Then, we demonstrate its applicability for taint analysis over a sample application of our making. Our CACG construction algorithm accurately identifies service call targets 81.07% of the time on average. Our taint analysis obtains a F-Measure of 95.60% over a benchmark. The use of a CACG, compared to a naive approach, improves the F-Measure of a taint analysis from 66.67% to 100.00% for our sample application.}, 
keywords={Web services;Benchmark testing;Java;Security;Manuals;Algorithm design and analysis;Androids}, 
doi={10.1109/SANER.2015.7081882}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{9557519, 
author={Hielscher, Leon and Bloeck, Alexander and Viehl, Alexander and Reiter, Sebastian and Staiger, Marc and Bringmann, Oliver}, 
booktitle={2021 IEEE 19th International Conference on Industrial Informatics (INDIN)}, 
title={Platform Generation for Edge AI Devices with Custom Hardware Accelerators}, 
year={2021}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={In recent years artificial neural networks (NNs) have been at the center of research on data processing. However, their high computational demand often prohibits deployment on resource-constrained Industrial IoT Systems. Custom hardware accelerators can enable real-time NN processing on small-scale edge devices but are generally hard to develop and integrate. In this paper we present a hardware generation approach to rapidly create, test, and deploy entire SoC platforms with application-specific NN hardware accelerators. The feasibility of the approach is demonstrated by the generation of a condition monitoring system for high-speed valves.}, 
keywords={Condition monitoring;Conferences;Artificial neural networks;Valves;Data processing;Real-time systems;Informatics;Hardware Acceleration;Hardware Generation;Neural Networks;Edge Devices}, 
doi={10.1109/INDIN45523.2021.9557519}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{9680292, 
author={Paduraru, Ciprian and Paduraru, Miruna and Stefanescu, Alin}, 
booktitle={2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW)}, 
title={Automated game testing using computer vision methods}, 
year={2021}, 
volume={}, 
number={}, 
pages={65-72}, 
abstract={Video game development is a growing industry nowadays with high revenues. However, even if there are many resources invested in the software development process, many games still contain bugs or performance issues that affect the user experience. This paper presents ideas on how computer vision methods can be used to automate the process of game testing. The goal is to replace the parts of the testing process that require human users (testers) with machines as much as possible, in order to reduce costs and perform more tests in less time by scaling with hardware resources. The focus is on solving existing real-world problems that have emerged from several discussions with industry partners. We base our methods on previous work in this area using intelligent agents playing video games and deep learning methods that interpret feedback from their actions based on visual output. The paper proposes several methods and a set of open-source tools, independent of the operating system or deployment platform, to evaluate the efficiency of the presented methods.}, 
keywords={Industries;Computer vision;Visualization;Operating systems;Games;User experience;Intelligent agents;AI agents;game testing;automated testing;deep learning;reinforcement learning;software architecture}, 
doi={10.1109/ASEW52652.2021.00024}, 
ISSN={2151-0830}, 
month={Nov},}
@INPROCEEDINGS{9340228, 
author={Nikiforova, Anastasija and Bicevskis, Janis and Bicevska, Zane and Oditis, Ivo}, 
booktitle={2020 7th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)}, 
title={Data Quality Model-based Testing of Information Systems: the Use-case of E-scooters}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The paper proposes a data quality model-based testing methodology aimed at improving testing methodology of information systems (IS) using previously proposed data quality model. The solution supposes creation of a description of the data to be processed by IS and the data quality requirements used for the development of the tests, followed by performing an automated test of the system on the generated tests verifying the correctness of data to be entered and stored in the database. The generation of tests for all possible data quality conditions creates a complete set of tests that verify the operation of the IS under all possible data quality conditions. The proposed solution is demonstrated by the real example of the system dealing with e-scooters. Although the proposed solution is demonstrated by applying it to the system that is already in use, it can also be used when developing a new system.}, 
keywords={Databases;Data integrity;Syntactics;Data models;Software;Testing;Information systems;complete test set;data quality model;e-scooters;Internet of Things;IoT;Internet of Vehicles;model-based testing;symbolic execution}, 
doi={10.1109/IOTSMS52051.2020.9340228}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6595796, 
author={Diepenbeck, Melanie and Soeken, Mathias and Grobe, Daniel and Drechsler, Rolf}, 
booktitle={2013 8th International Workshop on Automation of Software Test (AST)}, 
title={Towards automatic scenario generation from coverage information}, 
year={2013}, 
volume={}, 
number={}, 
pages={82-88}, 
abstract={Nowadays, the design of software systems is pushed towards agile development practices. One of its most fundamental approaches is Test Driven Development (TDD). This procedure is based on test cases which are incrementally written prior to the implementation. Recently, Behavior Driven Development (BDD) has been introduced as an extension of TDD, in which natural language scenarios are the starting point for the test cases. This description offers a ubiquitous communication mean for both the software developers and stakeholders. Following the BDD methodology thoroughly, one would expect 100 % code coverage, since code is only written to make the test cases pass. However, as we show in an empirical study this expectation is not valid in practice. It becomes even worse in the process of development, i.e. the coverage decreases over time. To close the coverage gap, we sketch an algorithm that generates BDD-style scenarios based on uncovered code.}, 
keywords={Data structures;Boolean functions;Natural languages;Software;Testing;Unified modeling language;Context}, 
doi={10.1109/IWAST.2013.6595796}, 
ISSN={}, 
month={May},}
@ARTICLE{9324982, 
author={Laibinis, Linas and Iliasov, Alexei and Romanovsky, Alexander}, 
journal={IEEE Transactions on Reliability}, 
title={Mutation Testing for Rule-Based Verification of Railway Signaling Data}, 
year={2021}, 
volume={70}, 
number={2}, 
pages={676-691}, 
abstract={Industry applications of formal verification to signaling control tables require formulation of a large number of mathematical conjectures expressing verification rules. It is paramount to establish the validity and completeness of these conjectures. This article discusses a mutation-based validation technique that guides domain experts in the construction of such verification rules. Furthermore, we use genetic programming to quickly generate millions of well-formed data mutations of control tables and to synthesize mutation programs. The technique is illustrated by a synthetic running example and a discussion of our experience in using it in the industrial setting.}, 
keywords={Rail transportation;Safety;Layout;Testing;Semantics;Junctions;Topology;Formal verification;mutation testing;railway;signaling;verification conditions}, 
doi={10.1109/TR.2020.3047462}, 
ISSN={1558-1721}, 
month={June},}
@INPROCEEDINGS{776011, 
author={Hodge, Y. and Bajpay, P. and Chao, C.-W. and Grammer, G. and Kan, H. and Nadle, D.}, 
booktitle={IEEE GLOBECOM 1998 (Cat. NO. 98CH36250)}, 
title={AT&T service maintenance platform for next century}, 
year={1998}, 
volume={6}, 
number={}, 
pages={3757-3762 vol.6}, 
abstract={With rapid deployment of new services and increasing competitive pressure have come new challenges in the telecommunications management arena. This paper presents an evolved service maintenance platform intended to streamline, simplify and automate network management operations. A unified Business Maintenance Platform (BMP) for AT&T voice and data services is a key enabler for supporting AT&T continuous commitment to quality of service (QoS). The BMP is critical to the seamless and cost effective integration of voice, data and frame relay services and provides a flexible platform to encompass local, ATM, wireless services and new services in the future.}, 
keywords={Quality of service;Costs;Frame relay;Asynchronous transfer mode;Customer service;SONET;Availability;Chaos;Laboratories;Pressing}, 
doi={10.1109/GLOCOM.1998.776011}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7338253, 
author={Rodriguez-Echeverria, Roberto and Macias, Fernando}, 
booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
title={A statistical analysis approach to assist model transformation evolution}, 
year={2015}, 
volume={}, 
number={}, 
pages={226-235}, 
abstract={Model Driven Engineering (MDE) is essentially based in metamodel definition, model edition and the specification of model transformations (MT) among these. In many cases the development, evolution and adaptation of these transformations is still carried out without the support of proper methods and tools to reduce the effort and related costs to these activities. In this work, a novel model testing approach specifically designed to assist the engineer in model transformation evolution is presented. A statistical analysis of the actual behavior of the transformations is performed by means of the computation of well-known information extraction metrics. In order to assist the MT adaptation, a detailed interpretation of the possible results of those metrics is also presented. And finally, the results of applying this approach on a Model-Driven Reverse Engineering (MDRE) scenario defined in the context of the MIGRARIA project are discussed.}, 
keywords={Adaptation models;Concrete;Contracts;Testing;Context modeling;Measurement;Unified modeling language;Model Transformation;Model Transformation Evolution;Model Transformation Testing;Testing Oracle}, 
doi={10.1109/MODELS.2015.7338253}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{9606984, 
author={Han, Zhao and Wang, Deyan and Rutsch, Gabriel and Li, Bowen and Prebeck, Sebastian Siegfried and Lopera, Daniela Sanchez and Devarajegowda, Keerthikumara and Ecker, Wolfgang}, 
booktitle={2021 IFIP/IEEE 29th International Conference on Very Large Scale Integration (VLSI-SoC)}, 
title={Aspect-Oriented Design Automation with Model Transformation}, 
year={2021}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Despite the high configurability of IPs and hardware generators, code modifications are still required to introduce aspect-oriented instrumentation to satisfy emerging design requirements such as on-chip debug and functional safety. These code modifications lead to escalated development, verification efforts and deteriorate the code reuse. This paper proposes a highly efficient aspect-oriented design automation approach that leverages graph-grammar-based model transformations. With the proposed approach, main design functionalities and aspect-oriented instrumentation are separately developed, automatically integrated and verified. To demonstrate the applicability, industrial SoCs were transformed to support on-chip debug. Experimental results confirm the efficiency of the approach. Further, reduced code is needed with the proposed automation approach, which also replaces the error-prone manual RTL coding. Finally, the transformation scripts are applicable to different SoCs, which promotes the overall code reuse.}, 
keywords={Codes;Design automation;Automation;Instruments;Manuals;Very large scale integration;Hardware;Electronic Design Automation;Aspect-Oriented Programming;Model-Driven Architecture}, 
doi={10.1109/VLSI-SoC53125.2021.9606984}, 
ISSN={2324-8440}, 
month={Oct},}
@INPROCEEDINGS{9183509, 
author={Schiewe, Alexander and Ruck, Andreas and Duffley, Gordon and Butson, Christopher R. and Krüger, Jens}, 
booktitle={2017 IEEE 10th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS)}, 
title={VisAnalyticsKit: User Logging for Mobile Visualization Applications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={This paper introduces a full-stack self-hosted user logging system for mobile devices, available to the open-source community. It excels through minimal integration and administration efforts for developers or researchers who use our system. VisAnalyticsKit features a W3C conform provenance data model and two-way data replication, rarely found among other analytics services. It facilitates pushing data from mobile devices to back-end servers and pulling data from data centers to the users' or analysts' devices. This allows replaying and reviewing previously captured log sessions in the sense of What You See Now, Is What I Saw Then (WYSNIWIST). Further, we evaluate our system by instrumenting a mobile visualization application, publicly available on the App Store.}, 
keywords={Servers;Open source software;Mobile handsets;Databases;Data visualization;Tools;W3C;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/methodology;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction techniques, D.1.m [Programming Techniques]: Miscellaneous—;K.6.2 [Management of Computing and Information Systems]: Installation Management—Performance and usage measurement}, 
doi={10.1109/SEARIS41720.2017.9183509}, 
ISSN={2328-7829}, 
month={March},}
@INPROCEEDINGS{6229805, 
author={Schiller, Todd W. and Lucia, Brandon}, 
booktitle={2012 Second International Workshop on Developing Tools as Plug-Ins (TOPI)}, 
title={Playing cupid: The IDE as a matchmaker for plug-ins}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={We describe a composable, data-driven, plug-in ecosystem for IDEs. Inspired by Unix's and Windows Power-Shell's pipeline communication models, each plug-in declares data-driven capabilities. Developers can then seamlessly mix, match, and combine plug-in capabilities to produce new insight, without modifying the plug-ins. We formalize the architecture using the polymorphic lambda calculus, with special types for source and source locations; the type system prevents nonsensical plug-in combinations, and helps to inform the design of new tools and plug-ins. To illustrate the power of the formalism, we describe several synergies between existing plug-ins (and tools) made possible by the ecosystem.}, 
keywords={Java;Pipelines;Biological system modeling;Contracts;Cloning;Ecosystems;Debugging}, 
doi={10.1109/TOPI.2012.6229805}, 
ISSN={2327-0772}, 
month={June},}
@ARTICLE{6772080, 
author={Aprille, T. J. and Gupta, D. V. and St. Amand, P. G.}, 
journal={The Bell System Technical Journal}, 
title={D4 Digital Channel Bank Family: Dataport — Channel units for digital data system subrates}, 
year={1982}, 
volume={61}, 
number={9}, 
pages={2721-2740}, 
abstract={The single-channel dataports are a series of D4 channel units that convert the digital signal derived from one T-facility time slot by the D4 common circuits to an appropriate format at speeds of 64, 9.6, 4.8, or 2.4 kb/s for use in the Digital Data System (DDS). They come in two formats, the first being the DDS bipolar format for 64 kb/s and the second, for the remaining three speeds, being an EIA RS-449 format. Their error-correction feature ensures 10−8 error-rate performance for a 10−3 error-rate transmission channel. Advances in large-scale integration (LSI) technology have allowed the packaging of all the digital circuit functions needed into the space of a single channel unit. An on-board power converter unit generates the additional current required by the dataports over that needed by regular analog channel units. The local loop side of each channel unit uses integrated technology to achieve signal equalization and timing recovery. Standard DDS remote maintenance features are provided. The dataport channel units are easily installed and removed; they supply economical digital transmission.}, 
keywords={}, 
doi={10.1002/j.1538-7305.1982.tb03449.x}, 
ISSN={0005-8580}, 
month={Nov},}
@INPROCEEDINGS{8972261, 
author={Otte, Marcel and Rohjans, Sebastian and Andrén, Filip Pröstl and Strasser, Thomas I.}, 
booktitle={2019 IEEE 17th International Conference on Industrial Informatics (INDIN)}, 
title={Applying Machine Learning Concepts to Enhance the Smart Grid Engineering Process}, 
year={2019}, 
volume={1}, 
number={}, 
pages={1687-1693}, 
abstract={The expansion of renewable energy sources, as an effort to reduce global warming and to guarantee a sustainable energy supply, forces the electrical energy systems into enhanced complexity through new requirements, actors, technological approaches or business models. This complexity is also noticed in the smart grid engineering process, resulting in increasing effort and costs. By applying machine learning concepts on the engineering process it is possible to decrease the work-effort and minimize tedious and error prone manual tasks. This work introduces three machine learning concepts and shows how they can improve the smart grid engineering process by applying a clustering approach to give recommendations of standards that are useful for the developed use case. According to their implementation-feasibility an evaluation based on the state-of-the-art is pursued. Furthermore, a tool prototype indicates current and future application possibilities of machine learning in the smart grid engineering process.}, 
keywords={Renewable energy sources;Prototypes;Machine learning;Manuals;Tools;Smart grids;Complexity theory;Engineering process;machine Learning;smart grid;standardization;support systems}, 
doi={10.1109/INDIN41052.2019.8972261}, 
ISSN={2378-363X}, 
month={July},}
@INBOOK{5396736, 
author={Xiu, Liming}, 
booktitle={VLSI Circuit Design Methodology Demystified: A Conceptual Taxonomy}, 
title={CellBased ASIC Design Methodology}, 
year={2008}, 
volume={}, 
number={}, 
pages={73-188}, 
abstract={This chapter contains sections titled: What are the major tasks and personnel required in a chip design project? What are the major steps in ASIC chip construction? What is the ASIC design flow? What are the two major aspects of ASIC design flow? What are the characteristics of good design flow? What is the role of market research in an ASIC project? What is the optimal solution of an ASIC project? What is system-level study of a project? What are the approaches for verifying design at the system level? What is register-transfer-level (RTL) system-level description? What are methods of verifying design at the register-transfer-level? What is a test bench? What is code coverage? What is functional coverage? What is bug rate convergence? What is design planning? What are hard macro and soft macro? What is hardware description language (HDL)? What is register-transfer-level (RTL) description of hardware? What is standard cell? What are the differences among standard cell, gate-array, and sea-of-gate approaches? What is an ASIC library? What is logic synthesis? What are the optimization targets of logic synthesis? What is schematic or netlist? What is the gate count of a design? What is the purpose of test insertion during logic synthesis? What is the most commonly used model in VLSI circuit testing? What are controllability and observability in a digital circuit? What is a testable circuit? What is the aim of scan insertion? What is fault coverage? What is defect part per million (DPPM)? Why is design for testability important for a product's financial success? What is chip power usage analysis? What are the major components of CMOS power consumption? What is power optimization? What is VLSI physical design? What are the problems that make VLSI physical design so challenging? What is floorplanning? What is the placement process? What is the routing process? What is a power network? What is clock distribution? What are the key requirements for constructing a clock tree? What is the difference between time skew and length skew in a clock tree? What is scan chain? What is scan chain reordering? What is parasitic extraction? What is delay calculation? What is back annotation? What kind of signal integrity problems do place and route tools handle? What is cross-talk delay? What is cross-talk noise? What is IR drop? What are the major netlist formats for design representation? What is gate-level logic verification before tapeout? What is equivalence check? What is timing verification? What is design constraint? What is static timing analysis (STA)? What is simulation approach on timing verification? What is the logical-effort-based timing closure approach? What is physical verification? What are design rule check (DRC), design verification (DV), and geometry verification (GV)? What is schematic verification (SV) or layout versus schematic (LVS)? What is automatic test pattern generation (ATPG)? What is tapeout? What is yield? What are the qualities of a good IC implementation designer? }, 
keywords={}, 
doi={10.1002/9780470199114.ch4}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470199107}, 
url={https://ieeexplore.ieee.org/document/5396736},}
@INPROCEEDINGS{1607414, 
author={White, J. and Schmidt, D.C.}, 
booktitle={13th Annual IEEE International Symposium and Workshop on Engineering of Computer-Based Systems (ECBS'06)}, 
title={FireAnt: a tool for reducing enterprise product line architecture deployment, configuration, and testing costs}, 
year={2006}, 
volume={}, 
number={}, 
pages={2 pp.-508}, 
abstract={Product-line architectures (PLA)s are a paradigm for developing software families by customizing and composing reusable artifacts, rather than handcrafting software from scratch. Extensive testing is required to develop reliable PLAs. Each PLA may have hundreds of valid variants that can be constructed from the architecture's components. It is crucial that each of these variants be thoroughly tested to ensure the quality of these applications on multiple OS platforms and hardware configurations. Setting up test environments and running tests can become extremely complex and expensive as the number of variants and the complexity of their deployment and configuration increases. Once a variant is deemed ready for deployment and configuration in a production environment, it is crucial that these activities be done identically to the tested configurations and upholds the assumptions of the component developers. Rapidly setting up numerous distributed test environments and ensuring that they are deployed and configured correctly is hard. This poster paper presents FireAnt, which is a tool for the model-driven development (MDD) of PLA deployment plans}, 
keywords={Costs;Programmable logic arrays;Computer architecture;Software reusability;Software testing;Application software;Hardware;Production;Software packages;Packaging}, 
doi={10.1109/ECBS.2006.43}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{781322, 
author={Harbison, S.P.}, 
booktitle={Proceedings 1999 Design Automation Conference (Cat. No. 99CH36361)}, 
title={System-level hardware/software trade-offs}, 
year={1999}, 
volume={}, 
number={}, 
pages={258-259}, 
abstract={Operating systems and development tools can impose overly general requirements that prevent an embedded system from achieving its hardware performance entitlement. It is time for embedded processor designers to become more involved with system software and tools.}, 
keywords={Hardware;Software systems;System software;Operating systems;Computer architecture;Systolic arrays;Assembly;Real time systems;Embedded software;Digital signal processing}, 
doi={10.1109/DAC.1999.781322}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6614743, 
author={Dorn, Christoph and Egyed, Alexander}, 
booktitle={2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE)}, 
title={Towards collaboration-centric pattern-based software development support}, 
year={2013}, 
volume={}, 
number={}, 
pages={109-112}, 
abstract={Software engineering activities tend to be loosely coupled to allow for flexibly reacting to unforeseen development complexity, requirements changes, and progress delays. This flexibility comes a the price of hidden dependencies among design and code artifacts that make it difficult or even impossible to assess change impact. Incorrect change propagation subsequently results in costly errors. This position paper proposes a novel approach based on monitoring engineering activities for subsequent high-level pattern detection. Patterns of (i) collaboration structures, (ii) temporal action sequences, and (iii) artifact consistency constraints serve as input to recommendation and automatic reconfiguration algorithms for ultimately avoiding and correcting artifact inconsistencies.}, 
keywords={Software;Collaboration;Computer architecture;Unified modeling language;Uncertainty;Adaptation models;Software engineering;monitoring;pattern detection;software engineering;recommendation;collaboration structures}, 
doi={10.1109/CHASE.2013.6614743}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9449436, 
author={Lisboa, Luciano A. C.}, 
booktitle={15th International Conference on Developments in Power System Protection (DPSP 2020)}, 
title={Formal methods to power-system automation based on Petri Nets}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In this work, after surveying the current available methods and formats for specification of functional requirements of PAC systems used by the industry and analysing their advantages and disadvantages, a formal method approach based on Petri Nets is proposed to power-system automation.}, 
keywords={Automation;Formal languages;Petri Nets;Power transmission control;Substations}, 
doi={10.1049/cp.2020.0145}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{4293629, 
author={Kitiyakara, Narti and Graves, Joseph}, 
booktitle={Agile 2007 (AGILE 2007)}, 
title={Growing a Build Management System from Seed}, 
year={2007}, 
volume={}, 
number={}, 
pages={401-407}, 
abstract={This paper describes the authors' experiences creating a full build management system from a simple version control system. We will explore how the XP values of simplicity, feedback, communication, courage and respect play into making a system that provides the developers, testers and customer with excellent value showing how various XP principles (like baby steps and mutual benefit) come into play. We will also demonstrate how we are able to remain true to our XP values while still achieving ISO 9001- 2001 and CMM Level II certifications. Finally, we compare the build management system with other systems we have encountered that were not developed in accordance with XP values.}, 
keywords={System testing;Communication system control;Control systems;Feedback;Pediatrics;ISO standards;Coordinate measuring machines;Certification;Control system synthesis;Writing}, 
doi={10.1109/AGILE.2007.32}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7601515, 
author={Sroka, Michal and Fisch, Dominik and Nagy, Roman}, 
booktitle={2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)}, 
title={Impact of crossover and mutation on reproduction in evolutionary test model learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={39-44}, 
abstract={Automation in the software test design process has a significant impact on the software testing process and therefore also on the overall software development in the industry. The focus of this paper is on the automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is briefly described. This paper further investigates the impact of reproduction configuration on the evolutionary learning method.}, 
keywords={Biological cells;Sociology;Statistics;Software;Testing;Software algorithms;Evolutionary computation}, 
doi={10.1109/SISY.2016.7601515}, 
ISSN={1949-0488}, 
month={Aug},}
@INBOOK{5732852, 
author={Etemad, Kamran and Lai, Ming-Yee}, 
booktitle={WiMAX Technology and Network Evolution}, 
title={Overview of WiMAX Network Architecture and Evolution}, 
year={2010}, 
volume={}, 
number={}, 
pages={147-177}, 
abstract={This chapter contains sections titled: Introduction WiMAX Basic Network Reference Model WiMAX Network Roadmap: Release 1.0, 1.5, 1.6, and 2.0 Overview of Major Features in Release 1.0 Overview of Major Features in Release 1.5 Major Features in Network Release 1.6 Comparison of Mobile WiMAX and 3GPP/SAE Network Architecture Summary }, 
keywords={WiMAX;network architecture;service provider's working group;network working group;residential gateways}, 
doi={10.1002/9780470633021.ch6}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470633014}, 
url={https://ieeexplore.ieee.org/document/5732852},}
@INBOOK{8686312, 
author={Janevski, Toni}, 
booktitle={QoS for Fixed and Mobile Ultra-Broadband}, 
title={Broadband QoS Parameters, KPIs, and Measurements}, 
year={2019}, 
volume={}, 
number={}, 
pages={221-259}, 
abstract={Broadband and ultra‐broadband access networks provide capabilities for delivery of various services toward end‐users, which can have different quality of service (QoS) and quality of experience (QoE) requirements. The relationship between network layer QoS and the obtained QoE is strongly dependent on the given service and its application. The QoS parameters are defined to be used by service and network providers, to manage and improve the offering of their services. The main purpose of QoS parameters in an operational network is to be measured and then compared with reference values. There are two types of QoS parameters regarding measurement methods: objective QoS parameters and subjective QoS parameters. This chapter defines several important QoS parameters for time‐division multiplexing‐based points of interconnection, and their possible thresholds values. Such key performance indicators should be measurable and realistic to ensure effective interconnection regulation and mitigation of eventual disputes.}, 
keywords={Quality of service;Streaming media;Delays;Internet of Things;Broadband communication;Real-time systems}, 
doi={10.1002/9781119470519.ch7}, 
ISSN={}, 
publisher={Wiley}, 
isbn={9781119470496}, 
url={https://ieeexplore.ieee.org/document/8686312},}
@INBOOK{5238155, 
author={Sherif, Mostafa Hashem}, 
booktitle={Managing Projects in Telecommunication Services}, 
title={Standards and Innovation in Telecommunication Services}, 
year={2006}, 
volume={}, 
number={}, 
pages={19-41}, 
abstract={This chapter contains sections titled: The Two Dimensions of Telecommunication Projects Innovation in Telecommunication Services Phasic Relation Between Equipment and Services Standardization for Telecommunication Services Summary }, 
keywords={}, 
doi={10.1002/9780470047682.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470047675}, 
url={https://ieeexplore.ieee.org/document/5238155},}
@INBOOK{8040815, 
author={Pang, Sauming}, 
booktitle={Successful Service Design for Telecommunications: A comprehensive guide to design and implementation}, 
title={Glossary}, 
year={2009}, 
volume={}, 
number={}, 
pages={329-333}, 
abstract={}, 
keywords={}, 
doi={10.1002/9780470741207.gloss}, 
ISSN={}, 
publisher={Wiley}, 
isbn={9780470740828}, 
url={https://ieeexplore.ieee.org/document/8040815},}
@INPROCEEDINGS{7980399, 
author={Contan, Andrei and Miclea, Liviu and Dehelean, Catalin}, 
booktitle={2017 14th International Conference on Engineering of Modern Electric Systems (EMES)}, 
title={Automated testing framework development based on social interaction and communication principles}, 
year={2017}, 
volume={}, 
number={}, 
pages={136-139}, 
abstract={The speed of development of the IT industry as well as the computational power which are increasing exponentially, create great competitiveness in the process of development but also in the launching of software products on the market. Automated testing comes to help with these challenges by trying to increase the speed of development by offering fast feedback and trustworthy quality by means of repeated runs of the implemented tests. This isn't a problem just on a technical level, but also on a social level, especially in the area of communication and understanding the requirements of the client. This work presents the implementation of an automated testing framework which also addresses the social problems. BDD or “Behavior Driven Development” includes an approach which would like to line up the area of client requests to the technical area, offering a uniform platform of collaboration and development. The implementation of this principle is applied in an MVP (Minimum Viable Product) type project which is meant to demonstrate the technical solution which may draw together, both socially and communication wise, the business teams and the technical implementation teams.}, 
keywords={Testing;Software;Business;Documentation;Collaboration;Automation;Libraries;testing process;BDD;automated testing;Gherkin language}, 
doi={10.1109/EMES.2017.7980399}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1677379, 
author={Smidts, C.}, 
booktitle={RAMS '06. Annual Reliability and Maintainability Symposium, 2006.}, 
title={Research in software reliability engineering}, 
year={2006}, 
volume={}, 
number={}, 
pages={228-233}, 
abstract={Our research has focused on development of an approach to predicting software reliability based on a systematic identification of software process failure modes and their likelihoods. A direct consequence of the approach and its supporting data collection efforts is the identification of weak areas in the software development process. A Bayes framework for the quantification of software process failure mode probabilities can be useful since it allows use of historical data that are only partially relevant to the software at hand. The approach has been applied in the context of a waterfall life-cycle and for failure modes related to the requirements phase}, 
keywords={Software reliability;Reliability engineering;Programming;Risk management;Predictive models;Software tools;Software testing;Mechanical engineering;Computer science education;Educational programs}, 
doi={10.1109/RAMS.2006.1677379}, 
ISSN={0149-144X}, 
month={Jan},}
@BOOK{9453331, 
author={McRuer, Duane T. and Graham, Dunstan and Ashkenas, Irving}, 
booktitle={Aircraft Dynamics and Automatic Control}, 
year={1974}, 
volume={}, 
number={}, 
pages={}, 
abstract={Aeronautical engineers concerned with the analysis of aircraft dynamics and the synthesis of aircraft flight control systems will find an indispensable tool in this analytical treatment of the subject. Approaching these two fields with the conviction that an understanding of either one can illuminate the other, the authors have summarized selected, interconnected techniques that facilitate a high level of insight into the essence of complex systems problems. These techniques are suitable for establishing nominal system designs, for forecasting off-nominal problems, and for diagnosing the root causes of problems that almost inevitably occur in the design process. A complete and self-contained work, the text discusses the early history of aircraft dynamics and control, mathematical models of linear system elements, feedback system analysis, vehicle equations of motion, longitudinal and lateral dynamics, and elementary longitudinal and lateral feedback control. The discussion concludes with such topics as the system design process, inputs and system performance assessment, and multi-loop flight control systems.Originally published in 1974.The Princeton Legacy Library uses the latest print-on-demand technology to again make available previously out-of-print books from the distinguished backlist of Princeton University Press. These editions preserve the original texts of these important books while presenting them in durable paperback and hardcover editions. The goal of the Princeton Legacy Library is to vastly increase access to the rich scholarly heritage found in the thousands of books published by Princeton University Press since its founding in 1905.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Princeton University Press}, 
isbn={9781400855988}, 
url={https://ieeexplore.ieee.org/document/9453331},}
@INPROCEEDINGS{404523, 
author={Parrella, E.L. and Sin-Min Chang}, 
booktitle={Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit}, 
title={Four channel DS1 framer}, 
year={1994}, 
volume={}, 
number={}, 
pages={445-448}, 
abstract={A four channel DS1 framer chip has been developed for deployment in multichannel T1 systems, SONET add-drop multiplexers, T3 multiplexes, and ATM over T1 applications. Area reduction was realized through the use of a high speed clock, permitting use of shared resources and construction of simple arbiters for single port RAM. For further gate reduction, a state-machine based framing algorithm utilizing RAM as next state memory was developed.<>}, 
keywords={Clocks;SONET;Costs;Buffer storage;Switches;Jitter;Application specific integrated circuits;Read-write memory;Add-drop multiplexers;Asynchronous transfer mode}, 
doi={10.1109/ASIC.1994.404523}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{9502466, 
author={Wolschke, Christian and Sangchoolie, Behrooz and Simon, Jacob and Marksteiner, Stefan and Braun, Tobias and Hamazaryan, Hayk}, 
booktitle={2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)}, 
title={SaSeVAL: A Safety/Security-Aware Approach for Validation of Safety-Critical Systems}, 
year={2021}, 
volume={}, 
number={}, 
pages={27-34}, 
abstract={Increasing communication and self-driving capabilities for road vehicles lead to threats which could potentially be exploited by attackers. Especially attacks leading to safety violations have to be identified to address them by appropriate measures. The impact of an attack depends on the threat exploited, potential countermeasures and the traffic situation. In order to identify such attacks and to use them for testing, we propose the systematic approach SaSeVAL for deriving attacks of autonomous vehicles.SaSeVAL is based on threats identification and safety-security analysis. The impact of automotive use cases to attacks is considered. The threat identification considers the attack interface of vehicles and classifies threat scenarios according to threat types, which are then mapped to attack types. The safety-security analysis identifies the necessary requirements which have to be tested based on the architecture of the system under test. It determines which safety impact a security violation may have, and in which traffic situations the highest impact is expected. Finally, the results of threat identification and safety-security analysis are used to describe attacks.The goal of SaSeVAL is to achieve safety validation of the vehicle w.r.t. security concerns. It traces safety goals to threats and to attacks explicitly. Hence, the coverage of safety concerns by security testing is assured. Two use cases of vehicle communication and autonomous driving are investigated to prove the applicability of the approach.}, 
keywords={Space vehicles;Privacy;Systematics;Road vehicles;Bandwidth;Safety;Security;safety;security testing;attack description;threats;threat library;risk assessment}, 
doi={10.1109/DSN-W52860.2021.00016}, 
ISSN={2325-6664}, 
month={June},}
@ARTICLE{4392504, 
author={}, 
journal={IEEE Unapproved Draft Std P2600/D30b, Nov 2007}, 
title={IEEE Standard for Information Technology: Hardcopy Device and System Security}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-172}, 
abstract={This standard defines security requirements (all aspects of security including but not limited to authentication, authorization, privacy, integrity, device management, physical security, and information security) for manufacturers, users, and others on the selection, installation, configuration, and usage of hardcopy devices (HCDs) and systems, including printers, copiers, and multifunction devices (MFDs), and the computer systems that support these devices. This standard identifies security exposures for these HCDs and systems, and instructs manufacturers and software developers on appropriate security capabilities to include in their devices and systems, and instructs users on appropriate ways to use these security capabilities.}, 
keywords={Standards;Security;IEEE Standards;Best practices;Patents;Licenses;Terminology;2600-2008;all-in-one;copier;facsimile;fax;hardcopy device;HCD;information security;MFD;MFP;multifunction device;multifunction product;printer;scanner}, 
doi={}, 
ISSN={}, 
month={Dec},}
@ARTICLE{8862808, 
author={Siddiqui, Sidra and Khan, Tamim Ahmed}, 
journal={IEEE Access}, 
title={Test Patterns for Cloud Applications}, 
year={2019}, 
volume={7}, 
number={}, 
pages={147060-147080}, 
abstract={Software systems are becoming graphical user intensive. They involve web technologies organized in the cloud platform which supports translation of services to a wider community. Such cloud applications are more vulnerable to misuse. Consequently, system development needs to focus on system security features in a comprehensive manner. Therefore, techniques that are based on test-driven development will be a good choice to use for the quality maintenance of such systems. We need checklists and mechanisms that provide identification and knowledge of best practices to maintain consistency in performing testing activities. We propose a test patterns-based technique which supports identification of test cases on the bases of specification and domain analysis of system under test. We provide a set of test patterns that support Test Driven Development (TDD) as well. We link misuse cases and security requirement to testing and provide test patterns for testing cloud applications. We consider threats associated with cloud applications and make use of case studies to evaluate and present results.}, 
keywords={Testing;Security;Software;Cloud computing;Unified modeling language;Graphical user interfaces;Test pattern;TDD (test driven development);misuse case;test last development (TLD)}, 
doi={10.1109/ACCESS.2019.2946315}, 
ISSN={2169-3536}, 
month={},}
@BOOK{9100572, 
author={Wrobel, Sharon and Wrobel, Leo}, 
booktitle={Disaster Recovery Planning for Communications and Critical Infrastructure}, 
year={2009}, 
volume={}, 
number={}, 
pages={}, 
abstract={Addressing the vulnerabilities in today's critical infrastructure to natural disasters and terrorism, this practical book describes what you should be doing to protect your infrastructure before the unthinkable happens. You learn how to maintain command and control in any disaster, and how to predict the probability of those disasters. Written by two highly regarded experts in the field, this one-of-a-kind book shows you how to simplify risk assessments and emergency response procedures to disasters affecting our critical national and local infrastructure. This practical resource helps you: Understand the latest technologies that help assure word gets out quickly after an act of terrorism, a severe weather occurance, or other destructive event occurs; Set up procedures for 4Ciù (Command, Control, Communications, Computers and intelligence); Assure that critical public services such as 911 centers will survive a catastrophic event; Learn the basics of what a good emergency response plan should contain for critical infrastructure providers; Create step-by-step plans and templates for assessing vulnerability in hospitals, government agencies, police and fire departments, EMT centers, water supplies, power grids, telecommunication networks, large business enterprises, and more; Develop safeguards and standards for critical infrastructure systems and write first alertù procedures; Discover ways to have seismic, weather and other alerts delivered to your telephone, wireless phone, blackberry or email, almost as they happen!}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781596934696}, 
url={https://ieeexplore.ieee.org/document/9100572},}
@INPROCEEDINGS{9323015, 
author={Parrott, Chester and Carver, Doris}, 
booktitle={2020 3rd International Conference on Data Intelligence and Security (ICDIS)}, 
title={Lodestone: A Streaming Approach to Behavior Modeling and Load Testing}, 
year={2020}, 
volume={}, 
number={}, 
pages={109-116}, 
abstract={It is evident that our technologically-dependent society rightly expects systems engineers to produce systems having increasing levels of security, performance, efficiency, and reliability. In addition, such systems must be able to handle sudden massive amounts of usage as well as withstand cyber-attacks such as Distributed Denial of Service (DDoS). As such, we must convolve academic and industrial data science approaches to provide theory, systems, and working technologies that can catalyze and propel engineers, developers, and technical professionals of various disciplines toward the ultimate goal of consistent delivery of quality systems. Tools and processes exist for improving the quality-oriented posture of the systems engineering industry; in practice, the most perpetual form of software testing continues to be the rote repetition of test cases through either manual testing or scripted automation of those same manual tests. We describe Lodestone: a real-time data science approach for generating workload in software systems. This real-time approach to load testing uses streaming log data to generate and dynamically update user behavior models, cluster them into similar behavior profiles, and instantiate distributed workload of software systems. We show that Lodestone outperforms Markov4JMeter on JMeter through a qualitative comparison of key feature parameters as well as experimentation based on shared data and models.}, 
keywords={Testing;Measurement;Data models;Tools;Load modeling;Adaptation models;Security;Software quality;Software performance;System performance;Performance analysis;Software testing;System testing;Automatic testing;Automatic test pattern generation}, 
doi={10.1109/ICDIS50059.2020.00021}, 
ISSN={}, 
month={June},}
@BOOK{9100252, 
author={Dias, Dileeka and Kularatna, Nihal}, 
booktitle={Essentials of Modern Telecommunications Systems}, 
year={2004}, 
volume={}, 
number={}, 
pages={}, 
abstract={In today's competitive and fast-changing telecom industry, most professionals find themselves in the difficult situation of having to sacrifice keeping on top of the latest technology because they are striving to meet another round of high-pressure deadlines. Essentials of Modern Telecommunications Systems offers you a solution to this problem, helping you quickly coming up to speed with the latest advances in your field. By cutting out arcane mathematics and management-speak jargon, it focuses on the essentials you need for rapidly understanding and mastering the latest implementation and development techniques. It provides the complete systems picture from semiconductors to end-to-end networking. Covered are such cutting-edge technologies as Next Generation Wireless, VoIP, optical transmission, and digital subscriber loop, as well as other broadband technologies. Special attention is given to systems software, especially applications software for 3G products. Throughout the book, the emphasis is on techniques you can use on the job for modeling, testing, and system management. By relating advanced technology to current technology already deployed, this book lets you quickly go from the familiar and the everyday to the cutting edge and the future.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580538848}, 
url={https://ieeexplore.ieee.org/document/9100252},}
@INBOOK{8044359, 
author={Flanagan, William A.}, 
booktitle={VoIP and Unified Communications: Internet Telephony and the Future Voice Network}, 
title={Index}, 
year={2011}, 
volume={}, 
number={}, 
pages={277-298}, 
abstract={}, 
keywords={}, 
doi={10.1002/9781118166048.index}, 
ISSN={}, 
publisher={Wiley}, 
isbn={9781118166017}, 
url={https://ieeexplore.ieee.org/document/8044359},}
@ARTICLE{4067154, 
author={}, 
journal={IEEE Std P1175.2/D12.2, Jul 2006}, 
title={IEEE Draft Recommended Practice for Case Tool Interconnection-Characterization of Interconnections}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{9796461, 
author={Liu, Yu and Yandrapally, Rahulkrishna and Kalia, Anup K. and Sinha, Saurabh and Tzoref-Brill, Rachel and Mesbah, Ali}, 
booktitle={2022 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
title={CRAWLABEL: Computing Natural-Language Labels for UI Test Cases}, 
year={2022}, 
volume={}, 
number={}, 
pages={103-114}, 
abstract={End-to-end test cases that exercise the application under test via its user interface (UI) are known to be hard for developers to read and understand; consequently, diagnosing failures in these tests and maintaining them can be tedious. Techniques for computing natural-language descriptions of test cases can help increase test readability. However, so far, such techniques have been developed for unit test cases; they are not applicable to end-to-end test cases. In this paper, we focus on the problem of computing naturallanguage labels for the steps of end-to-end UI test cases for web applications. We present two techniques that apply natural-language processing to information available in the browser document object model (DOM). The first technique is an instance of a supervised approach in which labeling-relevant DOM attributes are ranked via manual analysis and fed into label computation. However, supervised approach requires a training dataset. So we propose the second technique, which is unsupervised: it leverages probabilistic context-free grammar learning to compute dominant DOM attributes automatically. We implemented these techniques, along with two simpler baseline techniques, in a tool called CRAWLABEL (available as a plugin to Crawljax, a state-of-the-art UI test-generation tool for web applications) and evaluated their effectiveness on open-source web applications. Our results indicate that the supervised approach can achieve precision, recall, and Fl-score of 83.38, 60.64, and 66.40, respectively. The unsupervised approach, although less effective, is competitive, achieving scores of 72.37, 58.12, and 59.77. We highlight key results and discuss the implications of our findings.}, 
keywords={Training;Automation;Computational modeling;Manuals;User interfaces;Probabilistic logic;Grammar}, 
doi={10.1145/3524481.3527229}, 
ISSN={}, 
month={May},}
@INBOOK{5238094, 
author={Sherif, Mostafa Hashem}, 
booktitle={Managing Projects in Telecommunication Services}, 
title={Index}, 
year={2006}, 
volume={}, 
number={}, 
pages={239-247}, 
abstract={No abstract.}, 
keywords={Indexes}, 
doi={10.1002/9780470047682.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470047675}, 
url={https://ieeexplore.ieee.org/document/5238094},}
@INBOOK{5273158, 
author={Kaplan, Steven M.}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={A}, 
year={2004}, 
volume={}, 
number={}, 
pages={1-53}, 
abstract={}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch1}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470547151}, 
url={https://ieeexplore.ieee.org/document/5273158},}
@INPROCEEDINGS{5463662, 
author={Palviainen, Marko}, 
booktitle={2010 Third International Conference on Software Testing, Verification, and Validation Workshops}, 
title={A Dynamic Behaviour and Reliability Evaluation Method for Applications That Are Based on Asynchronous Processing Nodes}, 
year={2010}, 
volume={}, 
number={}, 
pages={309-318}, 
abstract={Many embedded and distributed applications are based on processing nodes that perform parallel processing tasks. Unfortunately, it is difficult to evaluate the overall behaviour of this kind of applications because the overall behaviour consists of 1) the execution-paths of asynchronous processing nodes and of 2) messages that either activate or deactivate processing nodes to perform parallel processing tasks. In order to facilitate behaviour and reliability evaluation of applications doing parallel processing, we developed a method that: 1) is capable of composing an overall representation for parallel behaviours and recognizing both the defined use cases and undetermined behaviours from this representation and 2) supports calculation of use case-specific reliability values for components. In this paper, we describe the method, present a ComponentBee tool that implements the method and supports behaviour and reliability evaluation of multithreaded Java applications, and finally demonstrate the use of the method with a case study.}, 
keywords={Parallel processing;Application software;Software measurement;Java;Software systems;Software testing;Performance evaluation;Concurrent computing;Predictive models;behaviour evaluation;reliability evaluation;ComponentBee;parallel processing}, 
doi={10.1109/ICSTW.2010.45}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8432197, 
author={Rana, Rakesh and Lagercrantz, Tommy and Staron, Miroslaw}, 
booktitle={2018 IEEE International Conference on Software Architecture Companion (ICSA-C)}, 
title={Building an Effective Software Issues Scorecard: An Action Research Report from the Automotive Domain}, 
year={2018}, 
volume={}, 
number={}, 
pages={136-143}, 
abstract={A large number of mature software companies use data and analytic for status monitoring of their projects and to help improve their decision making at different levels within the organization. Dashboards or scorecards also provide common platform for different stakeholders to access information they need for tracking the status of projects of their interest. Further data from software issues database can provide real and observable indicators to track the quality of given product during its development and testing. The study presented here reports on distinct and evolution of information needs of different stakeholder groups interested in tracking such data. The action research report documents the evolution of software issues scorecard as it is extended to meet information need of specific user groups. A roadmap for future into how such scorecard can be made more effective is also presented.}, 
keywords={Software;Testing;Stakeholders;Monitoring;Automotive engineering;Companies;Automobiles;software issue;scorecard;software development;action research;information need;defect database}, 
doi={10.1109/ICSA-C.2018.00042}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7473044, 
author={Zawistowski, Piotr}, 
booktitle={2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)}, 
title={The Method of Measurement and Control Systems Design and Validation with Use of BRMS Systems}, 
year={2016}, 
volume={}, 
number={}, 
pages={324-332}, 
abstract={Quality of software has always been a problem in every area of software use. Lack of software can cause problems during software execution and lead to different failures. Measurement and control systems (MCS) are such a group of software which use laboratory devices to obtain measurement data or to control e.g. production processes. Bugs in software can lead to variety of problems, from incorrect measurement data to device damages. For this reason it is important to deliver software of good quality. Software development should be considered a process which consists of a sequence of steps and is supported by a tool or set of tools that interoperate to make the work with the software easier. The problem is that there is no such approach nor tools for MCS systems. For this reason, a suitable method of design and validation of this group of systems has been developed. Moreover, the proper solution for supporting the proposed method has been implemented and tested. The solution consists of Business Rule Management System (BMRS) used for validating software execution data in an efficient way.}, 
keywords={Software;Software measurement;Standards;Software engineering;Computer languages;Nickel;Control systems;BRMS;Drools;LabVIEW;measurement and control systems;software design;software quality assurance;software runtime validation}, 
doi={10.1109/SOSE.2016.61}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{9796424, 
author={Camilli, Matteo and Guerriero, Antonio and Janes, Andrea and Russo, Barbara and Russo, Stefano}, 
booktitle={2022 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
title={Microservices Integrated Performance and Reliability Testing}, 
year={2022}, 
volume={}, 
number={}, 
pages={29-39}, 
abstract={Continuous quality assurance for extra-functional properties of modern software systems is today a big challenge as their complexity is constantly increasing to satisfy market demands. This is the case of microservice systems. They provide high control on the scale of operation by means of fine-grained service decomposition, but this demands careful consideration of the relations between performance of individual microservices and service failures. In this work, we propose MlPaRT, a novel methodology, and platform to automatically test microservice operations for performance and reliability in combination. The proposed platform can be integrated into a DevOps cycle to support continuous testing and monitoring by the automatic (1) generation and execution of performance-reliability ex-vivo testing sessions, (2) collection of monitoring data, (3) computation of performance and reliability metrics, and (4) integrated visualization of the results. We apply our approach by operating the platform on an open source benchmark. Results show that our integrated approach can provide additional insights into the performance and reliability behaviour of microservices as well as their mutual relationships. CCS CONCEPTS • Software and its engineering →Software performance; Software reliability; Software verification and validation.}, 
keywords={Measurement;Q-factor;Quality assurance;Costs;Microservice architectures;Data visualization;Benchmark testing;Microservices systems;reliability testing;performance testing}, 
doi={10.1145/3524481.3527233}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9062726, 
author={Coon, Ethan T. and Elwasif, Wael R. and Pillai, Himanshu and Thornton, Peter E. and Painter, Scott L.}, 
booktitle={2019 IEEE/ACM Parallel Applications Workshop, Alternatives To MPI (PAW-ATM)}, 
title={Exploring the Use of Novel Programming Models in Land Surface Models}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={A wide range of programming models are currently under rapid development to meet the needs of application devel- opers looking to work on more complex machines. These models fill a variety of roles. Some look to abstract supercomputer architecture, including both processors and memory, to present a strategy for portable performance across a wide range of machines. Others look to expose concurrency by explicitly con- structing task-driven dependency graphs that allow a scheduler to find parallelism. Here we explore the implications for application codes of adopting two such programming models, Kokkos and Legion, one from each class of models. We specifically focus on the software design implications on refactoring existing applications, rather than the performance and performance tuning of these models. We identify a strategy for refactoring the Energy Exas- cale Earth System Model's Land Surface Model, an extremely complex code for climate applications, and prototype a series of mini-apps that explore the adoption of Kokkos and Legion. In doing this, we identify commonalities across the models, leading to a series of conclusions about application software design and refactoring for the adoption of novel programming models. Specifically, we find that refactoring efforts to abstract physics algorithms from data structures enable the use of a variety of programming models. With this refactoring done, we find that, at least in the case of Kokkos and Legion, these types of programming models are sufficiently mature for active use by even small application software development teams.}, 
keywords={Computational modeling;Biological system modeling;Programming;Mathematical model;Earth;Atmospheric modeling;Computer architecture;Software testing;Parallel processing;Distributed processing;Runtime;Coprocessors;Multitasking}, 
doi={10.1109/PAW-ATM49560.2019.00006}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8452099, 
author={Steffens, Andreas and Lichter, Horst and Döring, Jan Simon}, 
booktitle={2018 IEEE/ACM 4th International Workshop on Rapid Continuous Software Engineering (RCoSE)}, 
title={Designing a Next-Generation Continuous Software Delivery System: Concepts and Architecture}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Continuous Integration and Continuous Delivery are established practices in modern agile software development. The DevOps movement adapted theses practices and places the deployment pipeline at its heart as one of the main requirements to automate the software development process and to deliver and operate software in a more robust way with higher quality. Over the time a lot of systems and tools has been developed to implement the deployment pipeline and to support continuous delivery. But software development is complex, its process even more and due to the individual organization of software vendors no real all-in-one solution for CD exists. Literature identified a lot of challenges when adopting CD and DevOps in an organization. This paper presents a conceptual model and fundamental design decisions for a new generation of software delivery systems tackling some of these issues. Our approach focuses on two specific challenges for adopting CD. The first is the lack of flexibility and maintainability of software delivery systems. The second is the insufficient user support to model and manage delivery processes and pipelines. We introduce an automated mechanism to ease the effort for developers and other stakeholders. Based on these results this paper introduces an architectural proposal for a next-generation continuous software delivery system.}, 
keywords={Software;Pipelines;Logic gates;Computer architecture;Tools;Computational modeling;Next generation networking;Continuous Delivery;Continuous Integration;DevOps;Software Architecture;Microservices;Domain Driven Design;Domain Model;Software Delivery;Continuous Software Engineering;Software Design}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1214600, 
author={Gohar, I. and Mirza, A.}, 
booktitle={IEEE Students Conference, ISCON '02. Proceedings.}, 
title={Voice over asynchronous transfer modc(ATM)}, 
year={2002}, 
volume={2}, 
number={}, 
pages={11-12}, 
abstract={}, 
keywords={Asynchronous transfer mode;Software testing;Software performance;Transportation;Educational institutions;Shape;System testing;Fault detection;Speech analysis;Spine}, 
doi={10.1109/ISCON.2002.1214600}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{9921425, 
author={Binder, Christoph and Calà, Ambra and Vollmar, Jan and Neureiter, Christian and Lüder, Arndt}, 
booktitle={2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
title={Towards Round-trip Engineering to evolve Complex Production Systems by utilizing AutomationML}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Additive manufacturing and product configurations will become increasingly important in future production systems. This trend brings great opportunities for manufacturing companies, but also inherits challenges to overcome. Resulting from this, a heterogeneous tool-landscape has emerged, where each of the single tools is addressing a particular aspect of the value-creation network. An example for such a tool specifically targeting the engineering of such flexible production systems according to the Reference Architecture Model Industrie 4.0 (RAMI 4.0) has been proposed with the RAMI Toolbox. However, as universal frameworks are too general to deal with all aspects of developing such complex systems, other possibilities for conflictfree engineering of the system need to be available. Thus, the main contribution of this paper deals with proposing a Round-trip Engineering (RTE) approach, that allows to export previously modeled flexible production systems according to the peculiarities of RAMI 4.0 and subsequently reload external elaborated results. Thereby, a major benefit is the application of Model-based Systems Engineering (MBSE), which ensures the traceability to the remaining system components. The chosen methodology for bidirectionally exchanging the engineering data is AutomationML, which allows to store exported information from RAMI 4.0 or import such stored information into it. The RTE-approach is thereby evaluated with a real-world case study, the Siemens Fischertechnik industrial plant model.}, 
keywords={Industries;Production systems;Three-dimensional printing;Market research;Manufacturing;Industrial plants;Internet of Things;Reference Architecture Model Industrie 4.0 (RAMI 4.0);Model-based Systems Engineering (MBSE);Industrial Internet of Things (IIoT);AutomationML;Round-trip Engineering (RTE)}, 
doi={10.1109/ETFA52439.2022.9921425}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{9896170, 
author={Kotti, Zoe and Gousios, Georgios and Spinellis, Diomidis}, 
journal={IEEE Transactions on Software Engineering}, 
title={Impact of Software Engineering Research in Practice: A Patent and Author Survey Analysis}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-19}, 
abstract={Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4,354 SE patents citing 1,690 SE papers published in four leading SE venues between 1975–2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and awarded publications, achieving 26% response rate. Overall, researchers have equipped practitioners with various tools, processes, and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited funding resources or unfavorable cost-benefit trade-off of the proposed solutions. The need for higher SE research funding could be corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its definition. Therefore, academia and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic.}, 
keywords={Software;Patents;Industries;Companies;Software engineering;Interviews;Collaboration;software engineering;practical impact;empirical study;survey;patent citations}, 
doi={10.1109/TSE.2022.3208210}, 
ISSN={1939-3520}, 
month={},}
@INBOOK{6542510, 
author={Cambron, G. Keith}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Front Matter}, 
year={2013}, 
volume={}, 
number={}, 
pages={i-xxxviii}, 
abstract={The prelims comprise: Half-Title Page Title Page Copyright Page Dedication Page Table of Contents List of Figures About the Author Foreword Preface Acknowledgments List of Acronyms ]]>}, 
keywords={}, 
doi={10.1002/9781118394519.fmatter}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781118394526}, 
url={https://ieeexplore.ieee.org/document/6542510},}
@INPROCEEDINGS{8980532, 
author={Heilscher, Gerd and Kondzialka, Christoph and Chen, Shuo and Ebe, Falko and Hess, Sebastian and Lorenz, Heiko and Wening, Jens}, 
booktitle={2019 IEEE 46th Photovoltaic Specialists Conference (PVSC)}, 
title={Integration of Photovoltaic Systems into Smart Grids Demonstration of Solar-, Storage and E-Mobility Applications within a Secure Energy Information Network in Germany}, 
year={2019}, 
volume={}, 
number={}, 
pages={1541-1548}, 
abstract={The integration of decentralized renewable energy systems into our distribution networks leads to a need of more detailed information about local network structure and state estimation down to the low voltage level [1]. This enforces the transformation of today's distribution networks into smart grids. Smart Meters with Smart Meter Gateways (iMSys) and Controllable Local Systems (CLS) are the essential new bricks of the future smart grid. In Germany the new law "Digitalization of the Energiewende"[2] sets up the rules for network operators to establish this secure energy information system based on the smart meter infrastructure. During the last two years the authors developed and demonstrated on laboratory and field level such a secure energy information system. The main innovation of the project is the direct and secure communication with decentralized energy systems such as photovoltaic inverters, battery storage systems, E-mobility charging stations or power to heat applications within this new smart meter infrastructure, which has been defined by technical rules from the German regulator for data security (BSI) [3]. The two-way communication is able to read measurement values from the field as well as change set points or activate curtailment of decentralized energy systems (see figure 1).}, 
keywords={smart grids;distribution network;smart meter;decentralized energy systems;monitoring;control;data security;energy information system}, 
doi={10.1109/PVSC40753.2019.8980532}, 
ISSN={0160-8371}, 
month={June},}
@INBOOK{5396935, 
author={}, 
booktitle={A Guide to the Wireless Engineering Body of Knowledge (WEBOK)}, 
title={Fundamental Knowledge}, 
year={2009}, 
volume={}, 
number={}, 
pages={201-219}, 
abstract={This chapter contains sections titled: Introduction Contents Electrical Engineering Basics for Wireless Communications Signal Processing and Communication Systems RF Engineering Instruments And Measurements [Wit02] Communication Networks Other Communication Systems General Engineering Management and Economics Key References }, 
keywords={}, 
doi={10.1002/9780470439128.ch7}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470439111}, 
url={https://ieeexplore.ieee.org/document/5396935},}
@INPROCEEDINGS{6549304, 
author={Iyer, Ramkumar and Chandramouleeswaran, Balasundaram}, 
booktitle={International Conference on Software Engineering and Mobile Application Modelling and Development (ICSEMA 2012)}, 
title={Best practices and case study for open source middleware migration: Egate to apache camel migration}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Many legacy systems use message oriented middleware to communicate between themselves. Message oriented middleware are considered to be the most effective technology for enterprise integration. There are a lot of proprietary middleware solutions in the market that involve huge licensing costs, difficult maintenance procedures and niche skill sets. The usage of open source middleware to replace these proprietary solutions in a cost effective manner is an idea that can now bear fruition due to the relative maturity of such solutions. The use of open source reduces licensing cost, enables the developers to have greater insight into the working of the system and avail of the wide spread community support for such systems.}, 
keywords={Middleware;open source;Apache Camel;Egate;enterprise migration}, 
doi={10.1049/ic.2012.0140}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{5381646, 
author={Wieczorek, Sebastian and Stefanescu, Alin and Schieferdecker, Ina}, 
booktitle={2009 Testing: Academic and Industrial Conference - Practice and Research Techniques}, 
title={Model-Based Integration Testing of Enterprise Services}, 
year={2009}, 
volume={}, 
number={}, 
pages={56-60}, 
abstract={The success of service-oriented architectures (SOA) depends on faultless and seamless service integration. Formal modeling of global communication protocols between services enables a model-based integration testing (MBIT) approach. In this paper we present an MBIT approach based on SAP proprietary choreography models called message choreography models (MCM). We explain how MBIT fits into the SAP testing methodology for SOA and give some insights into the experience we gained from the work.}, 
keywords={Service oriented architecture;Enterprise resource planning;Automatic testing;Software testing;Protocols;Application software;Global communication;Unified modeling language;Java;Fault detection;Model-based Testing;Integration Testing;Service Oriented Architecture}, 
doi={10.1109/TAICPART.2009.11}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5457806, 
author={Bakera, Marco and Wagner, Christian and Margaria, Tiziana and Vassev, Emil and Hinchey, Mike and Steffen, Bernhard}, 
booktitle={2010 Seventh IEEE International Conference and Workshops on Engineering of Autonomic and Autonomous Systems}, 
title={Extracting Component-Oriented Behaviour for Self-Healing Enabling}, 
year={2010}, 
volume={}, 
number={}, 
pages={152-161}, 
abstract={Rich and multifaceted domain specific specification languages like the Autonomic System Specification Language (ASSL) help to design reliable systems with self-healing capabilities. The GEAR game-based Model Checker has been used successfully to investigate in depth properties of the ESA ExoMars Rover. We show here how to enable GEAR's game-based verification techniques for ASSL via systematic model extraction from a behavioral subset of the language, and illustrate it on a description of the Voyager II space mission. This way, we close the gap between the design-time and the run-time techniques provided in the SHADOWS platform for self-healing of concurrency, performance, and functional issues.}, 
keywords={Gears;Specification languages;Runtime;Concurrent computing;Software systems;Aerospace industry;Aerospace electronics;Conferences;Design engineering;Software engineering}, 
doi={10.1109/EASe.2010.23}, 
ISSN={2168-1872}, 
month={March},}
@ARTICLE{4152663, 
author={}, 
journal={IEEE Unapproved Std P487/D7 Feb 2007}, 
title={Unapproved IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{7503833, 
author={João, Ricardo and Marinheiro, Rui and Assunçõo, Pedro and Cruz, Luís}, 
booktitle={2016 IEEE International Conference on Communications Workshops (ICC)}, 
title={A flexible monitor for assessing 3D video QoE in real-time}, 
year={2016}, 
volume={}, 
number={}, 
pages={480-485}, 
abstract={With the evolution of 3D technology, 3D IPTV services may prove to be a common service widely distributed by operators. So it is important that they have the necessary means to easily and inexpensively monitor the Quality of Experience (QoE) of this new service. Deployment of 3D video QoE monitors anywhere in the network will enable operators to adapt their service and network infrastructure in order to guarantee a desired QoE level, e.g., in scenarios where 3D IPTV streaming is offered to users with multi-homed equipment and simultaneous access to the network by means of heterogeneous smartcells in the customer premises.}, 
keywords={Monitoring;Streaming media;Three-dimensional displays;Real-time systems;Mathematical model;Artificial neural networks;Computational modeling;3D video;G.1050;monitor;video-plus-depth;MVC}, 
doi={10.1109/ICCW.2016.7503833}, 
ISSN={}, 
month={May},}
@INBOOK{6078760, 
author={Jacobs, Stuart}, 
booktitle={Engineering Information Security: The Application of Systems Engineering Concepts to Achieve Information Assurance}, 
title={Index}, 
year={2011}, 
volume={}, 
number={}, 
pages={663-700}, 
abstract={No abstract.}, 
keywords={}, 
doi={10.1002/9780470947913.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470947838}, 
url={https://ieeexplore.ieee.org/document/6078760},}
@INBOOK{5273129, 
author={Kaplan, Steven M.}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={C}, 
year={2004}, 
volume={}, 
number={}, 
pages={88-162}, 
abstract={}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch3}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470547151}, 
url={https://ieeexplore.ieee.org/document/5273129},}
@ARTICLE{4040128, 
author={}, 
journal={IEEE Std P1175.2/D11.2}, 
title={Unapproved IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections (Superseded by P1175.2_D12.2)}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{9169459, 
author={}, 
booktitle={2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)}, 
title={ICBC 2020 Keynotes}, 
year={2020}, 
volume={}, 
number={}, 
pages={18-49}, 
abstract={Provides an abstract for each of the keynote presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.}, 
keywords={}, 
doi={10.1109/ICBC48266.2020.9169459}, 
ISSN={}, 
month={May},}
@ARTICLE{9760699, 
author={Wang, Ziyuan and Bu, Dexin and Sun, Aiyue and Gou, Shanyi and Wang, Yong and Chen, Lin}, 
journal={IEEE Transactions on Reliability}, 
title={An Empirical Study on Bugs in Python Interpreters}, 
year={2022}, 
volume={71}, 
number={2}, 
pages={716-734}, 
abstract={Python is an interpreted programming language that has been widely used in many fields. The successful execution of a Python program depends on both the correctness of Python program and the correctness of Python interpreter. As an infrastructure software, there are many bugs in the Python interpreter. Exploring the bugs in Python interpreters can help developers and maintainers of Python interpreters detect and fix bugs and help users of Python avoid risks. In this article, we conduct an empirical study on the bugs in two mainstream Python interpreters: CPython and PyPy. By analyzing 25 958 fixed bugs, 18 824 revisions, 2 116 test cases, and root causes of randomly sampled 510 bugs, we have summarized the following findings. 1)The distribution of bugs in the Python interpreter is so uneven that the vast majority of bugs are distributed in a few components and source files. 2)The scales of the testing programs that reveal bugs are small. 3)The fixing works seem to be not complicated since the number of modified source files and lines of code are limited; however, most bugs need a long time to be fixed; nearly 15% of the bugs need more than one year to fix. 4)The priorities of bugs are independent of their locations, but they significantly correlate with duration of bugs. 5)Semantic bugs are the most frequent root causes of bugs, and their proportion exceeds other types of root causes. These results could indicate some potential problems during the detecting and fixing of Python interpreter’s bugs, and provide some assistance to developers and maintainers of Python interpreters, users of Python, as well as researchers in related fields.}, 
keywords={Computer bugs;Python;Testing;Correlation;Sun;Software development management;Optimization;CPython;empirical study;PyPy;Python interpreter;software bug}, 
doi={10.1109/TR.2022.3159812}, 
ISSN={1558-1721}, 
month={June},}
@INPROCEEDINGS{554009, 
author={Tanir, O.}, 
booktitle={Wescon/96}, 
title={Specification driven design of complex systems}, 
year={1996}, 
volume={}, 
number={}, 
pages={327-332}, 
abstract={This paper examines the benefits of applying a specification driven approach and presents a framework for environments that can support the related design activities. The paper also outlines interrelated advanced topics such as intermediate architecture languages, model libraries and model verification issues.}, 
keywords={Costs;Hardware;Design engineering;Audio systems;Switches;Software performance;Software design;Libraries;Testing}, 
doi={10.1109/WESCON.1996.554009}, 
ISSN={1095-791X}, 
month={Oct},}
@INPROCEEDINGS{7352573, 
author={Bandung, Yoanes and Subekti, Luki Bangun and Gondokaryono, Yudi Satria}, 
booktitle={2015 International Conference on Electrical Engineering and Informatics (ICEEI)}, 
title={A design of teacher portal for supporting teacher's internet literacy web based solution for teacher learning}, 
year={2015}, 
volume={}, 
number={}, 
pages={618-623}, 
abstract={There are a lot of valuable materials available in the Internet to support learning and self-improvement. But in some developing countries like Indonesia, Internet penetration rate is still low and people still don't comprehend on how to get benefit from it. In this research, a web based solution to support teacher's Internet literacy is designed. This solution is a teacher portal system which provides featured contents and files management system for teacher. Some application modules have been developed to support this system those are web-based portal, file management system, chatting system, and content aggregator module as the main component of the system builder.}, 
keywords={Servers;Internet;Portals;Synchronization;Feature extraction;Computer architecture;Education;Internet literacy;teacher portal;files management;information and communication technology}, 
doi={10.1109/ICEEI.2015.7352573}, 
ISSN={2155-6830}, 
month={Aug},}
@INPROCEEDINGS{9320327, 
author={Leal, Lucas and Montecchi, Leonardo and Ceccarelli, Andrea and Martins, Eliane}, 
booktitle={2020 IEEE 25th Pacific Rim International Symposium on Dependable Computing (PRDC)}, 
title={Using Metamodels to Improve Model-Based Testing of Service Orchestrations}, 
year={2020}, 
volume={}, 
number={}, 
pages={130-139}, 
abstract={Online model-based testing is one of the most suitable techniques to assess the proper behavior of service orchestrations. However, the diverse panorama in terms of modeling languages and test case generation tools is a limitation to widespread adoption. We advocate that the application of Model-Driven Engineering principles as meta-modeling and model transformation can cope with this problem, improving the interoperability of artifacts in the test case generation process, thus bringing benefits in case of agile development processes, where system and technology evolution is frequent. In this paper, we present our contribution to this idea, introducing i) a reference metamodel, which stores the business process behavior and the information to generate input models for testing tools, and ii) transformations from orchestration languages towards testing tools. The proposed approach is implemented in a testing framework and evaluated on a case study where multiple orchestrations are expressed in two languages. Also, the paper presents how test cases are appropriately generated and successfully executed, starting from an orchestration model as a consequence of successful transformations.}, 
keywords={Testing;Unified modeling language;Tools;Service-oriented architecture;Business;Runtime;Software;Model-Driven Engineering;SOA;Meta-modeling;Model-Based Testing}, 
doi={10.1109/PRDC50213.2020.00024}, 
ISSN={2473-3105}, 
month={Dec},}
@INPROCEEDINGS{9787838, 
author={Paduraru, Ciprian and Paduraru, Miruna and Stefanescu, Alin}, 
booktitle={2022 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
title={RiverGame - a game testing tool using artificial intelligence}, 
year={2022}, 
volume={}, 
number={}, 
pages={422-432}, 
abstract={As is the case with any very complex and interactive software, many video games are released with various minor or major issues that can potentially affect the user experience, cause security issues for players, or exploit the companies that deliver the products. To test their games, companies invest important resources in quality assurance personnel who usually perform the testing mostly manually. The main goal of our work is to automate various parts of the testing process that involve human users (testers) and thus to reduce costs and run more tests in less time. The secondary goal is to provide mechanisms to make test specification writing easier and more efficient. We focus on solving initial real-world problems that have emerged from several discussions with industry partners. In this paper, we present RiverGame, a tool that allows game developers to automatically test their products from different points of view: the rendered output, the sound played by the game, the animation and movement of the entities, the performance and various statistical analyses. We also address the problem of input priorities, scheduling, and directing the testing effort towards custom and dynamic directions. At the core of our methods, we use state-of-the-art artificial intelligence methods for analysis and a behavior-driven development (BDD) methodology for test specifications. Our technical solution is open-source, independent of game engine, platform, and programming language.}, 
keywords={Software testing;Quality assurance;Statistical analysis;Games;Companies;Dynamic scheduling;User experience;game testing;automated testing;BDD;deep learning;reinforcement learning;computer vision}, 
doi={10.1109/ICST53961.2022.00048}, 
ISSN={2159-4848}, 
month={April},}
@INPROCEEDINGS{1552877, 
author={Fan, Y. and Kendall, E.A.}, 
booktitle={IEEE International Conference on e-Business Engineering (ICEBE'05)}, 
title={A hybrid dialogue strategy for speech-enabled mobile commerce}, 
year={2005}, 
volume={}, 
number={}, 
pages={110-117}, 
abstract={Designing a dialogue strategy for speech-enabled mobile commerce is a significant challenge due to the context. This paper introduces a hybrid dialogue strategy to overcome the inflexibility of application-directed interactions while avoiding the significant recognition difficulty of a full mixed-initiative style. The system uses N-gram grammars to govern the recognition at the request segment of a dialogue, and employs an application-directed strategy at the clarification discourse segment. The paper also details generating a corpus for the N-gram grammar through a case-based reasoning approach, and constructing application-directed dialogues with decision trees. Our preliminary testing indicates the strategy is a feasible and effective solution for voice-enabling mobile commerce applications}, 
keywords={Business;Natural languages;Computer networks;Speech;Cities and towns;Mobile computing;Decision trees;Testing;Standards development;Control systems}, 
doi={10.1109/ICEBE.2005.6}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4101803, 
author={Ring, Steven J.}, 
journal={IEEE Transactions on Aerospace and Electronic Systems}, 
title={A Distributed Intelligence Automatic Test System for PATRIOT}, 
year={1977}, 
volume={AES-13}, 
number={3}, 
pages={264-272}, 
abstract={An automatic test system supporting high volume production testing of diverse state-of-the-art electronic assemblies is described. The test complex consists of a centralized computer system communicating to a network of satellite stations, each structured as "Intelligent Test Centers" dedicated to a particular family of assemblies (e.g., analog, digital, microwave). Allocation of resources and tasks have been distributed for optimum efficiency of production testing. This paper describes the organization and characteristics of the test system. Test center operation is explained with emphasis given to unique man-machine interactive features designed for on-line generation, examination, and maintenance of Unit-Under-Test (UUT) programs. Details are presented of the test language, RATEL, used for UUT programming. Other aspects that are discussed include test data and UUT program characteristics.}, 
keywords={Intelligent systems;Automatic testing;System testing;Electronic equipment testing;Assembly systems;Production systems;Analog computers;Computer networks;Artificial satellites;Intelligent networks}, 
doi={10.1109/TAES.1977.308394}, 
ISSN={1557-9603}, 
month={May},}
@BOOK{9106157, 
author={Elbert, Bruce}, 
booktitle={The Satellite Communication Ground Segment and Earth Station Handbook}, 
year={2000}, 
volume={}, 
number={}, 
pages={}, 
abstract={From international telephone network gateways to direct broadcast home receivers, today's broad range of ground systems and devices require satellite communication engineers and business managers to have a broad and sound understanding of the design and operating principles of earth stations and ground control facilities. The book is the first to explore the delivery end of the satellite link and its relationship to delivery of services. Authored by a leading authority in the field, the book provides you with the knowledge you need to devise your own approach to implementing and managing earth stations and the overall ground segment. You find practical guidance in an array of critical areas, including: preparing requirements, performing preliminary analyses, reviewing hardware designs, managing the introduction of the overall ground segment, and more. In over 400 pages with nearly 200 illustrations, this valuable book covers dozens of essential topics, including: Earth station and ground segment design philosophy and principles; GEO and non-GEO satellite orbit configurations; Video, data, and digital audio, and multimedia transmission; Uplink power control, RF Filtering, decoding, and Internet interfaces; Major earth station configurations and antennas; Building design, environmental concerns, and O&M principles; Supplier negotiations, cost evaluation, and support services. Plus, you find informative case studies, insightful ideas for future-proofing earth stations, and much more!}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580533973}, 
url={https://ieeexplore.ieee.org/document/9106157},}
@BOOK{9101136, 
author={Lehpamer, Harvey}, 
booktitle={Introduction to Power Utility Communications}, 
year={2016}, 
volume={}, 
number={}, 
pages={}, 
abstract={This timely new book is a cutting edge resource for engineers involved in the electric utility industry. This one-of-a-kind resource explores the planning, design, and deployment of communications networks, including fiber, microwave, RF, and Ethernet in electric utility spaces as related to Smart Grid. Readers are presented with an introduction to power utility communications, providing a thorough overview of data transmission media, electrical grid, and power grid modernization. Communication fundamentals and fiber-optic radio system design are also covered. Network performance and reliability considerations are discussed including channel protection, system latency, and cyber and grid security. Clear examples and calculations are presented to demonstrate reliability and availability measures for fiber-optic systems.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781630810078}, 
url={https://ieeexplore.ieee.org/document/9101136},}
@INBOOK{7406343, 
author={Jacobs, Stuart}, 
booktitle={Engineering Information Security: The Application of Systems Engineering Concepts to Achieve Information Assurance}, 
title={Index}, 
year={2016}, 
volume={}, 
number={}, 
pages={725-752}, 
abstract={No abstract.}, 
keywords={}, 
doi={10.1002/9781119104728.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781119104711}, 
url={https://ieeexplore.ieee.org/document/7406343},}
@INPROCEEDINGS{8449448, 
author={Kröher, Christian and El-Sharkawy, Sascha and Schmid, Klaus}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={KernelHaven – An Experimentation Workbench for Analyzing Software Product Lines}, 
year={2018}, 
volume={}, 
number={}, 
pages={73-76}, 
abstract={Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem. KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.}, 
keywords={Data mining;Pipelines;Data models;Feature extraction;Tools;Analytical models;Software engineering;Software product line analysis;variability extraction;static analysis;empirical software engineering}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@INPROCEEDINGS{9843780, 
author={Mukherji, Shubhodeep and Khan, Shaheer and Voskanian, Vicken and Su, Laura}, 
booktitle={2022 IEEE Aerospace Conference (AERO)}, 
title={Feedback-Directed Random Sequence Generation for Verifying Spacecraft Flight Rule Violations}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={The Psyche: Journey to a Metal World mission will be launched in 2022 to study the largest metal asteroid in the main asteroid belt, (16) Psyche. The spacecraft will perform a Mars Flyby in 2023 and enter (16) Psyche's orbit in 2026. Throughout the mission, safely operating the spacecraft will require abiding by a set of flight rules that can be defined at any point in the mission lifecycle. These flight rules ensure that the spacecraft is operating within allowed regimes and a discrete event simulation tool, SEQGEN, will be used to model all command sequences prior to uplink. One of SEQGEN's responsibilities is to determine if a command sequence violates any flight rules. For each flight rule, the necessary logic to determine if a violation has occurred is implemented in the SEQGEN adaptation, which is maintained by the mission. This adaptation must be tested thoroughly to ensure that the flight rule logic was interpreted and implemented correctly. This work describes a tool, RandSEQ, that autogenerates a suite of flight rule violating test sequences for each flight rule implemented in SEQGEN, and can be used by any mission using SEQGEN. The Psyche SEQGEN adaptation is still being developed, but so far, RandSEQ has been used to generate 532 test cases, that can be reviewed by various stakeholders, for 40 flight rules. The ability to autogenerate these test cases has significantly reduced the amount of time required to implement and test each flight rule.}, 
keywords={Space vehicles;Schedules;Metals;Writing;Software;Solar system;Stakeholders}, 
doi={10.1109/AERO53065.2022.9843780}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{6781158, 
author={Battaglia, Patricia A. and Byers, Charles C. and Guth, Leslie A. and Holliday, Albert and Spinelli, Claudio and Tong, Jason J.}, 
journal={Bell Labs Technical Journal}, 
title={Modular platform vision and strategy}, 
year={2004}, 
volume={9}, 
number={1}, 
pages={121-142}, 
abstract={A platform is a set of hardware, software, and process building blocks that can form the basis of many different products. The goals of platforms are to reuse and/or share assets wherever possible, to save on development costs, to spread fixed development and production costs across the largest volumes, and to offer highly integrated solutions, all while maintaining critical differentiation of products. Using platforms correctly can produce substantial life-cycle cost benefits and can lead to enhanced supplier management and customer satisfaction. This paper will begin by describing the general concept of platforms. It will then consider their economic benefits to development teams, suppliers, and customers. Next, it will discuss the role of industry standards in forming the basis of a platform offer. It will consider the changing architectures of telecommunications networks, along with the contribution of platforms to these changes. Finally, the paper will outline an example set of platform building blocks and their requirements, along with some case studies of how to combine these building blocks into products.}, 
keywords={}, 
doi={10.1002/bltj.20009}, 
ISSN={1538-7305}, 
month={},}
@INPROCEEDINGS{9968371, 
author={Sarkar, Santonu and Stark, Katharina and Hoernicke, Mario}, 
booktitle={IECON 2022 – 48th Annual Conference of the IEEE Industrial Electronics Society}, 
title={Design of a Validator for Module Type Packages}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Modular plants build a production system by integrating a set of pre-designed modules. Integration of these modules, supplied by different vendors, is performed using various design tools during the engineering phase. The integration process must perform a rigorous validation of the correctness of a module specification (MTP). Otherwise, the integration process can fail without providing enough failure details. Consequently, Such a failure at the later stage can significantly impact implementation, testing, integration, and SAT. In this paper, we describe a validator tool, that allows a plant designer to define a set of invariants that must be satisfied so that an MTP can be deemed fit for integration. We have tested the validator on a set of MTPs and reported our findings. We expect that the use of such a validator can significantly reduce the possibility of introducing errors during the engineering phase.}, 
keywords={Production systems;Design tools;Testing;MTP;Validation;Expert System;Invariant}, 
doi={10.1109/IECON49645.2022.9968371}, 
ISSN={2577-1647}, 
month={Oct},}
@BOOK{9100514, 
author={Frey, Robert}, 
booktitle={Successful Proposal Strategies for Small Businesses: Using Knowledge Management to Win Government, Private-Sector, and International Contracts, Sixth Edition}, 
year={2012}, 
volume={}, 
number={}, 
pages={}, 
abstract={Here's your one-stop-shop for winning new business! The new, Sixth Edition of this perennial bestseller updates and expands all previous editions, making this volume the most exhaustive and definitive proposal strategy resource. Directly applicable for businesses of all sizes, Successful Proposal Strategies provides extensive and important context, field-proven approaches, and in-depth techniques for business success with the Federal Government, the largest buyer of services and products in the world. This popular book and its companion CD-ROM are highly accessible, self-contained desktop references developed to be informative, highly practical, and easy to use. Small companies with a viable service or product learn how to gain and keep a customer 's attention, even when working with only a few employees. Offering a greatly expanded linkage of proposals to technical processes and directions, the Sixth Edition includes a wealth of new material, adding important chapters on cost building and price volume, the criticality of business culture and investments in proposal success, the proposal solution development process, and developing key conceptual graphics. CD-ROM Download Included: Features useful proposal templates in Adobe Acrobat, platform-independent format, HTML pointers to Small Business Web Sites, a comprehensive, fully searchable listing Proposal and Contract Acronyms, and a sample architecture for a knowledge base or proposal library.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781608074754}, 
url={https://ieeexplore.ieee.org/document/9100514},}
@INPROCEEDINGS{9101221, 
author={Jasser, Stefanie}, 
booktitle={2020 IEEE International Conference on Software Architecture (ICSA)}, 
title={Enforcing Architectural Security Decisions}, 
year={2020}, 
volume={}, 
number={}, 
pages={35-45}, 
abstract={Software architects should specify security measures for a software system on an architectural level. However, the implementation often diverges from this intended architecture including its security measures. This may lead to severe vulnerabilities that have a wide impact on the system and are hard to fix afterwards. In this paper, we propose an approach for checking the implementation's conformance with the defined security measures using architectural security rules: We extend a controlled natural language approach to formalize these rules and use dynamic analysis techniques to extract information on the actual system behavior for the conformance check. We evaluate our approach by an industrial case study to show the applicability and flexibility of our conformance checking approach.}, 
keywords={Computer architecture;Authorization;Software measurement;Software architecture;Software systems;Software-Architecture;Security-Architecture;Architecture-Rules;Security-Rules;Security-by-Design;Architecture-Enforcement;Architecture-Conformance;CNL;Ontology}, 
doi={10.1109/ICSA47634.2020.00012}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{9652642, 
author={Cluzel, Guillaume and Georgiou, Kyriakos and Moy, Yannick and Zeller, Clément}, 
booktitle={2021 IEEE Secure Development Conference (SecDev)}, 
title={Layered Formal Verification of a TCP Stack}, 
year={2021}, 
volume={}, 
number={}, 
pages={86-93}, 
abstract={The Transmission Control Protocol (TCP) at the heart of TCP/IP protocol stacks is a critical part of our current digital infrastructure. In this article, we show how an existing professional-grade open source embedded TCP/IP library can benefit from a formally verified TCP reimplementation. Our approach is to apply formal verification to the TCP layer only, relying on validated models of the lower layers on which it depends.}, 
keywords={Heart;Protocols;Conferences;TCPIP;Libraries;Formal verification;Network protocols;deductive verification;symbolic execution}, 
doi={10.1109/SecDev51306.2021.00028}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8250775, 
author={Johnson, Jiya and Jai, Aarathi Elizabeth}, 
booktitle={2017 International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
title={Netact based test-automation framework development for IMS CMREPO}, 
year={2017}, 
volume={}, 
number={}, 
pages={518-522}, 
abstract={IP Multimedia Subsystem (IMS) is an architectural framework to provide VoLte and other multimedia services. It is based on ETSI standards like SIP for interfaces between architectural elements.[9] IMS(IP Multimedia subsysytem) was originally designed by the wireless standards body 3rd Generation Partnership Project (3GPP). IMS(IP Multimedia subsysytem) is the key element in the 3G architecture that makes it possible to provide cellular access to all the services that the Internet provides. It is considered as a bridge between[9] cellular network and internet. The introduction of Configuration Management (CM) Repository Server (CMRepo Server) is an important prerequisite for the mass roll out of network elements (NEs), such as, Call Session Control Function (CSCF) and Home Subscriber Server (HSS). It is also required to pre-administer important parameters of the NEs that are required for IMS functionality. Thus, CMRepo Server provides the central CM system to manage multiple NEs. NetAct functions as the central CM system for managing online changeable parameters (class D and class E parameters). Management of other parameters (class A, B, and C parameters) is done through the customization procedure, which is time consuming as well as complex. Management of these parameters in simplified with the introduction of CMRepo Server. NetAct is an OSS Platform. This environment giving access to statistic, performance monitoring, configuration management, user management, fault management and all OSS aspect for the overall subsystem. All tools which is available in NetAct mostly developed by Java platform. Using the Robot framework tool automating NetAct test cases in IMS(IP Multimedia subsysytem).}, 
keywords={Robots;Servers;Testing;Protocols;Multimedia communication;Logic gates;IMS;Robot framework;Netact;VOLTE;CMRepo}, 
doi={10.1109/ICCONS.2017.8250775}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{9812676, 
author={Thwe, May Myat and Belay, Zelalem Mihret and Jee, Eunkyoung and Bae, Doo-Hwan}, 
booktitle={2022 17th Annual System of Systems Engineering Conference (SOSE)}, 
title={Cybersecurity Vulnerability Identification in System-of-Systems using Model-based Testing}, 
year={2022}, 
volume={}, 
number={}, 
pages={317-322}, 
abstract={When operationally and managerially independent constituent systems are integrated to form a System of Systems (SoS), cybersecurity vulnerabilities can be exploited by cyber threats that can break the security requirements of SoS due to its collaborative nature. Using model-based testing to generate test cases automatically can potentially aid in discovering vulnerabilities. However, security test case generation is time-consuming, error-prone, and labor-intensive; therefore, it is desirable to fully or partially automate security testing processes. This paper proposes the automatic test data generation using formal models presented as communicating sequential processes. We use the model-checking technique that generates counterexamples when the specified security properties are violated. Our approach then converted those counterexamples into executable test data by applying the conversion rule and defined mapping algorithm. We demonstrate our approach with an experiment using an operation of an air traffic control (ATC) system, a representative of SoS. We developed an agent simulation program to test the operation of the ATC by using the generated test data and evaluating it in terms of vulnerability identification. We incorporated four attack types, and our experimental results show that the security tests generated from the models can identify the known vulnerabilities in the ATC system.}, 
keywords={Atmospheric modeling;Collaboration;Traffic control;Data models;Air traffic control;Computer security;Testing;Model-based testing;Vulnerability identification;Security testing;System of Systems;Air traffic control}, 
doi={10.1109/SOSE55472.2022.9812676}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1471346, 
author={Verdonck, C. and Vandenameele, D.}, 
booktitle={Proceedings of the 27th European Solid-State Circuits Conference}, 
title={A single chip configurable network processor with built in ADSL-modem in 0.18 µm CMOS}, 
year={2001}, 
volume={}, 
number={}, 
pages={109-112}, 
abstract={The SEA ASIC integrates a complete Discrete Multi-Tone (DMT) ADSL modem with an Asynchronous Transfer Mode (ATM) switch, an IEEE 802.3 Ethernet based packet switch and an ARM microcontroller into a single 0.18 µm CMOS chip. The device is a cost-effective platform for a complete range of Alcatel Customer Premises ADSL Equipment.}, 
keywords={Intelligent networks;CMOS process;Hardware;Application specific integrated circuits;Local area networks;Modems;Wide area networks;Switches;Ethernet networks;Protocols}, 
doi={}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{4197854, 
author={}, 
journal={IEEE Unapproved Draft Std P487/D8, Apr 2007}, 
title={Unapproved IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations (Revision of IEEE Std 487-2000)}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{9314622, 
author={Yeung, Yiksing and Lau, C. K.}, 
booktitle={2020 IEEE International Conference on Power and Energy (PECon)}, 
title={Exploring the Application of Phasor Measurement Units in the Distribution Network}, 
year={2020}, 
volume={}, 
number={}, 
pages={299-303}, 
abstract={Phasor Measurement Units or PMU is a measurement device that produces synchronised measurement of phasor, frequency and rate of change of frequency from voltage and/or current signals based on a common time source that typically is the one provided by the Global Positioning System UTC-GPS [1]. It enables operators to monitor the network operating status in real time so as to take corresponding actions promptly and effectively while the network can be properly protected and supply can be restored automatically when fault incidents happen. PMU system can also provide the dedicated information to enhance the system protection scheme, supply reliability and asset utilisation so that the operators can operate the network smartly. This paper discusses system architecture design on the appropriate PMU technology for CLP Power's system, acquisition, installation, operation and performance evaluation of the PMU system in CLP Power's distribution network.}, 
keywords={Phasor measurement units;Current measurement;State estimation;Voltage measurement;Global Positioning System;Synchronization;Substations;Phasor Measurement Unit;Distribution;Smart City;State Estimation}, 
doi={10.1109/PECon48942.2020.9314622}, 
ISSN={}, 
month={Dec},}
@INBOOK{5273677, 
author={Bhatnagar, P. K.}, 
booktitle={Engineering Networks for Synchronization, CCS 7, and ISDN: Standards, Protocols, Planning and Testing}, 
title={Testing in the ISDN}, 
year={1997}, 
volume={}, 
number={}, 
pages={437-456}, 
abstract={This chapter contains sections titled: Introduction Layer 1 Tests Layer 2 and Layer 3 Tests Maintenance of ISDN Access This chapter contains sections titled: References }, 
keywords={}, 
doi={10.1109/9780470544570.ch14}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470544570}, 
url={https://ieeexplore.ieee.org/document/5273677},}
@ARTICLE{278309, 
author={}, 
journal={IEEE Std 610.13-1993}, 
title={IEEE Standard Glossary of Computer Languages}, 
year={1993}, 
volume={}, 
number={}, 
pages={i-}, 
abstract={}, 
keywords={}, 
doi={10.1109/IEEESTD.1993.119224}, 
ISSN={}, 
month={},}
@ARTICLE{7752755, 
author={}, 
journal={ISO/IEC/IEEE FDIS P24765 FDIS, October 2016}, 
title={ISO/IEC/IEEE Draft Systems and Software Engineering - Vocabulary}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-554}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;ISO Standards;IEC Standards;Software engineering;Terminology;Dictionaries}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INBOOK{8044562, 
author={}, 
booktitle={LTE and the Evolution to 4G Wireless: Design and Measurement Challenges}, 
title={List of Acronyms}, 
year={2013}, 
volume={}, 
number={}, 
pages={601-611}, 
abstract={}, 
keywords={}, 
doi={10.1002/9781118799475.oth1}, 
ISSN={}, 
publisher={Wiley}, 
isbn={9781119967927}, 
url={https://ieeexplore.ieee.org/document/8044562},}
@INPROCEEDINGS{9302829, 
author={Pranata, Alif Akbar and Barais, Olivier and Bourcier, Johann and Noirie, Ludovic}, 
booktitle={2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)}, 
title={Misconfiguration Discovery with Principal Component Analysis for Cloud-Native Services}, 
year={2020}, 
volume={}, 
number={}, 
pages={269-278}, 
abstract={Cloud applications and services have significantly increased the importance of system and service configuration activities. These activities include updating (i) these services, (ii) their dependencies on third parties, (iii) their configurations, (iv) the configuration of the execution environment, (v) network configurations. The high frequency of updates results in significant configuration complexity that can lead to failures or performance drops. To mitigate these risks, service providers extensively rely on testing techniques, such as metamorphic testing, to detect these failures before moving to production. However, the development and maintenance of these tests are costly, especially the oracle, which must determine whether a system's performance remains within acceptable boundaries. This paper explores the use of a learning method called Principal Component Analysis (PCA) to learn about acceptable performance metrics on cloudnative services and identify a metamorphic relationship between the nominal service behavior and the value of these metrics. We investigate the following research question: Is it possible to combine the metamorphic testing technique with learning methods on service monitoring data to detect error-prone reconfigurations before moving to production? We remove the developers' burden to define a specific oracle in detecting these configuration issues. For validation, we applied this proposal on a distributed media streaming application whose authentication was managed by an external identity and access management services. This application illustrates both the heterogeneity of the technologies used to build this type of service and its large configuration space. Our proposal demonstrated the ability to identify error-prone reconfigurations using PCA.}, 
keywords={Principal component analysis;Testing;Measurement;Streaming media;Servers;Complexity theory;Cloud computing;Reconfigurations;Metamorphic testing;Principal component analysis;Cloud-native services}, 
doi={10.1109/UCC48980.2020.00045}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8882769, 
author={Butler, Michael and Dghaym, Dana and Hoang, Thai Son and Omitola, Tope and Snook, Colin and Fellner, Andreas and Schlick, Rupert and Tarrach, Thorsten and Fischer, Tomas and Tummeltshammer, Peter}, 
booktitle={2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)}, 
title={Behaviour-Driven Formal Model Development of the ETCS Hybrid Level 3}, 
year={2019}, 
volume={}, 
number={}, 
pages={97-106}, 
abstract={Behaviour driven formal model development (BDFMD) enables domain engineers to influence and validate mathematically precise and verified specifications. In previous work we proposed a process where manually authored scenarios are used initially to support the requirements and help the modeller. The same scenarios are used to verify behavioural properties of the model. The model is then mutated to automatically generate scenarios that have a more complete coverage than the manual ones. These automatically generated scenarios are used to animate the model in a final acceptance stage. In this paper, we discuss lessons learned from applying this BDFMD process to a real-life specification: The European Train Control Systems (ETCS) Hybrid Level 3. During the case study, we have developed our understanding of the process, modifying the way we do some stages and developing improved tool support to make the process more efficient. We discuss (1) the need for abstract scenarios during incremental model development and verification, (2) tools and techniques developed to make the running of scenarios more efficient, and (3) improvements to tools that generate new test cases to improve coverage.}, 
keywords={Unified modeling language;Object oriented modeling;Tools;Visualization;Mathematical model;Testing;Computational modeling;Event-B, UML-B, MoMuT, BDFMD, Scenario, ETCS Hybrid Level 3}, 
doi={10.1109/ICECCS.2019.00018}, 
ISSN={}, 
month={Nov},}
@INBOOK{8045719, 
author={Schulzrinne, Henning and Tschofenig, Hannes}, 
booktitle={Internet Protocol-based Emergency Services}, 
title={Architectures}, 
year={2013}, 
volume={}, 
number={}, 
pages={103-192}, 
abstract={}, 
keywords={Emergency services;IP networks;3GPP;Protocols;Internet;WiMAX}, 
doi={10.1002/9781119993858.ch3}, 
ISSN={}, 
publisher={Wiley}, 
isbn={9781118652473}, 
url={https://ieeexplore.ieee.org/document/8045719},}
@INPROCEEDINGS{9254687, 
author={Ghanem, Timothy and Zein, Samer}, 
booktitle={2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)}, 
title={A Model-based approach to assist Android Activity Lifecycle Development}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={In Android app development, conforming to the activity lifecycle model is imperative to maintain app robustness and reliability as well as avoiding many issues tied to lifecycle state transitions, such as memory leaks, data preservation, and app crashes. Previous studies have shown that Android developers possess limited understanding and awareness of the activity lifecycle model and the current state-of-the-art Android app development tools and methods provide developers with little support during activity lifecycle development. In this study, we present an approach and a framework that provides a dynamic visual view for the activity lifecycle state transitions during implementation. The approach follows model-based development utilizing DSVL (Domain Specific Visual Language) and is implemented as a proof-of-concept Android Studio plugin. We evaluated our approach through experimentation by real Android developers. Initial results show that our approach can be useful and effective in assisting Android developers.}, 
keywords={Visualization;Tools;Robustness;Data models;Computer crashes;model-based;model-driven;MDD;android lifecycle;android;android development;lifecycle development}, 
doi={10.1109/ISMSIT50672.2020.9254687}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{4766483, 
author={Sarkar, Santonu and Panayappan, Arun}, 
booktitle={TENCON 2008 - 2008 IEEE Region 10 Conference}, 
title={Formal architecture modeling of business application- software maintenance case study}, 
year={2008}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Maintenance of complex business applications is challenging for software services industry. The maintenance team inherits the software with little design and implementation knowledge. The client-facing team gathers an ad-hoc architectural description of some sort and communicates the same to the geographically distributed maintenance team through informal box and line diagrams. This information is poorly understood, and the underlying architectural constraints are never enforced. This paper proposes a type system to model the architecture of a complex enterprise IT system using Acme architecture description language and reports a modeling approach to capture various architectural design decisions architects perform as a part of the architecture review. An initial field-study to evaluate the usefulness of such modeling has been encouraging.}, 
keywords={Computer architecture;Software maintenance;Application software;Computer industry;Architecture description languages;Software design;Documentation;Logic design;Software development management;Programming}, 
doi={10.1109/TENCON.2008.4766483}, 
ISSN={2159-3450}, 
month={Nov},}
@INPROCEEDINGS{186190, 
author={Godon, F. and Al-Khalili, D. and Inkol, R.}, 
booktitle={Third Annual IEEE Proceedings on ASIC Seminar and Exhibit}, 
title={Multi circular buffer controller chip for advanced ESM system}, 
year={1990}, 
volume={}, 
number={}, 
pages={P14/5.1-P14/5.4}, 
abstract={A 90 K transistor 1.5 mu m CMOS integrated circuit that operates at a data transfer rate of 20 MHz and implements an array of variable size circular buffers mapped into a high-speed RAM through physical and virtual addressing techniques is discussed. The device is fully programmable with the capability of single and block data transfers. The target application is an advanced multiprocessor ESM system.<>}, 
keywords={Control systems;Buffer storage;Random access memory;Read-write memory;Pulse measurements;Counting circuits;Very large scale integration;Computer architecture;Registers;Logic arrays}, 
doi={10.1109/ASIC.1990.186190}, 
ISSN={}, 
month={Sep.},}
@INBOOK{5273145, 
author={Kaplan, Steven M.}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={R}, 
year={2004}, 
volume={}, 
number={}, 
pages={623-675}, 
abstract={}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch18}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470547151}, 
url={https://ieeexplore.ieee.org/document/5273145},}
@INPROCEEDINGS{6075019, 
author={Balaretnaraja, Dhananjeyan and Weerawarana, Shahani}, 
booktitle={2011 International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
title={A framework for managing persistence in distributed systems}, 
year={2011}, 
volume={}, 
number={}, 
pages={9-13}, 
abstract={Enterprise applications today have acquired the need to be distributed due various demanding reasons. Such systems are developed with focus on distributed concerns than on the application logic. This diverted the developers from the functional requirement of the system and burdened them with the responsibility of developing and maintaining code related to distributed concerns. The main intention of this research is to facilitate development of distributed systems without any consideration for distributed concerns. We suggest a way where the application is initially designed without them and later enabled by integrating the framework proposed in this research. We confine our interest in separating persistence and replication among other distributed concerns. The motivation for this research comes by recognizing the fact that such a framework drastically reduces the code and complexity involved to make a distributed application resilient to failures and thereby to minimize the effort necessary to debug, deploy and maintain.}, 
keywords={Peer to peer computing;Robustness;Indexing;Servers;distributed computing;group communication framework;peer to peer overlay;JGroups;FreePastry}, 
doi={10.1109/ICTer.2011.6075019}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{9985149, 
author={Datar, Advaita and Zare, Amey and A, Asia and Venkatesh, R and Kumar, Shrawan and Shrotri, Ulka}, 
booktitle={2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={Automated Validation of Insurance Applications against Calculation Specifications}, 
year={2022}, 
volume={}, 
number={}, 
pages={55-60}, 
abstract={Insurance companies rely on their Legacy Insurance System (LIS) to govern day-to-day operations. These LIS operate as per the company's business rules that are formally specified in Calculation Specification (CS) sheets. To meet ever-changing business demands, insurance companies are increasingly trans-forming their outdated LIS to modern Policy Administration Systems (PAS). Quality Assurance (QA) of such PAS involves manual validation of calculations' implementation against the corresponding CS sheets from the LIS. This manual QA approach is effort-intensive and error-prone, which may fail to detect inconsistencies in PAS implementations and ultimately result in monetary loss. To address this challenge, we propose a novel low-code/no-code technique to automatically validate PAS imple-mentation against CS sheets. Our technique has been evaluated on a digital transformation project of a large insurance company on 12 real-world calculations through 254 policies. The evaluation resulted in effort savings of approximately 92 percent against the conventional manual validation approach.}, 
keywords={Quality assurance;Digital transformation;Conferences;Insurance;Manuals;Companies;Software reliability;Automated Validation;Formal Specification;Low-code/No-code;Insurance Applications}, 
doi={10.1109/ISSREW55968.2022.00039}, 
ISSN={}, 
month={Oct},}
@ARTICLE{4519427, 
author={}, 
journal={IEEE Unapproved Draft Std P11073-00101/D03, Sep 2007}, 
title={Health Informatics - Point-Of-Care Medical Device Communication - Technical Report - Guidelines for the Use of RF Wireless Technology}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={The following Guidance document addresses the use of radio frequency 1 (RF) wireless technology for the transport of medical data both to and from point-of-care (PoC) medical devices. The context of such wireless medical data transport can range from home- or mobile-based healthcare to in hospital ambulatory and stationary situations. The intent of the guidance document is to be global with respect to wireless spectrum and equipment, although working group participation and expertise have favored detail of scenarios from the US. At the time of this Guidance document several applicable RF wireless technologies exist with a range of capabilities and characteristics, and in different stages of maturity, standardization, and adoption in healthcare. It is recognized that RF technologies are rapidly evolving, and new options may become available (or sufficiently established) after the publication of this Guidance document. The recommendations, therefore, avoid being overly prescriptive and instead attempt to assist medical device manufacturers, wireless equipment manufacturers, healthcare providers, government agencies and any other end-user of this document to make reasonable judgments regarding performance and practical implementation of wireless solutions. The Guidance document defines specific use cases to estimate, compare, and contrast performance of known technologies operating on wireless personal area (WPAN), wireless local area (WLAN), wireless metropolitan area (WMAN), and wireless wide area (WWAN) networks. Major considerations are 1) the quality-of-service (QoS) requirements (reliability, latency, priority, bandwidth) associated with the data being transported, 2) the expected performance (power, link range, throughput, link establishment and maintenance) of the wireless technology, and 3) the specific needs and resources of the end user. Related issues include network architecture, EMI/EMC, coexistence with other data streams, security, cost, power consumption, and technology configurability. Performance summaries for specific wireless technologies that support defined use cases are not intended as an endorsement of optimal solution because different needs, resources, sizes, and environments cannot be comprehensively addressed. This overview document is meant to be a foundation and reference for several follow-on IEEE 11073.3.5.x standards that will profile specific classes of off-the-shelf RF wireless technologies for medical data transport. Importantly, this guidance document is not envisioned to be periodically updated, but instead will act as a source of information for the follow-on IEEE 11073-0305.x standards that will supplant it. Periodic updates will be performed on the IEEE 11073- 0305.x standards only.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{9984994, 
author={Drusinsky, Doron and Michael, James Bret and Litton, Matthew}, 
booktitle={2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={Machine-Learned Specifications for the Verification and Validation of Autonomous Cyberphysical Systems}, 
year={2022}, 
volume={}, 
number={}, 
pages={333-341}, 
abstract={Machine learning classifiers can be used as speci-fications for runtime monitoring (RM), which in turn supports evaluating autonomous systems during design-time and detecting/responding to exceptional situations during system operation. In this paper we describe how the use of machine-learned specifications enhances the effectiveness of RM for verification and validation (V & V) of autonomous cyberphysical systems (CPSs). In addition, we show that the development of machine-learned specifications has a predictable cost, at less than $100 per specification, using 2022 cloud computing pricing. Finally, a key benefit of our approach is that developing specifications by training ML models brings the task of developing robust specifications from the realm of doctoral-level experts into the domain of system developers and engineers.}, 
keywords={Training;Runtime;Systems operation;Pricing;Medical services;Machine learning;Cyber-physical systems;verification and validation;formal methods;machine learning;specifications;autonomous systems;cyberphysical systems}, 
doi={10.1109/ISSREW55968.2022.00089}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7018532, 
author={Ulrich, Andreas and Jell, Sylvia and Votintseva, Anjelika and Kull, Andres}, 
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={The ETSI Test Description Language TDL and its application}, 
year={2014}, 
volume={}, 
number={}, 
pages={601-608}, 
abstract={The wide-scale introduction of model-based testing techniques in an industrial context faces many obstacles. One of the obstacles is the existing methodology gap between informally described test purposes and formally defined test descriptions used as the starting point for test automation. The provision of an explicit test description becomes increasingly essential when integrating complex, distributed systems and providing support for conformance and interoperability tests of such systems. The upcoming ETSI standard on the Test Definition Language (TDL) covers this gap. It allows describing scenarios on a higher abstraction level than programming or scripting languages. Furthermore, TDL can be used as an intermediate representation of tests generated from other sources, e.g. simulators, test case generators, or logs from previous test runs. TDL is based on a meta-modelling approach that expresses its abstract syntax. Deploying this design approach, individual concrete syntaxes of TDL can be designed for different application domains. The paper provides an overview of TDL and discusses its application on a use case from the rail domain.}, 
keywords={Testing;Unified modeling language;Concrete;Telecommunication standards;Syntactics;Semantics;Abstracts;Model-based Testing;Domain-Specific Languages;Meta-modelling;Rail Application}, 
doi={}, 
ISSN={}, 
month={Jan},}
@ARTICLE{7361678, 
author={}, 
journal={IEEE Std 487-2015 (Revision of IEEE Std 487-2007) - Redline}, 
title={IEEE Standard for the Electrical Protection of Communications Facilities Serving Electric Supply Locations -- General Considerations - Redline}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-324}, 
abstract={General considerations are presented for the electrical protection of telecommunications facilities serving electric supply locations. This standard contains material that is common to the IEEE 487(TM) family of standards (i.e., dot-series) including fundamental protection theory; basic electrical protection philosophy, concepts, and designs; protection apparatus; service types; reliability; service performance objective (SPO) classifications; and transmission considerations. In general, special protective measures, handling procedures, and administrative procedures are necessary to provide electrical protection against damage to telecommunications facilities and equipment, maintain reliability of service, and ensure the safety of personnel.}, 
keywords={IEEE Standards;Electricity supply industry;Voltage control;Power transmission lines;Power stations;electric supply locations;high-voltage tower;IEEE 487(TM);power stations;protection;wire-line telecommunications}, 
doi={}, 
ISSN={}, 
month={July},}
@ARTICLE{9852775, 
author={Grandmaison, Arnaud de and Heydemann, Karine and Meunier, Quentin L.}, 
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
title={ARMISTICE: Microarchitectural Leakage Modeling for Masked Software Formal Verification}, 
year={2022}, 
volume={41}, 
number={11}, 
pages={3733-3744}, 
abstract={Side-channel attacks are powerful attacks for retrieving secret data by exploiting physical measurements, such as power consumption or electromagnetic emissions. Masking is a popular countermeasure as it can be proven secure against an attacker model. In practice, software-masked implementations suffer from a security reduction due to a mismatch between the considered leakage sources in the security proof and the real ones, which depend on the microarchitecture. We propose ARMISTICE, a framework for formally verifying the absence of leakage in first-order masked implementations taking into account modeled microarchitectural sources of leakage. As a proof of concept, we present the modeling of an Arm Cortex-M3 core from its RTL description and leakage test vectors, as well as the modeling of the memory of an STM32F1 board, exclusively using leakage test vectors. We show that, with these models, ARMISTICE pinpoints vulnerable instructions in real-world masked implementations and helps the design of masked software implementations which are practically secure.}, 
keywords={Computational modeling;Software;Registers;Codes;Security;Software algorithms;Integrated circuit modeling;Masking;microarchitectural leakage;side-channel attacks (SCA);verification}, 
doi={10.1109/TCAD.2022.3197507}, 
ISSN={1937-4151}, 
month={Nov},}
@INBOOK{5273147, 
author={Kaplan, Steven M.}, 
booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
title={B}, 
year={2004}, 
volume={}, 
number={}, 
pages={54-87}, 
abstract={}, 
keywords={Dictionaries}, 
doi={10.1109/9780470547151.ch2}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470547151}, 
url={https://ieeexplore.ieee.org/document/5273147},}
@INPROCEEDINGS{8051361, 
author={Terber, Matthias}, 
booktitle={2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
title={Function-Oriented Decomposition for Reactive Embedded Software}, 
year={2017}, 
volume={}, 
number={}, 
pages={288-295}, 
abstract={Due to C's overwhelming dominance in industry, reactive embedded applications usually rely on conventional sequential programming. Adopted approaches favor event-driven paradigms which prevent function-oriented code decomposition in particular. This encourages the violation of fundamental software engineering principles. The reactive programming paradigm is proposed as a general solution. However, most reactive languages cannot keep up with C's practical advantages. It appears, that the subfamily of synchronous languages provides promising features but real-world deployments and evaluations are rarely reported in literature. On this account, we make two major contributions in this paper. First, we elaborate how the lack of function-oriented software decomposition manifests in a real-life industrial application. Second, we provide a corresponding re-implementation which illustrates the deployment and discusses the gained engineering benefits provided by the third-party, synchronous-reactive programming language Céu. We believe that our work generally reveals a practicable way of improving embedded software quality in industrial applications.}, 
keywords={Programming;Logic gates;Software;Software engineering;Heating systems;Mirrors;Switches;synchronous reactive programming;Céu;software decomposition;software quality}, 
doi={10.1109/SEAA.2017.42}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7323164, 
author={De, Tran Cao}, 
booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
title={Applying model-driven development to environment monitoring System}, 
year={2015}, 
volume={}, 
number={}, 
pages={577-584}, 
abstract={Environmental monitoring is critical in understanding whether the quality of our environment is getting better or worse. Information gathered by using an environmental monitoring system is important to make decisions. Vietnam is a vulnerable country of climate change. Specially, in the South of Vietnam, the Mekong delta is known as the region getting the most impact of sea level rise in Vietnam. That leads to a lot of problems making the worst effects to residents in the area, who are mainly still very poor. On the other hand, Vietnam is going on industrialization process that makes a strong effect on the environment. To deal with these challenges, different projects of environment management have been proposed and implemented and many monitoring systems have been built in those projects. Those systems are basically sensor networks with high cost in developing and maintaining. They are related to modern technology such as cloud, communication mobile and wireless. They provide the data for large community for different purposes. Therefore, building such a system is normally a long term project that requires an incremental and modular development for a complex system. This paper, on one hand, represents some common characteristics of an environment monitoring system that requires more study to develop a formal model and a methodology for their specifications, implementations and verification. On the other hand, we would like to adapt the formal model approach proposed for Intelligent Transport Systems (ITS) to an environmental monitoring system. The framework of Baobab is also introduced as an example for transformation from model to code.}, 
keywords={Unified modeling language;Cities and towns;Environmental monitoring;Hardware;Mobile communication;Adaptation models;Climate change;Environment Management;Environment Monitoring System;Formal Method;Model-Driven Development;Sensor System}, 
doi={}, 
ISSN={}, 
month={Feb},}
@INBOOK{6544996, 
author={Cambron, G. Keith}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Integration and Innovation}, 
year={2013}, 
volume={}, 
number={}, 
pages={269-296}, 
abstract={This chapter contains sections titled: Technology Integration Lifecycle Support Invention and Innovation Summary References ]]>}, 
keywords={Benchmark testing}, 
doi={10.1002/9781118394519.ch13}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781118394526}, 
url={https://ieeexplore.ieee.org/document/6544996},}
@ARTICLE{9847081, 
author={Zhong, Ziyuan and Kaiser, Gail and Ray, Baishakhi}, 
journal={IEEE Transactions on Software Engineering}, 
title={Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-15}, 
abstract={Self-driving cars and trucks, autonomous vehicles (AVs), should not be accepted by regulatory bodies and the public until they have much higher confidence in their safety and reliability - which can most practically and convincingly be achieved by testing. But existing testing methods are inadequate for checking the end-to-end behaviors of AV controllers against complex, real-world corner cases involving interactions with multiple independent agents such as pedestrians and human-driven vehicles. While test-driving AVs on streets and highways fails to capture many rare events, existing simulation-based testing methods mainly focus on simple scenarios and do not scale well for complex driving situations that require sophisticated awareness of the surroundings. To address these limitations, we propose a new fuzz testing technique, called AutoFuzz, which can leverage widely-used AV simulators' API grammars to generate semantically and temporally valid complex driving scenarios (sequences of scenes). To efficiently search for traffic violations-inducing scenarios in a large search space, we propose a constrained neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation of our prototype on one state-of-the-art learning-based controller, two rule-based controllers, and one industrial-grade controller in five scenarios shows that AutoFuzz efficiently finds hundreds of traffic violations in high-fidelity simulation environments. For each scenario, AutoFuzz can find on average 10-39% more unique traffic violations than the best-performing baseline method. Further, fine-tuning the learning-based controller with the traffic violations found by AutoFuzz successfully reduced the traffic violations found in the new version of the AV controller software.}, 
keywords={Automobiles;Testing;Fuzzing;Vehicle crash testing;Grammar;Artificial neural networks;Roads;Search-based Software Engineering;Evolutionary Algorithms;Neural Networks;Software Testing;Test Generation;Autonomous Vehicles}, 
doi={10.1109/TSE.2022.3195640}, 
ISSN={1939-3520}, 
month={},}
@BOOK{8824174, 
author={Ringer, Talia and Palmskog, Karl and Sergey, Ilya and Gligoric, Milos and Tatlock, Zachary}, 
booktitle={QED at Large: A Survey of Engineering of Formally Verified Software}, 
year={2019}, 
volume={}, 
number={}, 
pages={}, 
abstract={Development of formal proofs of correctness of programs can increase actual and perceived reliability and facilitate better understanding of program specifications and their underlying assumptions. Tools supporting such development have been available for over 40 years but have only recently seen wide practical use. Projects based on construction of machine-checked formal proofs are now reaching an unprecedented scale, comparable to large software projects, which leads to new challenges in proof development and maintenance. Despite its increasing importance, the field of proof engineering is seldom considered in its own right; related theories, techniques, and tools span many fields and venues. QED at Large covers the timeline and research literature concerning proof development for program verification, including theories, languages, and tools. It emphasizes challenges and breakthroughs at each stage in history and highlights challenges that are currently present due to the increasing scale of proof developments. This monograph is intended for use by researchers and students who are new to the field. It provides the reader with an insightful overview of the work that has led to modern-day techniques for formally verifying software. In times of increasing automation, this underpins many software systems so future trends are also highlighted.}, 
keywords={}, 
doi={10.1561/2500000045}, 
ISSN={}, 
publisher={now}, 
isbn={9781680835953}, 
url={https://ieeexplore.ieee.org/document/8824174},}
@ARTICLE{4068343, 
author={}, 
journal={IEEE Std 1175.2-2006}, 
title={IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections}, 
year={2007}, 
volume={}, 
number={}, 
pages={1-45}, 
abstract={This recommended practice describes interconnections that need to be understood and evaluated when buying, building, testing, or using computer-aided software engineering (CASE) tools. CASE tools are developed for use in creating computing systems. By assisting users to reach a clear understanding of the context of operation for a computing system tool, this recommended practice contributes to the effective implementation and application of computing system tools. This recommended practice does not describe the processes of evaluating, acquiring, or adopting CASE tools. This recommended practice is limited to the technical aspects of CASE tools. It does not include issues in the management, marketing, or training domains.}, 
keywords={IEEE standards;Context;Computer aided software engineering;Trademarks;Standards Board;Patents;Computer-Aided Software Engineering (CASE) tools;tool communications;tool interconnections}, 
doi={10.1109/IEEESTD.2007.288641}, 
ISSN={}, 
month={Jan},}
@BOOK{9100418, 
author={Schulmeyer, G.}, 
booktitle={Handbook of Software Quality Assurance, Fourth Edition}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={This thoroughly revised fourth edition of the popular book, Handbook of Software Quality Assurance, brings together the latest SQA (software quality assurance) methods, recognizing the importance of CMMI and the ISO 900-3 standard. This unique book offers you a wide spectrum of experiences and issues presented in papers from leading experts in SQA, DQA (development quality assurance), and software development and management. The fourth edition is a significant update to past editions, bringing you the very latest on current best practices in the field. You learn the role of SQA/DQA with regard to ISO 9001-2000 requirements and the criteria from the Software Engineering Institute for the various levels of CMMI. You also find an updated discussion on the American Society for Quality (ASQ) SQA certification program, covering the benefits of becoming an ASQ certified software quality engineer. This practical resource shows you how to move an organization from CMMI software quality assurance compliance to developmental quality assurance compliance. The book covers the commercial standards and modern development methods of SQA and DQA, and details how SQA can be implemented in organizations large and small. This volume also helps you better understand the requirements of the ASQ's CSQE examination. From quality management concepts for IT, teaching SQA in an industrial environment, and the inspection process, to the impact of SQA certification on the hiring process, software quality metrics recommendations, and software reliability, this invaluable book serves as your a one-stop resource for complete and current software quality assurance knowledge.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781596931879}, 
url={https://ieeexplore.ieee.org/document/9100418},}
@INBOOK{6542353, 
author={Cambron, G. Keith}, 
booktitle={Global Networks: Engineering, Operations and Design}, 
title={Disasters and Outages}, 
year={2013}, 
volume={}, 
number={}, 
pages={297-316}, 
abstract={This chapter contains sections titled: Disasters Outages The Vicious Cycle Summary References ]]>}, 
keywords={Tornadoes;Earthquakes;Copper;Storms;Power cables;Bandwidth;Organizations}, 
doi={10.1002/9781118394519.ch14}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781118394526}, 
url={https://ieeexplore.ieee.org/document/6542353},}
@INPROCEEDINGS{7321627, 
author={Bicevska, Zane and Bicevskis, Janis and Oditis, Ivo}, 
booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
title={Smart technologies for improved software maintenance}, 
year={2015}, 
volume={}, 
number={}, 
pages={1533-1538}, 
abstract={Steadily increasing complexity of software systems makes them difficult to configure and use without special IT knowledge. One of the solutions is to improve software systems making them “smarter”, i.e. to supplement software systems with features of self-management, at least partially. This paper describes several software components known as smart technologies, which facilitate software use and maintenance. As to date smart technologies incorporate version updating, execution environment testing, self-testing, runtime verification and business process execution. The proposed approach has been successfully applied in several software projects.}, 
keywords={Software;Information systems;Business;Built-in self-test;Runtime;Complexity theory;Autonomic computing;smart technologies;self-managing systems;software maintenance}, 
doi={10.15439/2015F170}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7878375, 
author={Fangohr, Hans and Albert, Maximilian and Franchin, Matteo}, 
booktitle={2016 IEEE/ACM International Workshop on Software Engineering for Science (SE4Science)}, 
title={Nmag Micromagnetic Simulation Tool — Software Engineering Lessons Learned}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={We review design and development decisions and their impact for the open source code Nmag from a software engineering in computational science point of view. We summarise lessons learned and recommendations for future computational science projects. Key lessons include that encapsulating the simulation functionality in a library of a general purpose language, here Python, provides great flexibility in using the software. The choice of Python for the top-level user interface was very well received by users from the science and engineering community. The from-source installation in which required external libraries and dependencies are compiled from a tarball was remarkably robust. In places, the code is a lot more ambitious than necessary, which introduces unnecessary complexity and reduces main- tainability. Tests distributed with the package are useful, although more unit tests and continuous integration would have been desirable. The detailed documentation, together with a tutorial for the usage of the system, was perceived as one of its main strengths by the community.}, 
keywords={Libraries;Software;Mathematical model;Computational modeling;Software engineering;Micromagnetics;Magnetization;Nmag;Computational Science Software Engineering;Python;Finite Elements}, 
doi={}, 
ISSN={}, 
month={May},}
@ARTICLE{4040327, 
author={}, 
journal={IEEE Std P487/Draft6}, 
title={IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={This recommended practice presents engineering design practices for special high-voltage protection systems intended to protect wire-line telecommunication facilities serving electric supply locations. The following topics are included in this document: a) A description of the electric supply locations environment, i.e., ground potential rise (GPR), induced voltages, lightning, and switching transients; b)A discussion of special high-voltage protection devices; c)Definitions of service types and service performance objectives for electric supply locations telecommunication services; d)Special protection theory and philosophy; e)Special protection system design guidelines; f)Personnel safety considerations; g)Grounding; h)Cables with metallic members. Other telecommunication alternatives such as radio and optica fiber systems are excluded from this document.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@BOOK{9100206, 
author={Lehpamer, Harvey}, 
booktitle={Transmission Systems Design Handbook for Wireless Networks}, 
year={2002}, 
volume={}, 
number={}, 
pages={}, 
abstract={This practical new resource gives you a comprehensive understanding of the design and deployment of transmission networks for wireless applications. From principles and design, to equipment procurement, project management, testing, and operation, it's a practical, hands-on engineering guide with numerous real-life examples of turn-key operations in the wireless networking industry. This book, written for both technical and non-technical professionals, helps you deal with the costs and difficulties involved in setting up the local access with technologies that are still in the evolutionary stage. Issues involved in the deployment of various transmission technologies, and their impact on the overall wireless network topology are discussed. Strategy and approach to transmission network planning, design and deployment are explored. The book offers practical guidelines and advice derived from the author's own experience on projects worldwide. You gain a solid grounding in third generation wireless networks with increased capacity requirements, while learning about packet data architecture, and how it will impact future transmission network design and deployment.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580535540}, 
url={https://ieeexplore.ieee.org/document/9100206},}
@BOOK{9100210, 
author={Hassler, Vesna}, 
booktitle={Security Fundamentals for E-Commerce}, 
year={2000}, 
volume={}, 
number={}, 
pages={}, 
abstract={If you're charged with maintaining the security of e-commerce sites, you need this unique book that provides an in-depth understanding of basic security problems and relevant e-commerce solutions, while helping you implement today's most advanced security technologies. From designing secure Web, e-commerce, and mobile commerce applications - to securing your internal network - to providing secure employee/user authentication, this cutting-edge book gives you a valuable security perspective you won't find in other resources. Flexibly structured to give you a comprehensive overview or to help you quickly pinpoint topics of immediate concern, the book includes sections on basic security mechanisms, the specific requirements of electronic payment systems, address communication security, and Web- and Java-related security issues. A full section is devoted to the security aspects of code and customer mobility, specifically mobile agents, mobile devices, and smart cards. Over 70 illustrations help clarify important points throughout the book.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580534062}, 
url={https://ieeexplore.ieee.org/document/9100210},}
@ARTICLE{4040044, 
author={}, 
journal={IEEE Std P1073.0.1.1/D01J}, 
title={Unapproved IEEE Draft Guide for Health Informaticspoint-Of-Care Medical Device Communicationtechnical Reportguidelines for the Use of RF Wireless Technology}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{8973034, 
author={Moran, Kevin and Bernal-Cárdenas, Carlos and Linares-Vásquez, Mario and Poshyvanyk, Denys}, 
booktitle={2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)}, 
title={Overcoming Language Dichotomies: Toward Effective Program Comprehension for Mobile App Development}, 
year={2018}, 
volume={}, 
number={}, 
pages={7-711}, 
abstract={Mobile devices and platforms have become an established target for modern software developers due to performant hardware and a large and growing user base numbering in the billions. Despite their popularity, the software development process for mobile apps comes with a set of unique, domain-specific challenges rooted in program comprehension. Many of these challenges stem from developer difficulties in reasoning about different representations of a program, a phenomenon we define as a "language dichotomy". In this paper, we reflect upon the various language dichotomies that contribute to open problems in program comprehension and development for mobile apps. Furthermore, to help guide the research community towards effective solutions for these problems, we provide a roadmap of directions for future work.}, 
keywords={Program Comprehension;Mobile;Android;Natural Language;Code}, 
doi={}, 
ISSN={2643-7171}, 
month={May},}
@INPROCEEDINGS{9586183, 
author={Mondelli, Andrea and Gazzillo, Paul and Solihin, Yan}, 
booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)}, 
title={SeMPE: Secure Multi Path Execution Architecture for Removing Conditional Branch Side Channels}, 
year={2021}, 
volume={}, 
number={}, 
pages={973-978}, 
abstract={One prevalent source of side channel vulnerabilities is the secret-dependent behavior of conditional branches (SDBCB). The state-of-the-art solution relies on Constant-Time Expressions, which require high programming effort and incur high performance overheads. In this paper, we propose SeMPE, an architecture support to eliminate SDBCB without requiring much programming effort while incurring low performance overheads. When a secret-dependent branch is encountered, SeMPE fetches, executes, and commits both paths of the branch, preventing the adversary from inferring secret values from the branching behavior of the program. SeMPE outperforms code generated by FaCT, a constant-time expression language, by up to 18×.}, 
keywords={Codes;Design automation;Programming;Hardware;Registers;side channel;conditional branch;multipath execution;microarchitecture}, 
doi={10.1109/DAC18074.2021.9586183}, 
ISSN={0738-100X}, 
month={Dec},}
@BOOK{9106105, 
author={Demott, Jared and Miller, Charles and Takanen, Ari}, 
booktitle={Fuzzing for Software Security Testing and Quality Assurance}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={Learn the code cracker's malicious mindset, so you can find worn-size holes in the software you are designing, testing, and building. Fuzzing for Software Security Testing and Quality Assurance takes a weapon from the black-hat arsenal to give you a powerful new tool to build secure, high-quality software. This practical resource helps you add extra protection without adding expense or time to already tight schedules and budgets. The book shows you how to make fuzzing a standard practice that integrates seamlessly with all development activities. This comprehensive reference goes through each phase of software development and points out where testing and auditing can tighten security. It surveys all popular commercial fuzzing tools and explains how to select the right one for a software development project. The book also identifies those cases where commercial tools fall short and when there is a need for building your own fuzzing tools.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781596932159}, 
url={https://ieeexplore.ieee.org/document/9106105},}
@INPROCEEDINGS{7784207, 
author={Sroka, Michal and Fisch, Dominik and Nagy, Roman}, 
booktitle={2016 6th International Conference on Information Communication and Management (ICICM)}, 
title={Localised mutation in evolutionary test model learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={13-18}, 
abstract={The focus of this paper is on automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is described. In an experiment the impact of localised mutational changes on the evolutionary learning method is investigated.}, 
keywords={Biological cells;Sociology;Statistics;Testing;Software;Algorithm design and analysis;Software algorithms;model-based testing;evolutionary test model learning;localised mutation;reproduction operator}, 
doi={10.1109/INFOCOMAN.2016.7784207}, 
ISSN={}, 
month={Oct},}
@BOOK{9100180, 
author={Elbert, Bruce}, 
booktitle={The Satellite Communication Applications Handbook, Second Edition}, 
year={2003}, 
volume={}, 
number={}, 
pages={}, 
abstract={Since the publication of the best-selling first edition of The Satellite Communication Applications Handbook, the satellite industry has experienced explosive growth thanks to a flood of innovations in consumer electronics, broadcasting, the Internet, transportation, and broadband telecommunications. This second edition covers all the latest advances in satellite technology and applications and features new chapters on mobile digital audio radio and VSAT networks. It updates and expands upon the engineering and management topics that made the first edition a must-have for every satellite communications professional as well as network architects. Engineers get the latest technical details into operations, architectures, and systems components. Managers are brought up to date with the latest business applications as well as regulatory and legal decisions affecting domestic and international markets. The treatment is also of value to marketing, legal, regulatory, and financial and operations professionals who must gain a clear understanding of the capabilities and issues associated with satellite space and ground facilities and services. You get real-world, first-hand insight into: defining a satellite network architecture to meet your organization's business or operational requirements; engineering criteria and design principles for TV and radio broadcasting, mobile and fixed telephony, and VSAT data communications; and addressing business and regulatory issues to ensure a successful satellite application. Whether you are new to the satellite industry and need a quick and thorough understanding of how satellite communications operate or are a veteran professional needing a refresher on issues not encountered day to day, The Satellite Communication Applications Handbook, Second Edition is an indispensable resource to be referred to again and again.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580538084}, 
url={https://ieeexplore.ieee.org/document/9100180},}
@ARTICLE{7891866, 
author={}, 
journal={ISO/IEC/IEEE P24765/D3:2017}, 
title={ISO/IEC/IEEE Approved Draft International Standard - Systems and Software Engineering - Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-570}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;IEC Standards;ISO Standards;Terminology;Dictionaries;Software engineering}, 
doi={}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{9953718, 
author={Sánchez M, Daniel E. and Vega L, Jeison A. and Rueda, Diego F. and Rodriguez F, Andres A.}, 
booktitle={2022 Congreso Internacional de Innovación y Tendencias en Ingeniería (CONIITI)}, 
title={Remote Monitoring of RF Amplifiers in HFC Networks: Voltage Drop Detection due to Power Blackouts}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Hybrid Fiber–Coaxial (HFC) networks are widely used around the world to deliver voice, video, and data services. These networks are made up of both passive and active elements, most of which are managed remotely by the network operation centers. However, RF amplifiers do not have a mechanism to notify metrics or alarms that allow network operators to segment and locate the point of failure in an efficient and timely manner. This means that subscribers first detect the fault before the network operator, translating into an increase in complaints to the call center. This paper proposes an electronic system for remote monitoring of RF amplifiers in an HFC network that detects voltage drops due to power outages and notifies an alarm to the technical support staff. The results show that the designed device efficiently detects the voltage drop and sends the alarm notification through a text message, contributing to improve response times to subscribers when this type of incident occurs.}, 
keywords={Radio frequency;Optical fiber amplifiers;Measurement;Voltage;Hybrid fiber coaxial cables;Optical fiber networks;Power system reliability;voltage drop;HFC networks;power blackouts;remote monitoring;RF amplifiers}, 
doi={10.1109/CONIITI57704.2022.9953718}, 
ISSN={2539-4320}, 
month={Oct},}
@BOOK{9101054, 
author={Sendin, Alberto and Sanchez-Fornie, Miguel and Berganza, Inigo and Simon, Javier and Urrutia, Iker}, 
booktitle={Telecommunictaion Networks for the Smart Grid}, 
year={2016}, 
volume={}, 
number={}, 
pages={}, 
abstract={This comprehensive new resource demonstrates how to build smart grids utilizing the latest telecommunications technologies. Readers find practical coverage of PLC and wireless for smart grid and are given concise excerpts of the different technologies, networks, and services around it. Design and planning guidelines are shown through the combination of electricity grid and telecommunications technologies that support the reliability, performance and security requirements needed in smart grid applications. This book covers a wide range of critical topics, including telecommunications for power engineers, power engineering for telecommunications engineers, utility applications projecting in smart grids, technologies for smart grid networks, and telecommunications architecture. This practical reference is supported with in-depth case studies.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781630813734}, 
url={https://ieeexplore.ieee.org/document/9101054},}
@INPROCEEDINGS{9172613, 
author={}, 
booktitle={2020 IEEE Aerospace Conference}, 
title={Conference Digest}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-310}, 
abstract={full conference PDF}, 
keywords={Space vehicles;Mars;Moon;Robots;Conferences;Space missions;Legged locomotion}, 
doi={10.1109/AERO47225.2020.9172613}, 
ISSN={1095-323X}, 
month={March},}
@ARTICLE{7839172, 
author={}, 
journal={ISO/IEC/IEEE FDIS P24765:2016(E), January 2017}, 
title={ISO/IEC/IEEE Draft Systems and Software Engineering - Vocabulary}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-568}, 
abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.}, 
keywords={IEEE Standards;IEC Standards;ISO Standards;Dictionaries;Terminology;Software engineering}, 
doi={}, 
ISSN={}, 
month={Jan},}
@ARTICLE{9734792, 
author={Cofer, Darren and Amundson, Isaac and Babar, Junaid and Hardin, David and Slind, Konrad and Alexander, Perry and Hatcliff, John and Robby and Klein, Gerwin and Lewis, Corey and Mercer, Eric and Shackleton, John}, 
journal={IEEE Security & Privacy}, 
title={Cyberassured Systems Engineering at Scale}, 
year={2022}, 
volume={20}, 
number={3}, 
pages={52-64}, 
abstract={Our team has developed a model-based systems engineering environment that integrates formal methods at all levels of system design. Our methodology and tools enable systems engineers to address cybersecurity concerns early in the development of complex high-assurance systems.}, 
keywords={Computational modeling;Codes;Modeling;Contracts;Analytical models;Software;Computer architecture}, 
doi={10.1109/MSEC.2022.3151733}, 
ISSN={1558-4046}, 
month={May},}
@INPROCEEDINGS{9283630, 
author={Lahiri, Shuvendu K. and Lal, Akash and Gopinath, Sridhar and Nutz, Alexander and Levin, Vladimir and Kumar, Rahul and Deisinger, Nate and Lichtenberg, Jakob and Bansal, Chetan}, 
booktitle={2020 Formal Methods in Computer Aided Design (FMCAD)}, 
title={Angelic Checking within Static Driver Verifier: Towards high-precision defects without (modeling) cost}, 
year={2020}, 
volume={}, 
number={}, 
pages={169-178}, 
abstract={Microsoft's Static Driver Verifier (SDV) pioneered the use of software model checking for ensuring that device drivers correctly use operating system (OS) APIs. However, the verification methodology has been difficult to extend in order to support either (a) new classes of drivers for which SDV does not already have a harness and stubs, or (b) memory-corruption properties. Any attempt to apply SDV out-of-the-box results in either false alarms due to the lack of environment modeling, or scalability issues when finding deeply nested bugs in the presence of a very large number of memory accesses. In this paper, we describe our experience designing and shipping a new class of checks known as angelic checks through SDV with the aid of angelic verification (AV) [1] technology, over a period of 4 years. AV pairs a precise inter-procedural assertion checker with automatic inference of likely specifications for the environment. AV helps compensate for the lack of environment modeling and regains scalability by making it possible to find deeply nested bugs, even for complex memory-corruption properties. These new rules have together found over a hundred confirmed defects during internal deployment at Microsoft, including several previously unknown high-impact potential security vulnerabilities. AV considerably increases the reach of SDV, both in terms of drivers as well as rules that it can support effectively.}, 
keywords={Scalability;Computer bugs;Writing;Tools;Software;Space exploration;Security}, 
doi={10.34727/2020/isbn.978-3-85448-042-6_24}, 
ISSN={2708-7824}, 
month={Sep.},}
@ARTICLE{4152660, 
author={}, 
journal={IEEE Unapproved Std P11073-00101/D02J, Feb 2007}, 
title={Unapproved IEEE Draft Guide for Health Informatics-Point-Of-Care Medical Device Communication-Technical Report-Guidelines for the Use of RF Wireless Technology}, 
year={2007}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{9504763, 
author={Moghadam, Mahshid Helali and Hamidi, Golrokh and Borg, Markus and Saadatmand, Mehrdad and Bohlin, Markus and Lisper, Björn and Potena, Pasqualina}, 
booktitle={2021 IEEE Congress on Evolutionary Computation (CEC)}, 
title={Performance Testing Using a Smart Reinforcement Learning-Driven Test Agent}, 
year={2021}, 
volume={}, 
number={}, 
pages={2385-2394}, 
abstract={Performance testing with the aim of generating an efficient and effective workload to identify performance issues is challenging. Many of the automated approaches mainly rely on analyzing system models, source code, or extracting the usage pattern of the system during the execution. However, such information and artifacts are not always available. Moreover, all the transactions within a generated workload do not impact the performance of the system the same way, a finely tuned workload could accomplish the test objective in an efficient way. Model-free reinforcement learning is widely used for finding the optimal behavior to accomplish an objective in many decision-making problems without relying on a model of the system. This paper proposes that if the optimal policy (way) for generating test workload to meet a test objective can be learned by a test agent, then efficient test automation would be possible without relying on system models or source code. We present a self-adaptive reinforcement learning-driven load testing agent, RELOAD, that learns the optimal policy for test workload generation and generates an effective workload efficiently to meet the test objective. Once the agent learns the optimal policy, it can reuse the learned policy in subsequent testing activities. Our experiments show that the proposed intelligent load test agent can accomplish the test objective with lower test cost compared to common load testing procedures, and results in higher test efficiency.}, 
keywords={Analytical models;Automation;Transfer learning;Decision making;Reinforcement learning;Knowledge representation;Evolutionary computation;performance testing;load testing;workload generation;reinforcement learning;autonomous testing}, 
doi={10.1109/CEC45853.2021.9504763}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7943664, 
author={Bhatt, Devesh and Hall, Brendan and Murugesan, Anitha and Oglesby, David and Bush, Eric and Engstrom, Eric and Mueller, Joseph and Pelican, Michael}, 
booktitle={2017 IEEE Aerospace Conference}, 
title={Opportunities and challenges for formal methods tools in the certification of avionics software}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-20}, 
abstract={Formal methods tools, whose underlying principles are based on mathematics and formal logic, are considered one of the most effective and rigorous means of verifying system properties and assuring the absence of undesirable system behavior. The use of such tools seem to squarely fit the needs of those aiming to develop and certify avionic software as per the DO-178C standard, the set of objectives laid out by FAA to achieve a high level of confidence on the systems. However, our recent work on a NASA-funded research project revealed that there are practical considerations and additional complexities involved in using formal method tools to provide the level of assurance as exemplified by the DO-178C. In this paper we discuss one of the key concerns with formal tools: its soundness — the characteristic of a tool to never permit the verified system property be declared true when it is actually not true. We explored two major classes of formal methods tools — namely model checkers and static analyzers — and observed several threats to their soundness such as tool fallacies and failure modes that could lead to misplaced confidence in the verified system. We present various strategies to mitigate them, including an assurance case framework to verify that potential risks are all mitigated. The intent of this paper is not to discourage but encourage scrupulous use of formal tools to certify critical avionic software by being wary of the subtle but serious issues that may be overlooked.}, 
keywords={Tools;Software;Analytical models;Model checking;Aerospace electronics;Safety;Algorithm design and analysis}, 
doi={10.1109/AERO.2017.7943664}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{4599483, 
author={King, Paul and Smith, Craig}, 
booktitle={Agile 2008 Conference}, 
title={Technical lessons Learned Turning the Agile Dials to Eleven!}, 
year={2008}, 
volume={}, 
number={}, 
pages={233-238}, 
abstract={This report outlines technical lessons learnt by about 20 of Australiapsilas most experienced agile specialists over several years across several projects within an organization which aggressively applied the agile practices with much success. In these projects the agile dials were cranked to eleven to achieve very high levels of quality. Most of the specialists involved believe that they produced the highest quality software of their careers with some of the highest productivity they have ever experienced.}, 
keywords={Production;Radiation detectors;Complexity theory;Testing;Libraries;Productivity;Writing;Agile;Coverage;Mocking;TDD;Pair-programming}, 
doi={10.1109/Agile.2008.15}, 
ISSN={}, 
month={Aug},}
@ARTICLE{4040125, 
author={}, 
journal={IEEE Std P1175.2/D8.0}, 
title={Unapproved IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections (Superseded by P1175.2_D11.2)}, 
year={2006}, 
volume={}, 
number={}, 
pages={}, 
abstract={}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@INBOOK{6354029, 
author={Jajszczyk, Andrzej}, 
booktitle={A Guide to the Wireless Engineering Body of Knowledge (WEBOK)}, 
title={Fundamental Knowledge}, 
year={2012}, 
volume={}, 
number={}, 
pages={265-282}, 
abstract={This chapter contains sections titled: Introduction Electrical and RF Engineering Communication Engineering Engineering Management ]]>}, 
keywords={Microwave filters;Radio frequency;RLC circuits;Filtering theory;Wireless communication;Band-pass filters;Microwave circuits}, 
doi={10.1002/9781118444221.ch7}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781118444245}, 
url={https://ieeexplore.ieee.org/document/6354029},}
@BOOK{9101062, 
author={Rao, Sudhakar and Shafai, Lotfollah and Sharma, Satish}, 
booktitle={Handbook of Reflector Antennas and Feed Systems Volume III: Applications of Reflectors}, 
year={2013}, 
volume={}, 
number={}, 
pages={}, 
abstract={This is the first truly comprehensive and most up-to-date handbook available on modern reflector antennas and feed sources for diversified space and ground applications. There has never been such an all-encompassing reflector handbook in print, and no currently available title offers coverage of such recent research developments. The Handbook consists of three volumes. Volume III focuses on the range of reflector antenna applications, including space, terrestrial, and radar. The intent of this book volume is to provide practical applications and design information on reflector antennas used for several communications systems. This book covers recent developments of reflector antennas used for satellite communications, terrestrial communications, and remote sensing applications. New subjects are introduced for the first time, including satellite antennas, Terahertz antennas, PIM, multipaction, corona, deployable mesh reflector antennas, and mechanical aspects of reflector antennas. In addition, this book contains a separate topic on integrated feed assembly for reflector antennas covering analysis, design, fabrication, and test.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781608075201}, 
url={https://ieeexplore.ieee.org/document/9101062},}
@INPROCEEDINGS{4813952, 
author={Borchardt, Frank}, 
booktitle={2009 IET Smart Metering - Making It Happen}, 
title={Meeting the operational and logistical challenges of smart meter roll-out - the European experience (“It's not just hanging meters on walls”)}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-20}, 
abstract={A collection of slides on meeting the operational and logistical challenges of smart meter roll-out by Frank Borchardt, was given.}, 
keywords={}, 
doi={}, 
ISSN={0537-9989}, 
month={Feb},}
@ARTICLE{9933038, 
author={Zamprogno, Lucas and Hall, Braxton and Holmes, Reid and Atlee, Joanne M}, 
journal={IEEE Transactions on Software Engineering}, 
title={Dynamic Human-in-the-Loop Assertion Generation}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-15}, 
abstract={Test cases use assertions to check program behaviour. While these assertions may not be complex, they are themselves code that must be written correctly in order to determine whether a test case should pass or fail. We claim that most test assertions are relatively repetitive and straight-forward, making their construction well suited to automation and that this automation can reduce developer effort while improving assertion quality. Examining 33,873 assertions from 105 projects revealed that developer-written assertions fall into twelve high-level categories, confirming that the vast majority (>90%) of test assertions are fairly simple in practice. We created AutoAssert, a human-in-the-loop tool to fit naturally into a developer's test-writing workflow by automatically generating assertions for JavaScript and TypeScript test cases. A developer invokes AutoAssert by identifying the variable they want validated; AutoAssert uses dynamic analysis to generate assertions relevant for this variable and its runtime values, injecting the assertions into the test case for the developer to accept, modify, delete. Comparing AutoAssert's assertions to those written by developers, we found that the assertions generated by AutoAssert are the same kind of assertion as was written by developers 84% of the time in a sample of over 1,000 assertions. Additionally we validated the utility of AutoAssert-generated assertions with 17 developers who found the majority of generated assertions to be useful and expressed considerable interest in using such a tool for their own projects.}, 
keywords={Complexity theory;Codes;Semantics;Human in the loop;Testing;Runtime;Libraries}, 
doi={10.1109/TSE.2022.3217544}, 
ISSN={1939-3520}, 
month={},}
@INPROCEEDINGS{6581637, 
author={Neufeldt, Holger and Stanzel, Stefan}, 
booktitle={2013 14th International Radar Symposium (IRS)}, 
title={An operational WAM in frankfurt airspace}, 
year={2013}, 
volume={2}, 
number={}, 
pages={561-566}, 
abstract={Extending the capacity of Frankfurt airport with an additional runway to the north of the existing runway system, the surveillance capability in Frankfurt Terminal Maneuvering Area (TMA) had to be adapted as well. Based on their surveillance strategy, the DFS invested in multilateration technology to establish a Precision Approach Monitor (PAM) system and integrate it into the existing surveillance infrastructure. After a phase of thorough planning and preparation and an open tender process, Thales was contracted to implement the PAM FRA system which is now going into operation. This paper reports the successful implementation and testing of an operational WAM system in one of the most congested airspaces of the world. Transponder anomalies found as well as methods and strategies to achieve required performances are presented.}, 
keywords={Surveillance;Airports;Accuracy;Radar tracking;Transponders;Receivers}, 
doi={}, 
ISSN={2155-5753}, 
month={June},}
@INBOOK{6671244, 
author={Jacobs, Stuart}, 
booktitle={Security Management of Next Generation Telecommunications Networks and Services}, 
title={Index}, 
year={2014}, 
volume={}, 
number={}, 
pages={365-373}, 
abstract={No abstract.}, 
keywords={}, 
doi={10.1002/9781118741580.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781118741665}, 
url={https://ieeexplore.ieee.org/document/6671244},}
@INPROCEEDINGS{9843626, 
author={Khan, Shaheer and Mukherji, Deep and Lawler, Christopher and Kruger, Andrew and Voskanian, Vicken and Schellpfeffer, Maria and Alibay, Farah and Hwangpo, Nari and Weise, Tim}, 
booktitle={2022 IEEE Aerospace Conference (AERO)}, 
title={Flight Rule Design, Implementation, Verification, and Validation for the Psyche Mission}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-11}, 
abstract={NASA Jet Propulsion Lab (JPL)'s upcoming mission Psyche will begin its journey to the asteroid (16) Psyche in late 2022 in an effort to better understand its origins and, in turn, better understand our own. Operating the spacecraft safely will require the dedicated efforts of a small team that understands the spacecraft's operational constraints, as well as a set of powerful spacecraft models designed to catch command errors that can pose risks to mission success. One of the responsibilities of the operations team is to ensure adherence to a set of Flight Rules written by spacecraft and instrument experts that are designed to mitigate these risks. Psyche's innovations in Flight Rule design principles and advancements in the tools and processes used to implement and check Flight Rules are discussed. A comparison of Psyche's approach to Flight Rules to other JPL missions will provide lessons learned for future missions that must perform constraint checking during operations. Flight Rule development faces several major challenges. First, Flight Rule developers must work with Subject Matter Experts (SME) to write the rules in a way that captures the intent of the constraint in a straightforward, enforceable manner. Second, software developers must correctly interpret Flight Rules into code so that it meets the original intent of the SME. Finally, a means must be provided for SMEs to validate Flight Rule implementations without requiring them to understand the underlying software. Innovative processes intended to efficiently close the loop between stakeholders and software developers are described, such as the use of test-driven development to provide stakeholders with easy-to-review implementations. New guidelines for Flight Rule writing, designed to address these challenges, are described for future missions to adopt and build upon. All missions must perform detailed constraint checking, so a comparison of Psyche's approach to some of these items to the approaches taken by other missions such as Dawn, M2020, and Europa Clipper is done, specifically to examine SME-developer communication, tools used, and development process. Lessons learned from this comparison are be provided. Psyche Mission System has a variety of new and heritage tools that improve the Flight Rule validation and checking process. Psyche developed a powerful, new tool called RandSEQ and made significant improvements to OctopusJam, two valuable tools that aid the development of Flight Rule unit tests. Advancements in the models and processes for performing sequence validation with SEQuence GENerator (SEQGEN), the primary, high-heritage tool used for automated Flight Rule checks on Psyche, are described. The development of new software and the advancements to existing software put Psyche at the forefront of Flight Rule technology.}, 
keywords={Space vehicles;Technological innovation;NASA;Writing;Propulsion;Software;Solar system}, 
doi={10.1109/AERO53065.2022.9843626}, 
ISSN={1095-323X}, 
month={March},}
@INPROCEEDINGS{5381934, 
author={Xin, Chen}, 
booktitle={2009 International Conference on Wireless Networks and Information Systems}, 
title={Wireless Communications Trends}, 
year={2009}, 
volume={}, 
number={}, 
pages={278-281}, 
abstract={This paper reviews the wireless communications roadmap, and discusses the trends of wireless communications, including: higher and higher data rates, ubiquity of wireless devices, smart antennas, faster, smaller, cheaper hardware, frequency congestion, and multiple-input, multiple-output systems. Finally, this paper discusses the 4G wireless evolution.}, 
keywords={Wireless communication;Internet telephony;Cable TV;Communication cables;Land mobile radio;Business;Cellular phones;Web and internet services;Hardware;Wires;wireless communication;4G;telecommunication;evolution}, 
doi={10.1109/WNIS.2009.53}, 
ISSN={}, 
month={Dec},}
@BOOK{9100440, 
author={Miceli, Andrew}, 
booktitle={Wireless Technicians Handbook, Second Edition}, 
year={2003}, 
volume={}, 
number={}, 
pages={}, 
abstract={This new second edition of the Artech House classic, Wireless Technician's Handbook applies up-to-date knowledge of wireless communications formats to the real-world situations you encounter everyday. Featuring brand new material on such critical technologies as GPRS, EDGE, CDMA-2000, and WCDMA, this single, easy-to-understand volume collects the comprehensive information that is essential for your work in the field today. Covering dBm versus Watts, the differences between analog, TDMA and CDMA, as well as the evolution to 2.5 and 3G standards such as EDGE, WCDMA and CDMA-2000, and testing of handsets and base stations, this handy resource explains all first-tier wireless formats in clear, concise language, helping you to become more knowledgeable and productive. The book focuses on testing concepts and procedures, and types of testing equipment, including spectrum analyzers and power meters and their applications. Moreover, it reviews AMPS, CDMA, IS-136 and GSM, and includes important charts and screen shots from common test instruments. An invaluable reference for the field-based technician, the book also serves as an excellent guide for non-technical wireless professionals.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580537216}, 
url={https://ieeexplore.ieee.org/document/9100440},}
@INPROCEEDINGS{9095732, 
author={Dou, Jinfeng and Cao, Jiabao and Li, Xin and Wang, Lijuan and Tang, Shuya}, 
booktitle={2020 IEEE 5th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA)}, 
title={Big Data Oriented Light-Load Embedded Performance Modeling}, 
year={2020}, 
volume={}, 
number={}, 
pages={476-481}, 
abstract={With increasing development of big data, the performance assessment and optimization face with a big challenge. The traditional methods widely use delivery-testing-analysis-solving (DTAS) ring. In big data area, big data environment is necessary for the testing phase in DTAS, which results in the big cost in both time and hardware. This paper proposes the big data oriented light-load embedded performance modeling. It ascertains the performance criteria to set the Capacity and Performance (C&P) factors. These factors will be embedded into the software with an on-off switch during the architecture, design and developing phases before DTAS phase. After the software coding done with embedded C&P factors, a small traffic load is run to collect the C&P data. The collected data will be used for the performance bottleneck finding, performance optimization, and forecasting the capacity and performance for various customers' scenarios. Since the data easily help locate the issue, the required running traffic is small, and the problem solving is done before the traditional DTAS, this study is more suitable for the big data application. It can save more than 50% of time, decrease the software development efforts, and reduce the lab resources occupation. Finally, the proposed method is employed in the real prototype of an Internet of Things application, obtains the better capacity and performance, and the experiment data verify its effectiveness.}, 
keywords={Software;Testing;Data models;Big Data;Optimization;Telecommunication traffic;Load modeling;Big data;capacity and performance;light-load;performance modeling;performance optimization}, 
doi={10.1109/ICCCBDA49378.2020.9095732}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{1316702, 
author={Sprinkle, J.}, 
booktitle={Proceedings. 11th IEEE International Conference and Workshop on the Engineering of Computer-Based Systems, 2004.}, 
title={Improving CBS tool development with technological spaces}, 
year={2004}, 
volume={}, 
number={}, 
pages={218-224}, 
abstract={The complexity of computer based systems (CBSs) requires that multiple levels of abstraction be available to a designer in order to facilitate their formal specification. Generating final executable code from the model of the system is preferred to hand-coding the implementation, but this is seldom done in one step-usually there are several cascading transformations that eventually result in the executable system. We explain how the concept of the technological space (TS) can be used to define and describe the layers between cascading transformations, and the transformations themselves. TSs are also shown as a categorization that better distinguishes between a domain and the technology used to store information in a domain.}, 
keywords={Space technology;Software engineering;Software systems;Systems engineering and theory;Computer architecture;Software performance;Programming;Design engineering;Object oriented modeling;Large-scale systems}, 
doi={10.1109/ECBS.2004.1316702}, 
ISSN={}, 
month={May},}
@ARTICLE{4447435, 
author={}, 
journal={IEEE Unapproved Draft Std P2600_D33b, Feb 2008}, 
title={IEEE Standard for Information Technology: Hardcopy Device and System Security}, 
year={2008}, 
volume={}, 
number={}, 
pages={1-173}, 
abstract={This standard defines security requirements (all aspects of security including but not limited to authentication, authorization, privacy, integrity, device management, physical security, and information security) for manufacturers, users, and others on the selection, installation, configuration, and usage of hardcopy devices (HCDs) and systems, including printers, copiers, and multifunction devices (MFDs), and the computer systems that support these devices. This standard identifies security exposures for these HCDs and systems, and instructs manufacturers and software developers on appropriate security capabilities to include in their devices and systems, and instructs users on appropriate ways to use these security capabilities.}, 
keywords={Standards;Security;IEEE Standards;Best practices;Patents;Licenses;Certification;2600-2008;all-in-one;copier;facsimile;fax;hardcopy device;HCD;information security;MFD;MFP;multifunction device;multifunction product;printer;scanner}, 
doi={}, 
ISSN={}, 
month={Dec},}
@INBOOK{5273597, 
author={Hargrave, Frank}, 
booktitle={Hargrave's Communications Dictionary}, 
title={Index}, 
year={2001}, 
volume={}, 
number={}, 
pages={849-915}, 
abstract={}, 
keywords={Dictionaries;Indexes;Communication systems}, 
doi={10.1109/9780470544822.index}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9780470544822}, 
url={https://ieeexplore.ieee.org/document/5273597},}
@INPROCEEDINGS{9371550, 
author={Badaroux, Marie and Pétrot, Frédéric}, 
booktitle={2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)}, 
title={Arbitrary and Variable Precision Floating-Point Arithmetic Support in Dynamic Binary Translation}, 
year={2021}, 
volume={}, 
number={}, 
pages={325-330}, 
abstract={Floating-point hardware support has more or less been settled 35 years ago by the adoption of the IEEE 754 standard. However, many scientific applications require higher accuracy than what can be rep-resented on 64 bits, and to that end make use of dedicated arbitrary precision software libraries. To reach a good performance/accuracy trade-off, developers use variable precision, requiring e.g. more accuracy as the computation progresses. Hardware accelerators for this kind of computations do not exist yet, and independently of the actual quality of the underlying arithmetic computations, defining the right instruction set architecture, memory representations, etc, for them is a challenging task. We investigate in this paper the support for arbitrary and variable precision arithmetic in a dynamic binary translator, to help gain an insight of what such an accelerator could provide as an interface to compilers, and thus programmers. We detail our design and present an implementation in QEMU using the MPRF library for the RISC-V processor1.}, 
keywords={Software libraries;Production;Hardware;Space exploration;Task analysis;Standards;Testing;System-Level Simulation;Dynamic Binary Translation;Arbitrary Precision Floating-Point}, 
doi={}, 
ISSN={2153-697X}, 
month={Jan},}
@ARTICLE{6770564, 
author={DeDuck, Peter F. and Johnson, Steven R.}, 
journal={AT&T Technical Journal}, 
title={The FT-2000 OC-48 lightwave system}, 
year={1992}, 
volume={71}, 
number={1}, 
pages={14-22}, 
abstract={Next generation terrestrial lightwave terminals must do more than transport digital information from one location to another. FT-2000, AT&T's newest high-capacity lightwave transmission system, is designed to meet the needs of customers into the next century. It combines a flexible hardware platform and a powerful software-based architecture. As an intelligent lightwave system, FT-2000 can operate in sophisticated self-healing networks, and is managed by an advanced control system that simplifies installing, provisioning, monitoring, and maintaining it. It is fully compliant with the American National Standards Institute (ANSI) optical interface standard, the Synchronous Optical Network (SONET). We explore the broad range of applications and customer needs that drove the specification of FT-2000, and present the architectural solution that achieves the flexibility to meet those specifications.}, 
keywords={}, 
doi={10.1002/j.1538-7305.1992.tb00143.x}, 
ISSN={8756-2324}, 
month={Jan},}
@ARTICLE{4559609, 
author={}, 
journal={IEEE Unapproved Draft Std P11073-00101/D5, Jun 2008}, 
title={IEEE Draft Health Informatics - Point-Of-Care Medical Device Communication - Technical Report - Guidelines for the Use of RF Wireless Technology}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={The following Guidance document addresses the use of radio frequency 1 (RF) wireless technology for the transport of medical data both to and from point-of-care (PoC) medical devices. The context of such wireless medical data transport can range from home- or mobile-based healthcare to in hospital ambulatory and stationary situations. The intent of the guidance document is to be global with respect to wireless spectrum and equipment, although working group participation and expertise have favored detail of scenarios from the US. At the time of this Guidance document several applicable RF wireless technologies exist with a range of capabilities and characteristics, and in different stages of maturity, standardization, and adoption in healthcare. It is recognized that RF technologies are rapidly evolving, and new options may become available (or sufficiently established) after the publication of this Guidance document. The recommendations, therefore, avoid being overly prescriptive and instead attempt to assist medical device manufacturers, wireless equipment manufacturers, healthcare providers, government agencies and any other end-user of this document to make reasonable judgments regarding performance and practical implementation of wireless solutions. The Guidance document defines specific use cases to estimate, compare, and contrast performance of known technologies operating on wireless personal area (WPAN), wireless local area (WLAN), wireless metropolitan area (WMAN), and wireless wide area (WWAN) networks. Major considerations are 1) the quality-of-service (QoS) requirements (reliability, latency, priority, bandwidth) associated with the data being transported, 2) the expected performance (power, link range, throughput, link establishment and maintenance) of the wireless technology, and 3) the specific needs and resources of the end user. Related issues include network architecture, EMI/EMC, coexistence with other data streams, security, cost, power consumption, and technology configurability. Performance summaries for specific wireless technologies that support defined use cases are not intended as an endorsement of optimal solution because different needs, resources, sizes, and environments cannot be comprehensively addressed. This overview document is meant to be a foundation and reference for several follow-on IEEE 11073.3.5.x standards that will profile specific classes of off-the-shelf RF wireless technologies for medical data transport. Importantly, this guidance document is not envisioned to be periodically updated, but instead will act as a source of information for the follow-on IEEE 11073-0305.x standards that will supplant it. Periodic updates will be performed on the IEEE 11073-0305.x standards only.}, 
keywords={}, 
doi={}, 
ISSN={}, 
month={},}
@ARTICLE{9509755, 
author={Cheng, Lin and Pan, Peitian and Zhao, Zhongyuan and Ranjan, Krithik and Weber, Jack and Veluri, Bandhav and Ehsani, Seyed Borna and Ruttenberg, Max and Jung, Dai Cheol and Ivanov, Preslav and Richmond, Dustin and Taylor, Michael B. and Zhang, Zhiru and Batten, Christopher}, 
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
title={A Tensor Processing Framework for CPU-Manycore Heterogeneous Systems}, 
year={2022}, 
volume={41}, 
number={6}, 
pages={1620-1635}, 
abstract={Future CPU-manycore heterogeneous systems can provide high peak throughput by integrating thousands of simple, independent, energy-efficient cores in a single die. However, there are two key challenges to translating this high peak throughput into improved end-to-end workload performance: 1) manycore co-processors rely on simple hardware putting significant demands on the software programmer and 2) manycore co-processors use in-order cores that struggle to tolerate long memory latencies. To address the manycore programmability challenge, this article presents a dense and sparse tensor processing framework based on PyTorch that enables domain experts to easily accelerate off-the-shelf workloads on CPU-manycore heterogeneous systems. To address the manycore memory latency challenge, we use our extended PyTorch framework to explore the potential for decoupled access/execute (DAE) software and hardware mechanisms. More specifically, we propose two software-only techniques, naïve-software DAE and systolic-software DAE, along with a lightweight hardware access accelerator to further improve area-normalized throughput. We evaluate our techniques using a combination of PyTorch operator microbenchmarking and real-world PyTorch workloads running on a detailed register-transfer-level model of a 128-core manycore architecture. Our evaluation on three real-world dense and sparse tensor workloads suggests these workloads can achieve approximately 2– $6\times $ performance improvement when scaled to a future 2000-core CPU-manycore heterogeneous system compared to an 18-core out-of-order CPU baseline, while potentially achieving higher area-normalized throughput and improved energy efficiency compared to general-purpose graphics processing units.}, 
keywords={Computer architecture;Software;Hardware;Throughput;Multicore processing;Tensors;Central Processing Unit;Accelerator architectures;open source software;parallel programming;software libraries}, 
doi={10.1109/TCAD.2021.3103825}, 
ISSN={1937-4151}, 
month={June},}
@INPROCEEDINGS{10000806, 
author={Kozak, Ilona and Berko, Andrii}, 
booktitle={2022 IEEE 17th International Conference on Computer Sciences and Information Technologies (CSIT)}, 
title={Three-module framework for automated software testing}, 
year={2022}, 
volume={}, 
number={}, 
pages={454-457}, 
abstract={Different types of automated test frameworks are used for different purposes, i.e., to investigate features of the interface, functionality, etc. Each of these frameworks has different approaches to the testing process, and therefore, own advantages and disadvantages. Repositories seems to be helpful tool while developing various types of software testing systems. Such software testing system should meet business requirements, as well as user, functional, and non-functional requirements. Such requirements can be grouped into technological, financial, ergonomic, time, etc. The designed and developed three-module framework for automated software testing was uploaded to the repository and a sub-project was prepared.}, 
keywords={Software testing;Computer languages;Ergonomics;Random access memory;Software;Selenium;Information technology;automated test framework;test approaches;testing tools;remote storage}, 
doi={10.1109/CSIT56902.2022.10000806}, 
ISSN={2766-3639}, 
month={Nov},}
@BOOK{9106123, 
author={Frey, Robert}, 
booktitle={Successful Proposal Strategies for Small Businesses: Using Knowledge Management to Win Government, Private-Sector, and International Contracts, Fifth Edition}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
abstract={Winning new business presents significant challenges. The new, Fifth Edition of this perennial bestseller updates and expands upon previous editions. The result is the ultimate resource for small and mid-sized businesses, as well as non-profit organizations and public-sector agencies, looking to achieve effective, efficient, and disciplined business development, proposal development, and knowledge management (KM) processes that in turn support winning new business. This popular book and its companion CD-ROM are highly accessible, self-contained desktop references developed to be informative, highly practical, and easy to use. Among the extensive array of new material, the Fifth Edition covers how to establish an internal rapid-response task order proposal "engine" for GWACs and ID/Iqs, prepare for successful graduation from the U.S. Small Business Administration 8(a) Program, and succeed in the world of very small businesses.The CD-ROM included features useful proposal templates in Adobe Acrobat, platform-independent format; HTML pointers to Small Business Web Sites; a comprehensive, fully searchable listing Proposal and Contract Acronyms; and a sample architecture for a knowledge base or proposal library.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781596932272}, 
url={https://ieeexplore.ieee.org/document/9106123},}
@ARTICLE{9796038, 
author={Lei, Zhanyao and Chen, Yixiong and Yang, Yang and Xia, Mingyuan and Qi, Zhengwei}, 
journal={IEEE Transactions on Software Engineering}, 
title={Bootstrapping Automated Testing for RESTful Web Services}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-19}, 
abstract={Modern RESTful services expose RESTful APIs to integrate with diversified applications. Most RESTful API parameters are weakly typed, which greatly increases the possible input value space. Weakly-typed parameters pose difficulties for automated testing tools to generate effective test cases to reveal web service defects related to parameter validation. We call this phenomenon the type collapse problem. To remedy this problem, we introduce FET (Format-encoded Type) techniques, including the FET, the FET lattice, and the FET inference to model fine-grained information for API parameters. Inferred FET can enhance parameter validation, such as generating a parameter validator for a certain RESTful server. Enhanced by FET techniques, automated testing tools can generate targeted test cases. We demonstrate Leif, a trace-driven fuzzing tool, as a proof-of-concept implementation of FET techniques. Experiment results on 27 commercial services show that FET inference precisely captures documented parameter definitions, which helps Leif discover 11 new bugs and reduce $72\% - 86\%$ fuzzing time compared to state-of-the-art fuzzers. Leveraged by the inter-parameter dependency inference, Leif saves 15% fuzzing time.}, 
keywords={Field effect transistors;Lattices;Testing;Fuzzing;Codes;Web services;Restful API;Fuzz Testing;RESTful Web Service;Type Inference}, 
doi={10.1109/TSE.2022.3182663}, 
ISSN={1939-3520}, 
month={},}
@INPROCEEDINGS{9742049, 
author={Hu, Chi and Ma, Siyou and Yang, Wansheng and Sun, Zhe and Deng, Fei and Yang, Yonghui}, 
booktitle={2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={Software Test Data Reuse Based on Domain Ontology Construction}, 
year={2021}, 
volume={}, 
number={}, 
pages={279-284}, 
abstract={CPS software has more specific defects and coupling defects than traditional computer systems. The testing and validation of CPS software are seriously affected by the cognitive level of individual testers, thus the quality of testing is difficult to guarantee. We believe that the knowledge reuse technology can address this problem to some extent. To achieve that, a construction method of CPS testing knowledge graph was carried out by our team, and the domain ontology of knowledge graph was organized and illustrated. After that, the key technologies such as automatic data extraction and intelligent retrieval were researched. Finally, a test data reuse approach based on knowledge graph was designed. With this approach, we expect to achieve the goal of turning the isolated island of test data into the semantic net of knowledge. Furthermore, increasing the reusability of test knowledge and reducing the defect omissions caused by cognitive deficiency of testers. In the end, this paper concludes the effect of the knowledge reuse-based testing method.}, 
keywords={Knowledge engineering;Semantics;Software quality;Ontologies;Turning;Reliability engineering;Data processing;CPS testing;test data reuse;domain ontology;knowledge graph}, 
doi={10.1109/QRS-C55045.2021.00049}, 
ISSN={2693-9371}, 
month={Dec},}
@BOOK{9100656, 
author={Elbert, Bruce}, 
booktitle={The Satellite Communication Ground Segment and Earth Station Handbook, Second Edition}, 
year={2014}, 
volume={}, 
number={}, 
pages={}, 
abstract={This updated and expanded second edition reflects the state of earth station design and ground segment architecture. From international telephone network gateways to direct broadcast home receivers, today's broad range of ground systems and devices require satellite communication engineers and business managers to have a broad and sound understanding of the design and operating principles of earth stations and ground control facilities. This book explores the delivery end of the satellite link and its relationship to delivery of services. Authored by a leading authority in the field, the book provides engineers and managers with the knowledge they need to devise their own approach to implementing and managing earth stations and the overall ground segment. Readers find practical guidance in an array of critical areas, including: preparing requirements, performing preliminary analyses, reviewing hardware designs, managing the introduction of the overall ground segment, and more.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781608076741}, 
url={https://ieeexplore.ieee.org/document/9100656},}
@ARTICLE{6768354, 
author={McGee, Andrew R. and Vasireddy, S. Rao and Johnson, K. Jeffrey and Chandrashekhar, Uma and Richman, Steven H. and El-Sayed, Mohamed}, 
journal={Bell Labs Technical Journal}, 
title={Dynamic virtual private networks}, 
year={2002}, 
volume={6}, 
number={2}, 
pages={116-135}, 
abstract={Modifications to a virtual private network's (VPN's) topology, security, service provisioning options, or quality of service (QoS) typically require an end-user request to their service provider, whose personnel currently perform the VPN management. This process incurs more provisioning delay and is more costly than user self-provisioning. This paper presents a new service approach and dynamic virtual private network (D-VPN) technology that marries VPNs with directory enabled networking and Web-based subscriber service selection. It places VPN management into the hands of the user to produce instantaneous results, lowering service-provider operations costs, and subsequently reducing the cost to the end user. The paper also describes the target architecture and framework as well as the initial types of services that could be supported by D-VPN technology.3}, 
keywords={}, 
doi={10.1002/bltj.9}, 
ISSN={1538-7305}, 
month={},}
@INBOOK{8671453, 
author={Farhangi, Hassan and Joos, Geza}, 
booktitle={Microgrid Planning and Design: A Concise Guide}, 
title={Front Matter}, 
year={2019}, 
volume={}, 
number={}, 
pages={i-xxxii}, 
abstract={The prelims comprise: Half‐Title Page Title Page Copyright Dedication Contents About the Authors Disclaimer List of Figures List of Tables Foreword Preface Acknowledgments Acronyms and Abbreviations },
keywords={}, 
doi={10.1002/9781119453550.fmatter}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781119453536}, 
url={https://ieeexplore.ieee.org/document/8671453},}
@INPROCEEDINGS{9794080, 
author={Noller, Yannic and Shariffdeen, Ridwan and Gao, Xiang and Roychoudhury, Abhik}, 
booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)}, 
title={Trust Enhancement Issues in Program Repair}, 
year={2022}, 
volume={}, 
number={}, 
pages={2228-2240}, 
abstract={Automated program repair is an emerging technology that seeks to automatically rectify bugs and vulnerabilities using learning, search, and semantic analysis. Trust in automatically generated patches is necessary for achieving greater adoption of program repair. Towards this goal, we survey more than 100 software practitioners to understand the artifacts and setups needed to enhance trust in automatically generated patches. Based on the feedback from the survey on developer preferences, we quantitatively evaluate existing test-suite based program repair tools. We find that they cannot produce high-quality patches within a top-10 ranking and an acceptable time period of 1 hour. The developer feedback from our qualitative study and the observations from our quantitative examination of existing repair tools point to actionable insights to drive program repair research. Specifically, we note that producing repairs within an acceptable time-bound is very much dependent on leveraging an abstract search space representation of a rich enough search space. Moreover, while additional developer inputs are valuable for generating or ranking patches, developers do not seem to be interested in a significant human-in-the-loop interaction.}, 
keywords={Semantics;Computer bugs;Maintenance engineering;Software;Human in the loop;Software engineering;program repair}, 
doi={10.1145/3510003.3510040}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{6201497, 
author={Hui, Zan and Lei, Pan and Yifei, Wang}, 
booktitle={2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)}, 
title={Design & implementation of laboratory information management system based on agile method}, 
year={2012}, 
volume={}, 
number={}, 
pages={2490-2493}, 
abstract={Agile software development is a new methodology of developing high quality software timely when facing significant change; it is convenient for managers to accomplish the collection, disposal, output and other work to the data. There are chart and table functions to the output data and information of laboratories, functions of the output of original and final reports of laboratories, making it affiance to precede the quality control of the data and to help managers arrange analytical plans, staff and other daily work.}, 
keywords={Laboratories;Programming;Information management;Software;Quality assurance;Servers;agile Method;management system;LMIS;agile software development}, 
doi={10.1109/CECNet.2012.6201497}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{140801, 
author={Godon, F. and Al-Khalili, D. and Inkol, R.}, 
booktitle={Proceedings of the 33rd Midwest Symposium on Circuits and Systems}, 
title={A memory controller for mapping an array of circular buffers into a RAM}, 
year={1990}, 
volume={}, 
number={}, 
pages={645-648 vol.2}, 
abstract={A 1.5- mu m CMOS ASIC with a total complexity of over 22000 gates has been developed to generate and keep track of the offsets within 32 circular buffers. It offers a fair arbitration of interleaved read/write operations at a maximum data transfer rate of 20 MHz. Although the device is intended for a specialized electronic warfare system application, the design features incorporated make it generic and suitable for other applications such as communications interfaces in multiprocessor systems.<>}, 
keywords={Random access memory;Read-write memory;Buffer storage;Radar;Very large scale integration;Military computing;Process control;Computer architecture;Counting circuits;Physics computing}, 
doi={10.1109/MWSCAS.1990.140801}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{9789811, 
author={da Silva, Anderson Santos and Schaeffer-Filho, Alberto}, 
booktitle={NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium}, 
title={NetWords: Enabling the Understanding of Network Property Violation Occurrences}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={A clear trend within the context of computer networks is the use of software as an alternative to specialized hardware. The benefit of this trend include an enhancement of flexibility, modularity, and maintainability of network components. Simultaneously, it is challenging to determine if everything is happening correctly in a computer network where software, possibly with bugs, is very present. Research in this field frequently tries to improve network testing with the use of formal verification techniques or network monitoring to detect property violations, such as configuration errors or policy conflicts. However, formal verification by itself cannot detect a property violation that was not anticipated and included in the model. Similarly, network monitoring needs to wait for a property violation to occur to detect it. Consequently, both enhancement efforts fail to achieve a complete result. In this paper, we investigate the problem of guaranteeing the absence of network connectivity property violations by combining the advantages of network monitoring for detecting property violations with the advantages of formal verification to model the network. A highlight related to the success of such a combination is the use of a model based on grammars to capture the communication patterns existing on the network. Our preliminary analysis allows the evaluation of high-level properties such as "Can network component x send HTTP packets?" and the detection of property violations, such as conflicting forwarding rules, as soon they occur in the network.}, 
keywords={Computer bugs;Market research;Software;Computer networks;Hardware;Grammar;Monitoring}, 
doi={10.1109/NOMS54207.2022.9789811}, 
ISSN={2374-9709}, 
month={April},}
@INPROCEEDINGS{9432331, 
author={Sudarsanam, P. and R, Anand and Banerjee, Sumanta and R, Hemanth}, 
booktitle={2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
title={Resilience Test case Automation for LTE Femtocell Networks}, 
year={2021}, 
volume={}, 
number={}, 
pages={622-628}, 
abstract={The automated test case generation includes the manual test cases, which are converted to automated test cases through executable test scripts. It is more essential to bring out an effective resilience testing for network gateways. The automation testing plays a major role in performing time constrained and skillful work. Moreover, the network gateways will not take part in onetime investment, even when the technology or environment evolves, the test scripts are continuously patched based on the market demands. Thus, the automation testing is remaining as a challenging task to define and test automatically in a cost-effective mode. The developed device is called as femtocell, which helps to effectively densify the network and deliver a great customer experience in small indoor environments. It is checked to meet the real world scenarios under resilience test cases. It is performed by various manual test cases. Moreover, this project automates the manual test cases.}, 
keywords={Technological innovation;Automation;Manuals;Logic gates;Real-time systems;Time factors;Task analysis;Long Term Evolution;Femtocell;Gateway;SAM Client;Wireless Provisioning System}, 
doi={10.1109/ICICCS51141.2021.9432331}, 
ISSN={}, 
month={May},}
@BOOK{9100014, 
author={Liotine, Matthew}, 
booktitle={Mission-Critical Network Planning}, 
year={2003}, 
volume={}, 
number={}, 
pages={}, 
abstract={Whether a terrorist attack, fiber cut, security breach, natural disaster or traffic overload, today's networks must be designed to withstand adverse conditions and provide continuous service. This comprehensive, leading-edge book reveals the techniques and strategies to help you keep enterprise data and voice networks in service under critical circumstances. You learn numerous ways to minimize single points of failure through redundancy and backups, and discover how to select the right networking technologies to improve survivability and performance. This unique and timely resource shows you how to spot vulnerabilities in a network, use the protective features of different technologies for continuity and security, and build survivable network infrastructure. Supported with over 150 illustrations, this handy reference goes far beyond typical books dealing with only disaster recovery, to offer you a complete avoidance approach that proactively implements measures to protect infrastructure and systems from unplanned events.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580535595}, 
url={https://ieeexplore.ieee.org/document/9100014},}
@INPROCEEDINGS{9282802, 
author={Wang, Rui and Artho, Cyrille and Kristensen, Lars Michael and Stolz, Volker}, 
booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)}, 
title={Multi-objective Search for Model-based Testing}, 
year={2020}, 
volume={}, 
number={}, 
pages={130-141}, 
abstract={This paper presents a search-based approach relying on multi-objective reinforcement learning and optimization for test case generation in model-based software testing. Our approach considers test case generation as an exploration versus exploitation dilemma, and we address this dilemma by implementing a particular strategy of multi-objective multi-armed bandits with multiple rewards. After optimizing our strategy using the jMetal multi-objective optimization framework, the resulting parameter setting is then used by an extended version of the Modbat tool for model-based testing. We experimentally evaluate our search-based approach on a collection of examples, such as the ZooKeeper distributed service and PostgreSQL database system, by comparing it to the use of random search for test case generation. Our results show that test cases generated using our search-based approach can obtain more predictable and better state/transition coverage, find failures earlier, and provide improved path coverage.}, 
keywords={Software testing;Software algorithms;Software quality;Tools;Software reliability;Security;Optimization;model-based testing;test case generation;bandit-based methods;multi-objective optimization;genetic algorithm;search-based software testing}, 
doi={10.1109/QRS51102.2020.00029}, 
ISSN={}, 
month={Dec},}
@ARTICLE{9548078, 
author={Berquand, Audrey and Darm, Paul and Riccardi, Annalisa}, 
journal={IEEE Access}, 
title={SpaceTransformers: Language Modeling for Space Systems}, 
year={2021}, 
volume={9}, 
number={}, 
pages={133111-133122}, 
abstract={The transformers architecture and transfer learning have radically modified the Natural Language Processing (NLP) landscape, enabling new applications in fields where open source labelled datasets are scarce. Space systems engineering is a field with limited access to large labelled corpora and a need for enhanced knowledge reuse of accumulated design data. Transformers models such as the Bidirectional Encoder Representations from Transformers (BERT) and the Robustly Optimised BERT Pretraining Approach (RoBERTa) are however trained on general corpora. To answer the need for domain-specific contextualised word embedding in the space field, we propose SpaceTransformers, a novel family of three models, SpaceBERT, SpaceRoBERTa and SpaceSciBERT, respectively further pre-trained from BERT, RoBERTa and SciBERT on our domain-specific corpus. We collect and label a new dataset of space systems concepts based on space standards. We fine-tune and compare our domain-specific models to their general counterparts on a domain-specific Concept Recognition (CR) task. Our study rightly demonstrates that the models further pre-trained on a space corpus outperform their respective baseline models in the Concept Recognition task, with SpaceRoBERTa achieving significant higher ranking overall.}, 
keywords={Modeling;Task analysis;Bit error rate;Training;Data models;Transfer learning;Transformers;Language model;transformers;space systems;concept recognition;requirements}, 
doi={10.1109/ACCESS.2021.3115659}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{6716404, 
author={Kolek, Jozef and Jovanović, Zoran and Šljivić, Nenad and Narančić, Dragan}, 
booktitle={2013 21st Telecommunications Forum Telfor (TELFOR)}, 
title={Adding microMIPS backend to the LLVM compiler infrastructure}, 
year={2013}, 
volume={}, 
number={}, 
pages={1015-1018}, 
abstract={This work describes extending of the LLVM Compiler Infrastructure with the new backend support for microMIPS, which is an architecture from MIPS family of architectures. New backend consists of 16- and 32-bit instructions, out of which 180 of 32-bit instructions are recoded MIPS32 instructions, and 14 of 32-bit instructions are new microMIPS instructions. There are the 39 highly optimized 16-bit instructions.}, 
keywords={Encoding;Registers;Computer architecture;Libraries;Generators;Switches;Computers;Compilers;LLVM;microMIPS}, 
doi={10.1109/TELFOR.2013.6716404}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{271291, 
author={Dai, H. and Choo, M. and Starzyk, J.A.}, 
booktitle={[1992] Proceedings of the 35th Midwest Symposium on Circuits and Systems}, 
title={Noninvasive voltage measurement through an on-chip test structure (IC testing)}, 
year={1992}, 
volume={}, 
number={}, 
pages={340-343 vol.1}, 
abstract={A method to evaluate internal voltages through a built-in test structure is presented. Multiplexers are used to increase accessibility. The test structure does not affect normal operation of the circuit. Individual subcircuits can be tested selectively based on evaluated internal voltages.<>}, 
keywords={Integrated circuit testing;Voltage measurement;Circuit testing;Multiplexing;Large-scale systems;MOSFET circuits;MOS capacitors;Built-in self-test;Performance evaluation;Analog circuits}, 
doi={10.1109/MWSCAS.1992.271291}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6188784, 
author={}, 
booktitle={2012 International Conference on Devices, Circuits and Systems (ICDCS)}, 
title={2012 International Conference on Devices, Circuits and Systems (ICDCS)}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-748}, 
abstract={Provides the entire conference content.}, 
keywords={CMOS integrated circuits;CMOS technology;MESFETs;MOSFETs;Optical imaging;Random access memory;Field programmable gate arrays}, 
doi={10.1109/ICDCSyst.2012.6188784}, 
ISSN={}, 
month={March},}
@BOOK{9100016, 
author={Muller, Nathan}, 
booktitle={LANs to WANs: The Complete Management Guide}, 
year={2003}, 
volume={}, 
number={}, 
pages={}, 
abstract={Empowered by today's high-performance computers interconnected over LANs and WANs, companies are faced with the daunting task of bringing workability to the diversity and complexity of today's data communications landscape. This new, comprehensive resource addresses key network management challenges, showing you how to: tie together incompatible LANs, meld legacy systems and LANs, extend the reach of LANs with wireless links, protect information assets from various disaster scenarios, and consolidate multi-protocol traffic over a single WAN backbone in a way that guarantees appropriate service levels. Moreover, this hands-on guide defines management system requirements for the enterprise, and identifies the tools and expertise companies need to manage systems and networks in wired and wireless environments. You will learn how to design and test networks before implementation, and harden systems and networks against internal and external attacks with effective security administration and the application of specific tools. Putting infrastructure issues into proper perspective, this book will help you succeed in managing advanced communications systems and networks.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580535731}, 
url={https://ieeexplore.ieee.org/document/9100016},}
@BOOK{9100768, 
author={Caballero, Jose and Guimera, Andreu and Hens, Francisco and Segura, Roger}, 
booktitle={SONET/SDH, ATM and ADSL: Installation and Maintenance}, 
year={2003}, 
volume={}, 
number={}, 
pages={}, 
abstract={Service level agreements guaranteeing quality of service have helped your organization to keep old customers and win new ones over. Although it may be easy for the sales department to ink a service level agreement, you have to handle the constant problems of phase fluctuations, jitter, and wander, that threaten the quality of service spelled out in these service level agreements. By showing you how to properly set up a network, test its performance, and troubleshoot any systems glitches, this book is an on-the-job companion that you can turn to time after time to ensure quality network service. This implementation, maintenance, and troubleshooting manual goes beyond overview books and standards guides to give you the technical insight and hands-on knowledge you need to know to deploy today's digital networks and keep them running 24/7. From the principles of digital technology and SDH, to network synchronization and ADSL qualification, this practical resource provides in-depth coverage of the most critical areas. Over 300 illustrations support key topics.}, 
keywords={}, 
doi={}, 
ISSN={}, 
publisher={Artech}, 
isbn={9781580536981}, 
url={https://ieeexplore.ieee.org/document/9100768},}
@INPROCEEDINGS{4063813, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={1}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284569}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4064168, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={3}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284942}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{9905133, 
author={}, 
booktitle={2022 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
title={SCSE 2022 Conference Proceedings}, 
year={2022}, 
volume={5}, 
number={}, 
pages={i-cdii}, 
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.}, 
keywords={}, 
doi={10.1109/SCSE56529.2022.9905133}, 
ISSN={2613-8662}, 
month={Sep.},}
@INPROCEEDINGS{4063976, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={2}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.284735}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4064349, 
author={}, 
booktitle={2006 International Conference on Communications, Circuits and Systems}, 
title={Technical Program of 2006 ICCCAS}, 
year={2006}, 
volume={4}, 
number={}, 
pages={25-85}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICCCAS.2006.285102}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7927913, 
author={}, 
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Table of contents}, 
year={2017}, 
volume={}, 
number={}, 
pages={v-x}, 
abstract={The following topics are dealt with: Presents the table of contents/splash page of the proceedings record.}, 
keywords={}, 
doi={10.1109/ICST.2017.4}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{5501146, 
author={}, 
booktitle={2009 5th Central and Eastern European Software Engineering Conference in Russia (CEE-SECR)}, 
title={[Title page]}, 
year={2009}, 
volume={}, 
number={}, 
pages={i-ii}, 
abstract={The following topics are dealt with: crisis-time distributed systems development; regression test selection technique; agile project management; software project feasibility study; graphical processing units; industrial C/C++ software; video registration and security systems; reliable software development; industrial Java applications; parallel programs; e-government and outsourcing; program reliability; operation-friendly software development; software product management; SaaS concept; SOA testing stack; complex hardware-software systems; UML-model; Microsoft DSL technology; Microsoft.NET micro framework; WBEM/CIM & WS-MAN technology application; and agile Web development.}, 
keywords={}, 
doi={10.1109/CEE-SECR.2009.5501146}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{9318176, 
author={Imran, Hamza Ali and Latif, Usama and Ikram, Ataul Aziz and Ehsan, Maryam and Ikram, Ahmed Jamal and Khan, Waleed Ahmad and Wazir, Saad}, 
booktitle={2020 IEEE 23rd International Multitopic Conference (INMIC)}, 
title={Multi-Cloud: A Comprehensive Review}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In the span of a decade, innovations in cloud computing have led to a new understanding of computing to be used as a utility. Majority of cloud service providers are making the service better and competitive for end-user. Aside from the number of services introduced by these providers, users are feeling uneasy and are unaware of consequences while switching from one service to another. Internal architecture of the cloud makes it difficult for end-users to understand. To overcome this issue a new concept of multi-cloud has been introduced. In multi-cloud technology, we can use multiple clouds from different vendors without platform complexity. Hence summarized, Multi-cloud is the usage of autonomous cloud platforms with one interface which may clue to different administrative and implementation domains. This paper reviews the literature of recently presented solutions and architectures for multi-cloud platforms.}, 
keywords={Cloud Computing;Multi-Cloud;Ubiquitous Computing;High Performance Computing;Anything as a Service}, 
doi={10.1109/INMIC50486.2020.9318176}, 
ISSN={2049-3630}, 
month={Nov},}
@INPROCEEDINGS{8491782, 
author={}, 
booktitle={2018 21st Euromicro Conference on Digital System Design (DSD)}, 
title={Table of contents}, 
year={2018}, 
volume={}, 
number={}, 
pages={5-18}, 
abstract={Presents the table of contents/splash page of the proceedings record.}, 
keywords={}, 
doi={10.1109/DSD.2018.00004}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8449407, 
author={}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
title={Table of contents}, 
year={2018}, 
volume={}, 
number={}, 
pages={5-26}, 
abstract={Presents the table of contents/splash page of the proceedings record.}, 
keywords={}, 
doi={}, 
ISSN={2574-1934}, 
month={May},}
@INPROCEEDINGS{8509742, 
author={}, 
booktitle={2018 40th Electrical Overstress/Electrostatic Discharge Symposium (EOS/ESD)}, 
title={EOS/ESD 2018 Bios}, 
year={2018}, 
volume={}, 
number={}, 
pages={i-xxiv}, 
abstract={Presents an index of the authors whose articles are published in the conference proceedings record.}, 
keywords={}, 
doi={10.23919/EOS/ESD.2018.8509742}, 
ISSN={0739-5159}, 
month={Sep.},}
@INPROCEEDINGS{9241331, 
author={}, 
booktitle={2020 42nd Annual EOS/ESD Symposium (EOS/ESD)}, 
title={Bios}, 
year={2020}, 
volume={}, 
number={}, 
pages={1-27}, 
abstract={Lists the authors included in the conference proceedings.}, 
keywords={}, 
doi={}, 
ISSN={0739-5159}, 
month={Sep.},}
@INPROCEEDINGS{8878768, 
author={}, 
booktitle={2019 IEEE International Conference on System, Computation, Automation and Networking (ICSCAN)}, 
title={Table of contents}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={Presents the cover/table of contents for this issue of the periodical.}, 
keywords={}, 
doi={10.1109/ICSCAN.2019.8878768}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{9169403, 
author={}, 
booktitle={2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)}, 
title={ICBC 2020 Table of Contents}, 
year={2020}, 
volume={}, 
number={}, 
pages={i-xii}, 
abstract={Presents the table of contents/splash page of the proceedings record.}, 
keywords={}, 
doi={10.1109/ICBC48266.2020.9169403}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{9969363, 
author={}, 
booktitle={2022 IEEE 13th International Green and Sustainable Computing Conference (IGSC)}, 
title={Contents}, 
year={2022}, 
volume={}, 
number={}, 
pages={1-189}, 
abstract={Presents the conference table of contents.}, 
keywords={}, 
doi={10.1109/IGSC55832.2022.9969363}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{9169409, 
author={}, 
booktitle={2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)}, 
title={ICBC 2020 Final Program}, 
year={2020}, 
volume={}, 
number={}, 
pages={i-l}, 
abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.}, 
keywords={}, 
doi={10.1109/ICBC48266.2020.9169409}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8715180, 
author={}, 
booktitle={2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)}, 
title={[Front cover]}, 
year={2019}, 
volume={}, 
number={}, 
pages={c1-c126}, 
abstract={The following topics are dealt with: learning (artificial intelligence); integrated circuit design; multiprocessing systems; neural nets; power aware computing; microprocessor chips; field programmable gate arrays; embedded systems; system-on-chip; optimisation.}, 
keywords={}, 
doi={10.23919/DATE.2019.8715180}, 
ISSN={1558-1101}, 
month={March},}
@INPROCEEDINGS{685775, 
author={}, 
booktitle={Proceedings. Fifth International Conference on Software Reuse (Cat. No.98TB100203)}, 
title={Subject index}, 
year={1998}, 
volume={}, 
number={}, 
pages={377-388}, 
abstract={The index contains an entry for all items that appeared in this publication.}, 
keywords={}, 
doi={10.1109/ICSR.1998.685775}, 
ISSN={1085-9098}, 
month={June},}
@INPROCEEDINGS{8719515, 
author={}, 
booktitle={2018 25th Asia-Pacific Software Engineering Conference (APSEC)}, 
title={Table of contents}, 
year={2018}, 
volume={}, 
number={}, 
pages={5-15}, 
abstract={Presents the table of contents/splash page of the proceedings record.}, 
keywords={}, 
doi={10.1109/APSEC.2018.00004}, 
ISSN={2640-0715}, 
month={Dec},}
@INPROCEEDINGS{9226308, 
author={}, 
booktitle={2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
title={Table of Contents}, 
year={2020}, 
volume={}, 
number={}, 
pages={i-xi}, 
abstract={Presents the table of contents/splash page of the proceedings record.}, 
keywords={}, 
doi={10.1109/SEAA51224.2020.00004}, 
ISSN={}, 
month={Aug},}