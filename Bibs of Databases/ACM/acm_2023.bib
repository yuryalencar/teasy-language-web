@inproceedings{10.1145/2072221.2072247,
author = {Solms, Fritz and Edwards, Craig and Paar, Alexander and Gruner, Stefan},
title = {A Domain-Specific Language for URDAD Based Requirements Elicitation},
year = {2011},
isbn = {9781450308786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072221.2072247},
doi = {10.1145/2072221.2072247},
abstract = {Use-Case Responsibility-Driven Analysis and Design (URDAD) is a service-oriented software analysis and design methodology. It is used by requirements engineers to develop technology-neutral, semi-formal platform-independent models (PIM) within the OMG's MDA. In the past, URDAD models were denoted in UML. However, that was tedious and error-prone. The resulting models were often of rather poor quality. In this paper we introduce and discuss a new Domain-Specific Language (DSL) for URDAD. Its meta model is consistent and satisfiable. We show that URDAD DSL specifications are simpler and allow for more complete service contract specifications than their corresponding UML expressions. They also enable traceability and test case generation.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists Conference on Knowledge, Innovation and Leadership in a Diverse, Multidisciplinary Environment},
pages = {224–230},
numpages = {7},
keywords = {service orientation, model driven development, platform independent model, domain specific language, requirements engineering, meta model},
location = {Cape Town, South Africa},
series = {SAICSIT '11}
}

@inproceedings{10.1145/2915970.2916010,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {An Integrated Tool Environment for Experimentation in Domain Specific Language Engineering},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2916010},
doi = {10.1145/2915970.2916010},
abstract = {Domain specific languages (DSLs) are widely used in practice and investigated in software engineering research. But so far, language workbenches do not provide sufficient built-in decision support for language design and improvement. Controlled experiments have the potential to provide appropriate, data-driven decision support for language engineers and researchers to compare different language features with evidence-based feedback. This paper provides an integrated end-to-end tool environment to perform controlled experiments in DSL engineering. The experiment environment is built on the basis and integrated into the language workbench Meta Programming System (MPS). The environment not only supports language design but also all steps of experimentation, i.e., planning, operation, analysis &amp; interpretation, as well as presentation &amp; package. The tool environment is presented by means of a running example experiment comparing the time taken to create system acceptance tests for web applications in two different DSLs.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {20},
numpages = {5},
keywords = {tool support, empirical evaluation, domain specific languages (DSLs), meta programming system (MPS), controlled experiment, language engineering, experimentation},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/2851613.2851749,
author = {Mohr, David and Stefanovic, Darko},
title = {Stella: A Python-Based Domain-Specific Language for Simulations},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851749},
doi = {10.1145/2851613.2851749},
abstract = {We wish to make it easier and quicker to write well-performing scientific simulations that (1) have single-thread performance competitive with low-level languages, (2) use object-oriented programming to properly structure the code, and (3) are very easy to develop. Instead of prototyping in a high-level language and then rewriting in a lower-level language, we created a DSL embedded in Python that is transparently usable, retains some OOP features, compiles to machine code, and executes at speed similar to C.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1952–1959},
numpages = {8},
keywords = {Python, domain-specific languages, scientific simulations},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1007/s00165-016-0359-1,
author = {Keshishzadeh, Sarmen and Mooij, Arjan J.},
title = {Formalizing and Testing the Consistency of DSL Transformations},
year = {2016},
issue_date = {Apr 2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-016-0359-1},
doi = {10.1007/s00165-016-0359-1},
abstract = {A domain specific language (DSL) focuses on the essential concepts in a specific problem domain, and abstracts from low-level implementation details. The development of DSLs usually centers around the meta-model, grammar and code generator, possibly extended with transformations to analysis models. Typically, little attention is given to the formal semantics of the language, whereas this is essential for reasoning about DSL models, and for assessing the correctness of the generated code and analysis models. We argue that the semantics of a DSL should be defined explicitly and independently of any code generator, to avoid all kinds of complexities from low-level implementation details. As the generated analysis models must reflect some of these implementation details, we propose to formalize them separately. To assess the correctness and consistency of the generated code and analysis models in a practical way, we use conformance testing. We extensively illustrate this general approach using specific formalizations for an industrial DSL on collision prevention. We do not aim for a generic semantic model for any DSL, but this specific DSL indicates the potential of a modular semantics to facilitate reuse among DSLs.},
journal = {Form. Asp. Comput.},
month = {apr},
pages = {181–206},
numpages = {26},
keywords = {Semantics, Domain specific language (DSL), Conformance testing, Code generation}
}

@inproceedings{10.1145/3510457.3513078,
author = {Elsner, Daniel and Wuersching, Roland and Schnappinger, Markus and Pretschner, Alexander and Graber, Maria and Dammer, Ren\'{e} and Reimer, Silke},
title = {Build System Aware Multi-Language Regression Test Selection in Continuous Integration},
year = {2022},
isbn = {9781450392266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510457.3513078},
doi = {10.1145/3510457.3513078},
abstract = {At IVU Traffic Technologies, continuous integration (CI) pipelines build, analyze, and test the code for inadvertent effects before pull requests are merged. However, compiling the entire code base and executing all regression tests for each pull request is infeasible due to prohibitively long feedback times. Regression test selection (RTS) aims to reduce the testing effort. Yet, existing safe RTS techniques are not suitable, as they largely rely on language-specific program analysis. The IVU code base consists of more than 13 million lines of code in Java or C/C++ and contains thousands of non-code artifacts. Regression tests commonly operate across languages, using cross-language links, or read from non-code artifacts. In this paper, we describe our build system aware multi-language RTS approach, which selectively compiles and executes affected code modules and regression tests, respectively, for a pull request. We evaluate our RTS technique on 397 pull requests, covering roughly 2,700 commits. The results show that we are able to safely exclude up to 75% of tests on average (no undetected real failures slip into the target branches) and thereby save 72% of testing time, whereas end-to-end CI pipeline time is reduced by up to 63% on average.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice},
pages = {87–96},
numpages = {10},
keywords = {software testing, continuous integration, regression test selection},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIP '22}
}

@inproceedings{10.1145/3460319.3464834,
author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
title = {Empirically Evaluating Readily Available Information for Regression Test Optimization in Continuous Integration},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464834},
doi = {10.1145/3460319.3464834},
abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90% of the failures, we show that practitioners can expect to save on average 84% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {491–504},
numpages = {14},
keywords = {software testing, regression test optimization, machine learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1145/2659118.2659136,
author = {Zhou, Jingang and Yin, Kun},
title = {Automated Web Testing Based on Textual-Visual UI Patterns: The UTF Approach},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2659118.2659136},
doi = {10.1145/2659118.2659136},
abstract = {Automated software testing is the only resort for delivering quality software, since there are usually large test suites to be executed, especially for regression testing. Though many automated testing tools and techniques have been developed, they still do not solve all problems like cost and maintenance, and they can even be brittle in some situations, thus confining their adoption. To address these issues, we develop a pattern-based automated testing framework, called UTF (User-oriented Testing Framework), for Web applications. UTF encodes textual-visual information about and relationships between widgets into a domain specific language for test scripts based on the underlying invariant structural patterns in the DOM, which allows test scripts to be easily created and maintained. In addition, UTF provides flexible extension and customization capabilities to make it adaptable for various Web-application scenarios. Our experiences show UTF can greatly reduce the cost of adopting automated testing and facilitate its institutionalization.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–6},
numpages = {6},
keywords = {user-interface pattern, domain-specific language, automated testing, web application}
}

@article{10.1145/1151695.1151697,
author = {Sinha, Avik and Smidts, Carol},
title = {HOTTest: A Model-Based Test Design Technique for Enhanced Testing of Domain-Specific Applications},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/1151695.1151697},
doi = {10.1145/1151695.1151697},
abstract = {Model-based testing is an effective black-box test generation technique for applications. Existing model-based testing techniques, however, fail to capture implicit domain-specific properties, as they overtly rely on software artifacts such as design documents, requirement specifications, etc., for completeness of the test model. This article presents a technique, HOTTest, which uses a strongly typed domain-specific language to model the system under test. This allows extraction of type-related system invariants, which can be related to various domain-specific properties of the application. Thus, using HOTTest, it is possible to automatically extract and embed domain-specific requirements into the test models. In this article we describe HOTTest, its principles and methodology, and how it is possible to relate domain-specific properties to specific type constraints. HOTTest is described using the example of HaskellDB, which is a Haskell-based embedded domain-specific language for relational databases. We present an example application of the technique and compare the results to some other commonly used Model-based test automation techniques like ASML-based testing, UML-based testing, and EFSM-based testing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
pages = {242–278},
numpages = {37},
keywords = {Haskell, Test case generation, model-based testing, HaskellDB, database-specific test case generation, test generation tools, domain-specific languages, domain-specific testing}
}

@inproceedings{10.1145/3183895.3183897,
author = {Schuts, Mathijs and Hooman, Jozef and Tielemans, Paul},
title = {Industrial Experience with the Migration of Legacy Models Using a DSL},
year = {2018},
isbn = {9781450363556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183895.3183897},
doi = {10.1145/3183895.3183897},
abstract = {Software departments of companies that exist for several decades often have to deal with legacy models. Important business assets have been modelled with tools that are no longer preferred within the company. Manually remodelling these models with a new tool would be too costly. In this paper, we describe an approach to migrate from Rhapsody models to models of another tool. To perform the migration, we created a Domain Specific Language (DSL) that accepts Rhapsody models as instances. A generator of this DSL can then produces model instances for the new tool. To get confidence in the transformation in a pragmatic way, we applied a combination of model learning and equivalence checking. Learning has been applied to both the source code generated by Rhapsody and the code generated by the new tool. The resulting models are compared using equivalence checking.},
booktitle = {Proceedings of the Real World Domain Specific Languages Workshop 2018},
articleno = {1},
numpages = {10},
keywords = {Legacy, Model-based development, Domain specific languages, Model transformation, Tool migration},
location = {Vienna, Austria},
series = {RWDSL2018}
}

@inproceedings{10.1145/2422518.2422521,
author = {Sousa, Gustavo C. M. and Costa, F\'{a}bio M. and Clarke, Peter J. and Allen, Andrew A.},
title = {Model-Driven Development of DSML Execution Engines},
year = {2012},
isbn = {9781450318020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422518.2422521},
doi = {10.1145/2422518.2422521},
abstract = {The combination of domain-specific modeling languages and model-driven engineering techniques hold the promise of a breakthrough in the way applications are developed. By raising the level of abstraction and specializing in building blocks that are familiar in a particular domain, it has the potential to turn domain experts into application developers. Applications are developed as models, which in turn are interpreted at runtime by a specialized execution engine in order to produce the intended behavior. This approach has been successfully applied in different domains, such as communication and smart grid management to execute applications described by models that can be created and changed at runtime. However, each time the approach has to be realized in a different domain, substantial re-implementation has to take place in order to put together an execution engine for the respective DSML. In this paper, we present our work towards a generalization of the approach in the form of a metamodel which captures the domain-independent aspects of runtime model interpretation and allow the definition of domain-specific execution engines.},
booktitle = {Proceedings of the 7th Workshop on Models@run.Time},
pages = {10–15},
numpages = {6},
keywords = {model-driven engineering, models@run.time, domain-specific modeling languages, metamodeling},
location = {Innsbruck, Austria},
series = {MRT '12}
}

@article{10.1145/2693208.2693226,
author = {Bokil, Prasad and Krishnan, Padmanabhan and Venkatesh, R.},
title = {Achieving Effective Test Suites for Reactive Systems Using Specification Mining and Test Suite Reduction Techniques},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2693208.2693226},
doi = {10.1145/2693208.2693226},
abstract = {Failures in reactive embedded systems are often unacceptable. Moreover, effective testing of such systems to detect potential critical failures is a difficult task.We present an automated black box test suite generation technique for reactive systems. The technique is based on dynamic mining of specifications, in form of a finite state machine (FSM), from initial runs. The set of test cases thus produced contain several redundant test cases, many of which are eliminated by a simple greedy test suite reduction algorithm to give the final test suite. The effectiveness of tests generated by our technique was evaluated using five case studies from the reactive embedded domain. Results indicate that a test suite generated by our technique is promising in terms of effectiveness and scalability. While the test suite reduction algorithm removes redundant test cases, the change in effectiveness of test suites due to this reduction is examined in the experimentation.We present our specification mining based test suite generation technique, the test suite reduction technique and results on industrial case studies.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {1–8},
numpages = {8},
keywords = {test suite reduction, black box testing, specification mining}
}

@inproceedings{10.1145/2997364.2997382,
author = {Al-Sibahi, Ahmad Salim and Dimovski, Aleksandar S. and W\k{a}sowski, Andrzej},
title = {Symbolic Execution of High-Level Transformations},
year = {2016},
isbn = {9781450344470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2997364.2997382},
doi = {10.1145/2997364.2997382},
abstract = {Transformations form an important part of developing domain specific languages, where they are used to provide semantics for typing and evaluation. Yet, few solutions exist for verifying transformations written in expressive high-level transformation languages. We take a step towards that goal, by developing a general symbolic execution technique that handles programs written in these high-level transformation languages. We use logical constraints to describe structured symbolic values, including containment, acyclicity, simple unordered collections (sets) and to handle deep type-based querying of syntax hierarchies. We evaluate this symbolic execution technique on a collection of refactoring and model transformation programs, showing that the white-box test generation tool based on symbolic execution obtains better code coverage than a black box test generator for such programs in almost all tested cases.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {207–220},
numpages = {14},
keywords = {program transformation, model transformation, automated white-box test generation, symbolic execution},
location = {Amsterdam, Netherlands},
series = {SLE 2016}
}

@inproceedings{10.1145/3340433.3342825,
author = {Kessel, Marcus and Atkinson, Colin},
title = {A Platform for Diversity-Driven Test Amplification},
year = {2019},
isbn = {9781450368506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340433.3342825},
doi = {10.1145/3340433.3342825},
abstract = {Test amplification approaches take a manually written set of tests (input/output mappings) and enhance their effectiveness for some clearly defined engineering goal such as detecting faults. Conceptually, they can either achieve this in a ``black box'' way using only the initial ``seed'' tests or in a ``white box'' way utilizing additional inputs such as the source code or specification of the software under test. However, no fully black box approach to test amplification is currently available even though they can be used to enhance white box approaches. In this paper we introduce a new approach that uses the seed tests to search for existing redundant implementations of the software under test and leverages them as oracles in the generation and evaluation of new tests. The approach can therefore be used as a stand alone black box test amplification method or in tandem with other methods. In this paper we explain the approach, describe its synergies with other approaches and provide some evidence for its practical feasibility.},
booktitle = {Proceedings of the 10th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {35–41},
numpages = {7},
keywords = {mining software repositories, oracle problem, automated testing, test amplification, behavior, observations},
location = {Tallinn, Estonia},
series = {A-TEST 2019}
}

@inproceedings{10.1145/3053600.3053636,
author = {Ferme, Vincenzo and Pautasso, Cesare},
title = {Towards Holistic Continuous Software Performance Assessment},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053636},
doi = {10.1145/3053600.3053636},
abstract = {In agile, fast and continuous development lifecycles, software performance analysis is fundamental to confidently release continuously improved software versions. Researchers and industry practitioners have identified the importance of integrating performance testing in agile development processes in a timely and efficient way. However, existing techniques are fragmented and not integrated taking into account the heterogeneous skills of the users developing polyglot distributed software, and their need to automate performance practices as they are integrated in the whole lifecycle without breaking its intrinsic velocity. In this paper we present our vision for holistic continuous software performance assessment, which is being implemented in the BenchFlow tool. BenchFlow enables performance testing and analysis practices to be pervasively integrated in continuous development lifecycle activities. Users can specify performance activities (e.g., standard performance tests) by relying on an expressive Domain Specific Language for objective-driven performance analysis. Collected performance knowledge can be thus reused to speed up performance activities throughout the entire process.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {159–164},
numpages = {6},
keywords = {continuous software performance assessment, performance analysis, continuous integration, performance test},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/2993236.2993257,
author = {Makki, Majid and Van Landuyt, Dimitri and Joosen, Wouter},
title = {Automated Regression Testing of BPMN 2.0 Processes: A Capture and Replay Framework for Continuous Delivery},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993257},
doi = {10.1145/2993236.2993257},
abstract = {Regression testing is a form of software quality assurance (QA) that involves comparing the behavior of a newer version of a software artifact to its earlier correct behavior, and signaling the QA engineer when deviations are detected. Given the large potential in automated generation and execution of regression test cases for business process models in the context of running systems, powerful tools are required to make this practically feasible, more specifically to limit the potential impact on production systems, and to reduce the manual effort required from QA engineers. In this paper, we present a regression testing automation framework that implements the capture &amp; replay paradigm in the context of BPMN 2.0, a domain-specific language for modeling and executing business processes. The framework employs parallelization techniques and efficient communication patterns to reduce the performance overhead of capturing. Based on inputs from the QA engineer, it manipulates the BPMN2 model before executing tests for isolating the latter from external dependencies (e.g. human actors or expensive web services) and for avoiding undesired side-effects. Finally, it performs a regression detection algorithm and reports the results to the QA engineer. We have implemented our framework on top of a BPMN2-compliant execution engine, namely jBPM, and performed functional validations and evaluations of its performance and fault-tolerance. The results, indicating 3.9% average capturing performance overhead, demonstrate that the implemented framework can be the foundation of a practical regression testing tool for BPMN 2.0, and a key enabler for continuous delivery of business process-driven applications and services.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {178–189},
numpages = {12},
keywords = {BPMN 2.0, jBPM, Performance Overhead, Business Process Execution, Regression Testing, Node Mocking, Test Automation},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1145/3093335.2993257,
author = {Makki, Majid and Van Landuyt, Dimitri and Joosen, Wouter},
title = {Automated Regression Testing of BPMN 2.0 Processes: A Capture and Replay Framework for Continuous Delivery},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093335.2993257},
doi = {10.1145/3093335.2993257},
abstract = {Regression testing is a form of software quality assurance (QA) that involves comparing the behavior of a newer version of a software artifact to its earlier correct behavior, and signaling the QA engineer when deviations are detected. Given the large potential in automated generation and execution of regression test cases for business process models in the context of running systems, powerful tools are required to make this practically feasible, more specifically to limit the potential impact on production systems, and to reduce the manual effort required from QA engineers. In this paper, we present a regression testing automation framework that implements the capture &amp; replay paradigm in the context of BPMN 2.0, a domain-specific language for modeling and executing business processes. The framework employs parallelization techniques and efficient communication patterns to reduce the performance overhead of capturing. Based on inputs from the QA engineer, it manipulates the BPMN2 model before executing tests for isolating the latter from external dependencies (e.g. human actors or expensive web services) and for avoiding undesired side-effects. Finally, it performs a regression detection algorithm and reports the results to the QA engineer. We have implemented our framework on top of a BPMN2-compliant execution engine, namely jBPM, and performed functional validations and evaluations of its performance and fault-tolerance. The results, indicating 3.9% average capturing performance overhead, demonstrate that the implemented framework can be the foundation of a practical regression testing tool for BPMN 2.0, and a key enabler for continuous delivery of business process-driven applications and services.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {178–189},
numpages = {12},
keywords = {jBPM, Performance Overhead, BPMN 2.0, Business Process Execution, Test Automation, Node Mocking, Regression Testing}
}

@inproceedings{10.1145/2670979.2671004,
author = {Li, Kaituo and Joshi, Pallavi and Gupta, Aarti and Ganai, Malay K.},
title = {ReproLite: A Lightweight Tool to Quickly Reproduce Hard System Bugs},
year = {2014},
isbn = {9781450332521},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670979.2671004},
doi = {10.1145/2670979.2671004},
abstract = {Cloud systems have become ubiquitous today -- they are used to store and process the tremendous amounts of data being generated by Internet users. These systems run on hundreds of commodity machines, and have a huge amount of non-determinism (thousands of threads and hundreds of processes) in their execution. Therefore, bugs that occur in cloud systems are hard to understand, reproduce, and fix. The state-of-the-art of debugging in the industry is to log messages during execution, and refer to those messages later in case of errors. In ReproLite, we augment the already widespread process of debugging using logs by enabling testers to quickly and easily specify the conjectures that they form regarding the cause of an error (or bug) from execution logs, and to also automatically validate those conjectures.ReproLite includes a Domain Specific Language (DSL) that allows testers to specify all aspects of a potential scenario (e.g., specific workloads, execution operations and their orders, environment non-determinism) that causes a given bug. Given such a scenario, ReproLite can enforce the conditions in the scenario during system execution. Potential buggy scenarios can also be automatically generated from a sequence of log messages that a tester believes indicates the cause of the bug. We have experimented ReproLite with 11 bugs from two popular cloud systems, Cassandra and HBase. We were able to reproduce all of the bugs using ReproLite. We report on our experience with using ReproLite on those bugs.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {1–13},
numpages = {13},
keywords = {Lightweight, Hard System Bug, Debugging, Cloud Computing},
location = {Seattle, WA, USA},
series = {SOCC '14}
}

@inproceedings{10.1145/2884781.2884809,
author = {Menendez, David and Nagarakatte, Santosh},
title = {Termination-Checking for LLVM Peephole Optimizations},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884809},
doi = {10.1145/2884781.2884809},
abstract = {Mainstream compilers contain a large number of peephole optimizations, which perform algebraic simplification of the input program with local rewriting of the code. These optimizations are a persistent source of bugs. Our recent research on Alive, a domain-specific language for expressing peephole optimizations in LLVM, addresses a part of the problem by automatically verifying the correctness of these optimizations and generating C++ code for use with LLVM.This paper identifies a class of non-termination bugs that arise when a suite of peephole optimizations is executed until a fixed point. An optimization can undo the effect of another optimization in the suite, which results in non-terminating compilation. This paper (1) proposes a methodology to detect non-termination bugs with a suite of peephole optimizations, (2) identifies the necessary condition to ensure termination while composing peephole optimizations, and (3) provides debugging support by generating concrete input programs that cause non-terminating compilation. We have discovered 184 optimization sequences, involving 38 optimizations, that cause non-terminating compilation in LLVM with Alive-generated C++ code.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {191–202},
numpages = {12},
keywords = {compiler verification, alive, termination, peephole optimization},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1145/1416563.1416566,
author = {Huang, Shan Shan and Zook, David and Smaragdakis, Yannis},
title = {Domain-Specific Languages and Program Generation with Meta-AspectJ},
year = {2008},
issue_date = {November 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/1416563.1416566},
doi = {10.1145/1416563.1416566},
abstract = {Meta-AspectJ (MAJ) is a language for generating AspectJ programs using code templates. MAJ itself is an extension of Java, so users can interleave arbitrary Java code with AspectJ code templates. MAJ is a structured metaprogramming tool: a well-typed generator implies a syntactically correct generated program. MAJ promotes a methodology that combines aspect-oriented and generative programming. A valuable application is in implementing small domain-specific language extensions as generators using unobtrusive annotations for syntax extension and AspectJ as a back-end. The advantages of this approach are twofold. First, the generator integrates into an existing software application much as a regular API or library, instead of as a language extension. Second, a mature language implementation is easy to achieve with little effort since AspectJ takes care of the low-level issues of interfacing with the base Java language.In addition to its practical value, MAJ offers valuable insights to metaprogramming tool designers. It is a mature metaprogramming tool for AspectJ (and, by extension, Java): a lot of emphasis has been placed on context-sensitive parsing and error reporting. As a result, MAJ minimizes the number of metaprogramming (quote/unquote) operators and uses type inference to reduce the need to remember type names for syntactic entities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {6},
numpages = {32},
keywords = {program transformation, Metaprogramming, program verification, language extensions, program synthesis, domain-specific languages}
}

@inproceedings{10.1145/3011141.3011158,
author = {Longo, Douglas Hiura and Vilain, Patricia and da Silva, Lucas Pereira and Mello, Ronaldo dos Santos},
title = {A Web Framework for Test Automation: User Scenarios through User Interaction Diagrams},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011158},
doi = {10.1145/3011141.3011158},
abstract = {This paper presents a web framework for test automation. This framework automates tests specified using the technique User Scenarios through User Interaction Diagrams (US-UID). US-UIDs represent customer requirements and are created a priori to the development of the application code. The goal of this proposal is the execution of US-UIDs as acceptance tests to check the application code in the context of Acceptance Test-Driven Development (ATDD). The implementation of the proposed framework uses FitNesse as its base and adds an editor for creating and executing US-UIDs. The proposal is exemplified by US-UIDs of the 8-puzzle game and evaluates the code of a Web-based application. The results show that US-UID has an executable format as automated tests. In addition, the proposed web framework has the capability of indicating the success, error or failure of interaction between a US-UID and the application code.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {458–467},
numpages = {10},
keywords = {acceptance test, automated test, ATDD, user interaction diagram, FitNesse, user scenario, UID, US-UID},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@inproceedings{10.1145/1866307.1866358,
author = {Henecka, Wilko and K \"{o}gl, Stefan and Sadeghi, Ahmad-Reza and Schneider, Thomas and Wehrenberg, Immo},
title = {TASTY: Tool for Automating Secure Two-Party Computations},
year = {2010},
isbn = {9781450302456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866307.1866358},
doi = {10.1145/1866307.1866358},
abstract = {Secure two-party computation allows two untrusting parties to jointly compute an arbitrary function on their respective private inputs while revealing no information beyond the outcome. Existing cryptographic compilers can automatically generate secure computation protocols from high-level specifications, but are often limited in their use and efficiency of generated protocols as they are based on either garbled circuits or (additively) homomorphic encryption only.In this paper we present TASTY, a novel tool for automating, i.e., describing, generating, executing, benchmarking, and comparing, efficient secure two-party computation protocols. TASTY is a new compiler that can generate protocols based on homomorphic encryption and efficient garbled circuits as well as combinations of both, which often yields the most efficient protocols available today. The user provides a high-level description of the computations to be performed on encrypted data in a domain-specific language. This is automatically transformed into a protocol. TASTY provides most recent techniques and optimizations for practical secure two-party computation with low online latency. Moreover, it allows to efficiently evaluate circuits generated by the well-known Fairplay compiler.We use TASTY to compare protocols for secure multiplication based on homomorphic encryption with those based on garbled circuits and highly efficient Karatsuba multiplication. Further, we show how TASTY improves the online latency for securely evaluating the AES functionality by an order of magnitude compared to previous software implementations. TASTY allows to automatically generate efficient secure protocols for many privacy-preserving applications where we consider the use cases for private set intersection and face recognition protocols.},
booktitle = {Proceedings of the 17th ACM Conference on Computer and Communications Security},
pages = {451–462},
numpages = {12},
keywords = {secure function evaluation, compiler, garbled circuits, homomorphic encryption, cryptography},
location = {Chicago, Illinois, USA},
series = {CCS '10}
}

@inproceedings{10.1145/2038558.2038583,
author = {D\'{\i}az, Oscar and Puente, Gorka},
title = {Wiki Scaffolding: Helping Organizations to Set up Wikis},
year = {2011},
isbn = {9781450309097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038558.2038583},
doi = {10.1145/2038558.2038583},
abstract = {Organizational wikis are framed by an existing organization. This makes these wikis be especially vigilant upon (1) facilitating the alignment of the wiki with organizational practices, (2) engaging management or (3), promoting employees' participation. To this end, we advocate for the use of "wiki scaffoldings". A wiki scaffolding is a wiki installation that is provided at the onset, before any contribution is made. It aims to frame wiki contribution along the concerns already known in the hosting organization in terms of glossaries, schedules, organigrams and the like. Thus, wiki contributions do not start from scratch but within a known setting. This paper introduces a language to capture wiki scaffolding in terms of FreeMind's mind maps. These maps can later be mapped into wiki installations in MediaWiki. The paper seeks to validate the approach in a twofold manner. Firstly, by providing literature quotes that suggest the need for scaffolding. Secondly, by providing scaffolding examples for wikis reported in the literature. The findings suggest that wiki scaffolding can be useful to smoothly align wiki activity along the practices of the hosting organization from the onset.},
booktitle = {Proceedings of the 7th International Symposium on Wikis and Open Collaboration},
pages = {154–162},
numpages = {9},
keywords = {DSL, collaboration, wiki scaffolding, wikis},
location = {Mountain View, California},
series = {WikiSym '11}
}

@inproceedings{10.1145/3550355.3552451,
author = {Khorram, Faezeh and Bousse, Erwan and Mottu, Jean-Marie and Suny\'{e}, Gerson and G\'{o}mez-Abajo, Pablo and Ca\~{n}izares, Pablo C. and Guerra, Esther and de Lara, Juan},
title = {Automatic Test Amplification for Executable Models},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552451},
doi = {10.1145/3550355.3552451},
abstract = {Behavioral models are important assets that must be thoroughly verified early in the design process. This can be achieved with manually-written test cases that embed carefully hand-picked domain-specific input data. However, such test cases may not always reach the desired level of quality, such as high coverage or being able to localize faults efficiently. Test amplification is an interesting emergent approach to improve a test suite by automatically generating new test cases out of existing manually-written ones. Yet, while ad-hoc test amplification solutions have been proposed for a few programming languages, no solution currently exists for amplifying the test cases of behavioral models.In this paper, we fill this gap with an automated and generic approach. Given an executable DSL, a conforming behavioral model, and an existing test suite, our approach generates new regression test cases in three steps: (i) generating new test inputs by applying a set of generic modifiers on the existing test inputs; (ii) running the model under test with new inputs and generating assertions from the execution traces; and (iii) selecting the new test cases that increase the mutation score. We provide tool support for the approach atop the Eclipse GEMOC Studio1 and show its applicability in an empirical study. In the experiment, we applied the approach to 71 test suites written for models conforming to two different DSLs, and for 67 of the 71 cases, it successfully improved the mutation score between 3.17% and 54.11% depending on the initial setup.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {109–120},
numpages = {12},
keywords = {regression testing, test amplification, executable DSL, executable model},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.5555/2555523.2555535,
author = {da Silva, Elias Adriano Nogueira and Fortes, Renata P. M. and Lucr\'{e}dio, Daniel},
title = {A Model-Driven Approach for Promoting Cloud PaaS Portability},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Cloud computing has become an important research subject in software engineering. Among the many research gaps related to this new computing model is the lack of portability between cloud platforms, which generates the Lock-In problem. The Lock-In is the difficulty in migrating data and applications from a cloud platform to another. Current attempts to address this problem revolve around standardization of APIs and frameworks. We propose a different path, using model-driven engineering (MDE). We selected two cloud platforms and built a DSL and a set of automated transformations that generate code for each platform, based on a single portable model. We present the results of two studies. In a first study, subjects were asked to use the two versions of the same application, each one generated for a different platform from a single model. The subjects did not notice any difference between the two versions in terms of functionality. In a second study, we observed that besides facilitating cloud portability, MDE can increase productivity and reusability. These results indicate that MDE may be an alternative to standardization, not only helping to solve portability problems but also leading to additional benefits.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {92–105},
numpages = {14},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.1145/3152688.3152692,
author = {Gabrielova, Eugenia},
title = {End-to-End Regression Testing for Distributed Systems},
year = {2017},
isbn = {9781450351997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152688.3152692},
doi = {10.1145/3152688.3152692},
abstract = {Even with substantial advances in tools and research techniques, distributed systems remain challenging to test. One frustrating aspect of distributed systems development is the resurfacing of old problems due to code changes. Regression test suites replicate previously known bugs and ensure they do not resurface as the code evolves. Conventional unit regression tests miss a substantial amount of distributed system problems; end-to-end testing is almost always required in order to reproduce complex bugs. We describe a framework for regression testing that bridges a gap between local ad-hoc experiments and end-to-end stress testing, potentially lowering the recurrence of critical bugs.},
booktitle = {Proceedings of the 18th Doctoral Symposium of the 18th International Middleware Conference},
pages = {9–12},
numpages = {4},
location = {Las Vegas, Nevada},
series = {Middleware '17}
}

@inproceedings{10.1145/3393527.3393531,
author = {Zhang, Yuxiang and Chen, Kang and Liu, Weidong},
title = {Online Judge for FPGA-Based Lab Projects in Computer Organization Course},
year = {2020},
isbn = {9781450375344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393527.3393531},
doi = {10.1145/3393527.3393531},
abstract = {While online judge systems are widely used in programming related courses, they are rarely used in hardware-related courses such as the computer organization course requiring the digital circuit design. With the widely available FPGA hardware, students now have fewer difficulties in building hardware by only writing hardware description language (HDL) code. We have built a cloud-based lab environment that students can build CPUs online by submitting their Verilog HDL code. Our HDL online judge system is applied to test the submitted code. It greatly reduces the efforts of checking the code manually.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
pages = {15–20},
numpages = {6},
keywords = {Computer Organization, FPGA, Digital Circuit, Online Judge},
location = {Hefei, China},
series = {ACM TURC'20}
}

@inproceedings{10.1145/3293882.3330561,
author = {Golagha, Mojdeh and Lehnhoff, Constantin and Pretschner, Alexander and Ilmberger, Hermann},
title = {Failure Clustering without Coverage},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330561},
doi = {10.1145/3293882.3330561},
abstract = {Developing and integrating software in the automotive industry is a complex task and requires extensive testing. An important cost factor in testing and debugging is the time required to analyze failing tests. In the context of regression testing, usually, large numbers of tests fail due to a few underlying faults. Clustering failing tests with respect to their underlying faults can, therefore, help in reducing the required analysis time. In this paper, we propose a clustering technique to group failing hardware-in-the-loop tests based on non-code-based features, retrieved from three different sources. To effectively reduce the analysis effort, the clustering tool selects a representative test for each cluster. Instead of analyzing all failing tests, testers only inspect the representative tests to find the underlying faults. We evaluated the effectiveness and efficiency of our solution in a major automotive company using 86 regression test runs, 8743 failing tests, and 1531 faults. The results show that utilizing our clustering tool, testers can reduce the analysis time more than 60% and find more than 80% of the faults only by inspecting the representative tests.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {134–145},
numpages = {12},
keywords = {Failure Clustering, Debugging},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3183440.3195036,
author = {Santos, Ernani C\'{e}sar Dos and Vilain, Patr\'{\i}cia and Longo, Douglas Hiura},
title = {A Systematic Literature Review to Support the Selection of User Acceptance Testing Techniques},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3195036},
doi = {10.1145/3183440.3195036},
abstract = {User Acceptance Testing (UAT) aims to determine whether or not a software satisfies users acceptance criteria. Although some studies have used acceptance tests as software requirements, no previous study has collected information about available UAT techniques and established a comparison of them, to support an organization in the selection of one over another. This work presents a Systematic Literature Review on UAT to find out available techniques and compare their main features. We selected 80 studies and found out 21 UAT techniques. As result, we created a comparative table summarizing these techniques and their features.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {418–419},
numpages = {2},
keywords = {classification, features, techniques, user acceptance testing},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3425174.3425213,
author = {Toennemann, Jan and Anicul\u{a}esei, Adina and Rausch, Andreas},
title = {Asserting Functional Equivalence between C Code and SCADE Models in Code-to-Model Transformations},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425213},
doi = {10.1145/3425174.3425213},
abstract = {Model-based development is on the rise and tool chains employing automated code generation from models using certified code generators are getting increasingly common. We present an approach which enables the reverse operation and creates an ANSYS SCADE model that is functionally equivalent to the C code. The main motivation behind this development is to enable original equipment manufacturers (OEMs) to further use and maintain legacy code in new development environments, rather than having to re-develop the respective functionality from scratch.While the model transformation itself is performed manually, the testing process is fully automated and enabled the transfer of existing test cases for the C function to the SCADE Test Environment. The presented approach enables white-box testing of the model, requiring the original C implementation and its original test cases as well as a bi-directional mapping of variable names between C code and SCADE model. This is done by extending the original code in a way that generates SCADE test scenarios during runtime, allowing to use these white-box test scenarios to assert functional equivalence of code and model using empirical validation.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {60–68},
numpages = {9},
keywords = {test case generation, equivalence testing, embedded software, ANSYS SCADE model, C code, test automation, model-driven engineering, code-to-model transformation, white-box testing},
location = {Natal, Brazil},
series = {SAST 20}
}

@inproceedings{10.1145/3375959.3375967,
author = {Wolde, Behailu Getachew and Boltana, Abiot Sinamo},
title = {Combinatorial Testing Approach for Cloud Mobility Service},
year = {2020},
isbn = {9781450372633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375959.3375967},
doi = {10.1145/3375959.3375967},
abstract = {Currently, software product becomes an essential component in running many stakeholders' activities. For instance, the industries mostly use cloud services to execute their important business functionality. However, by a few input's parameter interacting, this functionality can be pended. Such constraint poses challenging to cover various features of failure especially in ensuring cloud application. One way is to devise a strategy to cover input parameters' characteristics based on Combinatorial testing approach. This technique includes all possible combinations of test inputs for detecting bugs on the System Under Test (SUT). The paper explains the Combinatorial covering arrays to generate relatively exhaustive testing by modeling features of sample services using Feature IDE plugin in Eclipse IDE. This way, we build the input domain model to represent coverage of the existing mobility service running on NEMo Mobility cloud platform. Using this model, covering arrays is applied to generate t-way test cases by leveraging IPOg algorithm, which is implemented in a CiTLab. As a test case management, the JUnit testing framework uses test stubs to validate the test methods of generated test cases on the specified service (SUT).},
booktitle = {Proceedings of the 2019 2nd Artificial Intelligence and Cloud Computing Conference},
pages = {6–13},
numpages = {8},
keywords = {CiTLAB, Software Testing, Feature Model, Cloud Mobility Service, Combinatorial Testing},
location = {Kobe, Japan},
series = {AICCC 2019}
}

@inproceedings{10.1145/2997364.2997367,
author = {Meyers, Bart and Denil, Joachim and D\'{a}vid, Istv\'{a}n and Vangheluwe, Hans},
title = {Automated Testing Support for Reactive Domain-Specific Modelling Languages},
year = {2016},
isbn = {9781450344470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2997364.2997367},
doi = {10.1145/2997364.2997367},
abstract = {Domain-specific modelling languages (DSML) enable domain users to model systems in their problem domain, using concepts and notations they are familiar with. The process of domain-specific modelling (DSM) consists of two stages: a language engineering stage where a DSML is created, and a system modelling stage where the DSML is used. Because techniques such as metamodelling and model transformation allow for a efficient creation of DSMLs, and using DSMLs significantly increases productivity, DSM is very suitable for early prototyping. Many systems that are modelled using DSMLs are reactive, meaning that during their execution, they respond to external input. Because of the complexity of input and response behaviour of reactive systems, it is desirable to test models as early as possible. However, while dedicated testing support for specific DSMLs has been provided, no systematic support exists for testing DSML models according to DSM principles. In this paper, we introduce a technique to automatically generate a domain-specific testing framework from an annotated DSML definition. In our approach, the DSML definition consists of a metamodel, a concrete syntax definition and operational semantics described as a schedule of graph rewrite rules, thus covering a large class of DSMLs. Currently, DSMLs with deterministic behaviour are supported, but we provide an outlook to other (nondeterministic, real-time or continuous-time) DSMLs. We illustrate the approach with a DSML for describing an elevator controller. We evaluate the approach and conclude that compared to the state-of-the-art, our testing support is significantly less costly, and similar or better (according to DSM principles) testing support is achieved. Additionally, the generative nature of the approach makes testing support for DSMLs less error-prone while catering the need for early testing.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {181–194},
numpages = {14},
keywords = {Language Engineering, Domain-Specific Modelling, Verification},
location = {Amsterdam, Netherlands},
series = {SLE 2016}
}

@inproceedings{10.1145/3184558.3191656,
author = {Vu, Henry and Fertig, Tobias and Braun, Peter},
title = {Verification of Hypermedia Characteristic of RESTful Finite-State Machines},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191656},
doi = {10.1145/3184558.3191656},
abstract = {Being an architectural style rather than a specification or a standard, the proper design of REpresentational State Transfer (REST) APIs is not trivial, since developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature and especially, hypermedia testing is not mentioned at all. To deal with this state of affairs, we have elaborated a Model-Driven Software Development (MDSD) approach for creating RESTful APIs. As this project matured, we also explored the possibility of Model-Driven Testing (MDT). This work addresses the challenges of hypermedia testing and proposes approaches to overcome them with MDT techniques. We present the results of hypermedia testing for RESTful APIs using a model verification approach that were discovered within our research. MDT enables the verification of the underlying model of a RESTful API and ensuring its correctness before initiating any code generation. Therefore, we can prevent a poorly designed model from being transformed into a poorly designed RESTful API.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1881–1886},
numpages = {6},
keywords = {MDT, hypermedia testing, RESTful systems, MDSD, hypermedia, RESTful applications, REST},
location = {Lyon, France},
series = {WWW '18}
}

@proceedings{10.1145/2997364,
title = {SLE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
year = {2016},
isbn = {9781450344470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1145/3210256,
author = {Florence, Spencer P. and Fetscher, Burke and Flatt, Matthew and Temps, William H. and St-Amour, Vincent and Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte and Yarnold, Paul R. and Findler, Robert Bruce and Belknap, Steven M.},
title = {POP-PL: A Patient-Oriented Prescription Programming Language},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/3210256},
doi = {10.1145/3210256},
abstract = {A medical prescription is a set of health care instructions that govern the plan of care for an individual patient, which may include orders for drug therapy, diet, clinical assessment, and laboratory testing. Clinicians have long used algorithmic thinking to describe and implement prescriptions but without the benefit of a formal programming language. Instead, medical algorithms are expressed using a natural language patois, flowcharts, or as structured data in an electronic medical record system. The lack of a prescription programming language inhibits expressiveness; results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse; and increases the risk of fatal medical error.This article reports on the design and evaluation of Patient-Oriented Prescription Programming Language (POP-PL), a domain-specific programming language designed for expressing prescriptions. The language is based around the idea that programs and humans have complementary strengths that, when combined properly, can make for safer, more accurate performance of prescriptions. Use of POP-PL facilitates automation of certain low-level vigilance tasks, freeing up human cognition for abstract thinking, compassion, and human communication.We implemented this language and evaluated its design attempting to write prescriptions in the new language and evaluated its usability by assessing whether clinicians can understand and modify prescriptions written in the language. We found that some medical prescriptions can be expressed in a formal domain-specific programming language, and we determined that medical professionals can understand and correctly modify programs written in POP-PL. We also discuss opportunities for refining and further developing POP-PL.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jul},
articleno = {10},
numpages = {37},
keywords = {medical prescriptions, DSL design, medical programming languages, empirical evaluation}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00035,
author = {Giorgi, Fabio and Paulisch, Frances},
title = {Transition towards Continuous Delivery in the Healthcare Domain},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00035},
doi = {10.1109/ICSE-SEIP.2019.00035},
abstract = {Continuous Delivery is meanwhile well-established in many parts of the software industry. In a transition towards continuous delivery in the healthcare domain, there are a number of additional challenges that should be addressed. We present how we have addressed some of these challenges and highlight some potential research topics that could be addressed in this space to make further progress in this important area. Although our focus is on the healthcare domain, the approach and the research topics are applicable also to a broad range of other application domains.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {253–254},
numpages = {2},
keywords = {domain-driven design, deployment pipeline, test-driven development, test automation, agile, continuous delivery, behavior-driven development, pair-programming},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1145/3524481.3527239,
author = {Eisner, Daniel and Wuersching, Roland and Schnappinger, Markus and Pretschner, Alexander},
title = {Probe-Based Syscall Tracing for Efficient and Practical File-Level Test Traces},
year = {2022},
isbn = {9781450392860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524481.3527239},
doi = {10.1145/3524481.3527239},
abstract = {Efficiently collecting per-test execution traces is a common prerequisite of dynamic regression test optimization techniques. However, as these test traces are typically recorded through language-specific code instrumentation, non-code artifacts and multi-language source code are usually not included. In contrast, more complete test traces can be obtained by instrumenting operating system calls and thereby tracing all accessed files during a test's execution. Yet, existing test optimization techniques that use syscall tracing are impractical as they either modify the Linux kernel or operate in user space, thus raising transferability, performance, and security concerns. Recent advances in operating system development provide versatile, lightweight, and safe kernel instrumentation frameworks: They allow to trace syscalls by instrumenting probes in the operating system kernel. Probe-based Syscall Tracing (ProST), our novel technique, harnesses this potential to collect file-level test traces that go beyond language boundaries and consider non-code artifacts. To evaluate ProST's efficiency and the completeness of obtained test traces, we perform an empirical study on 25 multi-language open-source software projects and compare our approach to existing language-specific instrumentation techniques. Our results show that most studied projects use source files from multiple languages (22/25) or non-code artifacts during testing (22/25) that are missed by language-specific techniques. With the low execution time overhead of 4.6% compared to non-instrumented test execution, ProST is more efficient than language-specific instrumentation. Furthermore, it collects on average 89% more files on top of those collected by language-specific techniques. Consequently, ProST paves the way for efficiently extracting valuable information through dynamic analysis to better understand and optimize testing in multi-language software systems.},
booktitle = {Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test},
pages = {126–137},
numpages = {12},
keywords = {multi-language software, non-code artifacts, dynamic program analysis, software testing},
location = {Pittsburgh, Pennsylvania},
series = {AST '22}
}

@inproceedings{10.5555/2337223.2337371,
author = {Devos, Nicolas and Ponsard, Christophe and Deprez, Jean-Christophe and Bauvin, Renaud and Moriau, Benedicte and Anckaerts, Guy},
title = {Efficient Reuse of Domain-Specific Test Knowledge: An Industrial Case in the Smart Card Domain},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {While testing is heavily used and largely automated in software development projects, the reuse of test practices across similar projects in a given domain is seldom systematized and supported by adequate methods and tools. This paper presents a practical approach that emerged from a concrete industrial case in the smart card domain at STMicroelectronics Belgium in order to better address this kind of challenge. The central concept is a test knowledge repository organized as a collection of specific patterns named QPatterns. A systematic process was followed, first to gather, structure and abstract the test practices, then to produce and validate an initial repository, and finally to make it evolve later on Testers can then rely on this repository to produce high quality test plans identifying all the functional and non-functional aspects that have to be addressed, as well as the concrete tests that have to be developed within the context of a new project. A tool support was also developed and integrated in a traceable way into the existing industrial test environment. The approach was validated and is currently under deployment at STMicroelectronics Belgium.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1123–1132},
numpages = {10},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2607023.2610278,
author = {Hesenius, Marc and Griebe, Tobias and Gruhn, Volker},
title = {Towards a Behavior-Oriented Specification and Testing Language for Multimodal Applications},
year = {2014},
isbn = {9781450327251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2607023.2610278},
doi = {10.1145/2607023.2610278},
abstract = {Initiated by the ubiquity of mobile devices, human computer interaction has evolved beyond the classic PCs' mouse and keyboard setup. Smartphones and tablets introduced new interaction modalities to the mass market and created the need for specialized software engineering methods. While more and more powerful SDKs are released to develop interactive applications, specifying user interaction is still ambiguous and error-prone, causing software defects as well as misunderstandings and frustration among project team members and stakeholders. We present an approach addressing this problems by demonstrating how to incorporate multimodal interaction into user acceptance tests written in near-natural language using Gherkin and formal gesture descriptions.},
booktitle = {Proceedings of the 2014 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {117–122},
numpages = {6},
keywords = {multimodal user interfaces, specification, software engineering},
location = {Rome, Italy},
series = {EICS '14}
}

@inproceedings{10.1145/2134243.2134248,
author = {Sadowski, Caitlin and Yi, Jaeheon},
title = {<u>T</u>i<u>d</u>d<u>l</u>e: A <u>t</u>race <u>d</u>escription <u>l</u>anguage for Generating Concurrent Benchmarks to Test Dynamic Analyses},
year = {2009},
isbn = {9781605586564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2134243.2134248},
doi = {10.1145/2134243.2134248},
abstract = {Dynamic analysis is a promising technique for finding concurrency bugs in multithreaded programs. However, testing a dynamic analysis tool can be difficult. Researchers end up writing large amounts of small benchmark programs. Since the benchmarks themselves are concurrent programs, they may execute nondeterministically, complicating testing of the analysis tool.We propose testing dynamic analyses by writing traces in a simple trace description language, Tiddle. Our implementation, written in Haskell, generates deterministic multithreaded Java programs for testing dynamic analyses. We report that it is substantially easier to write programs with incriminating bugs such as race conditions in Tiddle than the corresponding Java source code version, reducing the amount of source code to maintain and understand. Although our implementation is targeted towards Java, the ideas extend to any other languages which support mutable fields and multiple threads.},
booktitle = {Proceedings of the Seventh International Workshop on Dynamic Analysis},
pages = {15–21},
numpages = {7},
keywords = {race conditions, atomicity violations, dynamic analysis, concurrency, traces},
location = {Chicago, Illinois},
series = {WODA '09}
}

@inproceedings{10.5555/2663575.2663588,
author = {Morrison, Patrick and Holmgreen, Casper and Massey, Aaron and Williams, Laurie},
title = {Proposing Regulatory-Driven Automated Test Suites for Electronic Health Record Systems},
year = {2013},
isbn = {9781467362825},
publisher = {IEEE Press},
abstract = {In regulated domains such as finance and health care, failure to comply with regulation can lead to financial, civil and criminal penalties. While systems vary from organization to organization, regulations apply across organizations. We propose the use of Behavior-Driven-Development (BDD) scenarios as the basis of an automated compliance test suite for standards such as regulation and interoperability. Such test suites could become a shared asset for use by all systems subject to these regulations and standards. Each system, then, need only create their own system-specific test driver code to automate their compliance checks. The goal of this research is to enable organizations to compare their systems to regulation in a repeatable and traceable way through the use of BDD. To evaluate our proposal, we developed an abbreviated HIPAA test suite and applied it to three open-source electronic health record systems. The scenarios covered all security behavior defined by the selected regulation. The system-specific test driver code covered all security behavior defined in the scenarios, and identified where the tested system lacked such behavior.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering in Health Care},
pages = {46–49},
numpages = {4},
keywords = {software engineering, healthcare it, behavior-driven-development, security, regulatory compliance, software testing},
location = {San Francisco, California},
series = {SEHC '13}
}

@inproceedings{10.1145/3379597.3387482,
author = {Pinto, Gustavo and Miranda, Breno and Dissanayake, Supun and d'Amorim, Marcelo and Treude, Christoph and Bertolino, Antonia},
title = {What is the Vocabulary of Flaky Tests?},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387482},
doi = {10.1145/3379597.3387482},
abstract = {Flaky tests are tests whose outcomes are non-deterministic. Despite the recent research activity on this topic, no effort has been made on understanding the vocabulary of flaky tests. This work proposes to automatically classify tests as flaky or not based on their vocabulary. Static classification of flaky tests is important, for example, to detect the introduction of flaky tests and to search for flaky tests after they are introduced in regression test suites.We evaluated performance of various machine learning algorithms to solve this problem. We constructed a data set of flaky and non-flaky tests by running every test case, in a set of 64k tests, 100 times (6.4 million test executions). We then used machine learning techniques on the resulting data set to predict which tests are flaky from their source code. Based on features, such as counting stemmed tokens extracted from source code identifiers, we achieved an F-measure of 0.95 for the identification of flaky tests. The best prediction performance was obtained when using Random Forest and Support Vector Machines. In terms of the code identifiers that are most strongly associated with test flakiness, we noted that job, action, and services are commonly associated with flaky tests. Overall, our results provides initial yet strong evidence that static detection of flaky tests is effective.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {492–502},
numpages = {11},
keywords = {Text classification, Regression testing, Test flakiness},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/331960.331968,
author = {Reichwein, James and Rothermel, Gregg and Burnett, Margaret},
title = {Slicing Spreadsheets: An Integrated Methodology for Spreadsheet Testing and Debugging},
year = {2000},
isbn = {1581132557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/331960.331968},
doi = {10.1145/331960.331968},
abstract = {Spreadsheet languages, which include commercial spreadsheets and various research systems, have proven to be flexible tools in many domain specific settings. Research shows, however, that spreadsheets often contain faults. We would like to provide at least some of the benefits of formal testing and debugging methodologies to spreadsheet developers. This paper presents an integrated testing and debugging methodology for spreadsheets. To accommodate the modeless and incremental development, testing and debugging activities that occur during spreadsheet creation, our methodology is tightly integrated into the spreadsheet environment. To accommodate the users of spreadsheet languages, we provide an interface to our methodology that does not require an understanding of testing and debugging theory, and that takes advantage of the immediate visual feedback that is characteristic of the spreadsheet paradigm.},
booktitle = {Proceedings of the 2nd Conference on Domain-Specific Languages},
pages = {25–38},
numpages = {14},
location = {Austin, Texas, USA},
series = {DSL '99}
}

@article{10.1145/331963.331968,
author = {Reichwein, James and Rothermel, Gregg and Burnett, Margaret},
title = {Slicing Spreadsheets: An Integrated Methodology for Spreadsheet Testing and Debugging},
year = {2000},
issue_date = {Jan. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/331963.331968},
doi = {10.1145/331963.331968},
abstract = {Spreadsheet languages, which include commercial spreadsheets and various research systems, have proven to be flexible tools in many domain specific settings. Research shows, however, that spreadsheets often contain faults. We would like to provide at least some of the benefits of formal testing and debugging methodologies to spreadsheet developers. This paper presents an integrated testing and debugging methodology for spreadsheets. To accommodate the modeless and incremental development, testing and debugging activities that occur during spreadsheet creation, our methodology is tightly integrated into the spreadsheet environment. To accommodate the users of spreadsheet languages, we provide an interface to our methodology that does not require an understanding of testing and debugging theory, and that takes advantage of the immediate visual feedback that is characteristic of the spreadsheet paradigm.},
journal = {SIGPLAN Not.},
month = {dec},
pages = {25–38},
numpages = {14}
}

@article{10.1145/1507195.1517461,
author = {Torkar, Richard and Gorschek, Tony and Feldt, Robert},
title = {Eight Conference on Software Engineering Research and Practice in Sweden (SERPS'08)},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1507195.1517461},
doi = {10.1145/1507195.1517461},
abstract = {The eight conference on software engineering research and practice in Sweden (SERPS'08) was held in Karlskrona, Sweden, on the 4th-5th of Nov. 2008. The aim with SERPS'08 is to bring researchers and industry practitioners together to discuss software engineering issues, problems, solutions and experiences, not necessarily from a Swedish perspective. During the conference a number of research and industry papers were presented and questions in connection to the presentations were discussed. This paper is a report on the discussions that took place, pointing towards needs and challenges as well as areas of interest in both academia and industry.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {31–33},
numpages = {3}
}

@inproceedings{10.1145/3350768.3351300,
author = {Kudo, Taciana Novo and Bulc\~{a}o-Neto, Renato F. and Vincenzi, Auri M. R.},
title = {A Conceptual Metamodel to Bridging Requirement Patterns to Test Patterns},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351300},
doi = {10.1145/3350768.3351300},
abstract = {Requirement patterns represent an abstraction of an application's behaviors and services that, in turn, may be replicated in similar applications. However, there has been a lack of efforts exploiting the benefits of requirement patterns in other phases of the software development life cycle, besides the requirements engineering itself. To address this gap, we propose the Software Pattern MetaModel (SoPaMM) that bridges requirement patterns to groups of scenarios with similar behaviors in the form of test patterns. SoPaMM allows the description of the behavior of a requirement pattern through a time executable and easy-to-use language aiming at the automatic generation of test patterns. Using SoPaMM, we model and implement a behavior-driven functional requirement pattern for a web-based user authentication application. Our preliminary results point out that a requirement pattern can be an executable specification capable of generating automated tests.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {155–160},
numpages = {6},
keywords = {requirement pattern, test pattern, reuse, behavior, metamodeling},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1145/3468504,
author = {Sundelin, Anders and Gonzalez-huerta, Javier and Wnuk, Krzysztof and Gorschek, Tony},
title = {Towards an Anatomy of Software Craftsmanship},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3468504},
doi = {10.1145/3468504},
abstract = {Context: The concept of software craftsmanship has early roots in computing, and in 2009, the Manifesto for Software Craftsmanship was formulated as a reaction to how the Agile methods were practiced and taught. But software craftsmanship has seldom been studied from a software engineering perspective.Objective: The objective of this article is to systematize an anatomy of software craftsmanship through literature studies and a longitudinal case study.Method: We performed a snowballing literature review based on an initial set of nine papers, resulting in&nbsp;18 papers and 11 books. We also performed a case study following seven years of software development of a product for the financial market, eliciting qualitative, and quantitative results. We used thematic coding to synthesize the results into categories.Results: The resulting anatomy is centered around four themes, containing 17 principles and 47 hierarchical practices connected to the principles. We present the identified practices based on the experiences gathered from the case study, triangulating with the literature results.Conclusion: We provide our systematically derived anatomy of software craftsmanship with the goal of inspiring more research into the principles and practices of software craftsmanship and how these relate to other principles within software engineering in general.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {6},
numpages = {49},
keywords = {principles of software development, deliberate practice, Software craftsmanship}
}

@inproceedings{10.1145/2628363.2628391,
author = {Hesenius, Marc and Griebe, Tobias and Gries, Stefan and Gruhn, Volker},
title = {Automating UI Tests for Mobile Applications with Formal Gesture Descriptions},
year = {2014},
isbn = {9781450330046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628363.2628391},
doi = {10.1145/2628363.2628391},
abstract = {Touch- and gesture-based interfaces are common in applications for mobile devices. By evolving into mass market products, smartphones and tablets created an increased need for specialized software engineering methods. To ensure high quality applications, constant and efficient testing is crucial in software development. However, testing mobile applications is still cumbersome, time-consuming and error-prone. One reason is the devices' focus on touch-based interaction - gestures cannot be easily incorporated into automated application tests. We present an extension to the popular Calabash testing framework solving this problem by allowing to describe gestures with a formal language in tests scripts.},
booktitle = {Proceedings of the 16th International Conference on Human-Computer Interaction with Mobile Devices &amp; Services},
pages = {213–222},
numpages = {10},
keywords = {gestures, software engineering, test automation, mobile applications, gesture formalization, testing},
location = {Toronto, ON, Canada},
series = {MobileHCI '14}
}

@inproceedings{10.1145/3350768.3350790,
author = {Diniz, Thomaz and Alves, Everton L.G. and Silva, Anderson G.F. and Andrade, Wilkerson L.},
title = {Reducing the Discard of MBT Test Cases Using Distance Functions},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3350790},
doi = {10.1145/3350768.3350790},
abstract = {Model-Based Testing (MBT) is used for generating test suites from system models. However, as software evolves, its models tend to be updated, which often leads to obsolete test cases that are discarded. Test case discard can be very costly since essential data, such as execution history, are lost. In this paper, we investigate the use of distance functions to help to reduce the discard of MBT tests. For that, we ran a series of empirical studies using artifacts from industrial systems, and we analyzed how ten distance functions can classify the impact of MBT-centred use case edits. Our results showed that distance functions are effective for identifying low impact edits that lead to test cases that can be updated with little effort. Moreover, we found the optimal configuration for each distance function. Finally, we ran a case study that showed that, by using distance functions, we could reduce the discard of test cases by 15%.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {337–346},
numpages = {10},
keywords = {agile development, MBT, test suite evolution, distance functions},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1145/2554850.2554942,
author = {Griebe, Tobias and Gruhn, Volker},
title = {A Model-Based Approach to Test Automation for Context-Aware Mobile Applications},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2554942},
doi = {10.1145/2554850.2554942},
abstract = {Current testing tools for mobile applications do not provide sufficient support for context-aware application testing. In addition to regular input vectors (e.g. touch events, text entry) context parameters must be considered (e.g. accelerometer data interpreted as shake gestures, GPS location data, etc.). A multitude of possible application faults resulting from these additional context parameters requires an appropriately selected set of test cases. In this paper, we propose a model-based approach to improve the testing of context-aware mobile applications by deducing test cases from design-time system models. Using a custom-built version of the calabash-android testing framework enhanced by an arbitrary context parameter facility, our approach to test case generation and automated execution is validated on a context-aware mobile application.},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {420–427},
numpages = {8},
keywords = {model-based, testing, context-awareness, mobile},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@inproceedings{10.1145/3213846.3213852,
author = {Shin, Seung Yeob and Nejati, Shiva and Sabetzadeh, Mehrdad and Briand, Lionel C. and Zimmer, Frank},
title = {Test Case Prioritization for Acceptance Testing of Cyber Physical Systems: A Multi-Objective Search-Based Approach},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213852},
doi = {10.1145/3213846.3213852},
abstract = {Acceptance testing validates that a system meets its requirements and determines whether it can be sufficiently trusted and put into operation. For cyber physical systems (CPS), acceptance testing is a hardware-in-the-loop process conducted in a (near-)operational environment. Acceptance testing of a CPS often necessitates that the test cases be prioritized, as there are usually too many scenarios to consider given time constraints. CPS acceptance testing is further complicated by the uncertainty in the environment and the impact of testing on hardware. We propose an automated test case prioritization approach for CPS acceptance testing, accounting for time budget constraints, uncertainty, and hardware damage risks. Our approach is based on multi-objective search, combined with a test case minimization algorithm that eliminates redundant operations from an ordered sequence of test cases. We evaluate our approach on a representative case study from the satellite domain. The results indicate that, compared to test cases that are prioritized manually by satellite engineers, our automated approach more than doubles the number of test cases that fit into a given time frame, while reducing to less than one third the number of operations that entail the risk of damage to key hardware components.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {49–60},
numpages = {12},
keywords = {Multi-objective Optimization, Search-based Software Engineering, Acceptance Testing, Test Case Prioritization, Cyber Physical Systems},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3238147.3240463,
author = {Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin},
title = {Achieving Test Automation with Testers without Coding Skills: An Industrial Report},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240463},
doi = {10.1145/3238147.3240463},
abstract = {We present a process driven test automation solution which enables delegating (part of) automation tasks from test automation engineer (expensive resource) to test analyst (non-developer, less expensive). In our approach, a test automation engineer implements test steps (or actions) which are executed automatically. Such automated test steps represent user actions in the system under test and specified by a natural language which is understandable by a non-technical person. Then, a test analyst with a domain knowledge organizes automated steps combined with test input to create an automated test case. It should be emphasized that the test analyst does not need to possess programming skills to create, modify or execute automated test cases. We refine benchmark test automation architecture to be better suitable for an effective separation and sharing of responsibilities between the test automation engineer (with coding skills) and test analyst (with a domain knowledge). In addition, we propose a metric to empirically estimate cooperation between test automation engineer and test analyst's works. The proposed automation solution has been defined based on our experience in the development and maintenance of Helsenorg, the national electronic health services in Norway which has had over one million of visits per month past year, and we still use it to automate the execution of regression tests.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {749–756},
numpages = {8},
keywords = {DSL for test automation, keyword-driven test automation, Test automation, process-driven test automation, Helsenorge},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.5555/227726.227842,
author = {Kieburtz, Richard B. and McKinney, Laura and Bell, Jeffrey M. and Hook, James and Kotov, Alex and Lewis, Jeffrey and Oliva, Dino P. and Sheard, Tim and Smith, Ira and Walton, Lisa},
title = {A Software Engineering Experiment in Software Component Generation},
year = {1996},
isbn = {0818672463},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.},
booktitle = {Proceedings of the 18th International Conference on Software Engineering},
pages = {542–552},
numpages = {11},
keywords = {reliability, software component generation, usability, flexibility, productivity},
location = {Berlin, Germany},
series = {ICSE '96}
}

@inproceedings{10.5555/2666719.2666727,
author = {Landh\"{a}u\ss{}er, Mathias and Genaid, Adrian},
title = {Connecting User Stories and Code for Test Development},
year = {2012},
isbn = {9781467317597},
publisher = {IEEE Press},
abstract = {User Stories are short feature descriptions from the user's point of view. Functional tests ensure that the feature described by a User Story is fully implemented.We present a tool that builds an ontology for code and links completed User Stories in natural language with the related code artifacts. The ontology also contains links to API components that were used to implement the functional tests. Preliminary results show that these links can be used to recommend reusable test steps for new User Stories.},
booktitle = {Proceedings of the Third International Workshop on Recommendation Systems for Software Engineering},
pages = {33–37},
numpages = {5},
keywords = {reasoning, traceability, functional testing, code mining, ontology},
location = {Zurich, Switzerland},
series = {RSSE '12}
}

@article{10.1145/2579281.2579312,
author = {Ionita, Anca Daniela and Lewis, Grace A. and Litoiu, Marin},
title = {Report of the 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013)},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579312},
doi = {10.1145/2579281.2579312},
abstract = {The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {34–37},
numpages = {4},
keywords = {services, software evolution, software maintenance, SOA, cloud computing, cloudbased systems, service-oriented systems, serviceoriented architecture}
}

@inproceedings{10.5555/2662413.2662432,
author = {Diepenbeck, Melanie and Soeken, Mathias and Gro\ss{}e, Daniel and Drechsler, Rolf},
title = {Towards Automatic Scenario Generation from Coverage Information},
year = {2013},
isbn = {9781467361613},
publisher = {IEEE Press},
abstract = {Nowadays, the design of software systems is pushed towards agile development practices. One of its most fundamental approaches is Test Driven Development (TDD). This procedure is based on test cases which are incrementally written prior to the implementation. Recently, Behavior Driven Development (BDD) has been introduced as an extension of TDD, in which natural language scenarios are the starting point for the test cases. This description offers a ubiquitous communication mean for both the software developers and stakeholders.Following the BDD methodology thoroughly, one would expect 100% code coverage, since code is only written to make the test cases pass. However, as we show in an empirical study this expectation is not valid in practice. It becomes even worse in the process of development, i.e. the coverage decreases over time. To close the coverage gap, we sketch an algorithm that generates BDD-style scenarios based on uncovered code.},
booktitle = {Proceedings of the 8th International Workshop on Automation of Software Test},
pages = {82–88},
numpages = {7},
location = {San Francisco, California},
series = {AST '13}
}

@article{10.1145/3523056,
author = {Troya, Javier and Segura, Sergio and Burgue\~{n}o, Lola and Wimmer, Manuel},
title = {Model Transformation Testing and Debugging: A Survey},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3523056},
doi = {10.1145/3523056},
abstract = {Model transformations are the key technique in Model-Driven Engineering (MDE) to manipulate and construct models. As a consequence, the correctness of software systems built with MDE approaches relies mainly on the correctness of model transformations, and thus, detecting and locating bugs in model transformations have been popular research topics in recent years. This surge of work has led to a vast literature on model transformation testing and debugging, which makes it challenging to gain a comprehensive view of the current state-of-the-art. This is an obstacle for newcomers to this topic and MDE practitioners to apply these approaches. This article presents a survey on testing and debugging model transformations based on the analysis of 140&nbsp;papers on the topics. We explore the trends, advances, and evolution over the years, bringing together previously disparate streams of work and providing a comprehensive view of these thriving areas. In addition, we present a conceptual framework to understand and categorize the different proposals. Finally, we identify several open research challenges and propose specific action points for the model transformation community.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {72},
numpages = {39},
keywords = {Model transformation, testing, debugging, survey}
}

@inproceedings{10.1145/3183440.3183480,
author = {Kr\"{o}her, Christian and El-Sharkawy, Sascha and Schmid, Klaus},
title = {KernelHaven: An Experimentation Workbench for Analyzing Software Product Lines},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183480},
doi = {10.1145/3183440.3183480},
abstract = {Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem.KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.Video: https://youtu.be/IbNc-H1NoZU},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {73–76},
numpages = {4},
keywords = {static analysis, variability extraction, empirical software engineering, software product line analysis},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2814204.2814221,
author = {Florence, Spencer P. and Fetscher, Bruke and Flatt, Matthew and Temps, William H. and Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte and Yarnold, Paul R. and Findler, Robert Bruce and Belknap, Steven M.},
title = {POP-PL: A Patient-Oriented Prescription Programming Language},
year = {2015},
isbn = {9781450336871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814204.2814221},
doi = {10.1145/2814204.2814221},
abstract = {Medical professionals have long used algorithmic thinking to describe and implement health care processes without the benefit of the conceptual framework provided by a programming language. Instead, medical algorithms are expressed using English, flowcharts, or data tables. This results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse. This paper reports on the design and evaluation of a domain-specific programming language, POP-PL for expressing medical algorithms. The design draws on the experience of researchers in two disciplines, programming languages and medicine. The language is based around the idea that programs and humans have complementary strengths, that when combined can make for safer, more accurate performance of prescriptions. We implemented a prototype of our language and evaluated its design by writing prescriptions in the new language and administering a usability survey to medical professionals. This formative evaluation suggests that medical prescriptions can be conveyed by a programming language's mode of expression and provides useful information for refining the language. Analysis of the survey results suggests that medical professionals can understand and correctly modify programs in POP-PL.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {131–140},
numpages = {10},
keywords = {Empirical Evaluation, DSL Design, Medical Programming Languages, Med- ical Prescriptions},
location = {Pittsburgh, PA, USA},
series = {GPCE 2015}
}

@article{10.1145/2936314.2814221,
author = {Florence, Spencer P. and Fetscher, Bruke and Flatt, Matthew and Temps, William H. and Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte and Yarnold, Paul R. and Findler, Robert Bruce and Belknap, Steven M.},
title = {POP-PL: A Patient-Oriented Prescription Programming Language},
year = {2015},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/2936314.2814221},
doi = {10.1145/2936314.2814221},
abstract = {Medical professionals have long used algorithmic thinking to describe and implement health care processes without the benefit of the conceptual framework provided by a programming language. Instead, medical algorithms are expressed using English, flowcharts, or data tables. This results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse. This paper reports on the design and evaluation of a domain-specific programming language, POP-PL for expressing medical algorithms. The design draws on the experience of researchers in two disciplines, programming languages and medicine. The language is based around the idea that programs and humans have complementary strengths, that when combined can make for safer, more accurate performance of prescriptions. We implemented a prototype of our language and evaluated its design by writing prescriptions in the new language and administering a usability survey to medical professionals. This formative evaluation suggests that medical prescriptions can be conveyed by a programming language's mode of expression and provides useful information for refining the language. Analysis of the survey results suggests that medical professionals can understand and correctly modify programs in POP-PL.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {131–140},
numpages = {10},
keywords = {Med- ical Prescriptions, DSL Design, Medical Programming Languages, Empirical Evaluation}
}

@inproceedings{10.1145/2678015.2682533,
author = {Li, Huiqing and Thompson, Simon},
title = {Safe Concurrency Introduction through Slicing},
year = {2015},
isbn = {9781450332972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678015.2682533},
doi = {10.1145/2678015.2682533},
abstract = {Traditional refactoring is about modifying the structure of existing code without changing its behaviour, but with the aim of making code easier to understand, modify, or reuse. In this paper, we introduce three novel refactorings for retrofitting concurrency to Erlang applications, and demonstrate how the use of program slicing makes the automation of these refactorings possible.},
booktitle = {Proceedings of the 2015 Workshop on Partial Evaluation and Program Manipulation},
pages = {103–113},
numpages = {11},
keywords = {refactoring, concurrency, functional programming, erlang, slicing, parallelisation},
location = {Mumbai, India},
series = {PEPM '15}
}

@inproceedings{10.1145/3468264.3468605,
author = {Wu, Xiuheng and Zhu, Chenguang and Li, Yi},
title = {DIFFBASE: A Differential Factbase for Effective Software Evolution Management},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468605},
doi = {10.1145/3468264.3468605},
abstract = {Numerous tools and techniques have been developed to extract and analyze information from software development artifacts. Yet, there is a lack of effective method to process, store, and exchange information among different analyses. In this paper, we propose differential factbase, a uniform exchangeable representation supporting efficient querying and manipulation, based on the existing concept of program facts. We consider program changes as first-class objects, which establish links between intra-version facts of single program snapshots and provide insights on how certain artifacts evolve over time via inter-version facts. We implement a series of differential fact extractors supporting different programming languages and platforms, and demonstrate with usage scenarios the benefits of adopting differential facts in supporting software evolution management.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {503–515},
numpages = {13},
keywords = {reverse engineering, program facts, software maintenance, Software evolution},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3356317.3356319,
author = {Eras, Eduardo Rohde and de Santiago, Valdivino Alexandre and dos Santos, Luciana Brasil Rebelo},
title = {Singularity: A Methodology for Automatic Unit Test Data Generation for C++ Applications Based on Model Checking Counterexamples},
year = {2019},
isbn = {9781450376488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356317.3356319},
doi = {10.1145/3356317.3356319},
abstract = {One of the most challenging task of testing activity is the generation of test cases/data. While there is significant amount of studies in this regard, there is still need to move towards approaches that can generate test case/data based only on source code since many software systems mostly have the source code available and no adequate documentation. In this paper a new methodology, called Singularity, is introduced to generate unit test data for C++ applications based on Model Checking, a popular technique for test case generation. Our approach, which is to be supported by a tool, automatically translates C++ code into a model which resembles a Statechart model and then into the notation of the NuSMV Model Checker. Later, we rely on a technique based on the HiMoST Method, producing counterexamples from the Model Checker that are, in fact, the test cases/data themselves. We have applied our approach to a few C++ case studies analyzing how feasible it is for automatic test data generation.},
booktitle = {Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing},
pages = {72–79},
numpages = {8},
keywords = {Model Checking, Automation, Tool, Counterexamples, Unit test},
location = {Salvador, Brazil},
series = {SAST 2019}
}

@inproceedings{10.1145/3510003.3510176,
author = {Gerten, Michael C. and Marsh, Alexis L. and Lathrop, James I. and Cohen, Myra B. and Miner, Andrew S. and Klinge, Titus H.},
title = {Inference and Test Generation Using Program Invariants in Chemical Reaction Networks},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510176},
doi = {10.1145/3510003.3510176},
abstract = {Chemical reaction networks (CRNs) are an emerging distributed computational paradigm where programs are encoded as a set of abstract chemical reactions. CRNs can be compiled into DNA strands which perform the computations in vitro, creating a foundation for intelligent nanodevices. Recent research proposed a software testing framework for stochastic CRN programs in simulation, however, it relies on existing program specifications. In practice, specifications are often lacking and when they do exist, transforming them into test cases is time-intensive and can be error prone. In this work, we propose an inference technique called ChemFlow which extracts 3 types of invariants from an existing CRN model. The extracted invariants can then be used for test generation or model validation against program implementations. We applied ChemFlow to 13 CRN programs ranging from toy examples to real biological models with hundreds of reactions. We find that the invariants provide strong fault detection and often exhibit less flakiness than specification derived tests. In the biological models we showed invariants to developers and they confirmed that some of these point to parts of the model that are biologically incorrect or incomplete suggesting we may be able to use ChemFlow to improve model quality.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1193–1205},
numpages = {13},
keywords = {chemical reaction networks, invariants, petri nets, test generation},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2814270.2814276,
author = {Voelter, Markus and Deursen, Arie van and Kolb, Bernd and Eberle, Stephan},
title = {Using C Language Extensions for Developing Embedded Software: A Case Study},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814276},
doi = {10.1145/2814270.2814276},
abstract = {We report on an industrial case study on developing the embedded software for a smart meter using the C programming language and domain-specific extensions of C such as components, physical units, state machines, registers and interrupts. We find that the extensions help significantly with managing the complexity of the software. They improve testability mainly by supporting hardware-independent testing, as illustrated by low integration efforts. The extensions also do not incur significant overhead regarding memory consumption and performance. Our case study relies on mbeddr, an extensible version of C. mbeddr, in turn, builds on the MPS language workbench which supports modular extension of languages and IDEs.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {655–674},
numpages = {20},
keywords = {Real-time and embedded systems, Extensible languages, Code Generation, Program Editors},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{10.1145/2858965.2814276,
author = {Voelter, Markus and Deursen, Arie van and Kolb, Bernd and Eberle, Stephan},
title = {Using C Language Extensions for Developing Embedded Software: A Case Study},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814276},
doi = {10.1145/2858965.2814276},
abstract = {We report on an industrial case study on developing the embedded software for a smart meter using the C programming language and domain-specific extensions of C such as components, physical units, state machines, registers and interrupts. We find that the extensions help significantly with managing the complexity of the software. They improve testability mainly by supporting hardware-independent testing, as illustrated by low integration efforts. The extensions also do not incur significant overhead regarding memory consumption and performance. Our case study relies on mbeddr, an extensible version of C. mbeddr, in turn, builds on the MPS language workbench which supports modular extension of languages and IDEs.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {655–674},
numpages = {20},
keywords = {Code Generation, Extensible languages, Program Editors, Real-time and embedded systems}
}

@inproceedings{10.1145/2002931.2002935,
author = {Choudhary, Shauvik Roy and Zhao, Dan and Versee, Husayn and Orso, Alessandro},
title = {WATER: Web Application TEst Repair},
year = {2011},
isbn = {9781450308083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002931.2002935},
doi = {10.1145/2002931.2002935},
abstract = {Web applications tend to evolve quickly, resulting in errors and failures in test automation scripts that exercise them. Repairing such scripts to work on the updated application is essential for maintaining the quality of the test suite. Updating such scripts manually is a time consuming task, which is often difficult and is prone to errors if not performed carefully. In this paper, we propose a technique to automatically suggest repairs for such web application test scripts. Our technique is based on differential testing and compares the behavior of the test case on two successive versions of the web application: first version in which the test script runs successfully and the second version in which the script results in an error or failure. By analyzing the difference between these two executions, our technique suggests repairs that can be applied to repair the scripts. To evaluate our technique, we implemented it in a tool called WATER and exercised it on real web applications with test cases. Our experiments show that WATER can suggest meaningful repairs for practical test cases, many of which correspond to those made later by developers themselves.},
booktitle = {Proceedings of the First International Workshop on End-to-End Test Script Engineering},
pages = {24–29},
numpages = {6},
keywords = {test repair, web testing},
location = {Toronto, Ontario, Canada},
series = {ETSE '11}
}

@article{10.1145/3105906,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
abstract = {This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {17},
numpages = {24},
keywords = {self-healing software, Program repair}
}

@inproceedings{10.1145/3338906.3338972,
author = {Dutta, Saikat and Zhang, Wenxian and Huang, Zixin and Misailovic, Sasa},
title = {Storm: Program Reduction for Testing and Debugging Probabilistic Programming Systems},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338972},
doi = {10.1145/3338906.3338972},
abstract = {Probabilistic programming languages offer an intuitive way to model uncertainty by representing complex probability models as simple probabilistic programs. Probabilistic programming systems (PP systems) hide the complexity of inference algorithms away from the program developer. Unfortunately, if a failure occurs during the run of a PP system, a developer typically has very little support in finding the part of the probabilistic program that causes the failure in the system. This paper presents Storm, a novel general framework for reducing probabilistic programs. Given a probabilistic program (with associated data and inference arguments) that causes a failure in a PP system, Storm finds a smaller version of the program, data, and arguments that cause the same failure. Storm leverages both generic code and data transformations from compiler testing and domain-specific, probabilistic transformations. The paper presents new transformations that reduce the complexity of statements and expressions, reduce data size, and simplify inference arguments (e.g., the number of iterations of the inference algorithm). We evaluated Storm on 47 programs that caused failures in two popular probabilistic programming systems, Stan and Pyro. Our experimental results show Storm’s effectiveness. For Stan, our minimized programs have 49% less code, 67% less data, and 96% fewer iterations. For Pyro, our minimized programs have 58% less code, 96% less data, and 99% fewer iterations. We also show the benefits of Storm when debugging probabilistic programs.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {729–739},
numpages = {11},
keywords = {Probabilistic Programming Languages, Software Testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2304656.2304658,
author = {Rupanov, Vladimir and Buckl, Christian and Fiege, Ludger and Armbruster, Michael and Knoll, Alois and Spiegelberg, Gernot},
title = {Early Safety Evaluation of Design Decisions in E/E Architecture According to ISO 26262},
year = {2012},
isbn = {9781450313476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304656.2304658},
doi = {10.1145/2304656.2304658},
abstract = {ISO 26262 addresses development of safe in-vehicle functions by specifying methods potentially used in the design and development lifecycle. It does not indicate what is sufficient and leaves room for interpretation. However, the architects of electric/electronic systems need design boundaries to make decisions during architecture evolution without adding a risk of late architectural changes. Designing and changing a system benefits from correct selection of safety mechanisms at early design stages. This paper presents an iterative architecture design and refinement process that is centered around ISO 26262 requirements. We propose a domain-specific modeling scheme and component repositories to build up a bottom-up analysis framework that allows early quantitative safety evaluation. To guarantee that the target ASIL level can be reached, we complement our design-time component-level analysis with conservative top-down analysis. Given that analysis starts at early design stages, evolution of the architecture is supported by different levels of detail used in the analysis framework.},
booktitle = {Proceedings of the 3rd International ACM SIGSOFT Symposium on Architecting Critical Systems},
pages = {1–10},
numpages = {10},
keywords = {architecture modeling, automotive systems, functional safety, integration of analysis techniques},
location = {Bertinoro, Italy},
series = {ISARCS '12}
}

@inproceedings{10.1145/1481848.1481860,
author = {Erk\"{o}k, Levent and Matthews, John},
title = {Pragmatic Equivalence and Safety Checking in Cryptol},
year = {2009},
isbn = {9781605583303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1481848.1481860},
doi = {10.1145/1481848.1481860},
abstract = {Cryptol is programming a language designed for specifying and programming cryptographic algorithms. In order to meet high-assurance requirements, Cryptol comes with a suite of formal-methods based tools allowing users to perform various program verification tasks. In the fully automated mode, Cryptol uses modern off-the-shelf SAT and SMT solvers to perform verification in a push-button manner. In the manual mode, Cryptol produces Isabelle/HOL specifications that can be interactively verified using the Isabelle theorem prover. In this paper, we provide an overview of Cryptol's verification toolset, describing our experiences with building a practical programming environment with dedicated support for formal verification.},
booktitle = {Proceedings of the 3rd Workshop on Programming Languages Meets Program Verification},
pages = {73–82},
numpages = {10},
keywords = {size polymorphism, cryptography, equivalence checking, sat/smt solving, theorem proving, formal methods},
location = {Savannah, GA, USA},
series = {PLPV '09}
}

@article{10.1145/3095807,
author = {Bowen, Judy and Reeves, Steve},
title = {Generating Obligations, Assertions and Tests from UI Models},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {EICS},
url = {https://doi.org/10.1145/3095807},
doi = {10.1145/3095807},
abstract = {Model-based development of interactive systems provides a number of benefits which can support the creation of robust and correct systems, particularly important when the interactive systems are safety-critical. Many different approaches have been proposed which target the models at different aspects of the development process (for example task analysis, interface layouts, functional behaviours etc.) and which can be used in different ways (verification of correctness, plasticity, usability).One of the aims for any modelling method should be simplicity - we are after all trying to hide complexity via abstraction in order to make reasoning about systems more tractable than working at the programming level. One of the challenges that exists however we do our modelling is ensuring the consistency between the model of the interface and interactivity and model of the functional behaviour of the system. This is primarily due to the different types of models that most naturally describe these different elements. In this paper we propose a method of tightening the integration of models of these different components of the system by generating obligations which explicitly describe the coupling of functional behaviour with interactive elements. We then show how these obligations can be used to support the development process during the programming and testing of the system.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {5},
numpages = {18},
keywords = {testing, model-driven development}
}

@article{10.1145/3428212,
author = {Sotiropoulos, Thodoris and Chaliasos, Stefanos and Mitropoulos, Dimitris and Spinellis, Diomidis},
title = {A Model for Detecting Faults in Build Specifications},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428212},
doi = {10.1145/3428212},
abstract = {Incremental and parallel builds are crucial features of modern build systems. Parallelism enables fast builds by running independent tasks simultaneously, while incrementality saves time and computing resources by processing the build operations that were affected by a particular code change. Writing build definitions that lead to error-free incremental and parallel builds is a challenging task. This is mainly because developers are often unable to predict the effects of build operations on the file system and how different build operations interact with each other. Faulty build scripts may seriously degrade the reliability of automated builds, as they cause build failures, and non-deterministic and incorrect outputs. To reason about arbitrary build executions, we present BuildFS, a generally-applicable model that takes into account the specification (as declared in build scripts) and the actual behavior (low-level file system operation) of build operations. We then formally define different types of faults related to incremental and parallel builds in terms of the conditions under which a file system operation violates the specification of a build operation. Our testing approach, which relies on the proposed model, analyzes the execution of single full build, translates it into BuildFS, and uncovers faults by checking for corresponding violations. We evaluate the effectiveness, efficiency, and applicability of our approach by examining 612 Make and Gradle projects. Notably, thanks to our treatment of build executions, our method is the first to handle JVM-oriented build systems. The results indicate that our approach is (1) able to uncover several important issues (247 issues found in 47 open-source projects have been confirmed and fixed by the upstream developers), and (2) much faster than a state-of-the-art tool for Make builds (the median and average speedup is 39X and 74X respectively).},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {144},
numpages = {30},
keywords = {incremental builds, parallel builds, Gradle, JVM-based builds, Make}
}

@inproceedings{10.1007/978-3-642-33666-9_2,
author = {S\'{a}nchez-Cuadrado, Jes\'{u}s and De Lara, Juan and Guerra, Esther},
title = {Bottom-up Meta-Modelling: An Interactive Approach},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_2},
doi = {10.1007/978-3-642-33666-9_2},
abstract = {The intensive use of models in Model-Driven Engineering (MDE) raises the need to develop meta-models with different aims, like the construction of textual and visual modelling languages and the specification of source and target ends of model-to-model transformations. While domain experts have the knowledge about the concepts of the domain, they usually lack the skills to build meta-models. These should be tailored according to their future usage and specific implementation platform, which demands knowledge available only to engineers with great expertise in MDE platforms. These issues hinder a wider adoption of MDE both by domain experts and software engineers.In order to alleviate this situation we propose an interactive, iterative approach to meta-model construction enabling the specification of model fragments by domain experts, with the possibility of using informal drawing tools like Dia. These fragments can be annotated with hints about the intention or needs for certain elements. A meta-model is automatically induced, which can be refactored in an interactive way, and then compiled into an implementation meta-model using profiles and patterns for different platforms and purposes.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {3–19},
numpages = {17},
keywords = {meta-model design exploration, meta-modelling, interactive meta-modelling, domain-specific modelling languages},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@article{10.1145/3394979,
author = {Rocha Silva, Thiago and Winckler, Marco and Tr\ae{}tteberg, Hallvard},
title = {Ensuring the Consistency between User Requirements and Task Models: A Behavior-Based Automated Approach},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {EICS},
url = {https://doi.org/10.1145/3394979},
doi = {10.1145/3394979},
abstract = {Evaluating and ensuring the consistency between user requirements and modeling artifacts is a long-time issue for model-based software design. Conflicts in requirements specifications can lead to many design errors and have a decisive impact on the quality of systems under development. This article presents an approach based on Behavior-Driven Development (BDD) to provide automated assessment for task models, which are intended to model the flow of user and system tasks in an interactive system. The approach has been evaluated by exploiting user requirements described by a group of experts in the domain of business trips. Such requirements gave rise to a set of BDD stories that have been used to automatically assess scenarios extracted from task models that were reengineered from an existing web system for booking business trips. The results have shown our approach, by performing a static analysis of the source files, was able to identify different types of inconsistencies between the user requirements and the set of task models analyzed.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {77},
numpages = {32},
keywords = {behavior-driven development (BDD), user stories, task models, automated requirements assessment}
}

@inproceedings{10.1145/3424771.3424821,
author = {Zimmermann, Olaf and Pautasso, Cesare and L\"{u}bke, Daniel and Zdun, Uwe and Stocker, Mirko},
title = {Data-Oriented Interface Responsibility Patterns: Types of Information Holder Resources},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424821},
doi = {10.1145/3424771.3424821},
abstract = {Remote Application Programming Interfaces (APIs) are used in almost any distributed system today, for instance in microservices-based systems, and are thus enablers for many digitalization efforts. API design not only impacts whether software provided as a service is easy and efficient to develop applications with, but also affects the long term evolution of the software system. In general, APIs are responsible for providing remote and controlled access to the functionality provided as services; however, APIs often are also used to expose and share information. We focus on such data-related aspects of microservice APIs in this paper. Depending on the life cycle of the information published through the API, its mutability and the endpoint role, data-oriented APIs can be designed following patterns such as Operational Data Holder, Master Data Holder, Reference Data Holder, Data Transfer Holder, and Link Lookup Resource. Known uses and examples of the patterns are drawn from public Web APIs as well as application development and integration projects we have been involved in.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {11},
numpages = {25},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@inproceedings{10.1145/3510003.3510040,
author = {Noller, Yannic and Shariffdeen, Ridwan and Gao, Xiang and Roychoudhury, Abhik},
title = {Trust Enhancement Issues in Program Repair},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510040},
doi = {10.1145/3510003.3510040},
abstract = {Automated program repair is an emerging technology that seeks to automatically rectify bugs and vulnerabilities using learning, search, and semantic analysis. Trust in automatically generated patches is necessary for achieving greater adoption of program repair. Towards this goal, we survey more than 100 software practitioners to understand the artifacts and setups needed to enhance trust in automatically generated patches. Based on the feedback from the survey on developer preferences, we quantitatively evaluate existing test-suite based program repair tools. We find that they cannot produce high-quality patches within a top-10 ranking and an acceptable time period of 1 hour. The developer feedback from our qualitative study and the observations from our quantitative examination of existing repair tools point to actionable insights to drive program repair research. Specifically, we note that producing repairs within an acceptable time-bound is very much dependent on leveraging an abstract search space representation of a rich enough search space. Moreover, while additional developer inputs are valuable for generating or ranking patches, developers do not seem to be interested in a significant human-in-the-loop interaction.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2228–2240},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2983990.2984038,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding Compiler Bugs via Live Code Mutation},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984038},
doi = {10.1145/2983990.2984038},
abstract = {Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions. This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable. We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {849–863},
numpages = {15},
keywords = {automated testing, Compiler testing, miscompilation, equivalent program variants},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@article{10.1145/3022671.2984038,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding Compiler Bugs via Live Code Mutation},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022671.2984038},
doi = {10.1145/3022671.2984038},
abstract = {Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions. This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable. We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {849–863},
numpages = {15},
keywords = {automated testing, miscompilation, Compiler testing, equivalent program variants}
}

@inproceedings{10.1145/2103656.2103709,
author = {Zhao, Jianzhou and Nagarakatte, Santosh and Martin, Milo M.K. and Zdancewic, Steve},
title = {Formalizing the LLVM Intermediate Representation for Verified Program Transformations},
year = {2012},
isbn = {9781450310833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103656.2103709},
doi = {10.1145/2103656.2103709},
abstract = {This paper presents Vellvm (verified LLVM), a framework for reasoning about programs expressed in LLVM's intermediate representation and transformations that operate on it. Vellvm provides a mechanized formal semantics of LLVM's intermediate representation, its type system, and properties of its SSA form. The framework is built using the Coq interactive theorem prover. It includes multiple operational semantics and proves relations among them to facilitate different reasoning styles and proof techniques.To validate Vellvm's design, we extract an interpreter from the Coq formal semantics that can execute programs from LLVM test suite and thus be compared against LLVM reference implementations. To demonstrate Vellvm's practicality, we formalize and verify a previously proposed transformation that hardens C programs against spatial memory safety violations. Vellvm's tools allow us to extract a new, verified implementation of the transformation pass that plugs into the real LLVM infrastructure; its performance is competitive with the non-verified, ad-hoc original.},
booktitle = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {427–440},
numpages = {14},
keywords = {LLVM, memory safety, Coq},
location = {Philadelphia, PA, USA},
series = {POPL '12}
}

@article{10.1145/2103621.2103709,
author = {Zhao, Jianzhou and Nagarakatte, Santosh and Martin, Milo M.K. and Zdancewic, Steve},
title = {Formalizing the LLVM Intermediate Representation for Verified Program Transformations},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/2103621.2103709},
doi = {10.1145/2103621.2103709},
abstract = {This paper presents Vellvm (verified LLVM), a framework for reasoning about programs expressed in LLVM's intermediate representation and transformations that operate on it. Vellvm provides a mechanized formal semantics of LLVM's intermediate representation, its type system, and properties of its SSA form. The framework is built using the Coq interactive theorem prover. It includes multiple operational semantics and proves relations among them to facilitate different reasoning styles and proof techniques.To validate Vellvm's design, we extract an interpreter from the Coq formal semantics that can execute programs from LLVM test suite and thus be compared against LLVM reference implementations. To demonstrate Vellvm's practicality, we formalize and verify a previously proposed transformation that hardens C programs against spatial memory safety violations. Vellvm's tools allow us to extract a new, verified implementation of the transformation pass that plugs into the real LLVM infrastructure; its performance is competitive with the non-verified, ad-hoc original.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {427–440},
numpages = {14},
keywords = {Coq, memory safety, LLVM}
}

@inproceedings{10.1145/2467307.2467316,
author = {Taromirad, Masoumeh and Paige, Richard F.},
title = {Agile Requirements Traceability Using Domain-Specific Modelling Languages},
year = {2012},
isbn = {9781450318044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2467307.2467316},
doi = {10.1145/2467307.2467316},
abstract = {Requirements traceability is an important mechanism for managing verification, validation and change impact analysis challenges in system engineering. Numerous model-based approaches have been proposed to support requirements traceability, but significant challenges remain, including finding the appropriate level of granularity for modelling traceability and coping with the lack of uniformity in requirements management tools. This paper argues for an agile modelling approach to managing requirements traceability and, in this context, proposes a domain/project-specific requirements traceability modelling approach. The preliminary approach is illustrated briefly in the context of the safety-critical systems engineering domain, where agile traceability from functional and safety requirements is necessary to underpin certification.},
booktitle = {Proceedings of the 2012 Extreme Modeling Workshop},
pages = {45–50},
numpages = {6},
location = {Innsbruck, Austria},
series = {XM '12}
}

@inproceedings{10.1145/3236024.3236055,
author = {Hu, Gang and Zhu, Linjie and Yang, Junfeng},
title = {AppFlow: Using Machine Learning to Synthesize Robust, Reusable UI Tests},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236055},
doi = {10.1145/3236024.3236055},
abstract = {UI testing is known to be difficult, especially as today’s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an “add to cart” test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides “smoke testing” requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {269–282},
numpages = {14},
keywords = {UI recognition, test reuse, mobile testing, machine learning, UI testing, test synthesis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.5555/800054.802007,
author = {Boehm, Barry W. and Gray, Terence E. and Seewaldt, Thomas},
title = {Prototyping vs. Specifying: A Multi-Project Experiment},
year = {1984},
isbn = {0818605286},
publisher = {IEEE Press},
abstract = {In this experiment, seven software teams developed versions of the same small-size (2000-4000 source instruction) application software product. Four teams used the Specifying approach. Three teams used the Prototyping approach.The main results of the experiment were:Prototyping yielded products with roughly equivalent performance, but with about 40% less code and 45% less effort.The prototyped products rated somewhat lower on functionality and robustness, but higher on ease of use and ease of learning.Specifying produced more coherent designs and software that was easier to integrate.The paper presents the experimental data supporting these and a number of additional conclusions.},
booktitle = {Proceedings of the 7th International Conference on Software Engineering},
pages = {473–484},
numpages = {12},
location = {Orlando, Florida, USA},
series = {ICSE '84}
}

@article{10.1145/3447680,
author = {Jeong, Eunjin and Jeong, Dowhan and Ha, Soonhoi},
title = {Dataflow Model–Based Software Synthesis Framework for Parallel and Distributed Embedded Systems},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3447680},
doi = {10.1145/3447680},
abstract = {Existing software development methodologies mostly assume that an application runs on a single device without concern about the non-functional requirements of an embedded system such as latency and resource consumption. Besides, embedded software is usually developed after the hardware platform is determined, since a non-negligible portion of the code depends on the hardware platform. In this article, we present a novel model-based software synthesis framework for parallel and distributed embedded systems. An application is specified as a set of tasks with the given rules for execution and communication. Having such rules enables us to perform static analysis to check some software errors at compile-time to reduce the verification difficulty. Platform-specific programs are synthesized automatically after the mapping of tasks onto processing elements is determined. The proposed framework is expandable to support new hardware platforms easily. The proposed communication code synthesis method is extensible and flexible to support various communication methods between devices. In addition, the fault-tolerant feature can be added by modifying the task graph automatically according to the selected fault-tolerance configurations by the user. The viability of the proposed software development methodology is evaluated with a real-life surveillance application that runs on six processing elements.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {jun},
articleno = {35},
numpages = {38},
keywords = {fault tolerance, synchronous dataflow, Code generation, embedded software development}
}

@article{10.1145/3530813,
author = {Fahmideh, Mahdi and Grundy, John and Ahmad, Aakash and Shen, Jun and Yan, Jun and Mougouei, Davoud and Wang, Peng and Ghose, Aditya and Gunawardana, Anuradha and Aickelin, Uwe and Abedin, Babak},
title = {Engineering Blockchain-Based Software Systems: Foundations, Survey, and Future Directions},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3530813},
doi = {10.1145/3530813},
abstract = {Many scientific and practical areas have shown increasing interest in reaping the benefits of blockchain technology to empower software systems. However, the unique characteristics and requirements associated with Blockchain-based Software (BBS) systems raise new challenges across the development lifecycle that entail an extensive improvement of conventional software engineering. This article presents a systematic literature review of the state-of-the-art in BBS engineering research from the perspective of the software engineering discipline. We characterize BBS engineering based on the key aspects of theoretical foundations, processes, models, and roles. Based on these aspects, we present a rich repertoire of development tasks, design principles, models, roles, challenges, and resolution techniques. The focus and depth of this survey not only give software engineering practitioners and researchers a consolidated body of knowledge about current BBS development but also underpin a starting point for further research in this field.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {110},
numpages = {44},
keywords = {Systems development methods, Software engineering, blockchain, blockchain-based software systems}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1145/3180495,
author = {Kulla, Christopher and Conty, Alejandro and Stein, Clifford and Gritz, Larry},
title = {Sony Pictures Imageworks Arnold},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/3180495},
doi = {10.1145/3180495},
abstract = {Sony Imageworks’ implementation of the Arnold renderer is a fork of the commercial product of the same name, which has evolved independently since around 2009. This article focuses on the design choices that are unique to this version and have tailored the renderer to the specific requirements of film rendering at our studio. We detail our approach to subdivision surface tessellation, hair rendering, sampling, and variance reduction techniques, as well as a description of our open source texturing and shading language components. We also discuss some ideas we once implemented but have since discarded to highlight the evolution of the software over the years.},
journal = {ACM Trans. Graph.},
month = {aug},
articleno = {29},
numpages = {18},
keywords = {rendering, Ray tracing, Monte Carlo, path tracing}
}

@inproceedings{10.1145/3297858.3304019,
author = {Banerjee, Subho S. and Kalbarczyk, Zbigniew T. and Iyer, Ravishankar K.},
title = {AcMC 2 : Accelerating Markov Chain Monte Carlo Algorithms for Probabilistic Models},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304019},
doi = {10.1145/3297858.3304019},
abstract = {Probabilistic models (PMs) are ubiquitously used across a variety of machine learning applications. They have been shown to successfully integrate structural prior information about data and effectively quantify uncertainty to enable the development of more powerful, interpretable, and efficient learning algorithms. This paper presents AcMC2, a compiler that transforms PMs into optimized hardware accelerators (for use in FPGAs or ASICs) that utilize Markov chain Monte Carlo methods to infer and query a distribution of posterior samples from the model. The compiler analyzes statistical dependencies in the PM to drive several optimizations to maximally exploit the parallelism and data locality available in the problem. We demonstrate the use of AcMC2 to implement several learning and inference tasks on a Xilinx Virtex-7 FPGA. AcMC2-generated accelerators provide a 47-100\texttimes{} improvement in runtime performance over a 6-core IBM Power8 CPU and a 8-18\texttimes{} improvement over an NVIDIA K80 GPU. This corresponds to a 753-1600\texttimes{} improvement over the CPU and 248-463\texttimes{} over the GPU in performance-per-watt terms.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {515–528},
numpages = {14},
keywords = {accelerator, markov chain monte carlo, probabilistic graphical models, probabilistic programming},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{10.1145/275295.275375,
author = {Wheeler, Sharon and Duggins, Sheryl},
title = {Improving Software Quality},
year = {1998},
isbn = {1581130309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/275295.275375},
doi = {10.1145/275295.275375},
booktitle = {Proceedings of the 36th Annual Southeast Regional Conference},
pages = {300–309},
numpages = {10},
series = {ACM-SE 36}
}

@article{10.1007/s00165-017-0443-1,
author = {Corrodi, Claudio and Heu\ss{}ner, Alexander and Poskitt, Christopher M.},
title = {A Semantics Comparison Workbench for a Concurrent, Asynchronous, Distributed Programming Language},
year = {2018},
issue_date = {Jan 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0443-1},
doi = {10.1007/s00165-017-0443-1},
abstract = {A number of high-level languages and libraries have been proposed that offer novel and simple to use abstractions for concurrent, asynchronous, and distributed programming. The execution models that realise them, however, often change over time—whether to improve performance, or to extend them to new language features—potentially affecting behavioural and safety properties of existing programs. This is exemplified by Scoop, a message-passing approach to concurrent object-oriented programming that has seen multiple changes proposed and implemented, with demonstrable consequences for an idiomatic usage of its core abstraction. We propose a semantics comparison workbench for Scoop with fully and semi-automatic tools for analysing and comparing the state spaces of programs with respect to different execution models or semantics. We demonstrate its use in checking the consistency of properties across semantics by applying it to a set of representative programs, and highlighting a deadlock-related discrepancy between the principal execution models of Scoop. Furthermore, we demonstrate the extensibility of the workbench by generalising the formalisation of an execution model to support recently proposed extensions for distributed programming. Our workbench is based on a modular and parameterisable graph transformation semantics implemented in the Groove tool. We discuss how graph transformations are leveraged to atomically model intricate language abstractions, how the visual yet&nbsp;algebraic nature of the model can be used to ascertain soundness, and highlight how the approach could be applied to similar languages.},
journal = {Form. Asp. Comput.},
month = {jan},
pages = {163–192},
numpages = {30},
keywords = {Object-oriented programming, Distributed programming with message passing, Graph transformation systems, Software engineering, Groove, Concurrency abstractions, Concurrent asynchronous programming, Runtime semantics, Operational semantics, Scoop, Verification/analysis parameterised by semantics}
}

@article{10.1145/3241743,
author = {Stol, Klaas-Jan and Fitzgerald, Brian},
title = {The ABC of Software Engineering Research},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3241743},
doi = {10.1145/3241743},
abstract = {A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {11},
numpages = {51},
keywords = {Research methodology, research strategy}
}

@inproceedings{10.1145/3328433.3328447,
author = {Stocco, Andrea},
title = {How Artificial Intelligence Can Improve Web Development and Testing},
year = {2019},
isbn = {9781450362573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328433.3328447},
doi = {10.1145/3328433.3328447},
abstract = {The Artificial Intelligence (AI) revolution in software development is just around the corner. With the rise of AI, developers are expected to play a different role from the traditional role of programmers, as they will need to adapt their know-how and skillsets to complement and apply AI-based tools and techniques into their traditional web development workflow. In this extended abstract, some of the current trends on how AI is being leveraged to enhance web development and testing are discussed, along with some of the main opportunities and challenges for researchers.},
booktitle = {Companion Proceedings of the 3rd International Conference on the Art, Science, and Engineering of Programming},
articleno = {13},
numpages = {4},
keywords = {web development, web testing, artificial intelligence},
location = {Genova, Italy},
series = {Programming '19}
}

@inproceedings{10.5555/2050655.2050704,
author = {Wilke, Claas and G\"{o}tz, Sebastian and Reimann, Jan and A\ss{}mann, Uwe},
title = {Vision Paper: Towards Model-Based Energy Testing},
year = {2011},
isbn = {9783642244841},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Today, energy consumption is one of the major challenges for optimisation of future software applications and ICT infrastructures. To develop software w.r.t. its energy consumption, testing is an essential activity, since testing allows quality assurance and thus, energy consumption reduction during the software's development. Although first approaches measuring and predicting software's energy consumption for its execution on a specific hardware platform exist, no model-based testing approach has been developed, yet. In this paper we present our vision of a model-based energy testing approach that uses a combination of abstract interpretation and run-time profiling to predict the energy consumption of software applications and to derive energy consumption test cases.},
booktitle = {Proceedings of the 14th International Conference on Model Driven Engineering Languages and Systems},
pages = {480–489},
numpages = {10},
keywords = {profiling, energy consumption testing, unit testing, abstract interpretation, model-based testing},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@inproceedings{10.5555/2819009.2819099,
author = {Abreu, Rui and Erdogmus, Hakan and Perez, Alexandre},
title = {CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts},
year = {2015},
publisher = {IEEE Press},
abstract = {Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plugin mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, CodeAware, for distributed and fine-grained artifact analysis. CodeAware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. CodeAware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems; (b) the ability to perform both static and dynamic analyses on these artifacts; and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of CodeAware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {551–554},
numpages = {4},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3323503.3360639,
author = {Pinto, Thiago Delgado and Gon\c{c}alves, Willian Inacio and Costa, Pablo Veiga},
title = {User Interface Prototype Generation from Agile Requirements Specifications Written in Concordia},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360639},
doi = {10.1145/3323503.3360639},
abstract = {User interface prototypes (UIP) are widely used to get feedback before building a software feature. They can prevent misunderstandings between the software development team and other stakeholders (e.g., users, investors) that lead to rework or a resulting software that does not meet their needs. UIP can also be a valuable resource in Agile software development, in which feedback is key. In this paper, we present an approach to generate UIP automatically from Agile requirements specifications written in Concordia and its corresponding prototype tool. The tool is able to generate UIP for web-based applications. We evaluated the approach and the tool with questionnaires, and the results revealed that: (i) the generated UIP are very similar to those drawn by respondents; (ii) the generated source code has good enough quality to be reused by developers; and (iii) they save design and development time.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {61–64},
numpages = {4},
keywords = {user interface, user story, generation, concordia, agile},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@article{10.14778/3415478.3415481,
author = {Khurana, Kapil and Haritsa, Jayant R.},
title = {UNMASQUE: A Hidden SQL Query Extractor},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415481},
doi = {10.14778/3415478.3415481},
abstract = {Given a database instance and a populated result, query reverse-engineering attempts to identify candidate SQL queries that produce this result on the instance. A variant of this problem arises when a ground-truth is additionally available, but hidden within an opaque database application. In this demo, we present UN-MASQUE, an extraction algorithm that is capable of precisely identifying a substantive class of such hidden queries. A hallmark of its design is that the extraction is completely non-invasive to the application. Specifically, it only examines the results obtained from application executions on databases derived with a combination of data mutation and data generation techniques, thereby achieving platform-independence. Further, potent optimizations, such as database size reduction to a few rows, are incorporated to minimize the extraction overheads. The demo showcases these features on both declarative and imperative applications.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {2809–2812},
numpages = {4}
}

@article{10.1145/351159.351173,
author = {Heering, Jan and Klint, Paul},
title = {Semantics of Programming Languages: A Tool-Oriented Approach},
year = {2000},
issue_date = {March 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/351159.351173},
doi = {10.1145/351159.351173},
abstract = {By paying more attention to semantics-based tool generation, programming language semantics can significantly increase its impact. Ultimately, this may lead to "Language Design Assistants" incorporating substantial amounts of semantic knowledge.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {39–48},
numpages = {10}
}

@inproceedings{10.1145/1869643.1869648,
author = {Rideau, Francois-Ren\'{e} and Goldman, Robert P.},
title = {Evolving ASDF: More Cooperation, Less Coordination},
year = {2010},
isbn = {9781450304702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869643.1869648},
doi = {10.1145/1869643.1869648},
abstract = {We present ASDF2, the current state of the art in CL build systems. From a technical standpoint, ASDF2 improves upon ASDF by integrating previous common extensions, making configuration easy, and fixing bugs. However the overriding concern driving these changes was social rather than technical: ASDF plays a central role in the CL community and we wanted to reduce the coordination costs that it imposed upon CL programmers. We outline ASDF's history and architecture, explain the link between the social issues we faced and the software features we added, and explore the technical challenges involved and lessons learned, notably involving inplace code upgrade of ASDF itself, backward compatibility, portability, testing and other coding best practices.},
booktitle = {Proceedings of the 2010 International Conference on Lisp},
pages = {29–42},
numpages = {14},
keywords = {dynamic code update., common lisp, build infrastructure, interaction design, code evolution},
location = {Reno/Tahoe, Nevada, USA},
series = {ILC '10}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/2983990,
title = {OOPSLA 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/1022685.1022944,
author = {Brini, Silvia and Benjelloun, Doha and Castanier, Fabien},
title = {A Flexible Virtual Platform for Computational and Communication Architecture Exploration of DMT VDSL Modems},
year = {2003},
isbn = {0769518702},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper a high-level SoC architecture exploration of DMT (Discrete Multitone) VDSL transceivers (Very high speed Digital Subscriber Line) is presented. A flexible and complete virtual platform was developed for the purpose, exploiting the paradigm of "orthogonalization of concerns" (functionality independent from architecture) in the framework of Cadence VCC system level design tool. An accurate processor model, obtained through the back-annotation of profiling results on a target DSP core, allowed the exploration of different HW/SW partitioning and the study of the computational units required. A transaction-accurate VCC bus model was developed for the investigation of the on-chip bus architecture and its relevant parameters dimensioning.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe: Designers' Forum - Volume 2},
pages = {20164},
series = {DATE '03}
}

@proceedings{10.1145/2837614,
title = {POPL '16: Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
year = {2016},
isbn = {9781450335492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. Petersburg, FL, USA}
}

@inproceedings{10.1145/1353482.1353484,
author = {Benz, Sebastian},
title = {AspectT: Aspect-Oriented Test Case Instantiation},
year = {2008},
isbn = {9781605580449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1353482.1353484},
doi = {10.1145/1353482.1353484},
abstract = {Test case instantiation is the transformation of abstract test cases into executable test scripts. Abstract test cases are either created during model based test case generation or are manually defined in a suitable modeling notation. The transformation varies depending on different testing concerns, such as test goal, test setup and test phase. Thus, for each testing concern a new transformation must be defined. This paper introduces AspectT, an aspect-oriented language for the instantiation of abstract test cases. We reduce the effort of test case instantiation by modularizing testing concerns in the form of aspects to enable their reuse in different testing contexts. The approach is implemented and integrated in an existing testing framework and has been successfully applied to test an electronic control unit of an automotive infotainment system at BMW Group.},
booktitle = {Proceedings of the 7th International Conference on Aspect-Oriented Software Development},
pages = {1–12},
numpages = {12},
keywords = {model-based testing, aspect-orientation, test case instantiation, test case generation},
location = {Brussels, Belgium},
series = {AOSD '08}
}

@inproceedings{10.1145/3239372.3239404,
author = {Jolak, Rodi and Ho-Quang, Truong and Chaudron, Michel R.V. and Schiffelers, Ramon R.H.},
title = {Model-Based Software Engineering: A Multiple-Case Study on Challenges and Development Efforts},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239404},
doi = {10.1145/3239372.3239404},
abstract = {A recurring theme in discussions about the adoption of Model-Based Engineering (MBE) is its effectiveness. This is because there is a lack of empirical assessment of the processes and (tool-)use of MBE in practice. We conducted a multiple-case study by observing 2 two-month MBE projects from which software for a Mars rover were developed. We focused on assessing the distribution of the total software development effort over different development activities. Moreover, we observed and collected challenges reported by the developers during the execution of projects. We found that the majority of the effort is spent on the collaboration and communication activities. Furthermore, our inquiry into challenges showed that tool-related challenges are the most encountered.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {213–223},
numpages = {11},
keywords = {Effort Distribution, Case Study Design, Model-Based Engineering, Software Engineering, MBE Challenges, Modeling Tools},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@article{10.1145/986913.986917,
author = {Sammet, Jean E.},
title = {Roster of Programming Languages for 1973},
year = {1974},
issue_date = {November 1974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {11},
issn = {0362-1340},
url = {https://doi.org/10.1145/986913.986917},
doi = {10.1145/986913.986917},
journal = {SIGPLAN Not.},
month = {nov},
pages = {18–31},
numpages = {14}
}

@inproceedings{10.1145/2652524.2652587,
author = {Rodrigues, Elder M. and Saad, Rodrigo S. and Oliveira, Flavio M. and Costa, Leandro T. and Bernardino, Maicon and Zorzo, Avelino F.},
title = {Evaluating Capture and Replay and Model-Based Performance Testing Tools: An Empirical Comparison},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652587},
doi = {10.1145/2652524.2652587},
abstract = {[Context] A variety of testing tools have been developed to support and automate software performance testing activities. These tools may use different techniques, such as Model-Based Testing (MBT) or Capture and Replay (CR). [Goal] For software companies, it is important to evaluate such tools w.r.t. the effort required for creating test artifacts using them; despite its importance, there are few empirical studies comparing performance testing tools, specially tools developed with different approaches. [Method] We are conducting experimental studies to provide evidence about the required effort to use CR-based tools and MBT tools. In this paper, we present our first results, evaluating the effort (time spent) when using LoadRunner and Visual Studio CR-based tools, and the PLeTsPerf MBT tool to create performance test scripts and scenarios to test Web applications, in the context of a collaboration project between Software Engineering Research Center at PUCRS and a technological laboratory of a global IT company. [Results] Our results indicate that, for simple testing tasks, the effort of using a CR-based tool was lower than using an MBT tool, but as the testing complexity increases tasks, the advantage of using MBT grows significantly. [Conclusions] To conclude, we discuss the lessons we learned from the design, operation, and analysis of our empirical experiment.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {9},
numpages = {8},
keywords = {testing tools, experiment, performance testing},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.1109/ICSE43902.2021.00118,
author = {Mayr-Dorn, Christoph and Vierhauser, Michael and Bichler, Stefan and Keplinger, Felix and Cleland-Huang, Jane and Egyed, Alexander and Mehofer, Thomas},
title = {Supporting Quality Assurance with Automated Process-Centric Quality Constraints Checking},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00118},
doi = {10.1109/ICSE43902.2021.00118},
abstract = {Regulations, standards, and guidelines for safety-critical systems stipulate stringent traceability but do not prescribe the corresponding, detailed software engineering process. Given the industrial practice of using only semi-formal notations to describe engineering processes, processes are rarely "executable" and developers have to spend significant manual effort in ensuring that they follow the steps mandated by quality assurance. The size and complexity of systems and regulations makes manual, timely feedback from Quality Assurance (QA) engineers infeasible. In this paper we propose a novel framework for tracking processes in the background, automatically checking QA constraints depending on process progress, and informing the developer of unfulfilled QA constraints. We evaluate our approach by applying it to two different case studies; one open source community system and a safety-critical system in the air-traffic control domain. Results from the analysis show that trace links are often corrected or completed after the fact and thus timely and automated constraint checking support has significant potential on reducing rework.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1298–1310},
numpages = {13},
keywords = {developer support, software engineering process, traceability},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/514144.514743,
author = {Wainer, Gabriel A. and Daicz, Sergio and De Simoni, Luis F. and Wassermann, Demian},
title = {Using the Alfa-1 Simulated Processor for Educational Purposes},
year = {2001},
issue_date = {December 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1531-4278},
url = {https://doi.org/10.1145/514144.514743},
doi = {10.1145/514144.514743},
abstract = {Alfa-1 is a simulated computer designed for computer organization courses. Alfa-1 and its accompanying toolkit allow students to acquire practical insights into developing hardware by extending existing components. The DEVS formalism is used to model individual components and to integrate them into a hierarchy that describes the detailed behavior of different levels of a computer's architecture. We introduce Alfa-1 and the toolkit, show how to extend existing components, and describe how to use Alfa-1 for educational purposes. We also explain how to assemble, link, and execute applications and how to test new extensions usingthe testing tools.},
journal = {J. Educ. Resour. Comput.},
month = {dec},
pages = {111–151},
numpages = {41},
keywords = {systems specification, modeling computer architectures, DEVS formalism}
}

@inproceedings{10.1145/2593743.2593747,
author = {Malakuti, Somayeh and Wilke, Claas},
title = {Energy Aspects: Modularizing Energy-Aware Applications},
year = {2014},
isbn = {9781450328449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593743.2593747},
doi = {10.1145/2593743.2593747},
abstract = {To effectively extend legacy applications with energy-awareness functionality, dedicated modularization mechanisms are required. This paper introduces the GreenDev framework, which integrates energy testing and event-based modularization for this matter. Energy testing facilitates identifying the energy-related interfaces of applications to the energy-awareness functionality, and event-based modularization facilitates modularizing this functionality from the base functionality of the applications. To maintain loose coupling among these, GreenDev offers a dedicated interface definition language, which enables defining the interfaces abstractly from the actual implementation of the applications. The applications are automatically augmented with these interfaces. We illustrate the applicability of GreenDev in implementing an energy-aware mobile emailing app.},
booktitle = {Proceedings of the 3rd International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {energy st-atechart, aspect-orientated programming, energy testing, Event-based modularization},
location = {Hyderabad, India},
series = {GREENS 2014}
}

@inproceedings{10.1145/3064899.3064907,
author = {Peijnenburg, Falco and Hage, Jurriaan and Serrano, Alejandro},
title = {Type Directives and Type Graphs in Elm},
year = {2016},
isbn = {9781450347679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064899.3064907},
doi = {10.1145/3064899.3064907},
abstract = {We introduce type graphs into Elm in order to improve type error messages for infinite types, and integrate type qualifiers (for type classes a la Haskell) and Elm's row polymorphism into type graphs. We also discuss how specialized type rules and siblings can be used to achieve domain-specific type error diagnosis in the context of Elm.},
booktitle = {Proceedings of the 28th Symposium on the Implementation and Application of Functional Programming Languages},
articleno = {2},
numpages = {12},
keywords = {type error diagnosis, embedded domain specific languages, type graphs, type classes, Elm},
location = {Leuven, Belgium},
series = {IFL 2016}
}

@inproceedings{10.1145/1478873.1478890,
author = {Allen, J. R. and Yau, S. S.},
title = {Real-Time Fault Detection for Small Computers},
year = {1971},
isbn = {9781450379090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1478873.1478890},
doi = {10.1145/1478873.1478890},
abstract = {Advancing technology and declining costs have led to a sharp increase in the number and variety of small computers in use. Because small computers are readily suited for many real-time applications, a great deal of work has been directed toward simplifying the interface between the computer and its peripherals. Hardware interrupting capability and a specially designed I/O bus are required for peripheral device interfacing in a real-time environment and such things as direct memory access, data channels, and multilevel hardware and software interrupt capability are common. These machines tend to be parallel, synchronous computers with a relatively simple architecture.},
booktitle = {Proceedings of the May 16-18, 1972, Spring Joint Computer Conference},
pages = {119–127},
numpages = {9},
location = {Atlantic City, New Jersey},
series = {AFIPS '72 (Spring)}
}

@inproceedings{10.1145/3242744.3242748,
author = {Breitner, Joachim},
title = {A Promise Checked is a Promise Kept: Inspection Testing},
year = {2018},
isbn = {9781450358354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242744.3242748},
doi = {10.1145/3242744.3242748},
abstract = {Occasionally, developers need to ensure that the compiler treats their code in a specific way that is only visible by inspecting intermediate or final compilation artifacts. This is particularly common with carefully crafted compositional libraries, where certain usage patterns are expected to trigger an intricate sequence of compiler optimizations – stream fusion is a well-known example. The developer of such a library has to manually inspect build artifacts and check for the expected properties. Because this is too tedious to do often, it will likely go unnoticed if the property is broken by a change to the library code, its dependencies or the compiler. The lack of automation has led to released versions of such libraries breaking their documented promises. This indicates that there is an unrecognized need for a new testing paradigm, inspection testing, where the programmer declaratively describes non-functional properties of an compilation artifact and the compiler checks these properties. We define inspection testing abstractly, implement it in the context of the Haskell Compiler GHC and show that it increases the quality of such libraries.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Symposium on Haskell},
pages = {14–25},
numpages = {12},
keywords = {Testing, Haskell, Compilers},
location = {St. Louis, MO, USA},
series = {Haskell 2018}
}

@article{10.1145/3299711.3242748,
author = {Breitner, Joachim},
title = {A Promise Checked is a Promise Kept: Inspection Testing},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/3299711.3242748},
doi = {10.1145/3299711.3242748},
abstract = {Occasionally, developers need to ensure that the compiler treats their code in a specific way that is only visible by inspecting intermediate or final compilation artifacts. This is particularly common with carefully crafted compositional libraries, where certain usage patterns are expected to trigger an intricate sequence of compiler optimizations – stream fusion is a well-known example. The developer of such a library has to manually inspect build artifacts and check for the expected properties. Because this is too tedious to do often, it will likely go unnoticed if the property is broken by a change to the library code, its dependencies or the compiler. The lack of automation has led to released versions of such libraries breaking their documented promises. This indicates that there is an unrecognized need for a new testing paradigm, inspection testing, where the programmer declaratively describes non-functional properties of an compilation artifact and the compiler checks these properties. We define inspection testing abstractly, implement it in the context of the Haskell Compiler GHC and show that it increases the quality of such libraries.},
journal = {SIGPLAN Not.},
month = {sep},
pages = {14–25},
numpages = {12},
keywords = {Haskell, Compilers, Testing}
}

@inproceedings{10.5555/2394101.2394117,
author = {Weil, Frank and Mastenbrook, Brian and Nelson, David and Dietz, Paul and Van Den Berg, Aswin},
title = {Automated Semantic Analysis of Design Models},
year = {2007},
isbn = {3540752080},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Based on several years of experience in generating code from large SDL and UML models in the telecommunications domain, it has become apparent that model analysis must be used to augment more traditional validation and testing techniques. While model correctness is extremely important, the difficulty of use and non-scalability of most formal verification techniques when applied to large-scale design models renders them insufficient for most applications. We have also repeatedly seen that even the most complete test coverage fails to find many problems. In contrast, sophisticated model analysis techniques can be applied without human interaction to large-scale models. A discussion of the model analysis techniques and the model defects that they can detect is provided, along with some real-world examples of defects that have been caught.},
booktitle = {Proceedings of the 10th International Conference on Model Driven Engineering Languages and Systems},
pages = {166–180},
numpages = {15},
location = {Nashville, TN},
series = {MODELS'07}
}

@inproceedings{10.1145/3213846.3213859,
author = {Strandberg, Per Erik and Ostrand, Thomas J. and Weyuker, Elaine J. and Sundmark, Daniel and Afzal, Wasif},
title = {Automated Test Mapping and Coverage for Network Topologies},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213859},
doi = {10.1145/3213846.3213859},
abstract = {Communication devices such as routers and switches play a critical role in the reliable functioning of embedded system networks. Dozens of such devices may be part of an embedded system network, and they need to be tested in conjunction with various computational elements on actual hardware, in many different configurations that are representative of actual operating networks. An individual physical network topology can be used as the basis for a test system that can execute many test cases, by identifying the part of the physical network topology that corresponds to the configuration required by each individual test case. Given a set of available test systems and a large number of test cases, the problem is to determine for each test case, which of the test systems are suitable for executing the test case, and to provide the mapping that associates the test case elements (the logical network topology) with the appropriate elements of the test system (the physical network topology). We studied a real industrial environment where this problem was originally handled by a simple software procedure that was very slow in many cases, and also failed to provide thorough coverage of each network's elements. In this paper, we represent both the test systems and the test cases as graphs, and develop a new prototype algorithm that a) determines whether or not a test case can be mapped to a subgraph of the test system, b) rapidly finds mappings that do exist, and c) exercises diverse sets of network nodes when multiple mappings exist for the test case. The prototype has been implemented and applied to over 10,000 combinations of test cases and test systems, and reduced the computation time by a factor of more than 80 from the original procedure. In addition, relative to a meaningful measure of network topology coverage, the mappings achieved an increased level of thoroughness in exercising the elements of each test system.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {73–83},
numpages = {11},
keywords = {testing, subgraph isomorphism, test coverage, network topology},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3007120.3007123,
author = {Dumont, Cyril and Mourlin, Fabrice and Nel, Laurent},
title = {A Mobile Distributed System for Remote Resource Access},
year = {2016},
isbn = {9781450348065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3007120.3007123},
doi = {10.1145/3007120.3007123},
abstract = {Mobile and distributed systems involve multiple mobile computers processing data and communicating the results to each other, such as in electronic commerce or online voting, where the users are geographically separated. Our contribution is on mobile distributed applications based on embedded platforms such as smartphones or tablets. We provide a definition of a protocol called MEXP which stands for Mobile Exchange eXperiment Protocol. It allows the exposure of local resources on a mobile device to other mobile computers of the distributed system. The kinds of resources are pictures and sounds which are recorded with a mobile device during lab activities. They require the use of a local Wi-Fi network for the security of the recorded data. The lab activities evolve over time and the observers have remote access to the pictures and sounds for validation and tagging. This work has resulted in the acceptance of our mobile distributed application by an academic training team in the Biology department.},
booktitle = {Proceedings of the 14th International Conference on Advances in Mobile Computing and Multi Media},
pages = {154–163},
numpages = {10},
keywords = {data collection, NFC exchange, distributed application, Mobile architecture, mobile REST services},
location = {Singapore, Singapore},
series = {MoMM '16}
}

@inproceedings{10.1145/2804302.2804313,
author = {McDonell, Trevor L. and Chakravarty, Manuel M. T. and Grover, Vinod and Newton, Ryan R.},
title = {Type-Safe Runtime Code Generation: Accelerate to LLVM},
year = {2015},
isbn = {9781450338080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2804302.2804313},
doi = {10.1145/2804302.2804313},
abstract = {Embedded languages are often compiled at application runtime; thus, embedded compile-time errors become application runtime errors. We argue that advanced type system features, such as GADTs and type families, play a crucial role in minimising such runtime errors. Specifically, a rigorous type discipline reduces runtime errors due to bugs in both embedded language applications and the implementation of the embedded language compiler itself. In this paper, we focus on the safety guarantees achieved by type preserving compilation. We discuss the compilation pipeline of Accelerate, a high-performance array language targeting both multicore CPUs and GPUs, where we are able to preserve types from the source language down to a low-level register language in SSA form. Specifically, we demonstrate the practicability of our approach by creating a new type-safe interface to the industrial-strength LLVM compiler infrastructure, which we used to build two new Accelerate backends that show competitive runtimes on a set of benchmarks across both CPUs and GPUs.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell},
pages = {201–212},
numpages = {12},
keywords = {Dynamic Compilation, GPGPU, Code Generation, Haskell, Data Parallelism, Embedded Language, LLVM, Arrays},
location = {Vancouver, BC, Canada},
series = {Haskell '15}
}

@article{10.1145/2887747.2804313,
author = {McDonell, Trevor L. and Chakravarty, Manuel M. T. and Grover, Vinod and Newton, Ryan R.},
title = {Type-Safe Runtime Code Generation: Accelerate to LLVM},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {12},
issn = {0362-1340},
url = {https://doi.org/10.1145/2887747.2804313},
doi = {10.1145/2887747.2804313},
abstract = {Embedded languages are often compiled at application runtime; thus, embedded compile-time errors become application runtime errors. We argue that advanced type system features, such as GADTs and type families, play a crucial role in minimising such runtime errors. Specifically, a rigorous type discipline reduces runtime errors due to bugs in both embedded language applications and the implementation of the embedded language compiler itself. In this paper, we focus on the safety guarantees achieved by type preserving compilation. We discuss the compilation pipeline of Accelerate, a high-performance array language targeting both multicore CPUs and GPUs, where we are able to preserve types from the source language down to a low-level register language in SSA form. Specifically, we demonstrate the practicability of our approach by creating a new type-safe interface to the industrial-strength LLVM compiler infrastructure, which we used to build two new Accelerate backends that show competitive runtimes on a set of benchmarks across both CPUs and GPUs.},
journal = {SIGPLAN Not.},
month = {aug},
pages = {201–212},
numpages = {12},
keywords = {GPGPU, Dynamic Compilation, Code Generation, Haskell, Embedded Language, Data Parallelism, LLVM, Arrays}
}

@inproceedings{10.1145/3314221.3314601,
author = {Dasgupta, Sandeep and Park, Daejun and Kasampalis, Theodoros and Adve, Vikram S. and Ro\c{s}u, Grigore},
title = {A Complete Formal Semantics of X86-64 User-Level Instruction Set Architecture},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314601},
doi = {10.1145/3314221.3314601},
abstract = {We present the most complete and thoroughly tested formal semantics of x86-64 to date. Our semantics faithfully formalizes all the non-deprecated, sequential user-level instructions of the x86-64 Haswell instruction set architecture. This totals 3155 instruction variants, corresponding to 774 mnemonics. The semantics is fully executable and has been tested against more than 7,000 instruction-level test cases and the GCC torture test suite. This extensive testing paid off, revealing bugs in both the x86-64 reference manual and other existing semantics. We also illustrate potential applications of our semantics in different formal analyses, and discuss how it can be useful for processor verification.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1133–1148},
numpages = {16},
keywords = {x86-64, Formal Semantics, ISA specification},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@article{10.1145/844128.844152,
author = {White, Brian and Lepreau, Jay and Stoller, Leigh and Ricci, Robert and Guruprasad, Shashi and Newbold, Mac and Hibler, Mike and Barb, Chad and Joglekar, Abhijeet},
title = {An Integrated Experimental Environment for Distributed Systems and Networks},
year = {2003},
issue_date = {Winter 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {SI},
issn = {0163-5980},
url = {https://doi.org/10.1145/844128.844152},
doi = {10.1145/844128.844152},
abstract = {Three experimental environments traditionally support network and distributed systems research: network emulators, network simulators, and live networks. The continued use of multiple approaches highlights both the value and inadequacy of each. Netbed, a descendant of Emulab, provides an experimentation facility that integrates these approaches, allowing researchers to configure and access networks composed of emulated, simulated, and wide-area nodes and links. Netbed's primary goals are ease of use, control, and realism, achieved through consistent use of virtualization and abstraction.By providing operating system-like services, such as resource allocation and scheduling, and by virtualizing heterogeneous resources, Netbed acts as a virtual machine for network experimentation. This paper presents Netbed's overall design and implementation and demonstrates its ability to improve experimental automation and efficiency. These, in turn, lead to new methods of experimentation, including automated parameter-space studies within emulation and straightforward comparisons of simulated, emulated, and wide-area scenarios.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {dec},
pages = {255–270},
numpages = {16}
}

@inproceedings{10.1145/1409944.1409969,
author = {Liu, Xin and Sridharan, Ashwin and Machiraju, Sridhar and Seshadri, Mukund and Zang, Hui},
title = {Experiences in a 3G Network: Interplay between the Wireless Channel and Applications},
year = {2008},
isbn = {9781605580968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1409944.1409969},
doi = {10.1145/1409944.1409969},
abstract = {We present an experimental characterization of the physical and MAC layers in CDMA 1xEV-DO and their impact on transport layer performance. The 1xEV-DO network is currently the fastest mobile broadband cellular network, offering data rates of up to 3.1 Mbps for both stationary and mobile users. These rates are achieved by using novel capacity enhancement techniques at the lower layers. Specifically, 1xEV-DO incorporates rapid channel rate adaptation in response to signal conditions, and opportunistic scheduling to exploit channel fluctuations. Although shown to perform well in isolation, there is no comprehensive literature that examines the impact of these features on transport layer and application performance in real networks.We take the first step in addressing this issue through a large set of experiments conducted on a commercial 1xEV-DO network. Our evaluation includes both stationary and mobile scenarios wherein we transferred data using four popular transport protocols: TCPReno, TCP-Vegas, TCP-Westwood, and TCP-Cubic, and logged detailed measurements about wireless channel level characteristics as well as transport layer performance. We analyzed data from several days of experiments and inferred the properties of the physical, MAC and transport layers, as well as potential interactions between them. We find that the wireless channel data rate shows significant variability over long time scales on the order of hours, but retains high memory and predictability over small time scales on the order of milliseconds. We also find that loss-based TCP variants are largely unaffected by channel variations due to the presence of large buffers, and hence able to achieve in excess of 80% of the system capacity.},
booktitle = {Proceedings of the 14th ACM International Conference on Mobile Computing and Networking},
pages = {211–222},
numpages = {12},
keywords = {TCP, CDMA, cross-layer, SINR, 3G, mobility, cellular, measurement, proportional fair (PF), DRC, 1xEV-DO},
location = {San Francisco, California, USA},
series = {MobiCom '08}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@article{10.1145/62764.1062251,
author = {Asher, Lisa},
title = {DA STANDARDS ACTIVITIES: Summary: Standard Package Position Papers VHDL Model Standards Group},
year = {1988},
issue_date = {December 1, 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3–4},
issn = {0163-5743},
url = {https://doi.org/10.1145/62764.1062251},
doi = {10.1145/62764.1062251},
abstract = {The following are abridged versions of position papers being used by the IEEE/DASS in developing the IEEE VHDL standard. These were supplied by Jim Armstrong who is Chairman of the VHDL Model Subgroup. Complete versions of the papers and minutes of the IEEE VHDL Subgroup are archived at the IEEE Computer Society. For further information call Rick Cain at 202-371-0101. I believe that sometime in the fall of 89 Design and Test will publish a special issue on the VHDL standard based on the final resolutions of the DASS. Special thanks to J. W. Smith for help in obtaining this material.},
journal = {SIGDA Newsl.},
month = {dec},
pages = {31},
numpages = {54}
}

@proceedings{10.1145/2970276,
title = {ASE 2016: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1145/3084225,
author = {Storer, Tim},
title = {Bridging the Chasm: A Survey of Software Engineering Practice in Scientific Programming},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3084225},
doi = {10.1145/3084225},
abstract = {The use of software is pervasive in all fields of science. Associated software development efforts may be very large, long lived, and complex, requiring the commitment of significant resources. However, several authors have argued that the “gap” or “chasm” between software engineering and scientific programming is a serious risk to the production of reliable scientific results, as demonstrated in a number of case studies. This article reviews the research that addresses the gap, exploring how both software engineering and research practice may need to evolve to accommodate the use of software in science.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {47},
numpages = {32},
keywords = {Software engineering, scientific programming}
}

@proceedings{10.1145/3508397,
title = {MEDES '22: Proceedings of the 14th International Conference on Management of Digital EcoSystems},
year = {2022},
isbn = {9781450392198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {During the past years, the International Conference on ManagEment of Digital EcoSystems (MEDES) has become one of the most important international scientific events bringing together researchers, developers, and practitioners to discuss latest research issues and experiences in developing advanced solutions that will help to design, deploy, exploit and tune emerging ecosystems.},
location = {Venice, Italy}
}

@proceedings{10.5555/3571885,
title = {SC '22: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {This volume, containing the accepted technical papers and ACM Gordon Bell prize finalists, captures the best current research in all aspects of High Performance Computing (HPC). The SC22 Archive at the conference web site sc22.supercomputing.org complements this volume by collecting other high quality, peer-reviewed material including research posters, the visualization &amp; data analytics showcase, panels, birds of a feather, workshops, and tutorials.},
location = {Dallas, Texas}
}

@proceedings{10.1145/2517349,
title = {SOSP '13: Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
year = {2013},
isbn = {9781450323888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP 2013), held at the Nemacolin Woodlands Resort, Farmington, Pennsylvania, USA. This year's program includes 30 papers, and touches on a wide range of computer systems topics, from kernels to big data, from responsiveness to correctness, and from devices to data centers. The program committee made every effort to identify and include some of the most creative and thought-provoking ideas in computer systems today. Each accepted paper was shepherded by a program committee member to make sure the papers are as readable and complete as possible. We hope you will enjoy the program as much as we did in selecting it.},
location = {Farminton, Pennsylvania}
}

@proceedings{10.1145/3544902,
title = {ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@proceedings{10.1145/3556223,
title = {ICCCM '22: Proceedings of the 10th International Conference on Computer and Communications Management},
year = {2022},
isbn = {9781450396349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Okayama, Japan}
}

@book{10.1145/3226595,
editor = {Brodie, Michael L.},
title = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
year = {2018},
isbn = {9781947487192},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {22},
abstract = {At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the "Nobel Prize" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker," the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource ("The Economist," May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.}
}

@proceedings{10.1145/3565387,
title = {CSAE '22: Proceedings of the 6th International Conference on Computer Science and Application Engineering},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, China}
}

