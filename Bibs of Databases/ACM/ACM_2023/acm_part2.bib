@inproceedings{10.5555/2666719.2666727,
author = {Landh\"{a}u\ss{}er, Mathias and Genaid, Adrian},
title = {Connecting User Stories and Code for Test Development},
year = {2012},
isbn = {9781467317597},
publisher = {IEEE Press},
abstract = {User Stories are short feature descriptions from the user's point of view. Functional tests ensure that the feature described by a User Story is fully implemented.We present a tool that builds an ontology for code and links completed User Stories in natural language with the related code artifacts. The ontology also contains links to API components that were used to implement the functional tests. Preliminary results show that these links can be used to recommend reusable test steps for new User Stories.},
booktitle = {Proceedings of the Third International Workshop on Recommendation Systems for Software Engineering},
pages = {33–37},
numpages = {5},
keywords = {reasoning, traceability, functional testing, code mining, ontology},
location = {Zurich, Switzerland},
series = {RSSE '12}
}

@article{10.1145/2579281.2579312,
author = {Ionita, Anca Daniela and Lewis, Grace A. and Litoiu, Marin},
title = {Report of the 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013)},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579312},
doi = {10.1145/2579281.2579312},
abstract = {The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {34–37},
numpages = {4},
keywords = {services, software evolution, software maintenance, SOA, cloud computing, cloudbased systems, service-oriented systems, serviceoriented architecture}
}

@inproceedings{10.5555/2662413.2662432,
author = {Diepenbeck, Melanie and Soeken, Mathias and Gro\ss{}e, Daniel and Drechsler, Rolf},
title = {Towards Automatic Scenario Generation from Coverage Information},
year = {2013},
isbn = {9781467361613},
publisher = {IEEE Press},
abstract = {Nowadays, the design of software systems is pushed towards agile development practices. One of its most fundamental approaches is Test Driven Development (TDD). This procedure is based on test cases which are incrementally written prior to the implementation. Recently, Behavior Driven Development (BDD) has been introduced as an extension of TDD, in which natural language scenarios are the starting point for the test cases. This description offers a ubiquitous communication mean for both the software developers and stakeholders.Following the BDD methodology thoroughly, one would expect 100% code coverage, since code is only written to make the test cases pass. However, as we show in an empirical study this expectation is not valid in practice. It becomes even worse in the process of development, i.e. the coverage decreases over time. To close the coverage gap, we sketch an algorithm that generates BDD-style scenarios based on uncovered code.},
booktitle = {Proceedings of the 8th International Workshop on Automation of Software Test},
pages = {82–88},
numpages = {7},
location = {San Francisco, California},
series = {AST '13}
}

@article{10.1145/3523056,
author = {Troya, Javier and Segura, Sergio and Burgue\~{n}o, Lola and Wimmer, Manuel},
title = {Model Transformation Testing and Debugging: A Survey},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3523056},
doi = {10.1145/3523056},
abstract = {Model transformations are the key technique in Model-Driven Engineering (MDE) to manipulate and construct models. As a consequence, the correctness of software systems built with MDE approaches relies mainly on the correctness of model transformations, and thus, detecting and locating bugs in model transformations have been popular research topics in recent years. This surge of work has led to a vast literature on model transformation testing and debugging, which makes it challenging to gain a comprehensive view of the current state-of-the-art. This is an obstacle for newcomers to this topic and MDE practitioners to apply these approaches. This article presents a survey on testing and debugging model transformations based on the analysis of 140&nbsp;papers on the topics. We explore the trends, advances, and evolution over the years, bringing together previously disparate streams of work and providing a comprehensive view of these thriving areas. In addition, we present a conceptual framework to understand and categorize the different proposals. Finally, we identify several open research challenges and propose specific action points for the model transformation community.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {72},
numpages = {39},
keywords = {Model transformation, testing, debugging, survey}
}

@inproceedings{10.1145/3183440.3183480,
author = {Kr\"{o}her, Christian and El-Sharkawy, Sascha and Schmid, Klaus},
title = {KernelHaven: An Experimentation Workbench for Analyzing Software Product Lines},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183480},
doi = {10.1145/3183440.3183480},
abstract = {Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem.KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.Video: https://youtu.be/IbNc-H1NoZU},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {73–76},
numpages = {4},
keywords = {static analysis, variability extraction, empirical software engineering, software product line analysis},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2814204.2814221,
author = {Florence, Spencer P. and Fetscher, Bruke and Flatt, Matthew and Temps, William H. and Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte and Yarnold, Paul R. and Findler, Robert Bruce and Belknap, Steven M.},
title = {POP-PL: A Patient-Oriented Prescription Programming Language},
year = {2015},
isbn = {9781450336871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814204.2814221},
doi = {10.1145/2814204.2814221},
abstract = {Medical professionals have long used algorithmic thinking to describe and implement health care processes without the benefit of the conceptual framework provided by a programming language. Instead, medical algorithms are expressed using English, flowcharts, or data tables. This results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse. This paper reports on the design and evaluation of a domain-specific programming language, POP-PL for expressing medical algorithms. The design draws on the experience of researchers in two disciplines, programming languages and medicine. The language is based around the idea that programs and humans have complementary strengths, that when combined can make for safer, more accurate performance of prescriptions. We implemented a prototype of our language and evaluated its design by writing prescriptions in the new language and administering a usability survey to medical professionals. This formative evaluation suggests that medical prescriptions can be conveyed by a programming language's mode of expression and provides useful information for refining the language. Analysis of the survey results suggests that medical professionals can understand and correctly modify programs in POP-PL.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {131–140},
numpages = {10},
keywords = {Empirical Evaluation, DSL Design, Medical Programming Languages, Med- ical Prescriptions},
location = {Pittsburgh, PA, USA},
series = {GPCE 2015}
}

@article{10.1145/2936314.2814221,
author = {Florence, Spencer P. and Fetscher, Bruke and Flatt, Matthew and Temps, William H. and Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte and Yarnold, Paul R. and Findler, Robert Bruce and Belknap, Steven M.},
title = {POP-PL: A Patient-Oriented Prescription Programming Language},
year = {2015},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/2936314.2814221},
doi = {10.1145/2936314.2814221},
abstract = {Medical professionals have long used algorithmic thinking to describe and implement health care processes without the benefit of the conceptual framework provided by a programming language. Instead, medical algorithms are expressed using English, flowcharts, or data tables. This results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse. This paper reports on the design and evaluation of a domain-specific programming language, POP-PL for expressing medical algorithms. The design draws on the experience of researchers in two disciplines, programming languages and medicine. The language is based around the idea that programs and humans have complementary strengths, that when combined can make for safer, more accurate performance of prescriptions. We implemented a prototype of our language and evaluated its design by writing prescriptions in the new language and administering a usability survey to medical professionals. This formative evaluation suggests that medical prescriptions can be conveyed by a programming language's mode of expression and provides useful information for refining the language. Analysis of the survey results suggests that medical professionals can understand and correctly modify programs in POP-PL.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {131–140},
numpages = {10},
keywords = {Med- ical Prescriptions, DSL Design, Medical Programming Languages, Empirical Evaluation}
}

@inproceedings{10.1145/2678015.2682533,
author = {Li, Huiqing and Thompson, Simon},
title = {Safe Concurrency Introduction through Slicing},
year = {2015},
isbn = {9781450332972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678015.2682533},
doi = {10.1145/2678015.2682533},
abstract = {Traditional refactoring is about modifying the structure of existing code without changing its behaviour, but with the aim of making code easier to understand, modify, or reuse. In this paper, we introduce three novel refactorings for retrofitting concurrency to Erlang applications, and demonstrate how the use of program slicing makes the automation of these refactorings possible.},
booktitle = {Proceedings of the 2015 Workshop on Partial Evaluation and Program Manipulation},
pages = {103–113},
numpages = {11},
keywords = {refactoring, concurrency, functional programming, erlang, slicing, parallelisation},
location = {Mumbai, India},
series = {PEPM '15}
}

@inproceedings{10.1145/3468264.3468605,
author = {Wu, Xiuheng and Zhu, Chenguang and Li, Yi},
title = {DIFFBASE: A Differential Factbase for Effective Software Evolution Management},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468605},
doi = {10.1145/3468264.3468605},
abstract = {Numerous tools and techniques have been developed to extract and analyze information from software development artifacts. Yet, there is a lack of effective method to process, store, and exchange information among different analyses. In this paper, we propose differential factbase, a uniform exchangeable representation supporting efficient querying and manipulation, based on the existing concept of program facts. We consider program changes as first-class objects, which establish links between intra-version facts of single program snapshots and provide insights on how certain artifacts evolve over time via inter-version facts. We implement a series of differential fact extractors supporting different programming languages and platforms, and demonstrate with usage scenarios the benefits of adopting differential facts in supporting software evolution management.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {503–515},
numpages = {13},
keywords = {reverse engineering, program facts, software maintenance, Software evolution},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3356317.3356319,
author = {Eras, Eduardo Rohde and de Santiago, Valdivino Alexandre and dos Santos, Luciana Brasil Rebelo},
title = {Singularity: A Methodology for Automatic Unit Test Data Generation for C++ Applications Based on Model Checking Counterexamples},
year = {2019},
isbn = {9781450376488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356317.3356319},
doi = {10.1145/3356317.3356319},
abstract = {One of the most challenging task of testing activity is the generation of test cases/data. While there is significant amount of studies in this regard, there is still need to move towards approaches that can generate test case/data based only on source code since many software systems mostly have the source code available and no adequate documentation. In this paper a new methodology, called Singularity, is introduced to generate unit test data for C++ applications based on Model Checking, a popular technique for test case generation. Our approach, which is to be supported by a tool, automatically translates C++ code into a model which resembles a Statechart model and then into the notation of the NuSMV Model Checker. Later, we rely on a technique based on the HiMoST Method, producing counterexamples from the Model Checker that are, in fact, the test cases/data themselves. We have applied our approach to a few C++ case studies analyzing how feasible it is for automatic test data generation.},
booktitle = {Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing},
pages = {72–79},
numpages = {8},
keywords = {Model Checking, Automation, Tool, Counterexamples, Unit test},
location = {Salvador, Brazil},
series = {SAST 2019}
}

@inproceedings{10.1145/3510003.3510176,
author = {Gerten, Michael C. and Marsh, Alexis L. and Lathrop, James I. and Cohen, Myra B. and Miner, Andrew S. and Klinge, Titus H.},
title = {Inference and Test Generation Using Program Invariants in Chemical Reaction Networks},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510176},
doi = {10.1145/3510003.3510176},
abstract = {Chemical reaction networks (CRNs) are an emerging distributed computational paradigm where programs are encoded as a set of abstract chemical reactions. CRNs can be compiled into DNA strands which perform the computations in vitro, creating a foundation for intelligent nanodevices. Recent research proposed a software testing framework for stochastic CRN programs in simulation, however, it relies on existing program specifications. In practice, specifications are often lacking and when they do exist, transforming them into test cases is time-intensive and can be error prone. In this work, we propose an inference technique called ChemFlow which extracts 3 types of invariants from an existing CRN model. The extracted invariants can then be used for test generation or model validation against program implementations. We applied ChemFlow to 13 CRN programs ranging from toy examples to real biological models with hundreds of reactions. We find that the invariants provide strong fault detection and often exhibit less flakiness than specification derived tests. In the biological models we showed invariants to developers and they confirmed that some of these point to parts of the model that are biologically incorrect or incomplete suggesting we may be able to use ChemFlow to improve model quality.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1193–1205},
numpages = {13},
keywords = {chemical reaction networks, invariants, petri nets, test generation},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2814270.2814276,
author = {Voelter, Markus and Deursen, Arie van and Kolb, Bernd and Eberle, Stephan},
title = {Using C Language Extensions for Developing Embedded Software: A Case Study},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814276},
doi = {10.1145/2814270.2814276},
abstract = {We report on an industrial case study on developing the embedded software for a smart meter using the C programming language and domain-specific extensions of C such as components, physical units, state machines, registers and interrupts. We find that the extensions help significantly with managing the complexity of the software. They improve testability mainly by supporting hardware-independent testing, as illustrated by low integration efforts. The extensions also do not incur significant overhead regarding memory consumption and performance. Our case study relies on mbeddr, an extensible version of C. mbeddr, in turn, builds on the MPS language workbench which supports modular extension of languages and IDEs.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {655–674},
numpages = {20},
keywords = {Real-time and embedded systems, Extensible languages, Code Generation, Program Editors},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{10.1145/2858965.2814276,
author = {Voelter, Markus and Deursen, Arie van and Kolb, Bernd and Eberle, Stephan},
title = {Using C Language Extensions for Developing Embedded Software: A Case Study},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814276},
doi = {10.1145/2858965.2814276},
abstract = {We report on an industrial case study on developing the embedded software for a smart meter using the C programming language and domain-specific extensions of C such as components, physical units, state machines, registers and interrupts. We find that the extensions help significantly with managing the complexity of the software. They improve testability mainly by supporting hardware-independent testing, as illustrated by low integration efforts. The extensions also do not incur significant overhead regarding memory consumption and performance. Our case study relies on mbeddr, an extensible version of C. mbeddr, in turn, builds on the MPS language workbench which supports modular extension of languages and IDEs.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {655–674},
numpages = {20},
keywords = {Code Generation, Extensible languages, Program Editors, Real-time and embedded systems}
}

@inproceedings{10.1145/2002931.2002935,
author = {Choudhary, Shauvik Roy and Zhao, Dan and Versee, Husayn and Orso, Alessandro},
title = {WATER: Web Application TEst Repair},
year = {2011},
isbn = {9781450308083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002931.2002935},
doi = {10.1145/2002931.2002935},
abstract = {Web applications tend to evolve quickly, resulting in errors and failures in test automation scripts that exercise them. Repairing such scripts to work on the updated application is essential for maintaining the quality of the test suite. Updating such scripts manually is a time consuming task, which is often difficult and is prone to errors if not performed carefully. In this paper, we propose a technique to automatically suggest repairs for such web application test scripts. Our technique is based on differential testing and compares the behavior of the test case on two successive versions of the web application: first version in which the test script runs successfully and the second version in which the script results in an error or failure. By analyzing the difference between these two executions, our technique suggests repairs that can be applied to repair the scripts. To evaluate our technique, we implemented it in a tool called WATER and exercised it on real web applications with test cases. Our experiments show that WATER can suggest meaningful repairs for practical test cases, many of which correspond to those made later by developers themselves.},
booktitle = {Proceedings of the First International Workshop on End-to-End Test Script Engineering},
pages = {24–29},
numpages = {6},
keywords = {test repair, web testing},
location = {Toronto, Ontario, Canada},
series = {ETSE '11}
}

@article{10.1145/3105906,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
abstract = {This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {17},
numpages = {24},
keywords = {self-healing software, Program repair}
}

@inproceedings{10.1145/3338906.3338972,
author = {Dutta, Saikat and Zhang, Wenxian and Huang, Zixin and Misailovic, Sasa},
title = {Storm: Program Reduction for Testing and Debugging Probabilistic Programming Systems},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338972},
doi = {10.1145/3338906.3338972},
abstract = {Probabilistic programming languages offer an intuitive way to model uncertainty by representing complex probability models as simple probabilistic programs. Probabilistic programming systems (PP systems) hide the complexity of inference algorithms away from the program developer. Unfortunately, if a failure occurs during the run of a PP system, a developer typically has very little support in finding the part of the probabilistic program that causes the failure in the system. This paper presents Storm, a novel general framework for reducing probabilistic programs. Given a probabilistic program (with associated data and inference arguments) that causes a failure in a PP system, Storm finds a smaller version of the program, data, and arguments that cause the same failure. Storm leverages both generic code and data transformations from compiler testing and domain-specific, probabilistic transformations. The paper presents new transformations that reduce the complexity of statements and expressions, reduce data size, and simplify inference arguments (e.g., the number of iterations of the inference algorithm). We evaluated Storm on 47 programs that caused failures in two popular probabilistic programming systems, Stan and Pyro. Our experimental results show Storm’s effectiveness. For Stan, our minimized programs have 49% less code, 67% less data, and 96% fewer iterations. For Pyro, our minimized programs have 58% less code, 96% less data, and 99% fewer iterations. We also show the benefits of Storm when debugging probabilistic programs.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {729–739},
numpages = {11},
keywords = {Probabilistic Programming Languages, Software Testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2304656.2304658,
author = {Rupanov, Vladimir and Buckl, Christian and Fiege, Ludger and Armbruster, Michael and Knoll, Alois and Spiegelberg, Gernot},
title = {Early Safety Evaluation of Design Decisions in E/E Architecture According to ISO 26262},
year = {2012},
isbn = {9781450313476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304656.2304658},
doi = {10.1145/2304656.2304658},
abstract = {ISO 26262 addresses development of safe in-vehicle functions by specifying methods potentially used in the design and development lifecycle. It does not indicate what is sufficient and leaves room for interpretation. However, the architects of electric/electronic systems need design boundaries to make decisions during architecture evolution without adding a risk of late architectural changes. Designing and changing a system benefits from correct selection of safety mechanisms at early design stages. This paper presents an iterative architecture design and refinement process that is centered around ISO 26262 requirements. We propose a domain-specific modeling scheme and component repositories to build up a bottom-up analysis framework that allows early quantitative safety evaluation. To guarantee that the target ASIL level can be reached, we complement our design-time component-level analysis with conservative top-down analysis. Given that analysis starts at early design stages, evolution of the architecture is supported by different levels of detail used in the analysis framework.},
booktitle = {Proceedings of the 3rd International ACM SIGSOFT Symposium on Architecting Critical Systems},
pages = {1–10},
numpages = {10},
keywords = {architecture modeling, automotive systems, functional safety, integration of analysis techniques},
location = {Bertinoro, Italy},
series = {ISARCS '12}
}

@inproceedings{10.1145/1481848.1481860,
author = {Erk\"{o}k, Levent and Matthews, John},
title = {Pragmatic Equivalence and Safety Checking in Cryptol},
year = {2009},
isbn = {9781605583303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1481848.1481860},
doi = {10.1145/1481848.1481860},
abstract = {Cryptol is programming a language designed for specifying and programming cryptographic algorithms. In order to meet high-assurance requirements, Cryptol comes with a suite of formal-methods based tools allowing users to perform various program verification tasks. In the fully automated mode, Cryptol uses modern off-the-shelf SAT and SMT solvers to perform verification in a push-button manner. In the manual mode, Cryptol produces Isabelle/HOL specifications that can be interactively verified using the Isabelle theorem prover. In this paper, we provide an overview of Cryptol's verification toolset, describing our experiences with building a practical programming environment with dedicated support for formal verification.},
booktitle = {Proceedings of the 3rd Workshop on Programming Languages Meets Program Verification},
pages = {73–82},
numpages = {10},
keywords = {size polymorphism, cryptography, equivalence checking, sat/smt solving, theorem proving, formal methods},
location = {Savannah, GA, USA},
series = {PLPV '09}
}

@article{10.1145/3095807,
author = {Bowen, Judy and Reeves, Steve},
title = {Generating Obligations, Assertions and Tests from UI Models},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {EICS},
url = {https://doi.org/10.1145/3095807},
doi = {10.1145/3095807},
abstract = {Model-based development of interactive systems provides a number of benefits which can support the creation of robust and correct systems, particularly important when the interactive systems are safety-critical. Many different approaches have been proposed which target the models at different aspects of the development process (for example task analysis, interface layouts, functional behaviours etc.) and which can be used in different ways (verification of correctness, plasticity, usability).One of the aims for any modelling method should be simplicity - we are after all trying to hide complexity via abstraction in order to make reasoning about systems more tractable than working at the programming level. One of the challenges that exists however we do our modelling is ensuring the consistency between the model of the interface and interactivity and model of the functional behaviour of the system. This is primarily due to the different types of models that most naturally describe these different elements. In this paper we propose a method of tightening the integration of models of these different components of the system by generating obligations which explicitly describe the coupling of functional behaviour with interactive elements. We then show how these obligations can be used to support the development process during the programming and testing of the system.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {5},
numpages = {18},
keywords = {testing, model-driven development}
}

@article{10.1145/3428212,
author = {Sotiropoulos, Thodoris and Chaliasos, Stefanos and Mitropoulos, Dimitris and Spinellis, Diomidis},
title = {A Model for Detecting Faults in Build Specifications},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428212},
doi = {10.1145/3428212},
abstract = {Incremental and parallel builds are crucial features of modern build systems. Parallelism enables fast builds by running independent tasks simultaneously, while incrementality saves time and computing resources by processing the build operations that were affected by a particular code change. Writing build definitions that lead to error-free incremental and parallel builds is a challenging task. This is mainly because developers are often unable to predict the effects of build operations on the file system and how different build operations interact with each other. Faulty build scripts may seriously degrade the reliability of automated builds, as they cause build failures, and non-deterministic and incorrect outputs. To reason about arbitrary build executions, we present BuildFS, a generally-applicable model that takes into account the specification (as declared in build scripts) and the actual behavior (low-level file system operation) of build operations. We then formally define different types of faults related to incremental and parallel builds in terms of the conditions under which a file system operation violates the specification of a build operation. Our testing approach, which relies on the proposed model, analyzes the execution of single full build, translates it into BuildFS, and uncovers faults by checking for corresponding violations. We evaluate the effectiveness, efficiency, and applicability of our approach by examining 612 Make and Gradle projects. Notably, thanks to our treatment of build executions, our method is the first to handle JVM-oriented build systems. The results indicate that our approach is (1) able to uncover several important issues (247 issues found in 47 open-source projects have been confirmed and fixed by the upstream developers), and (2) much faster than a state-of-the-art tool for Make builds (the median and average speedup is 39X and 74X respectively).},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {144},
numpages = {30},
keywords = {incremental builds, parallel builds, Gradle, JVM-based builds, Make}
}

@inproceedings{10.1007/978-3-642-33666-9_2,
author = {S\'{a}nchez-Cuadrado, Jes\'{u}s and De Lara, Juan and Guerra, Esther},
title = {Bottom-up Meta-Modelling: An Interactive Approach},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_2},
doi = {10.1007/978-3-642-33666-9_2},
abstract = {The intensive use of models in Model-Driven Engineering (MDE) raises the need to develop meta-models with different aims, like the construction of textual and visual modelling languages and the specification of source and target ends of model-to-model transformations. While domain experts have the knowledge about the concepts of the domain, they usually lack the skills to build meta-models. These should be tailored according to their future usage and specific implementation platform, which demands knowledge available only to engineers with great expertise in MDE platforms. These issues hinder a wider adoption of MDE both by domain experts and software engineers.In order to alleviate this situation we propose an interactive, iterative approach to meta-model construction enabling the specification of model fragments by domain experts, with the possibility of using informal drawing tools like Dia. These fragments can be annotated with hints about the intention or needs for certain elements. A meta-model is automatically induced, which can be refactored in an interactive way, and then compiled into an implementation meta-model using profiles and patterns for different platforms and purposes.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {3–19},
numpages = {17},
keywords = {meta-model design exploration, meta-modelling, interactive meta-modelling, domain-specific modelling languages},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@article{10.1145/3394979,
author = {Rocha Silva, Thiago and Winckler, Marco and Tr\ae{}tteberg, Hallvard},
title = {Ensuring the Consistency between User Requirements and Task Models: A Behavior-Based Automated Approach},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {EICS},
url = {https://doi.org/10.1145/3394979},
doi = {10.1145/3394979},
abstract = {Evaluating and ensuring the consistency between user requirements and modeling artifacts is a long-time issue for model-based software design. Conflicts in requirements specifications can lead to many design errors and have a decisive impact on the quality of systems under development. This article presents an approach based on Behavior-Driven Development (BDD) to provide automated assessment for task models, which are intended to model the flow of user and system tasks in an interactive system. The approach has been evaluated by exploiting user requirements described by a group of experts in the domain of business trips. Such requirements gave rise to a set of BDD stories that have been used to automatically assess scenarios extracted from task models that were reengineered from an existing web system for booking business trips. The results have shown our approach, by performing a static analysis of the source files, was able to identify different types of inconsistencies between the user requirements and the set of task models analyzed.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {77},
numpages = {32},
keywords = {behavior-driven development (BDD), user stories, task models, automated requirements assessment}
}

@inproceedings{10.1145/3424771.3424821,
author = {Zimmermann, Olaf and Pautasso, Cesare and L\"{u}bke, Daniel and Zdun, Uwe and Stocker, Mirko},
title = {Data-Oriented Interface Responsibility Patterns: Types of Information Holder Resources},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424821},
doi = {10.1145/3424771.3424821},
abstract = {Remote Application Programming Interfaces (APIs) are used in almost any distributed system today, for instance in microservices-based systems, and are thus enablers for many digitalization efforts. API design not only impacts whether software provided as a service is easy and efficient to develop applications with, but also affects the long term evolution of the software system. In general, APIs are responsible for providing remote and controlled access to the functionality provided as services; however, APIs often are also used to expose and share information. We focus on such data-related aspects of microservice APIs in this paper. Depending on the life cycle of the information published through the API, its mutability and the endpoint role, data-oriented APIs can be designed following patterns such as Operational Data Holder, Master Data Holder, Reference Data Holder, Data Transfer Holder, and Link Lookup Resource. Known uses and examples of the patterns are drawn from public Web APIs as well as application development and integration projects we have been involved in.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {11},
numpages = {25},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@inproceedings{10.1145/3510003.3510040,
author = {Noller, Yannic and Shariffdeen, Ridwan and Gao, Xiang and Roychoudhury, Abhik},
title = {Trust Enhancement Issues in Program Repair},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510040},
doi = {10.1145/3510003.3510040},
abstract = {Automated program repair is an emerging technology that seeks to automatically rectify bugs and vulnerabilities using learning, search, and semantic analysis. Trust in automatically generated patches is necessary for achieving greater adoption of program repair. Towards this goal, we survey more than 100 software practitioners to understand the artifacts and setups needed to enhance trust in automatically generated patches. Based on the feedback from the survey on developer preferences, we quantitatively evaluate existing test-suite based program repair tools. We find that they cannot produce high-quality patches within a top-10 ranking and an acceptable time period of 1 hour. The developer feedback from our qualitative study and the observations from our quantitative examination of existing repair tools point to actionable insights to drive program repair research. Specifically, we note that producing repairs within an acceptable time-bound is very much dependent on leveraging an abstract search space representation of a rich enough search space. Moreover, while additional developer inputs are valuable for generating or ranking patches, developers do not seem to be interested in a significant human-in-the-loop interaction.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2228–2240},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2983990.2984038,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding Compiler Bugs via Live Code Mutation},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984038},
doi = {10.1145/2983990.2984038},
abstract = {Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions. This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable. We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {849–863},
numpages = {15},
keywords = {automated testing, Compiler testing, miscompilation, equivalent program variants},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@article{10.1145/3022671.2984038,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding Compiler Bugs via Live Code Mutation},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022671.2984038},
doi = {10.1145/3022671.2984038},
abstract = {Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions. This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable. We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {849–863},
numpages = {15},
keywords = {automated testing, miscompilation, Compiler testing, equivalent program variants}
}

@inproceedings{10.1145/2103656.2103709,
author = {Zhao, Jianzhou and Nagarakatte, Santosh and Martin, Milo M.K. and Zdancewic, Steve},
title = {Formalizing the LLVM Intermediate Representation for Verified Program Transformations},
year = {2012},
isbn = {9781450310833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103656.2103709},
doi = {10.1145/2103656.2103709},
abstract = {This paper presents Vellvm (verified LLVM), a framework for reasoning about programs expressed in LLVM's intermediate representation and transformations that operate on it. Vellvm provides a mechanized formal semantics of LLVM's intermediate representation, its type system, and properties of its SSA form. The framework is built using the Coq interactive theorem prover. It includes multiple operational semantics and proves relations among them to facilitate different reasoning styles and proof techniques.To validate Vellvm's design, we extract an interpreter from the Coq formal semantics that can execute programs from LLVM test suite and thus be compared against LLVM reference implementations. To demonstrate Vellvm's practicality, we formalize and verify a previously proposed transformation that hardens C programs against spatial memory safety violations. Vellvm's tools allow us to extract a new, verified implementation of the transformation pass that plugs into the real LLVM infrastructure; its performance is competitive with the non-verified, ad-hoc original.},
booktitle = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {427–440},
numpages = {14},
keywords = {LLVM, memory safety, Coq},
location = {Philadelphia, PA, USA},
series = {POPL '12}
}

@article{10.1145/2103621.2103709,
author = {Zhao, Jianzhou and Nagarakatte, Santosh and Martin, Milo M.K. and Zdancewic, Steve},
title = {Formalizing the LLVM Intermediate Representation for Verified Program Transformations},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/2103621.2103709},
doi = {10.1145/2103621.2103709},
abstract = {This paper presents Vellvm (verified LLVM), a framework for reasoning about programs expressed in LLVM's intermediate representation and transformations that operate on it. Vellvm provides a mechanized formal semantics of LLVM's intermediate representation, its type system, and properties of its SSA form. The framework is built using the Coq interactive theorem prover. It includes multiple operational semantics and proves relations among them to facilitate different reasoning styles and proof techniques.To validate Vellvm's design, we extract an interpreter from the Coq formal semantics that can execute programs from LLVM test suite and thus be compared against LLVM reference implementations. To demonstrate Vellvm's practicality, we formalize and verify a previously proposed transformation that hardens C programs against spatial memory safety violations. Vellvm's tools allow us to extract a new, verified implementation of the transformation pass that plugs into the real LLVM infrastructure; its performance is competitive with the non-verified, ad-hoc original.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {427–440},
numpages = {14},
keywords = {Coq, memory safety, LLVM}
}

@inproceedings{10.1145/2467307.2467316,
author = {Taromirad, Masoumeh and Paige, Richard F.},
title = {Agile Requirements Traceability Using Domain-Specific Modelling Languages},
year = {2012},
isbn = {9781450318044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2467307.2467316},
doi = {10.1145/2467307.2467316},
abstract = {Requirements traceability is an important mechanism for managing verification, validation and change impact analysis challenges in system engineering. Numerous model-based approaches have been proposed to support requirements traceability, but significant challenges remain, including finding the appropriate level of granularity for modelling traceability and coping with the lack of uniformity in requirements management tools. This paper argues for an agile modelling approach to managing requirements traceability and, in this context, proposes a domain/project-specific requirements traceability modelling approach. The preliminary approach is illustrated briefly in the context of the safety-critical systems engineering domain, where agile traceability from functional and safety requirements is necessary to underpin certification.},
booktitle = {Proceedings of the 2012 Extreme Modeling Workshop},
pages = {45–50},
numpages = {6},
location = {Innsbruck, Austria},
series = {XM '12}
}

@inproceedings{10.1145/3236024.3236055,
author = {Hu, Gang and Zhu, Linjie and Yang, Junfeng},
title = {AppFlow: Using Machine Learning to Synthesize Robust, Reusable UI Tests},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236055},
doi = {10.1145/3236024.3236055},
abstract = {UI testing is known to be difficult, especially as today’s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an “add to cart” test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides “smoke testing” requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {269–282},
numpages = {14},
keywords = {UI recognition, test reuse, mobile testing, machine learning, UI testing, test synthesis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.5555/800054.802007,
author = {Boehm, Barry W. and Gray, Terence E. and Seewaldt, Thomas},
title = {Prototyping vs. Specifying: A Multi-Project Experiment},
year = {1984},
isbn = {0818605286},
publisher = {IEEE Press},
abstract = {In this experiment, seven software teams developed versions of the same small-size (2000-4000 source instruction) application software product. Four teams used the Specifying approach. Three teams used the Prototyping approach.The main results of the experiment were:Prototyping yielded products with roughly equivalent performance, but with about 40% less code and 45% less effort.The prototyped products rated somewhat lower on functionality and robustness, but higher on ease of use and ease of learning.Specifying produced more coherent designs and software that was easier to integrate.The paper presents the experimental data supporting these and a number of additional conclusions.},
booktitle = {Proceedings of the 7th International Conference on Software Engineering},
pages = {473–484},
numpages = {12},
location = {Orlando, Florida, USA},
series = {ICSE '84}
}

@article{10.1145/3447680,
author = {Jeong, Eunjin and Jeong, Dowhan and Ha, Soonhoi},
title = {Dataflow Model–Based Software Synthesis Framework for Parallel and Distributed Embedded Systems},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3447680},
doi = {10.1145/3447680},
abstract = {Existing software development methodologies mostly assume that an application runs on a single device without concern about the non-functional requirements of an embedded system such as latency and resource consumption. Besides, embedded software is usually developed after the hardware platform is determined, since a non-negligible portion of the code depends on the hardware platform. In this article, we present a novel model-based software synthesis framework for parallel and distributed embedded systems. An application is specified as a set of tasks with the given rules for execution and communication. Having such rules enables us to perform static analysis to check some software errors at compile-time to reduce the verification difficulty. Platform-specific programs are synthesized automatically after the mapping of tasks onto processing elements is determined. The proposed framework is expandable to support new hardware platforms easily. The proposed communication code synthesis method is extensible and flexible to support various communication methods between devices. In addition, the fault-tolerant feature can be added by modifying the task graph automatically according to the selected fault-tolerance configurations by the user. The viability of the proposed software development methodology is evaluated with a real-life surveillance application that runs on six processing elements.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {jun},
articleno = {35},
numpages = {38},
keywords = {fault tolerance, synchronous dataflow, Code generation, embedded software development}
}

@article{10.1145/3530813,
author = {Fahmideh, Mahdi and Grundy, John and Ahmad, Aakash and Shen, Jun and Yan, Jun and Mougouei, Davoud and Wang, Peng and Ghose, Aditya and Gunawardana, Anuradha and Aickelin, Uwe and Abedin, Babak},
title = {Engineering Blockchain-Based Software Systems: Foundations, Survey, and Future Directions},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3530813},
doi = {10.1145/3530813},
abstract = {Many scientific and practical areas have shown increasing interest in reaping the benefits of blockchain technology to empower software systems. However, the unique characteristics and requirements associated with Blockchain-based Software (BBS) systems raise new challenges across the development lifecycle that entail an extensive improvement of conventional software engineering. This article presents a systematic literature review of the state-of-the-art in BBS engineering research from the perspective of the software engineering discipline. We characterize BBS engineering based on the key aspects of theoretical foundations, processes, models, and roles. Based on these aspects, we present a rich repertoire of development tasks, design principles, models, roles, challenges, and resolution techniques. The focus and depth of this survey not only give software engineering practitioners and researchers a consolidated body of knowledge about current BBS development but also underpin a starting point for further research in this field.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {110},
numpages = {44},
keywords = {Systems development methods, Software engineering, blockchain, blockchain-based software systems}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1145/3180495,
author = {Kulla, Christopher and Conty, Alejandro and Stein, Clifford and Gritz, Larry},
title = {Sony Pictures Imageworks Arnold},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/3180495},
doi = {10.1145/3180495},
abstract = {Sony Imageworks’ implementation of the Arnold renderer is a fork of the commercial product of the same name, which has evolved independently since around 2009. This article focuses on the design choices that are unique to this version and have tailored the renderer to the specific requirements of film rendering at our studio. We detail our approach to subdivision surface tessellation, hair rendering, sampling, and variance reduction techniques, as well as a description of our open source texturing and shading language components. We also discuss some ideas we once implemented but have since discarded to highlight the evolution of the software over the years.},
journal = {ACM Trans. Graph.},
month = {aug},
articleno = {29},
numpages = {18},
keywords = {rendering, Ray tracing, Monte Carlo, path tracing}
}

@inproceedings{10.1145/3297858.3304019,
author = {Banerjee, Subho S. and Kalbarczyk, Zbigniew T. and Iyer, Ravishankar K.},
title = {AcMC 2 : Accelerating Markov Chain Monte Carlo Algorithms for Probabilistic Models},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304019},
doi = {10.1145/3297858.3304019},
abstract = {Probabilistic models (PMs) are ubiquitously used across a variety of machine learning applications. They have been shown to successfully integrate structural prior information about data and effectively quantify uncertainty to enable the development of more powerful, interpretable, and efficient learning algorithms. This paper presents AcMC2, a compiler that transforms PMs into optimized hardware accelerators (for use in FPGAs or ASICs) that utilize Markov chain Monte Carlo methods to infer and query a distribution of posterior samples from the model. The compiler analyzes statistical dependencies in the PM to drive several optimizations to maximally exploit the parallelism and data locality available in the problem. We demonstrate the use of AcMC2 to implement several learning and inference tasks on a Xilinx Virtex-7 FPGA. AcMC2-generated accelerators provide a 47-100\texttimes{} improvement in runtime performance over a 6-core IBM Power8 CPU and a 8-18\texttimes{} improvement over an NVIDIA K80 GPU. This corresponds to a 753-1600\texttimes{} improvement over the CPU and 248-463\texttimes{} over the GPU in performance-per-watt terms.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {515–528},
numpages = {14},
keywords = {accelerator, markov chain monte carlo, probabilistic graphical models, probabilistic programming},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{10.1145/275295.275375,
author = {Wheeler, Sharon and Duggins, Sheryl},
title = {Improving Software Quality},
year = {1998},
isbn = {1581130309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/275295.275375},
doi = {10.1145/275295.275375},
booktitle = {Proceedings of the 36th Annual Southeast Regional Conference},
pages = {300–309},
numpages = {10},
series = {ACM-SE 36}
}

@article{10.1007/s00165-017-0443-1,
author = {Corrodi, Claudio and Heu\ss{}ner, Alexander and Poskitt, Christopher M.},
title = {A Semantics Comparison Workbench for a Concurrent, Asynchronous, Distributed Programming Language},
year = {2018},
issue_date = {Jan 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0443-1},
doi = {10.1007/s00165-017-0443-1},
abstract = {A number of high-level languages and libraries have been proposed that offer novel and simple to use abstractions for concurrent, asynchronous, and distributed programming. The execution models that realise them, however, often change over time—whether to improve performance, or to extend them to new language features—potentially affecting behavioural and safety properties of existing programs. This is exemplified by Scoop, a message-passing approach to concurrent object-oriented programming that has seen multiple changes proposed and implemented, with demonstrable consequences for an idiomatic usage of its core abstraction. We propose a semantics comparison workbench for Scoop with fully and semi-automatic tools for analysing and comparing the state spaces of programs with respect to different execution models or semantics. We demonstrate its use in checking the consistency of properties across semantics by applying it to a set of representative programs, and highlighting a deadlock-related discrepancy between the principal execution models of Scoop. Furthermore, we demonstrate the extensibility of the workbench by generalising the formalisation of an execution model to support recently proposed extensions for distributed programming. Our workbench is based on a modular and parameterisable graph transformation semantics implemented in the Groove tool. We discuss how graph transformations are leveraged to atomically model intricate language abstractions, how the visual yet&nbsp;algebraic nature of the model can be used to ascertain soundness, and highlight how the approach could be applied to similar languages.},
journal = {Form. Asp. Comput.},
month = {jan},
pages = {163–192},
numpages = {30},
keywords = {Object-oriented programming, Distributed programming with message passing, Graph transformation systems, Software engineering, Groove, Concurrency abstractions, Concurrent asynchronous programming, Runtime semantics, Operational semantics, Scoop, Verification/analysis parameterised by semantics}
}

@article{10.1145/3241743,
author = {Stol, Klaas-Jan and Fitzgerald, Brian},
title = {The ABC of Software Engineering Research},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3241743},
doi = {10.1145/3241743},
abstract = {A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {11},
numpages = {51},
keywords = {Research methodology, research strategy}
}

@inproceedings{10.1145/3328433.3328447,
author = {Stocco, Andrea},
title = {How Artificial Intelligence Can Improve Web Development and Testing},
year = {2019},
isbn = {9781450362573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328433.3328447},
doi = {10.1145/3328433.3328447},
abstract = {The Artificial Intelligence (AI) revolution in software development is just around the corner. With the rise of AI, developers are expected to play a different role from the traditional role of programmers, as they will need to adapt their know-how and skillsets to complement and apply AI-based tools and techniques into their traditional web development workflow. In this extended abstract, some of the current trends on how AI is being leveraged to enhance web development and testing are discussed, along with some of the main opportunities and challenges for researchers.},
booktitle = {Companion Proceedings of the 3rd International Conference on the Art, Science, and Engineering of Programming},
articleno = {13},
numpages = {4},
keywords = {web development, web testing, artificial intelligence},
location = {Genova, Italy},
series = {Programming '19}
}

@inproceedings{10.5555/2050655.2050704,
author = {Wilke, Claas and G\"{o}tz, Sebastian and Reimann, Jan and A\ss{}mann, Uwe},
title = {Vision Paper: Towards Model-Based Energy Testing},
year = {2011},
isbn = {9783642244841},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Today, energy consumption is one of the major challenges for optimisation of future software applications and ICT infrastructures. To develop software w.r.t. its energy consumption, testing is an essential activity, since testing allows quality assurance and thus, energy consumption reduction during the software's development. Although first approaches measuring and predicting software's energy consumption for its execution on a specific hardware platform exist, no model-based testing approach has been developed, yet. In this paper we present our vision of a model-based energy testing approach that uses a combination of abstract interpretation and run-time profiling to predict the energy consumption of software applications and to derive energy consumption test cases.},
booktitle = {Proceedings of the 14th International Conference on Model Driven Engineering Languages and Systems},
pages = {480–489},
numpages = {10},
keywords = {profiling, energy consumption testing, unit testing, abstract interpretation, model-based testing},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@inproceedings{10.5555/2819009.2819099,
author = {Abreu, Rui and Erdogmus, Hakan and Perez, Alexandre},
title = {CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts},
year = {2015},
publisher = {IEEE Press},
abstract = {Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plugin mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, CodeAware, for distributed and fine-grained artifact analysis. CodeAware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. CodeAware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems; (b) the ability to perform both static and dynamic analyses on these artifacts; and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of CodeAware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {551–554},
numpages = {4},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3323503.3360639,
author = {Pinto, Thiago Delgado and Gon\c{c}alves, Willian Inacio and Costa, Pablo Veiga},
title = {User Interface Prototype Generation from Agile Requirements Specifications Written in Concordia},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360639},
doi = {10.1145/3323503.3360639},
abstract = {User interface prototypes (UIP) are widely used to get feedback before building a software feature. They can prevent misunderstandings between the software development team and other stakeholders (e.g., users, investors) that lead to rework or a resulting software that does not meet their needs. UIP can also be a valuable resource in Agile software development, in which feedback is key. In this paper, we present an approach to generate UIP automatically from Agile requirements specifications written in Concordia and its corresponding prototype tool. The tool is able to generate UIP for web-based applications. We evaluated the approach and the tool with questionnaires, and the results revealed that: (i) the generated UIP are very similar to those drawn by respondents; (ii) the generated source code has good enough quality to be reused by developers; and (iii) they save design and development time.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {61–64},
numpages = {4},
keywords = {user interface, user story, generation, concordia, agile},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@article{10.14778/3415478.3415481,
author = {Khurana, Kapil and Haritsa, Jayant R.},
title = {UNMASQUE: A Hidden SQL Query Extractor},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415481},
doi = {10.14778/3415478.3415481},
abstract = {Given a database instance and a populated result, query reverse-engineering attempts to identify candidate SQL queries that produce this result on the instance. A variant of this problem arises when a ground-truth is additionally available, but hidden within an opaque database application. In this demo, we present UN-MASQUE, an extraction algorithm that is capable of precisely identifying a substantive class of such hidden queries. A hallmark of its design is that the extraction is completely non-invasive to the application. Specifically, it only examines the results obtained from application executions on databases derived with a combination of data mutation and data generation techniques, thereby achieving platform-independence. Further, potent optimizations, such as database size reduction to a few rows, are incorporated to minimize the extraction overheads. The demo showcases these features on both declarative and imperative applications.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {2809–2812},
numpages = {4}
}

@article{10.1145/351159.351173,
author = {Heering, Jan and Klint, Paul},
title = {Semantics of Programming Languages: A Tool-Oriented Approach},
year = {2000},
issue_date = {March 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/351159.351173},
doi = {10.1145/351159.351173},
abstract = {By paying more attention to semantics-based tool generation, programming language semantics can significantly increase its impact. Ultimately, this may lead to "Language Design Assistants" incorporating substantial amounts of semantic knowledge.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {39–48},
numpages = {10}
}

@inproceedings{10.1145/1869643.1869648,
author = {Rideau, Francois-Ren\'{e} and Goldman, Robert P.},
title = {Evolving ASDF: More Cooperation, Less Coordination},
year = {2010},
isbn = {9781450304702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869643.1869648},
doi = {10.1145/1869643.1869648},
abstract = {We present ASDF2, the current state of the art in CL build systems. From a technical standpoint, ASDF2 improves upon ASDF by integrating previous common extensions, making configuration easy, and fixing bugs. However the overriding concern driving these changes was social rather than technical: ASDF plays a central role in the CL community and we wanted to reduce the coordination costs that it imposed upon CL programmers. We outline ASDF's history and architecture, explain the link between the social issues we faced and the software features we added, and explore the technical challenges involved and lessons learned, notably involving inplace code upgrade of ASDF itself, backward compatibility, portability, testing and other coding best practices.},
booktitle = {Proceedings of the 2010 International Conference on Lisp},
pages = {29–42},
numpages = {14},
keywords = {dynamic code update., common lisp, build infrastructure, interaction design, code evolution},
location = {Reno/Tahoe, Nevada, USA},
series = {ILC '10}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/2983990,
title = {OOPSLA 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/1022685.1022944,
author = {Brini, Silvia and Benjelloun, Doha and Castanier, Fabien},
title = {A Flexible Virtual Platform for Computational and Communication Architecture Exploration of DMT VDSL Modems},
year = {2003},
isbn = {0769518702},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper a high-level SoC architecture exploration of DMT (Discrete Multitone) VDSL transceivers (Very high speed Digital Subscriber Line) is presented. A flexible and complete virtual platform was developed for the purpose, exploiting the paradigm of "orthogonalization of concerns" (functionality independent from architecture) in the framework of Cadence VCC system level design tool. An accurate processor model, obtained through the back-annotation of profiling results on a target DSP core, allowed the exploration of different HW/SW partitioning and the study of the computational units required. A transaction-accurate VCC bus model was developed for the investigation of the on-chip bus architecture and its relevant parameters dimensioning.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe: Designers' Forum - Volume 2},
pages = {20164},
series = {DATE '03}
}

@proceedings{10.1145/2837614,
title = {POPL '16: Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
year = {2016},
isbn = {9781450335492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. Petersburg, FL, USA}
}

@inproceedings{10.1145/1353482.1353484,
author = {Benz, Sebastian},
title = {AspectT: Aspect-Oriented Test Case Instantiation},
year = {2008},
isbn = {9781605580449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1353482.1353484},
doi = {10.1145/1353482.1353484},
abstract = {Test case instantiation is the transformation of abstract test cases into executable test scripts. Abstract test cases are either created during model based test case generation or are manually defined in a suitable modeling notation. The transformation varies depending on different testing concerns, such as test goal, test setup and test phase. Thus, for each testing concern a new transformation must be defined. This paper introduces AspectT, an aspect-oriented language for the instantiation of abstract test cases. We reduce the effort of test case instantiation by modularizing testing concerns in the form of aspects to enable their reuse in different testing contexts. The approach is implemented and integrated in an existing testing framework and has been successfully applied to test an electronic control unit of an automotive infotainment system at BMW Group.},
booktitle = {Proceedings of the 7th International Conference on Aspect-Oriented Software Development},
pages = {1–12},
numpages = {12},
keywords = {model-based testing, aspect-orientation, test case instantiation, test case generation},
location = {Brussels, Belgium},
series = {AOSD '08}
}

@inproceedings{10.1145/3239372.3239404,
author = {Jolak, Rodi and Ho-Quang, Truong and Chaudron, Michel R.V. and Schiffelers, Ramon R.H.},
title = {Model-Based Software Engineering: A Multiple-Case Study on Challenges and Development Efforts},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239404},
doi = {10.1145/3239372.3239404},
abstract = {A recurring theme in discussions about the adoption of Model-Based Engineering (MBE) is its effectiveness. This is because there is a lack of empirical assessment of the processes and (tool-)use of MBE in practice. We conducted a multiple-case study by observing 2 two-month MBE projects from which software for a Mars rover were developed. We focused on assessing the distribution of the total software development effort over different development activities. Moreover, we observed and collected challenges reported by the developers during the execution of projects. We found that the majority of the effort is spent on the collaboration and communication activities. Furthermore, our inquiry into challenges showed that tool-related challenges are the most encountered.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {213–223},
numpages = {11},
keywords = {Effort Distribution, Case Study Design, Model-Based Engineering, Software Engineering, MBE Challenges, Modeling Tools},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@article{10.1145/986913.986917,
author = {Sammet, Jean E.},
title = {Roster of Programming Languages for 1973},
year = {1974},
issue_date = {November 1974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {11},
issn = {0362-1340},
url = {https://doi.org/10.1145/986913.986917},
doi = {10.1145/986913.986917},
journal = {SIGPLAN Not.},
month = {nov},
pages = {18–31},
numpages = {14}
}

@inproceedings{10.1145/2652524.2652587,
author = {Rodrigues, Elder M. and Saad, Rodrigo S. and Oliveira, Flavio M. and Costa, Leandro T. and Bernardino, Maicon and Zorzo, Avelino F.},
title = {Evaluating Capture and Replay and Model-Based Performance Testing Tools: An Empirical Comparison},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652587},
doi = {10.1145/2652524.2652587},
abstract = {[Context] A variety of testing tools have been developed to support and automate software performance testing activities. These tools may use different techniques, such as Model-Based Testing (MBT) or Capture and Replay (CR). [Goal] For software companies, it is important to evaluate such tools w.r.t. the effort required for creating test artifacts using them; despite its importance, there are few empirical studies comparing performance testing tools, specially tools developed with different approaches. [Method] We are conducting experimental studies to provide evidence about the required effort to use CR-based tools and MBT tools. In this paper, we present our first results, evaluating the effort (time spent) when using LoadRunner and Visual Studio CR-based tools, and the PLeTsPerf MBT tool to create performance test scripts and scenarios to test Web applications, in the context of a collaboration project between Software Engineering Research Center at PUCRS and a technological laboratory of a global IT company. [Results] Our results indicate that, for simple testing tasks, the effort of using a CR-based tool was lower than using an MBT tool, but as the testing complexity increases tasks, the advantage of using MBT grows significantly. [Conclusions] To conclude, we discuss the lessons we learned from the design, operation, and analysis of our empirical experiment.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {9},
numpages = {8},
keywords = {testing tools, experiment, performance testing},
location = {Torino, Italy},
series = {ESEM '14}
}

